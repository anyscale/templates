{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM offline batch inference with RayLLM-Batch APIs\n",
    "\n",
    "**⏱️ Time to complete**: 10 min\n",
    "\n",
    "\n",
    "<!-- TODO: add a link for the RayLLM-Batch API reference -->\n",
    "This template shows you how to run batch inference for LLMs using RayLLM-Batch.\n",
    "\n",
    "**Note:** This tutorial runs within a workspace. Review the `Introduction to Workspaces` template before this tutorial.\n",
    "\n",
    "\n",
    "### How to decide between online vs offline inference for LLM\n",
    "Online LLM inference (e.g. Anyscale Endpoint) should be used when you want to get real-time response for prompt or to interact with the LLM. Use online inference when you want to optimize latency of inference to be as quick as possible.\n",
    "\n",
    "On the other hand, offline LLM inference (also referred to as batch inference) should be used when you want to get reponses for a large number of prompts within some time frame, but not required to be real-time (minutes to hours granularity). Use offline inference when you want to:\n",
    "1. Scale your workload to large-scale datasets\n",
    "2. Optimize inference throughput and resource usage (for example, maximizing GPU utilization).\n",
    "\n",
    "In this tutorial, we will focus on the latter, using offline LLM inference for a summarization task using real-world news articles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up the workload\n",
    "\n",
    "RayLLM-Batch is a library for running batch inference for LLMs using Ray Data for data processing, and defines an easy and flexible interface for the user to define their own workload. In this tutorial, we will implement a workload based on the [`CNNDailyMail`](https://huggingface.co/datasets/abisee/cnn_dailymail) dataset, which is a collection of news articles. And we will summarize each article with our batch inferencing pipeline. We will cover more details on how to customize the workload in the later sections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rayllm_batch.workload import ChatWorkloadBase\n",
    "from typing import Optional, Dict, Any\n",
    "import ray \n",
    "from ray.data.dataset import Dataset\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CNNDailySummary(ChatWorkloadBase):\n",
    "    \"\"\"The CNN/DailyMail summarization workload.\"\"\"\n",
    "\n",
    "    # We directly load the dataset from Hugging Face in this example. You can specify the dataset file path in your workload.\n",
    "    dataset_file: Optional[str] = None\n",
    "    # We will load only a portion of the dataset to run inference faster for the tutorial.\n",
    "    dataset_fraction: float = 0.0003 # 0.03% of the 300K entries.\n",
    "    # The sampling params for the LLM inference workload.\n",
    "    sampling_params: Dict[str, Any] = field(default_factory=lambda: {\"max_tokens\": 150})\n",
    "\n",
    "    def load_dataset(self) -> Dataset:\n",
    "        # Load the dataset from Hugging Face into Ray Data. If you're using your own dataset,\n",
    "        # refer to Ray Data APIs https://docs.ray.io/en/latest/data/api/input_output.html to load it.\n",
    "        # For example, you can use ray.data.read_json(dataset_file) to load dataset in JSONL.\n",
    "        import datasets  # type: ignore\n",
    "\n",
    "        df = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "        return ray.data.from_huggingface(df[\"train\"])\n",
    "\n",
    "    def parse_row(self, row: dict[str, Any]) -> dict[str, Any]:\n",
    "        # Parse the row into the format expected by the model.\n",
    "        # We will use the article as the user prompt, and ask the model to \n",
    "        # generate a summary with the system prompt.\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a commentator. Your task is to \"\n",
    "                    \"summarize highlights from article.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"# Article:\\n{row['article']}\\n\\n\"\n",
    "                    \"#Instructions:\\nIn clear and concise language, \"\n",
    "                    \"summarize the highlights presented in the article.\",\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "workload = CNNDailySummary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the model configs\n",
    "\n",
    "We will also need to define the model configs for the LLM engine, which configures the model and compute resources needed for inference. \n",
    "\n",
    "Some models will require you to input your [Hugging Face user access token](https://huggingface.co/docs/hub/en/security-tokens). This will be used to authenticate/download the model and **is required for official LLaMA, Mistral, and Gemma models**. You can use one of the other models which don't require a token if you don't have access to this model (for example, `neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8`).\n",
    "\n",
    "Run the following cell to start the authentication flow. A VS Code overlay will appear and prompt you to enter your Hugging Face token if your selected model requires authentication. If you are using a model that does not require a token, you can skip this step. For this example, we will be using the `meta-llama/Meta-Llama-3.1-8B-Instruct` model, which requires a token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts the user for Hugging Face token if required by the model.\n",
    "from util.utils import prompt_for_hugging_face_token\n",
    "HF_TOKEN = prompt_for_hugging_face_token(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will be using the `meta-llama/Meta-Llama-3.1-8B-Instruct` model.\n",
    "We will also need to define a yaml configuration file associated with the model we want to use to configure the compute resources, engine arguments and other inference engine specific parameters. For more details on the the model configs, see the [API doc](https://docs.anyscale.com/llms/serving/guides/bring_any_model/) on bringing your own models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rayllm_batch import init_engine_from_config\n",
    "# Read the model configs from the path.\n",
    "model_config_path = \"configs/llama-3.1-8b-a10g.yaml\" \n",
    "# Use model_config_path=\"configs/llama-3.1-8b-l4.yaml\" if on GCP.\n",
    "\n",
    "# One could potentially override the engine configs by passing in a dictionary here.\n",
    "override = {\"runtime_env\": {\"env_vars\": {\"HF_TOKEN\": HF_TOKEN}}} # Override Ray's runtime env to include the Hugging Face token. Ray is being used under the hood to orchestrate the inference pipeline.\n",
    "engine_config = init_engine_from_config(config=model_config_path, override=override)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run the batch inference through RayLLM-Batch\n",
    "\n",
    "\n",
    "With the workload and model configs defined, we can now run the batch inference through RayLLM-Batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rayllm_batch import RayLLMBatch\n",
    "\n",
    "batch = RayLLMBatch(\n",
    "    engine_cfg=engine_config,\n",
    "    workload=workload,\n",
    "    # Specify the batch size for inference. Set the batch size to as large as possible without running out of memory to maximize the throughput.\n",
    "    # Meanwhile, a reasonable batch size can offer better fault tolerance. \n",
    "    batch_size=None,\n",
    "    # Set the number of replicas to use for the inference. Each replica will run one instance of inference pipeline.\n",
    "    num_replicas=1,\n",
    ")\n",
    "\n",
    "# This will runs until completion. If you specify output_path=..., then\n",
    "# the results will be written to local disk (local://) or AWS S3 (s3://).\n",
    "# In this example, we only keep results in memory for demo purpose.\n",
    "ds = batch.run()\n",
    "\n",
    "\n",
    "# Peak the first 3 entries. \n",
    "gen_texts = [r[\"generated_text\"] for r in ds.take(3)]\n",
    "print(\"==================GENERATED OUTPUT===============\")\n",
    "print(\"\\n\".join(gen_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing your LLM inference workload\n",
    "\n",
    "As shown in the example above, one can easily customize the workload by overriding the `load_dataset` and `parse_row` methods. The workload class is defined with the below APIs that one can override:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MyChatWorkload(ChatWorkloadBase):\n",
    "    \"\"\"My chat workload.\"\"\"\n",
    "\n",
    "    # Path to the dataset file.\n",
    "    dataset_file: Optional[str] = \"/path/to/dataset.jsonl\"\n",
    "\n",
    "    # Percentage of the dataset to use for the workload.\n",
    "    dataset_fraction: float = 1.0\n",
    "\n",
    "    # Sampling parameters such as max_tokens, temperature, etc.\n",
    "    sampling_params: Dict[str, Any] = field(\n",
    "        default_factory=lambda: {\"max_tokens\": 150, \"ignore_eos\": False}\n",
    "    )\n",
    "\n",
    "    # Other workload parameters\n",
    "    # ...\n",
    "\n",
    "    def load_dataset(self) -> Dataset:\n",
    "      \"\"\"Load dataset using Ray Data APIs.\"\"\"\n",
    "      pass\n",
    "\n",
    "    def parse_row(self, row: Dict[str, Any]) -> Dict[str, Any]:\n",
    "      \"\"\"Parse each row in the dataset to make them compatible with\n",
    "      OpenAI chat API messages. Specifically, the output row should only\n",
    "      include a single key \"messages\" with type List[Dict[str, Union[str, List[Dict]]]].      \n",
    "      \"\"\"\n",
    "      pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Monitoring the execution\n",
    "\n",
    "RayLLM-Batch uses Ray Data to implement the execution of the batch inference pipeline, and one can use the Ray Dashboard to monitor the execution. In the Ray Dashboard tab, navigate to the Job page and open the \"Ray Data Overview\" section. Click on the link for the running job, and open the \"Ray Data Overview\" section to view the details of the batch inference execution:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/templates/main/templates/batch-llm/assets/ray-data-jobs.png\" width=900px />\n",
    "\n",
    "### Handling GPU out-of-memory failures\n",
    "If you run into CUDA out of memory, your batch size is likely too large. Set an explicit small batch size or use a smaller model (or a larger GPU).\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "- Created a custom workload for the CNN/DailyMail summarization task.\n",
    "- Defined the model configs for the Meta Llama 3.1 8B model.\n",
    "- Ran the batch inference through RayLLM-Batch and monitored the execution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
