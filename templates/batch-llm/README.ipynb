{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rayllm_batch.workload import ChatWorkloadBase\n",
    "from typing import Optional, Dict, Any\n",
    "import ray \n",
    "from ray.data.dataset import Dataset\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CNNDailySummary(ChatWorkloadBase):\n",
    "    \"\"\"The CNN/DailyMail summarization workload.\"\"\"\n",
    "\n",
    "    # We directly load the dataset from Hugging Face.\n",
    "    dataset_file: Optional[str] = None\n",
    "    # We will load only a portion of the dataset to run inference faster for the tutorial.\n",
    "    dataset_fraction: float = 0.0005 # 0.2% of the 300K entries.\n",
    "    # The sampling params for the LLM inference workload.\n",
    "    sampling_params: Dict[str, Any] = field(default_factory=lambda: {\"max_tokens\": 200})\n",
    "\n",
    "    def load_dataset(self) -> Dataset:\n",
    "        # Load the dataset from Hugging Face into Ray Data.\n",
    "        import datasets  # type: ignore\n",
    "\n",
    "        df = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "        return ray.data.from_huggingface(df[\"train\"])\n",
    "\n",
    "    def parse_row(self, row: dict[str, Any]) -> dict[str, Any]:\n",
    "        # Parse the row into the format expected by the model.\n",
    "        # We will use the article as the user prompt, and ask the model to \n",
    "        # generate a summary with the system prompt.\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a commentator. Your task is to \"\n",
    "                    \"summarize highlights from article.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"# Article:\\n{row['article']}\\n\\n\"\n",
    "                    \"#Instructions:\\nIn clear and concise language, \"\n",
    "                    \"summarize the highlights presented in the article.\",\n",
    "                },\n",
    "            ]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts the user for Hugging Face token if required by the model.\n",
    "from util.utils import prompt_for_hugging_face_token\n",
    "HF_TOKEN = prompt_for_hugging_face_token(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rayllm_batch import init_engine_from_config\n",
    "# Read the model configs from the path.\n",
    "model_config_path = \"configs/llama-3.1-8b-a10g.yaml\"\n",
    "\n",
    "# One could potentially override the engine configs by passing in a dictionary here.\n",
    "override = {\"runtime_env\": {\"env_vars\": {\"HF_TOKEN\": HF_TOKEN}}} # Override Ray's runtime env to include the Hugging Face token. Ray is being used under the hood to orchestrate the inference pipeline.\n",
    "engine_config = init_engine_from_config(config=model_config_path, override=override)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rayllm_batch import RayLLMBatch\n",
    "\n",
    "\n",
    "workload = CNNDailySummary()\n",
    "batch = RayLLMBatch(\n",
    "    engine_cfg=engine_config,\n",
    "    workload=workload,\n",
    "    # Specify the batch size for inference. Set the batch size to as large as possible without running out of memory.\n",
    "    # If you encounter out-of-memory errors, decreasing batch_size may help. \n",
    "    batch_size=None,\n",
    "    # Set the number of replicas to use for the inference. Each replica will run one instance of inference pipeline.\n",
    "    num_replicas=1,\n",
    ")\n",
    "\n",
    "\n",
    "# This will runs until completion.\n",
    "ds = batch.run()\n",
    "\n",
    "\n",
    "# Read the results\n",
    "gen_texts = [r[\"generated_text\"] for r in ds.take_all()]\n",
    "print(gen_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
