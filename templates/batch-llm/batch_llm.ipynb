{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch LLM using vLLM and Ray Data\n",
    "\n",
    "This notebook will provide an example walking through how to use Ray Data's `map_batches` to run batch inference of a LLM.\n",
    "\n",
    "All dependencies are pre-installed on this cluster. Let's move on to some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "import ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Up defaults that will be used including\n",
    "* Your [Hugging Face user access token](https://huggingface.co/docs/hub/en/security-tokens). This will be used to download the model. This environment variable can also be set using a [Cluster Environment](https://docs.anyscale.com/configure/dependency-management/cluster-environments).\n",
    "* Which model to load\n",
    "* The sampling params object used by vLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Hugging Face token. Replace the following with your token.\n",
    "hf_token = \"<REPLACE_WITH_YOUR_HUGGING_FACE_USER_TOKEN>\"\n",
    "# Set to the model that you wish to use. Note that using the llama models will require a hugging face token to be set.\n",
    "hf_model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# Create a sampling params object.\n",
    "sampling_params = SamplingParams(temperature=0, max_tokens=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start up Ray, using the hf token as an environment variable so that it's available to all nodes in the cluster\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init(runtime_env={\"env_vars\":{\"HF_TOKEN\": hf_token}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some sample prompts, and use Ray Data to create a dataset for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "\"\"\"\n",
    "I always wanted to be a ...\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "The best way to learn a new language is ...\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "The biggest challenge facing our society today is ...\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "One thing I would change about my past is ...\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "The key to a happy life is ...\n",
    "\"\"\",\n",
    "]\n",
    "ds = ray.data.from_items(prompts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: This notebook uses local text data defined above, but it can easily be expanded to read from input data stored in cloud storage. To read text files from cloud storage (e.g., AWS S3), you can modify the dataset to be:\n",
    "\n",
    "```python\n",
    "ds = ray.data.read_text(\"s3://anonymous@air-example-data/prompts.txt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a class to define logic of batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMPredictor:\n",
    "    def __init__(self):\n",
    "        # Create an LLM.\n",
    "        self.llm = LLM(model=hf_model)\n",
    "\n",
    "    def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, list]:\n",
    "        # Generate texts from the prompts.\n",
    "        # The output is a list of RequestOutput objects that contain the prompt,\n",
    "        # generated text, and other information.\n",
    "        outputs = self.llm.generate(batch[\"item\"], sampling_params)\n",
    "        prompt = []\n",
    "        generated_text = []\n",
    "        for output in outputs:\n",
    "            prompt.append(output.prompt)\n",
    "            generated_text.append(' '.join([o.text for o in output.outputs]))\n",
    "        return {\n",
    "            \"prompt\": prompt,\n",
    "            \"generated_text\": generated_text,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply batch inference for all input data by using `map_batches`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map_batches(\n",
    "    LLMPredictor,\n",
    "    # Set the concurrency to the number of LLM instances.\n",
    "    concurrency=1,\n",
    "    # Specify the number of GPUs required per LLM instance.\n",
    "    num_gpus=1,\n",
    "    # Specify the batch size for inference.\n",
    "    batch_size=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to execute and view the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.take_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to an Anyscale Job\n",
    "\n",
    "To convert this to an Anyscale Job, you can use the `batch_llm.py` as a starting point. It contains the same code, but uses data from an S3 bucket for the prompts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Oct 25 2022, 14:13:24) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
