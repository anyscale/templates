model_id: meta-llama/Meta-Llama-3.1-8B-Instruct
llm_engine: vllm
accelerator_type: A10G
engine_kwargs:
  tensor_parallel_size: 1
  pipeline_parallel_size: 1
  max_num_seqs: 64
  use_v2_block_manager: True
  preemption_mode: "recompute"
  block_size: 16
  kv_cache_dtype: "auto"
  enforce_eager: False
  gpu_memory_utilization: 0.90
  enable_chunked_prefill: False
  max_seq_len_to_capture: 32768
runtime_env:
  env_vars:
    VLLM_ATTENTION_BACKEND: "FLASH_ATTN"
    ENABLE_ANYSCALE_PREFIX_OPTIMIZATIONS: "0"
