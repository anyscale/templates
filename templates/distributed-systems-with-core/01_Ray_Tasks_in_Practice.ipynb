{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc505cf",
   "metadata": {},
   "source": [
    "# Ray Tasks in Practice: Building Distributed Applications\n",
    "\n",
    "Â© 2025, Anyscale. All Rights Reserved\n",
    "\n",
    "This notebook provides a step-by-step introduction to Ray Tasks, the fundamental building block of Ray that enables distributed computing.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b> Here is the roadmap for this notebook </b>\n",
    "\n",
    "<ol>\n",
    "  <li>Overview and setup</li>\n",
    "  <li>Simple task submission (creating, executing, and getting results)</li>\n",
    "  <li>Task options and configuration</li>\n",
    "  <li>Object store and memory model</li>\n",
    "  <li>Chaining tasks and passing data</li>\n",
    "  <li>Error handling and task retries</li>\n",
    "  <li>Task runtime environments</li>\n",
    "  <li>Resource allocation and management</li>\n",
    "  <li>Pipeline data processing and waiting for results</li>\n",
    "  <li>Ray generators</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e0130e",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24c023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ray\n",
    "import requests\n",
    "import ray.runtime_context\n",
    "from ray import tune\n",
    "from ray.util.scheduling_strategies import NodeAffinitySchedulingStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6031c002",
   "metadata": {},
   "source": [
    "## 1. Overview and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692df836",
   "metadata": {},
   "source": [
    "### 1.1. Ray Core at a glance\n",
    "\n",
    "- **Scales your code** across many CPU cores, machines, and accelerators.  \n",
    "- **Schedules arbitrary task graphs** thanks to its distributed scheduler.\n",
    "- **Hides distributed-system overhead** with built-ins for  \n",
    "  - fast data serialization and transfer,  \n",
    "  - smart task placement, \n",
    "  - distributed memory & reference counting.\n",
    "\n",
    "Ray's higher-level libraries build on Ray Core to offer ready-made APIs for common workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021e9ce9",
   "metadata": {},
   "source": [
    "### 1.2. When to use Ray Tasks\n",
    "\n",
    "Ray Tasks are ideal for:\n",
    "- **Parallelizing computationally expensive functions** across multiple cores or machines\n",
    "- **Processing large datasets** by distributing work across workers\n",
    "- **Building complex task dependency graphs** (DAGs) for data pipelines\n",
    "- **Scaling existing Python code** with minimal changes\n",
    "\n",
    "**When NOT to use Ray Tasks:**\n",
    "- Functions that execute in < 1ms (overhead not worth it)\n",
    "- Very fine-grained parallelism (use NumPy/Pandas instead)\n",
    "- When you need mutable shared state (use Ray Actors instead)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece05983",
   "metadata": {},
   "source": [
    "### 1.3. Ray cluster architecture\n",
    "\n",
    "Before diving into tasks, let's understand the key components of a Ray cluster.\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/ray-cluster.svg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fae29a0",
   "metadata": {},
   "source": [
    "A Ray cluster consists of:\n",
    "- One or more **worker nodes**, where each worker node consists of the following processes:\n",
    "    - **worker processes** responsible for task submission and execution.\n",
    "    - A **raylet** responsible for:\n",
    "      - resource management and task placement.\n",
    "      - shared memory management through an object store \n",
    "- One of the worker nodes is designated a **head node** and is responsible for running \n",
    "  - A **global control service** responsible for keeping track of the **cluster-level state** that is not supposed to change too frequently.\n",
    "  - An **autoscaler** service responsible for adding and removing worker nodes by integrating with different infrastructure providers (e.g. AWS, GCP, ...) to match the resource requirements of the cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9b2120",
   "metadata": {},
   "source": [
    "### 1.4. Initializing Ray\n",
    "\n",
    "Before using Ray, you must initialize it. Here are common initialization patterns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418c3864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to existing Ray cluster or create a new one in local development (single machine)\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03037069",
   "metadata": {},
   "source": [
    "## 2. Simple task submission (creating, executing, and getting results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777c595c",
   "metadata": {},
   "source": [
    "### 2.1. Creating remote functions\n",
    "\n",
    "The first step in using Ray is to create remote functions. A remote function is a regular Python function that can be executed on any process in your cluster.\n",
    "\n",
    "Given a simple Python function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65953716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564ce0b5",
   "metadata": {},
   "source": [
    "Decorate the function with `@ray.remote` to turn it into a remote function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6516f6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def remote_add(a, b):\n",
    "    return a + b\n",
    "\n",
    "remote_add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce88bd77",
   "metadata": {},
   "source": [
    "### 2.2. Executing remote functions (asynchronous by default)\n",
    "\n",
    "Native python functions are invoked by calling them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6447d801",
   "metadata": {},
   "outputs": [],
   "source": [
    "add(1, 2)  # Returns 3 immediately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbddedf",
   "metadata": {},
   "source": [
    "Remote ray functions are executed as tasks by calling them with `.remote()` suffix:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85d58d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_add.remote(1, 2)  # Returns ObjectRef immediately, computation happens async"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3113392a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://docs.ray.io/en/latest/ray-core/key-concepts.html#tasks\" target=\"_blank\">Tasks</a></strong> is a remote, stateless Python function invocation.\n",
    "</div>\n",
    "\n",
    "Here is what happens when you call `{remote_function}.remote`:\n",
    "1. Ray schedules the function execution as a task in a separate process in the cluster\n",
    "2. Ray returns an `ObjectRef` (a reference to the future result) to you **immediately** \n",
    "3. The cluster executes the actual computation in the background\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e749d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = remote_add.remote(1, 2)\n",
    "ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7acb089",
   "metadata": {},
   "source": [
    "**Think of `ObjectRef` as a future/promise**: it's a placeholder for a value that will be computed later.\n",
    "\n",
    "Here is a map of how Python code is translated into Ray tasks:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-core/python_to_ray_task_map_v2.png\" alt=\"Python to Ray Task Map\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c858eeba",
   "metadata": {},
   "source": [
    "### 2.3. Getting results\n",
    "\n",
    "If we want to wait (block) and retrieve the corresponding object, we can use `ray.get`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae98426",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.get(ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e129e3",
   "metadata": {},
   "source": [
    "`ray.get()` works with single ObjectRefs or lists:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a0c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single result\n",
    "result = ray.get(ref)\n",
    "\n",
    "# Multiple results\n",
    "refs = [remote_add.remote(i, i) for i in range(5)]\n",
    "results = ray.get(refs)  # Wait for all to complete\n",
    "print(results)  # [0, 2, 4, 6, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9adf3bb",
   "metadata": {},
   "source": [
    "### 2.4. Putting it all together\n",
    "\n",
    "Here are the three steps:\n",
    "1. Create the remote function\n",
    "2. Execute it remotely (non-blocking)\n",
    "3. Get the result when needed (blocking)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "__Activity: define and invoke a Ray task__\n",
    "\n",
    "Define a remote function `sqrt_add` that accepts two arguments and performs the following steps:\n",
    "1. computes the square-root of the first\n",
    "2. adds the second\n",
    "3. returns the result\n",
    "\n",
    "Execute it with 2 different sets of parameters and collect the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46aacf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: define the below as a remote function\n",
    "def sqrt_add(a, b):\n",
    "    ... \n",
    "\n",
    "# Hint: invoke it as a remote task and collect the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83411a27",
   "metadata": {},
   "source": [
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ba9404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63589c2e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary> Click to see solution </summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a49260",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def sqrt_add(a, b):\n",
    "    return math.sqrt(a) + b\n",
    "\n",
    "ray.get([sqrt_add.remote(2, 3), sqrt_add.remote(5, 4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af3e4eb",
   "metadata": {},
   "source": [
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5ca3ea",
   "metadata": {},
   "source": [
    "### 2.5. Understanding asynchronous execution\n",
    "\n",
    "The key difference between regular Python and Ray is that `.remote()` **does not block**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e24a984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slow_function(x):\n",
    "    time.sleep(3)\n",
    "    return x * x\n",
    "\n",
    "# Sequential Python (blocks for each call)\n",
    "start = time.time()\n",
    "results = [slow_function(i) for i in range(4)]  # Would take 12 seconds!\n",
    "print(f\"Sequential: {time.time() - start:.2f}s\")\n",
    "\n",
    "slow_function = ray.remote(slow_function)\n",
    "\n",
    "# Distributed Ray (non-blocking)\n",
    "start = time.time()\n",
    "refs = [slow_function.remote(i) for i in range(4)]  # Returns immediately!\n",
    "print(f\"Task submission: {time.time() - start:.2f}s\")  # < 0.01s\n",
    "\n",
    "# Now wait for results (blocks until all complete)\n",
    "results = ray.get(refs)\n",
    "print(f\"Total with parallelism: {time.time() - start:.2f}s\")  # ~3s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3f712c",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc511b00",
   "metadata": {},
   "source": [
    "### 2.6. What can be passed to Ray tasks? (Serialization)\n",
    "\n",
    "Ray uses **cloudpickle** to serialize function arguments and return values. Most Python objects work, but there are limitations:\n",
    "\n",
    "**â Can serialize:**\n",
    "- Basic types: int, float, str, bool, None\n",
    "- Collections: list, dict, tuple, set\n",
    "- NumPy arrays, Pandas DataFrames\n",
    "- Most custom classes\n",
    "- ObjectRefs (passed efficiently by reference)\n",
    "\n",
    "**â Cannot serialize:**\n",
    "- File handles (`open()` objects)\n",
    "- Network sockets\n",
    "- Threading locks\n",
    "\n",
    "**Example of serialization issues:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd055365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â BAD: File handle won't serialize\n",
    "file = open(\"/tmp/data.txt\", \"w\")\n",
    "\n",
    "@ray.remote\n",
    "def read_file(f):\n",
    "    return f.read()\n",
    "\n",
    "# ref = read_file.remote(file)  # Will fail with a PicklingError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1ba7fa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>ð¡ Troubleshooting:</b> If you see <code>pickle.PicklingError</code> or <code>TypeError: cannot pickle</code>, check if you're passing non-serializable objects to your task.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a68ce5",
   "metadata": {},
   "source": [
    "### 2.7 Task submission sequence\n",
    "\n",
    "Here is the sequence of events when you submit a Ray task:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-core/task-submission_old.gif\" alt=\"Task Submission Sequence\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004446d9",
   "metadata": {},
   "source": [
    "### 2.8 Task submission under the hood\n",
    "\n",
    "When a task is submitted, here is how resource fulfillment works\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-core/normal-task-resource-fullfilment.svg\" width=\"700\" alt=\"Resource fulfillment and execution of `double(2)` in a Ray cluster.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0f375e",
   "metadata": {},
   "source": [
    "Tthe caller must choose **which node (raylet)** should schedule it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103e7313",
   "metadata": {},
   "source": [
    "#### 1ï¸â£ Choosing the Preferred Raylet\n",
    "| **Rule**          | **When Used**                    | **How It Works**                                                                                    |\n",
    "| ----------------- | -------------------------------- | --------------------------------------------------------------------------------------------------- |\n",
    "| **Data locality** | Task has large input objects.    | Pick node holding the most object bytes locally (from the object directory, may be slightly stale). |\n",
    "| **Node affinity** | Task specifies a target node.    | Use the node from `NodeAffinitySchedulingStrategy`.                                                 |\n",
    "| **Default**       | No data or affinity preferences. | Use the **local raylet**.                                                                           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e171e309",
   "metadata": {},
   "source": [
    "#### 2ï¸â£ Request â Lease â Worker\n",
    "- Caller sends a **resource request** to the preferred raylet.  \n",
    "- If granted, the raylet **leases a local worker** and returns its address.  \n",
    "- The **lease stays active** while both caller and worker are alive.  \n",
    "- Idle or unused leases are returned after a short timeout (~hundreds of ms).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf240e38",
   "metadata": {},
   "source": [
    "#### 3ï¸â£ Task Execution on the Leased Worker\n",
    "The caller can schedule **multiple compatible tasks** on the same worker without re-contacting the scheduler.\n",
    "\n",
    "Compatibility means matching:\n",
    "- **Resource shape**, e.g. `{\"CPU\": 1}`\n",
    "- **Shared-memory arguments** (large objects must be local; small ones are inlined)\n",
    "- **Runtime environment**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b83927",
   "metadata": {},
   "source": [
    "#### 4ï¸â£ Optimization Insight\n",
    "Worker leases act as a *cache* for scheduling decisions â similar tasks can reuse the same worker for lower latency and higher throughput.\n",
    "\n",
    "Note also that the caller can hold multiple worker leases to increase parallelism. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ceee48",
   "metadata": {},
   "source": [
    "<!-- TODO - perhaps add the example debug logs here -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c5db94",
   "metadata": {},
   "source": [
    "## 3. Task options and configuration\n",
    "\n",
    "You can dynamically configure tasks using the `.options()` method without redefining the function. This is useful for adjusting resources, retries, or other settings per task invocation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0849bb2e",
   "metadata": {},
   "source": [
    "### 3.1. Basic usage of .options()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea3857b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def flexible_task(x):\n",
    "    return x * 2\n",
    "\n",
    "# Use default configuration (1 CPU)\n",
    "ref1 = flexible_task.remote(5)\n",
    "\n",
    "# Override to use 2 CPUs for this specific invocation\n",
    "ref2 = flexible_task.options(num_cpus=2).remote(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97362f0e",
   "metadata": {},
   "source": [
    "### 3.2. Common options\n",
    "\n",
    "**Resource options:**\n",
    "- `num_cpus`: Number of CPUs (can be fractional, e.g., 0.5)\n",
    "- `num_gpus`: Number of GPUs (can be fractional)\n",
    "- `memory`: Memory in bytes\n",
    "- `resources`: Dict of custom resources\n",
    "\n",
    "**Fault tolerance options:**\n",
    "- `max_retries`: Max number of retries (default: 3 for system errors)\n",
    "- `retry_exceptions`: List of exception types to retry on\n",
    "\n",
    "**Execution options:**\n",
    "- `runtime_env`: Dict specifying runtime environment\n",
    "- `scheduling_strategy`: Control task placement\n",
    "- `name`: Name for debugging/monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19525630",
   "metadata": {},
   "source": [
    "### 3.3. Scheduling strategies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fd4924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default: Ray decides based on data locality and load\n",
    "flexible_task.remote(4)\n",
    "\n",
    "# SPREAD: Distribute tasks across nodes\n",
    "flexible_task.options(scheduling_strategy=\"SPREAD\").remote(5)\n",
    "\n",
    "# Node affinity: Run on specific node\n",
    "strategy = NodeAffinitySchedulingStrategy(\n",
    "    node_id=ray.get_runtime_context().get_node_id(),\n",
    "    soft=True  # soft=True allows fallback if node unavailable\n",
    ")\n",
    "flexible_task.options(scheduling_strategy=strategy).remote(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06006e6c",
   "metadata": {},
   "source": [
    "## 4. Object store and memory model\n",
    "\n",
    "Each worker node has its own object store, and collectively, these form a shared object store across the cluster.\n",
    "\n",
    "Remote objects are immutable. That is, their values cannot be changed after creation. This allows remote objects to be replicated in multiple object stores without needing to synchronize the copies.\n",
    "\n",
    "| <img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/ray-core/ray-cluster.png\" width=\"700px\" loading=\"lazy\"> |\n",
    "| :---------------------------------------------------------------------------------------------------------------------------- |\n",
    "| A Ray cluster with a head node and two worker nodes. Highlighted in orange is distributed object store.                       |\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://docs.ray.io/en/latest/ray-core/key-concepts.html#objects\" target=\"_blank\">Object</a></strong> - tasks and actors create and work with remote objects, which can be stored anywhere in a cluster. These objects are accessed using <strong>ObjectRef</strong> and are cached in a distributed shared-memory <strong>object store</strong>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0948b25b",
   "metadata": {},
   "source": [
    "### 4.1. Ray memory model\n",
    "\n",
    "Ray manages memory in several ways to efficiently handle distributed tasks:\n",
    "\n",
    "1. **Heap memory**:\n",
    "   - Used by workers to execute tasks and actors.\n",
    "   - Used to store small objects (less than 100KB) and Ray metadata.\n",
    "   - High memory pressure can cause Ray to terminate some tasks to free up resources.\n",
    "\n",
    "2. **Shared memory (Object Store)**:\n",
    "   - Serves as the medium for passing data between tasks.\n",
    "   - Large objects (greater than 100KB) are stored in a shared memory space, using up to 30% of a node's memory.\n",
    "   - If more space is needed, objects can be spilled to disk or stored on disk in a slower-access format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab84eec",
   "metadata": {},
   "source": [
    "#### When objects are stored in the object store\n",
    "\n",
    "**Understanding the 100KB threshold:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f2da89",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def small_return():\n",
    "    return 42  # < 100KB: Returned inline (not stored in object store)\n",
    "\n",
    "@ray.remote\n",
    "def large_return():\n",
    "    return np.random.rand(1000, 1000)  # 8MB: Implicitly stored in object store\n",
    "\n",
    "# Explicitly store in object store\n",
    "large_data = np.random.rand(10000, 10000)  # 800MB\n",
    "ref = ray.put(large_data)  # Stored in object store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bdffdb",
   "metadata": {},
   "source": [
    "**Flow diagram:**\n",
    "```\n",
    "Task returns value\n",
    "     â\n",
    "     âââ < 100KB? ââ Yes ââ Send inline to caller (fast)\n",
    "     â\n",
    "     âââ No ââ Store in object store (shared memory)\n",
    "                    â\n",
    "                    âââ Caller gets ObjectRef\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f43c954",
   "metadata": {},
   "source": [
    "#### Design tradeoffs: Choosing where to store objects\n",
    "\n",
    "The following table outlines the tradeoffs between storing objects in the heap memory (in-process store) versus the shared memory (distributed object store):\n",
    "\n",
    "| **Design Aspect**           | **In-Process Store**                      | **Distributed Object Store**                                |\n",
    "| --------------------------- | ----------------------------------------- | ----------------------------------------------------------- |\n",
    "| **Resolution Time**         | Fast (direct memory copy)                 | Slower (requires RPCs)                                      |\n",
    "| **Memory Footprint**        | Higher (multiple copies across processes) | Lower (shared memory reduces duplication)                   |\n",
    "| **Throughput**              | Limited by owner process                  | Scales with number of nodes                                 |\n",
    "| **Data Sharing**            | Copies needed for multiple processes      | Shared memory allows multiple processes to access           |\n",
    "| **Object Size Constraints** | Limited by machine's memory capacity      | Can reference objects larger than a single machine's memory |\n",
    "\n",
    "This setup allows Ray to optimize performance and resource usage in distributed applications.\n",
    "\n",
    "Here is a simplified diagram of the Ray memory model:\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/memory.svg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5480621c",
   "metadata": {},
   "source": [
    "#### Object store capacity and OOM errors\n",
    "\n",
    "The object store has limited capacity (default: 30% of RAM). When full:\n",
    "\n",
    "1. Ray will spill objects to disk (slower access)\n",
    "2. If disk is also full, you'll get `ObjectStoreFullError`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6f4c77",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>ð¡ Troubleshooting:</b> If you see <code>ObjectStoreFullError</code>, you're creating objects faster than they're being consumed. \n",
    "\n",
    "Solutions:\n",
    "<ul>\n",
    "  <li>Process results in batches</li>\n",
    "  <li>Use Ray Generators for streaming</li>\n",
    "  <li>Increase object store size with <code>ray.init(object_store_memory=...)</code></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b4749c",
   "metadata": {},
   "source": [
    "### 4.2. Example: Producer-consumer pattern with numpy arrays\n",
    "\n",
    "This example demonstrates how Ray transfers data in the distributed object store. The `producer_task` creates a 4 GiB numpy array, and the `consumer_task` accesses it with zero-copy deserialization when on the same node:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66293ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def producer_task(size_mb: int = 4 * 1024) -> np.ndarray:\n",
    "    array = np.random.rand((1024**2 * size_mb // 8)).astype(np.float64)\n",
    "    return array\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def consumer_task(array: np.ndarray) -> None:\n",
    "    assert isinstance(array, np.ndarray)\n",
    "    assert not array.flags.owndata  # Confirms zero-copy\n",
    "\n",
    "arr_ref = producer_task.remote()  # Produce a 4 GiB array\n",
    "output_ref = consumer_task.remote(arr_ref)  # Pass ObjectRef to consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e04f54",
   "metadata": {},
   "source": [
    "**What happens under the hood:**\n",
    "\n",
    "1. **Producer task** creates the array in heap memory, then Ray stores it in the shared object store (large objects > 100KB)\n",
    "2. **Consumer task** receives the `ObjectRef` and directly accesses the array from shared memory with zero-copy deserialization (if on same node)\n",
    "3. If tasks run on different nodes, Ray copies the array across the network only once\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-data-deep-dive/producer-consumer-object-store-v2.png\" width=\"600\">\n",
    "\n",
    "To see memory usage in action, run this inspection script:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cfa010",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/memory_inspection.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845041de",
   "metadata": {},
   "source": [
    "#### Deep dive: How zero-copy deserialization works\n",
    "\n",
    "Ray uses **cloudpickle** and **pickle 5** for zero-copy deserialization. When workers on the same node access an object, Ray returns a **view** of shared memory instead of copying data to the process heap.\n",
    "\n",
    "**Zero-copy protocol requirements:**\n",
    "\n",
    "Zero-copy deserialization requires that the object supports the **Pickle 5 out-of-band buffer protocol**. This protocol allows the serializer to separate the object's metadata from its raw buffer data, enabling shared memory access.\n",
    "\n",
    "**When zero-copy is used:**\n",
    "- Large objects shared by workers on the same node\n",
    "- Write-once, read-many-times semantics\n",
    "- Contiguous numpy arrays (currently the main supported type)\n",
    "\n",
    "**How Ray transfers code and data:**\n",
    "\n",
    "1. **Code transfer (functions)**:\n",
    "   - Functions are pickled and stored in the Global Control Store (GCS)\n",
    "   - Subsequent calls fetch the function definition from a cache\n",
    "\n",
    "2. **Data transfer (arguments/return values)**:\n",
    "   - **Small objects (< 100 KB)**: Pickled and transferred inline with the task metadata\n",
    "   - **Large objects (> 100 KB)**: Pickled, stored in shared memory (object store), and only the `ObjectRef` is transferred\n",
    "\n",
    "**Performance considerations:**\n",
    "  \n",
    "- **Zero-copy limitations**:\n",
    "  - Currently works only for **contiguous numpy arrays**\n",
    "  - Cannot modify shared objects directly (requires copy-on-write)\n",
    "  - Does not work for PyTorch tensors or other array types\n",
    "  - **Workaround**: Convert objects to contiguous numpy arrays before passing to tasks:\n",
    "    ```python\n",
    "    # Convert PyTorch tensor to numpy\n",
    "    numpy_array = tensor.cpu().numpy()\n",
    "    \n",
    "    # Ensure contiguous layout\n",
    "    contiguous_array = np.ascontiguousarray(numpy_array)\n",
    "    \n",
    "    ref = ray.put(contiguous_array)\n",
    "    ```\n",
    "\n",
    "- **Cloudpickle overhead**: Cloudpickle has fixed serialization overhead. For small objects where performance matters, consider using raw `bytes` and performing your own serialization to bypass cloudpickle.\n",
    "\n",
    "To read more about object serialization in Ray, see [this documentation page here](https://docs.ray.io/en/latest/ray-core/objects/serialization.html).\n",
    "\n",
    "**Mutability semantics:**\n",
    "\n",
    "Objects in the object store are **immutable once sealed**. This design choice enables safe sharing across multiple processes without locks, but means in-place updates won't propagate to other workers accessing the same `ObjectRef`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8489c5",
   "metadata": {},
   "source": [
    "### 4.3. Usecase: Hyper-parameter tuning\n",
    "\n",
    "**The Problem**: When running hyperparameter tuning or experimentation, you often need to use the same dataset across dozens or hundreds of trials. If you pass the dataset by value to each training function, Ray will serialize it repeatedly, wasting memory and time.\n",
    "\n",
    "**The Solution**: Store the dataset once in the object store using `ray.put()`, then pass only the lightweight `ObjectRef` to each trial. All workers can access the same data without duplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f6e63c",
   "metadata": {},
   "source": [
    "#### Real-world scenario: Grid search with shared data\n",
    "\n",
    "Imagine running 20 experiments on a 100MB training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f9b07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a 100MB training dataset\n",
    "df = pd.DataFrame(np.random.rand(100 * 1024 ** 2 // 8))\n",
    "\n",
    "@ray.remote\n",
    "def train_model(data, learning_rate, batch_size):\n",
    "    # Simulate model training\n",
    "    result = data.mean().sum() * learning_rate / batch_size\n",
    "    time.sleep(20)\n",
    "    return {\"lr\": learning_rate, \"batch_size\": batch_size, \"score\": result}\n",
    "\n",
    "# Grid search: 20 different hyperparameter combinations\n",
    "hyperparameters = [\n",
    "    {\"lr\": lr, \"batch_size\": bs}\n",
    "    for lr in [0.001, 0.01, 0.1, 0.5]\n",
    "    for bs in [32, 64, 128, 256, 512]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c5ecf8",
   "metadata": {},
   "source": [
    "Here is an efficient way to run the experiments by passing the dataset by reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53ce3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â Memory Efficient: Pass ObjectRef (100 MB total memory)\n",
    "# Ray serializes once, all workers share the same data\n",
    "df_ref = ray.put(df)\n",
    "[\n",
    "    train_model.remote(df_ref, hp[\"lr\"], hp[\"batch_size\"]) \n",
    "    for hp in hyperparameters\n",
    "]\n",
    "print(\"Pass by reference: ~100 MiB memory used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fb25ba",
   "metadata": {},
   "source": [
    "Let's inspect the object store, we should only see the same 100MiB object being used across tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0fca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ray list objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4242c4",
   "metadata": {},
   "source": [
    "Here is the inefficient way by passing the dataset by value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d17730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â Memory inefficient: Pass dataframe by value (2 GB total memory!)\n",
    "# Ray serializes 100MB Ã 20 times = 2 GB of redundant data\n",
    "[\n",
    "    train_model.remote(df, hp[\"lr\"], hp[\"batch_size\"]) \n",
    "    for hp in hyperparameters\n",
    "]\n",
    "print(\"Pass by value: ~2GiB memory used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f74e45",
   "metadata": {},
   "source": [
    "Let's inspect the object store, we should now see different 100MiB objects being used across tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeb8f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ray list objects "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f809cbdc",
   "metadata": {},
   "source": [
    "**Performance comparison:**\n",
    "- **Pass by value**: 2 GB memory used (20Ã serialization overhead)\n",
    "- **Pass by reference**: 100 MB memory used (1Ã serialization)\n",
    "\n",
    "**Rule of thumb**: Pass by value only for small literals (< 100 KiB); otherwise, pass by reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35172a75",
   "metadata": {},
   "source": [
    "#### How Ray Tune leverages this pattern\n",
    "\n",
    "Ray Tune uses `tune.with_parameters()` to automatically pass large constant objects via the object store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c17cd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainable(config, data):\n",
    "    # Each trial receives a reference to the shared data\n",
    "    model = train(data, lr=config[\"lr\"], epochs=config[\"epochs\"])\n",
    "    return {\"accuracy\": model.eval()}\n",
    "\n",
    "# Tune automatically stores train_data in the object store\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_parameters(trainable, data=df),  # Passed by reference\n",
    "    param_space={\"lr\": tune.grid_search([0.001, 0.01, 0.1]), \"epochs\": tune.choice([10, 20, 50])},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9390674f",
   "metadata": {},
   "source": [
    "Without `tune.with_parameters()`, each trial would receive a separate copy of `train_data`, multiplying memory usage by the number of concurrent trials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1389b521",
   "metadata": {},
   "source": [
    "### 4.4. Distributed ownership and fate-sharing\n",
    "\n",
    "Ray uses a **distributed ownership model** to manage objects efficiently across the cluster. Understanding this concept is crucial for building robust distributed applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c3e69e",
   "metadata": {},
   "source": [
    "#### How distributed ownership works\n",
    "\n",
    "In Ray, the process that creates or submits a task becomes the **owner** of the task's result. The owner maintains critical metadata about the object, including:\n",
    "- Object location(s) in the cluster\n",
    "- Reference counts\n",
    "- Object size and other properties\n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/distributed_ownership_overview_v4.svg\" width=\"800px\">\n",
    "\n",
    "**Benefits of distributed ownership:**\n",
    "- **Lower latency**: No need to communicate all ownership information back to a central node\n",
    "- **Better scalability**: No single bottleneck as every worker maintains its own ownership information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c4c739",
   "metadata": {},
   "source": [
    "Here is a diagram that explains how distributed ownership works in a Ray cluster:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-core/distributed-ownership.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c160be6e",
   "metadata": {},
   "source": [
    "Below is some code based on the above diagram to illustrate distributed ownership:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46367280",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def b():\n",
    "    size_mib = 19\n",
    "    return np.ones(1024 ** 2 // 8 *  size_mib)\n",
    "\n",
    "@ray.remote\n",
    "def a(dep):\n",
    "    z = b.remote()\n",
    "\n",
    "    ip = ray.util.get_node_ip_address()\n",
    "    print(f\"{ip=}\")\n",
    "\n",
    "    time.sleep(20)\n",
    "    return dep.sum() / ray.get(z).sum() \n",
    "\n",
    "size_mib = 33\n",
    "arr = np.ones(1024 ** 2 // 8 *  size_mib)\n",
    "x = ray.put(arr)\n",
    "y = a.remote(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5585796a",
   "metadata": {},
   "source": [
    "We can verify that the the 19 MB array is owned by the worker that submitted task `b` - i.e. the worker executing task `a` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c448b460",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ray list objects --filter TASK_STATUS!=NIL --filter TYPE=WORKER  # TASK_STATUS = NIL is non-owners given Task only the owner tracks task status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe352c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.get(y), 33 / 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003bc670",
   "metadata": {},
   "source": [
    "#### The fate-sharing limitation\n",
    "\n",
    "The main tradeoff of distributed ownership is **fate-sharing**: objects are tied to the lifetime of their owner process.\n",
    "\n",
    "**What this means:**\n",
    "- Even if an object is stored in the object store on a different node, if the owner process dies, the object becomes unreachable\n",
    "- The owner maintains critical metadata (locations, reference counts) that other processes need to access the object\n",
    "- When the owner fails, this metadata is lost, making the object inaccessible even if copies exist elsewhere\n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/distributed_ownership_fate_share_with_owner_v4.svg\" width=\"900px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdf45ae",
   "metadata": {},
   "source": [
    "#### Example: Demonstrating fate-sharing\n",
    "\n",
    "This example creates two actors: an **Owner** that creates an object reference, and a **Borrower** that tries to access it. We'll see what happens when the Owner is terminated:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc3ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def f(data):\n",
    "    return data\n",
    "\n",
    "@ray.remote\n",
    "class Owner:\n",
    "    def __init__(self):\n",
    "        self.ref = None\n",
    "\n",
    "    def set_object_ref(self, data):\n",
    "        self.ref = f.remote(data)\n",
    "        return self.ref\n",
    "    \n",
    "    def is_alive(self): \n",
    "        return True\n",
    "\n",
    "@ray.remote\n",
    "class Borrower:\n",
    "    def get_object(self, ref):\n",
    "        return ray.get(ref)\n",
    "\n",
    "owner = Owner.remote()\n",
    "borrower = Borrower.remote()\n",
    "assert ray.get(owner.is_alive.remote())\n",
    "\n",
    "object_ref = owner.set_object_ref.remote(data=\"test1\")\n",
    "data = ray.get(borrower.get_object.remote(object_ref))\n",
    "assert data == \"test1\"\n",
    "print(f\"â Successfully retrieved data while Owner is alive: {data}\")\n",
    "\n",
    "ray.kill(owner)\n",
    "time.sleep(2)\n",
    "\n",
    "try:\n",
    "    ray.get(borrower.get_object.remote(object_ref))\n",
    "    print(\"â Unexpected: Should have failed!\")\n",
    "except Exception as e:\n",
    "    print(\"â Failed as expected after owner termination:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b306bc",
   "metadata": {},
   "source": [
    "**What happens:**\n",
    "1. While the Owner is alive, the Borrower can successfully retrieve the object using the `ObjectRef`\n",
    "2. After the Owner is killed, the Borrower still has the `ObjectRef`, but attempting to access the object fails\n",
    "3. Even though the object data may still exist in the object store, the ownership metadata is lost\n",
    "\n",
    "**Key takeaway:** In Ray's distributed ownership model, object lifetime is tied to the owner's lifetime. When building fault-tolerant applications:\n",
    "- Keep important owners alive (e.g., use long-running actors or the driver process)\n",
    "- Consider checkpointing critical data outside Ray's object store for durability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aa24ed",
   "metadata": {},
   "source": [
    "### 4.5 Lineage Reconstruction\n",
    "\n",
    "If instead, the owner is still alive, but the object is lost (e.g., due to node failure), Ray can reconstruct the object by either\n",
    "\n",
    "1. Finding any secondary copies in the object store (if they exist) and returning one of those\n",
    "2. Re-executing the task or task chain that created it. This is known as **lineage reconstruction**.\n",
    "\n",
    "See the below test inspired from the [Ray test suite](https://github.com/ray-project/ray/blob/a04cb06bb1a2c09e93b882b611492d62b8d1837a/python/ray/tests/test_reconstruction.py#L126) for an example of lineage reconstruction:\n",
    "\n",
    "```python\n",
    "@pytest.mark.parametrize(\"reconstruction_enabled\", [False, True])\n",
    "def test_basic_reconstruction(config, ray_start_cluster, reconstruction_enabled):\n",
    "    cluster = ray_start_cluster\n",
    "    # Head node with no resources.\n",
    "    cluster.add_node(\n",
    "        num_cpus=0,\n",
    "        _system_config=config,\n",
    "        enable_object_reconstruction=reconstruction_enabled,\n",
    "    )\n",
    "    ray.init(address=cluster.address)\n",
    "    # Node to place the initial object.\n",
    "    node_to_kill = cluster.add_node(\n",
    "        num_cpus=1, resources={\"node1\": 1}, object_store_memory=10**8\n",
    "    )\n",
    "    cluster.wait_for_nodes()\n",
    "\n",
    "    @ray.remote(max_retries=1 if reconstruction_enabled else 0)\n",
    "    def create_large_object():\n",
    "        return np.zeros(10**7, dtype=np.uint8)\n",
    "\n",
    "    @ray.remote\n",
    "    def process_large_object(x):\n",
    "        return\n",
    "\n",
    "    # Create a large object on node1\n",
    "    obj = create_large_object.options(resources={\"node1\": 1}).remote()\n",
    "    # Create a dependent task that will use the large object\n",
    "    ray.get(process_large_object.options(resources={\"node1\": 1}).remote(obj))\n",
    "    # Remove the node that has the large object\n",
    "    cluster.remove_node(node_to_kill, allow_graceful=False)\n",
    "    # Add a new node with the same resource config\n",
    "    node_to_kill = cluster.add_node(\n",
    "        num_cpus=1, resources={\"node1\": 1}, object_store_memory=10**8\n",
    "    )\n",
    "\n",
    "    if reconstruction_enabled:\n",
    "        # If the large object is lost and reconstruction is enabled, the following call will succeed\n",
    "        # i.e create_large_object will get called again and the object will be reconstructed\n",
    "        # then process_large_object can run successfully\n",
    "        ray.get(process_large_object.remote(obj))\n",
    "    else:\n",
    "        # Both the dependent task and the object will be lost\n",
    "        with pytest.raises(ray.exceptions.RayTaskError):\n",
    "            ray.get(process_large_object.remote(obj))\n",
    "        with pytest.raises(ray.exceptions.ObjectLostError):\n",
    "            ray.get(obj)\n",
    "\n",
    "    # Losing the object a second time will cause reconstruction to fail because\n",
    "    # we have reached the max task retries.\n",
    "    cluster.remove_node(node_to_kill, allow_graceful=False)\n",
    "    cluster.add_node(num_cpus=1, resources={\"node1\": 1}, object_store_memory=10**8)\n",
    "\n",
    "    if reconstruction_enabled:\n",
    "        with pytest.raises(\n",
    "            ray.exceptions.ObjectReconstructionFailedMaxAttemptsExceededError\n",
    "        ):\n",
    "            ray.get(obj)\n",
    "    else:\n",
    "        with pytest.raises(ray.exceptions.ObjectLostError):\n",
    "            ray.get(obj)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8930d3",
   "metadata": {},
   "source": [
    "### 4.6 ObjectRef lifecycle and garbage collection\n",
    "\n",
    "Objects in the object store are automatically garbage collected when teir distributed reference count drops to zero. This happens when all `ObjectRef`s pointing to the object are deleted or go out of scope.\n",
    "\n",
    "**Example:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1114c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_returns=2)\n",
    "def create_object():\n",
    "    task_id = ray.runtime_context.get_runtime_context().get_task_id()\n",
    "    return np.random.rand(1024 ** 2 // 8 * 20), task_id\n",
    "\n",
    "# Object created and stored\n",
    "ref1, ref2 = create_object.remote()\n",
    "\n",
    "# Object still in memory\n",
    "result = ray.get(ref1)\n",
    "task_id = ray.get(ref2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00095044",
   "metadata": {},
   "source": [
    "Let's inspect the returned objects in the store - note in this case we leverage the object id specification to find the object in the store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede5d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ray list objects --filter TASK_STATUS!=NIL --filter TYPE=DRIVER --filter OBJECT_ID={task_id}01000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5763b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release reference (allows GC)\n",
    "del ref1\n",
    "del ref2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2419381",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ray list objects --filter TASK_STATUS!=NIL --filter TYPE=DRIVER --filter OBJECT_ID={task_id}02000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e85952",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>â¡ Performance Tip:</b> For long-running applications, explicitly delete ObjectRefs you no longer need to free up object store memory.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8bc6c3",
   "metadata": {},
   "source": [
    "## 5. Chaining tasks and passing data\n",
    "\n",
    "Let's say we now want to execute a graph of two tasks:\n",
    "1. Square a value using `expensive_square`\n",
    "2. Add 1 to the `expensive_square` result, by using `remote_add`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1555f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def expensive_square(x):\n",
    "    time.sleep(1)\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74925204",
   "metadata": {},
   "source": [
    "This can be achieved without fetching an intermediate result.\n",
    "\n",
    "**â Anti-pattern:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e05b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st task\n",
    "square_ref = expensive_square.remote(2)\n",
    "square_value = ray.get(square_ref)  # wait to get the value\n",
    "\n",
    "# 2nd task\n",
    "sum_ref = remote_add.remote(1, square_value)  # pass value from 1st task\n",
    "sum_value = ray.get(sum_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc19a2f",
   "metadata": {},
   "source": [
    "**â Better:** Chain the tasks by passing the `ObjectRef` directly to the second task:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf99caeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "square_ref = expensive_square.remote(2)\n",
    "sum_ref = remote_add.remote(1, square_ref)  # Pass ObjectRef, not value!\n",
    "sum_value = ray.get(sum_ref)  # Wait only at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a82592",
   "metadata": {},
   "source": [
    "**Why this is better:**\n",
    "- No unnecessary data transfer (ObjectRef is just an ID)\n",
    "- Ray automatically handles dependencies\n",
    "- Second task waits for first task to complete\n",
    "- More efficient scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ca0fa8",
   "metadata": {},
   "source": [
    "### 5.1. Common task graph patterns\n",
    "\n",
    "Ray excels at executing complex directed acyclic graphs (DAGs) of tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d9da37",
   "metadata": {},
   "source": [
    "#### Pattern 1: Linear chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce432405",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def step1(data):\n",
    "    return process_a(data)\n",
    "\n",
    "@ray.remote\n",
    "def step2(data):\n",
    "    return process_b(data)\n",
    "\n",
    "@ray.remote\n",
    "def step3(data):\n",
    "    return process_c(data)\n",
    "\n",
    "# Chain tasks\n",
    "# ref1 = step1.remote(input_data)\n",
    "# ref2 = step2.remote(ref1)\n",
    "# ref3 = step3.remote(ref2)\n",
    "# final_result = ray.get(ref3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ac25a4",
   "metadata": {},
   "source": [
    "#### Pattern 2: Fan-out / Fan-in (MapReduce)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1603d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def map_task(chunk):\n",
    "    return process_chunk(chunk)\n",
    "\n",
    "@ray.remote\n",
    "def reduce_task(results):\n",
    "    return aggregate(results)\n",
    "\n",
    "# Map phase (fan-out)\n",
    "# map_refs = [map_task.remote(chunk) for chunk in data_chunks]\n",
    "\n",
    "# Reduce phase (fan-in)\n",
    "# final_result = ray.get(reduce_task.remote(map_refs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3534cea7",
   "metadata": {},
   "source": [
    "#### Pattern 3: Tree reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d3828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def pairwise_sum(a, b):\n",
    "    return a + b\n",
    "\n",
    "\n",
    "refs = [ray.put(i) for i in range(16)]  # Initial values\n",
    "\n",
    "# Tree reduction (depth = log2(16) = 4)\n",
    "while len(refs) > 1:\n",
    "    refs = [pairwise_sum.remote(refs[i], refs[i + 1]) for i in range(0, len(refs), 2)]\n",
    "\n",
    "result = ray.get(refs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaca2cc",
   "metadata": {},
   "source": [
    "### 5.2. Nested tasks\n",
    "\n",
    "Tasks can submit other tasks, enabling dynamic workflows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1c7c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def main():\n",
    "    square_ref_1 = expensive_square.remote(1)\n",
    "    square_ref_2 = expensive_square.remote(2)\n",
    "    add_ref = remote_add.remote(square_ref_1, square_ref_2)\n",
    "    return ray.get(add_ref)\n",
    "\n",
    "ray.get(main.remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a359eed8",
   "metadata": {},
   "source": [
    "**Avoiding deadlocks:** Ray automatically yields CPU resources when blocked on `ray.get()`, preventing deadlocks when nested tasks need the same resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31b1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster has 2 CPUs total\n",
    "\n",
    "@ray.remote(num_cpus=2)\n",
    "def outer_task():\n",
    "    inner_refs = [inner_task.remote() for _ in range(10)]\n",
    "    return ray.get(inner_refs)  # Ray yields the 2 CPUs while waiting\n",
    "\n",
    "@ray.remote(num_cpus=1)\n",
    "def inner_task():\n",
    "    return \n",
    "\n",
    "ray.get(outer_task.remote())  # Works! No deadlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655b349b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Read more about <strong><a href=\"https://docs.ray.io/en/latest/ray-core/tasks/nested-tasks.html#yielding-resources-while-blocked\" target=\"_blank\">yielding resources while blocked</a></strong>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4223e3",
   "metadata": {},
   "source": [
    "## 6. Error handling and task retries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efba8c50",
   "metadata": {},
   "source": [
    "### 6.1. Understanding exception types\n",
    "\n",
    "Let's consider two types of exceptions:\n",
    "1. **System errors**: Worker node dies, out of memory, network issues\n",
    "2. **Application-level errors**: Python exceptions in your code (ValueError, TypeError, etc.)\n",
    "\n",
    "Ray will automatically **retry a task up to 3 times** if it fails due to a system error (e.g., a worker node dies)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b3772f",
   "metadata": {},
   "source": [
    "### 6.2. Handling application exceptions\n",
    "\n",
    "Below task won't be retried by default because it's an application failure:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19b6581",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def incorrect_square(x: int, prob: float) -> int:\n",
    "    if random.random() < prob:\n",
    "        raise ValueError(\"Random failure\")\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e81976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ray.get([incorrect_square.remote(x=4, prob=0.5) for _ in range(10)])\n",
    "except ray.exceptions.RayTaskError as e:\n",
    "    print(f\"Task failed with: {e}\")\n",
    "    print(f\"Original exception: {e.cause}\")  # Access underlying exception"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b79b37",
   "metadata": {},
   "source": [
    "**Exception propagation:**\n",
    "- Exceptions in tasks are wrapped in `RayTaskError`\n",
    "- The original exception is available via `.cause` attribute\n",
    "- `ray.get()` will raise the exception\n",
    "- ObjectRefs remain valid, but getting them raises the exception"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf1e934",
   "metadata": {},
   "source": [
    "### 6.3. Configuring retries\n",
    "\n",
    "Ray lets you specify how to handle retries when an exception is encountered:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925e5181",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(retry_exceptions=[ValueError])\n",
    "def correct_square(x: int, prob: float) -> int:\n",
    "    if random.random() < prob:\n",
    "        raise ValueError(\"Random failure\")\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ec7bda",
   "metadata": {},
   "source": [
    "Note we did not have to re-define the remote function, instead we can create an updated version using `.options`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb165c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_square_mod = correct_square.options(\n",
    "    retry_exceptions=[ValueError],\n",
    "    max_retries=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3ef844",
   "metadata": {},
   "source": [
    "Let's try it out:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495ae251",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    outputs = ray.get([correct_square_mod.remote(x=4, prob=0.5) for _ in range(10)])\n",
    "    print(f\"Success! Results: {outputs}\")\n",
    "except ray.exceptions.RayTaskError:\n",
    "    print(\"At least one of the tasks failed after all retries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efcf039",
   "metadata": {},
   "source": [
    "### 6.4. Idempotency: Critical for reliable retries\n",
    "\n",
    "**â ï¸ WARNING:** Only retry tasks that are **idempotent** (can be safely run multiple times).\n",
    "\n",
    "**â Non-idempotent (dangerous to retry):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c16a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(retry_exceptions=[ValueError])\n",
    "def append_to_file(data):\n",
    "    with open(\"/tmp/data.txt\", \"a\") as f:\n",
    "        f.write(data)  # Will duplicate data on retry!\n",
    "    if random.random() < 0.5:\n",
    "        raise ValueError(\"Simulated failure\")\n",
    "    return \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c922b552",
   "metadata": {},
   "source": [
    "**If this task fails and retries:**\n",
    "1. First attempt: Writes \"hello\" â fails\n",
    "2. Retry: Writes \"hello\" again â file now has \"hellohello\"\n",
    "\n",
    "**â Idempotent (safe to retry):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649cc865",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(retry_exceptions=[ValueError])\n",
    "def write_to_file_safe(data, unique_id):\n",
    "    filename = f\"data_{unique_id}.txt\"\n",
    "    with open(filename, \"w\") as f:  # Overwrites on retry\n",
    "        f.write(data)\n",
    "    if random.random() < 0.5:\n",
    "        raise ValueError(\"Simulated failure\")\n",
    "    return \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622e9ada",
   "metadata": {},
   "source": [
    "**Other idempotent operations:**\n",
    "- Reading from database\n",
    "- GET requests (not POST/PUT/DELETE)\n",
    "- Mathematical computations\n",
    "- Overwriting files (not appending)\n",
    "\n",
    "**Non-idempotent operations to avoid retrying:**\n",
    "- Appending to files/databases\n",
    "- Sending emails/notifications\n",
    "- Charging credit cards\n",
    "- Incrementing counters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bae9e5",
   "metadata": {},
   "source": [
    "### 6.5. Task timeouts and cancellation\n",
    "\n",
    "Sometimes you want to set a maximum execution time or cancel tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff86c90e",
   "metadata": {},
   "source": [
    "#### Setting timeouts with ray.get()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39856f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def slow_task():\n",
    "    time.sleep(100)\n",
    "    return \"done\"\n",
    "\n",
    "ref = slow_task.remote()\n",
    "\n",
    "try:\n",
    "    result = ray.get(ref, timeout=5)  # Wait max 5 seconds\n",
    "except ray.exceptions.GetTimeoutError:\n",
    "    print(\"Task took too long!\")\n",
    "    ray.cancel(ref)  # Cancel the task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf62465e",
   "metadata": {},
   "source": [
    "#### Cancelling tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f2b516",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def long_running_task(duration):\n",
    "    time.sleep(duration)\n",
    "    return \"completed\"\n",
    "\n",
    "refs = [long_running_task.remote(10) for _ in range(5)]\n",
    "\n",
    "# Cancel all tasks\n",
    "for ref in refs:\n",
    "    ray.cancel(ref)\n",
    "\n",
    "# Check if cancelled\n",
    "try:\n",
    "    ray.get(refs[0])\n",
    "except ray.exceptions.TaskCancelledError:\n",
    "    print(\"Task was cancelled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ab92af",
   "metadata": {},
   "source": [
    "**Important notes about cancellation:**\n",
    "- Cancellation is best-effort, not guaranteed\n",
    "- Task might complete before cancellation takes effect\n",
    "- Dependent tasks are also cancelled\n",
    "- Use for cleanup, not critical functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549b38f5",
   "metadata": {},
   "source": [
    "#### Using ray.wait() with timeouts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c952d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = [slow_task.remote() for _ in range(10)]\n",
    "\n",
    "# Wait for at least 3 tasks to complete, timeout after 30 seconds\n",
    "ready, not_ready = ray.wait(refs, num_returns=3, timeout=30, fetch_local=False)\n",
    "\n",
    "print(f\"Completed: {len(ready)}, Still running: {len(not_ready)}\")\n",
    "\n",
    "# Cancel remaining tasks\n",
    "for ref in not_ready:\n",
    "    ray.cancel(ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d32e33",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Refer to the <strong><a href=\"https://docs.ray.io/en/latest/ray-core/tasks/retries.html\" target=\"_blank\">retries</a></strong> and <strong><a href=\"https://docs.ray.io/en/latest/ray-core/tasks/fault-tolerance.html\" target=\"_blank\">fault tolerance</a></strong> documentation to learn more.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b048b107",
   "metadata": {},
   "source": [
    "## 7. Task runtime environments\n",
    "\n",
    "A runtime environment defines dependencies such as files, packages, and environment variables needed for a Python script to run.\n",
    "\n",
    "- **Runtime Environment Management**:\n",
    "  - Managed by a `RuntimeEnvAgent` gRPC server on each node.\n",
    "  - The `RuntimeEnvAgent` fate-shares with the raylet, simplifying the failure model and ensuring it is a core component for task and actor scheduling.\n",
    "\n",
    "- **Environment Creation**:\n",
    "  - Triggered by the raylet via a gRPC request to the `RuntimeEnvAgent` when a task or actor requires a runtime environment.\n",
    "  - May involve:\n",
    "    - Installing packages using `pip install`.\n",
    "    - Setting environment variables for Ray worker processes.\n",
    "    - Activating conda environments with `conda activate`.\n",
    "    - Downloading files from remote cloud storage.\n",
    "\n",
    "- **Resource Caching**:\n",
    "  - Runtime environment resources, such as downloaded files and installed conda environments, are cached on each node.\n",
    "  - The cache allows sharing of resources between different tasks, actors, and jobs.\n",
    "  - When the cache size limit is exceeded, resources not currently in use are deleted to free up space.\n",
    "\n",
    "Here is a diagram showcasing the above concepts:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-core/runtime_env.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c94b89",
   "metadata": {},
   "source": [
    "### 7.1. Setting environment variables\n",
    "\n",
    "For example, we can set an environment variable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eda2fb7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "@ray.remote(runtime_env={\"env_vars\": {\"my_custom_env\": \"prod\"}})\n",
    "def f():\n",
    "    env = os.environ[\"my_custom_env\"]\n",
    "    return f\"My custom env is {env}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcbc6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.get(f.remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e54f91c",
   "metadata": {},
   "source": [
    "### 7.2. Installing pip dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc3d0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(runtime_env={\"pip\": [\"requests\", \"pandas==1.5.0\"]})\n",
    "def fetch_data(url):\n",
    "    import requests\n",
    "    return requests.get(url).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c038ae",
   "metadata": {},
   "source": [
    "### 7.3. Working directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facfd84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(runtime_env={\"working_dir\": \"s3://my-bucket/project/my_directory.zip\"})\n",
    "def load_config():\n",
    "    with open(\"config.yaml\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318fc286",
   "metadata": {},
   "source": [
    "### 7.4. Note about pip dependencies\n",
    "\n",
    "pip dependencies in task runtime environments don't come for free. They add to the startup time of the worker process.\n",
    "\n",
    "**Performance impact:**\n",
    "- First task: 10-60 seconds (installing dependencies)\n",
    "- Subsequent tasks: < 1 second (cached environment)\n",
    "\n",
    "**Best practices:**\n",
    "- If you need the same dependencies across many tasks, bake them into your cluster image\n",
    "- Use conda environments for better caching\n",
    "- Group tasks with same dependencies together\n",
    "\n",
    "If you find yourself needing to install the same dependencies across many tasks, consider baking them into the image you use to start your Ray cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b36796",
   "metadata": {},
   "source": [
    "## 8. Resource allocation and management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e52076",
   "metadata": {},
   "source": [
    "### 8.1. Understanding logical vs physical resources\n",
    "\n",
    "Ray resource specifications are **logical**, not physical:\n",
    "\n",
    "- **Logical resources**: Used by Ray scheduler for placement decisions (default: `num_cpus=1`)\n",
    "- **Physical resources**: Actual CPU/GPU/memory usage by your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a0b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_cpus=1)  # Ray reserves 1 CPU slot for scheduling\n",
    "def cpu_intensive_task():\n",
    "    # Ray sets OMP_NUM_THREADS=1 to match num_cpus\n",
    "    return np.dot(large_matrix_a, large_matrix_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9dbe72",
   "metadata": {},
   "source": [
    "**Key points:**\n",
    "- `num_cpus` is a scheduling hint, not a hard limit\n",
    "- Ray automatically sets `OMP_NUM_THREADS` to match `num_cpus` to prevent oversubscription\n",
    "\n",
    "You can override this if needed (may cause oversubscription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9c92e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_cpus=1)\n",
    "def mm(n: int = 4000):\n",
    "    return np.dot(np.random.rand(n, n), np.random.rand(n, n))\n",
    "\n",
    "# Override to use 8 threads (caution: may oversubscribe)\n",
    "ray.get(mm.options(runtime_env={\"env_vars\": {\"OMP_NUM_THREADS\": \"8\"}}).remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf89b1b0",
   "metadata": {},
   "source": [
    "Note assigning \"GPU\" resources to a task, Ray will automatically set the `CUDA_VISIBLE_DEVICES` env var within the worker to limit it to specific GPU ids."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06dece7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Learn more about <strong><a href=\"https://docs.ray.io/en/latest/ray-core/scheduling/resources.html#physical-resources-and-logical-resources\" target=\"_blank\">physical resources and logical resources</a></strong>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6599165c",
   "metadata": {},
   "source": [
    "### 8.2. Fractional resources for I/O-bound tasks\n",
    "\n",
    "Ray supports **fractional CPU requests** (e.g., `num_cpus=0.1`, `num_cpus=0.5`) to enable efficient oversubscription of I/O-bound tasks.\n",
    "\n",
    "**When to use fractional CPUs:**\n",
    "\n",
    "Tasks that spend most of their time waiting (not computing) can share CPU slots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab6b70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moderately I/O-bound: Some computation, some I/O\n",
    "@ray.remote(num_cpus=0.5)  # Allow 2 tasks per CPU core\n",
    "def download_and_parse(url):\n",
    "    data = requests.get(url).text\n",
    "    return parse_large_file(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e8ec33",
   "metadata": {},
   "source": [
    "**Benefits:**\n",
    "- **Higher throughput**: Run more tasks concurrently when they're waiting on I/O\n",
    "- **Better resource utilization**: Don't waste CPU cores on tasks that are mostly idle\n",
    "- **Cost efficiency**: Process more work on the same hardware\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Note:** Don't abuse fractional resources and fall into the anti-pattern of launching too many small tasks. Instead, batch work and leverage multi-threading within tasks when possible.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127b8a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU model serving: Load 4 small models on one GPU\n",
    "@ray.remote(num_gpus=0.25)\n",
    "def run_small_model(model, input_data):\n",
    "    return model.predict(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6482dc23",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Fractional resources include support for <strong><a href=\"https://docs.ray.io/en/latest/ray-core/scheduling/accelerators.html#fractional-accelerators\" target=\"_blank\">multiple accelerators</a></strong>, allowing users to load multiple smaller models onto a single GPU. Learn more about <strong><a href=\"https://docs.ray.io/en/latest/ray-core/scheduling/resources.html#fractional-resource-requirements\" target=\"_blank\">fractional resource requirements</a></strong>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79167387",
   "metadata": {},
   "source": [
    "### 8.3. Checking available resources\n",
    "\n",
    "<p>During the <em>scheduling stage</em>, Ray evaluates the <strong>resource requirements</strong> specified via the <code>@ray.remote</code> decorator or within the <code>resources={...}</code> argument. These requirements may include:</p>\n",
    "\n",
    "<ul>\n",
    "    <li><strong>CPU</strong> e.g., <code>@ray.remote(num_cpus=2)</code>)</li>\n",
    "    <li><strong>GPU</strong> e.g., <code>@ray.remote(num_gpus=1)</code>)</li>\n",
    "    <li><strong>Custom resources</strong>: User-defined custom resources like <code>\"TPU\"</code></li>\n",
    "    <li><strong>Memory</strong></li>\n",
    "</ul>\n",
    "\n",
    "<p>Ray's scheduler checks the <strong>resource specification</strong> (sometimes referred to as <strong>resource shape</strong>) to match tasks and actors with available resources in the cluster. If the exact resource combination is unavailable, Ray may autoscale the cluster.</p>\n",
    "\n",
    "<p>You can inspect the current resource availability using:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414abb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total cluster resources\n",
    "ray.cluster_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dd876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently available (not reserved)\n",
    "ray.available_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd3acd6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<strong>Pattern:</strong> configure the head node to be unavailable for compute tasks.\n",
    "\n",
    "When scaling to large clusters, it's important to ensure that the <strong>head node</strong> does not handle any compute tasks. Users can indicate that the head node is unavailable for compute by setting its resources:\n",
    "\n",
    "```resources: {\"CPU\": 0}```\n",
    "\n",
    "Learn more about <strong><a href=\"https://docs.ray.io/en/latest/cluster/vms/user-guides/large-cluster-best-practices.html#configuring-the-head-node\" target=\"_blank\">configuring the head node</a></strong>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f90a0fe",
   "metadata": {},
   "source": [
    "## 9. Pipeline data processing and waiting for results\n",
    "\n",
    "After launching a number of tasks, you may want to know which ones have finished executing without blocking on all of them. This could be achieved by `ray.wait()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fe8e23",
   "metadata": {},
   "source": [
    "### 9.1. Understanding ray.wait()\n",
    "\n",
    "`ray.wait()` is a powerful primitive for building pipelines and managing task completion:\n",
    "\n",
    "**Signature:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbc7cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def remote_fn(x):\n",
    "    time.sleep(random.uniform(2, 10))\n",
    "    return x\n",
    "\n",
    "refs = [remote_fn.remote(i) for i in range(10)]\n",
    "\n",
    "ready, not_ready = ray.wait(\n",
    "    refs,\n",
    "    num_returns=1,      # Number of references to wait for\n",
    "    timeout=None,       # Max time to wait for (seconds)\n",
    "    fetch_local=True    # Whether to fetch objects to the local node or not\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f36a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1411f9a",
   "metadata": {},
   "source": [
    "**Returns:**\n",
    "- `ready`: List of ObjectRefs that are ready\n",
    "- `not_ready`: List of ObjectRefs still pending"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4fa01d",
   "metadata": {},
   "source": [
    "### 9.2. Pipeline pattern with ray.wait()\n",
    "\n",
    "\n",
    "| <img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/ray-core/pipeline-data-processing.png\" width=\"400px\" loading=\"lazy\">                                                                               |\n",
    "| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| (top panel) Execution timeline when using ray.get() to wait for all results before calling process results. (bottom panel) Execution timeline when using ray.wait() to process results as soon as they become available. |\n",
    "\n",
    "Here are functions to match the above diagram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9994497",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def do_some_work(x):\n",
    "    time.sleep(x)\n",
    "    return x\n",
    "\n",
    "@ray.remote\n",
    "def process_incremental(result):\n",
    "    time.sleep(1)\n",
    "    return result * 2\n",
    "\n",
    "@ray.remote\n",
    "def process_results(results):\n",
    "    out = []\n",
    "    for result in results:\n",
    "        time.sleep(1)\n",
    "        out.append(result * 2)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898ec696",
   "metadata": {},
   "source": [
    "This is the **naive approach:** block until all tasks are complete and then process the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a3122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [2, 3, 1, 4]\n",
    "start = time.time()\n",
    "data_list = ray.get([do_some_work.remote(x) for x in inputs])\n",
    "output = ray.get(process_results.remote(data_list))\n",
    "print(\"duration =\", time.time() - start, \"\\nresult = \", output)\n",
    "# Duration: ~8 seconds (4s max task + 4s processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9545b1",
   "metadata": {},
   "source": [
    "This is the **pipelined** approach: process items as soon as they become available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0ba879",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "result_ids = [do_some_work.remote(x) for x in inputs]\n",
    "refs = []\n",
    "while len(result_ids):\n",
    "    done_id, result_ids = ray.wait(result_ids, num_returns=1)\n",
    "    print(done_id)\n",
    "    refs.append(process_incremental.remote(done_id))\n",
    "output = ray.get(refs)\n",
    "print(\"duration =\", time.time() - start, \"\\nresult = \", sum)\n",
    "# Duration: ~5 seconds (overlapping ~4s computation and 1s processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf78b01d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Read more about the <strong><a href=\"https://docs.ray.io/en/latest/ray-core/tips-for-first-time.html#tip-4-pipeline-data-processing\" target=\"_blank\">pipeline data processing</a></strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cb0443",
   "metadata": {},
   "source": [
    "## 10. Ray generators\n",
    "\n",
    "[Ray Generators](https://docs.ray.io/en/latest/ray-core/ray-generator.html) are a way to make use of the python generator pattern to generate data.\n",
    "\n",
    "They are useful for:\n",
    "- Reducing worker heap memory usage **by** avoiding building up a large in-memory collection\n",
    "- Reducing object store memory usage **by** allowing for garbage collection of objects that are processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d98fec",
   "metadata": {},
   "source": [
    "### 10.1. Why use Ray Generators?\n",
    "\n",
    "**Problem with regular tasks:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0a86bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def produce_large_dataset():\n",
    "    # Creates all data in memory at once\n",
    "    results = []\n",
    "    for i in range(100):\n",
    "        results.append(np.random.rand(1000, 1000))  # Each object is ~8MB\n",
    "    return results  # ~800MB in memory!\n",
    "\n",
    "# High memory pressure\n",
    "ref = produce_large_dataset.remote()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62e4b20",
   "metadata": {},
   "source": [
    "**Solution with generators:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc0b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def produce_large_dataset():\n",
    "    # Yields one object at a time\n",
    "    for i in range(100):\n",
    "        yield np.random.rand(1000, 1000)  # Only ~8MB at a time\n",
    "\n",
    "# Process streaming\n",
    "for obj_ref in produce_large_dataset.remote():\n",
    "    result = ray.get(obj_ref)\n",
    "    process(result)\n",
    "    # Previous objects can be garbage collected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6709d2",
   "metadata": {},
   "source": [
    "### 10.2. Python generator recap\n",
    "\n",
    "Let's start with a sample python generator function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bbd470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_function():\n",
    "    for i in range(10):\n",
    "        time.sleep(1)\n",
    "        yield i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcc1e6e",
   "metadata": {},
   "source": [
    "Here is how we can iterate over the generator function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a0e36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in generator_function():\n",
    "    print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600dd0dd",
   "metadata": {},
   "source": [
    "### 10.3. Converting to Ray generator\n",
    "\n",
    "Converting into a Ray generator function is straightforward:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d389bcad",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def generator_function():\n",
    "    for i in range(10):\n",
    "        time.sleep(1)\n",
    "        yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b36501",
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj_ref in generator_function.remote():\n",
    "    print(obj_ref)  # Prints ObjectRef\n",
    "\n",
    "result = ray.get(obj_ref)\n",
    "print(result)  # Prints actual value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b625a8",
   "metadata": {},
   "source": [
    "### 10.4. Memory usage comparison\n",
    "\n",
    "See the below script which shows the memory consumption when running with and without a generator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087746f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!RAY_DEDUP_LOGS=0 python scripts/ray_generator_object_store_diff.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e49deca",
   "metadata": {},
   "source": [
    "### 10.5. Key differences from Python generators\n",
    "\n",
    "Unlike python generators, Ray generators:\n",
    "- **Don't pause execution** - i.e. they don't require `__next__` to be called to yield the next element\n",
    "- **Don't support all APIs** like send and throw\n",
    "- **Execute eagerly** - the task runs to completion regardless of consumer speed\n",
    "\n",
    "Given that Ray eagerly executes a generator task to completion regardless of whether the caller is polling the partial results or not, it might lead to object store spilling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ccb26f",
   "metadata": {},
   "source": [
    "### 10.6. Backpressure control\n",
    "\n",
    "To backpressure the generator, we can specify the `_generator_backpressure_num_objects` argument in the `@ray.remote` decorator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3869ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_returns=\"streaming\", _generator_backpressure_num_objects=10)\n",
    "def generate_data():\n",
    "    for i in range(1000):\n",
    "        yield expensive_square(i)\n",
    "        # Will pause if consumer is slow and 10 objects are buffered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75158bd2",
   "metadata": {},
   "source": [
    "Below is a script that shows execution with and without backpressure:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e45fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/ray_generator_backpressure_diff.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595dd0eb",
   "metadata": {},
   "source": [
    "### 10.7. When to use Ray Generators\n",
    "\n",
    "**â Use Ray Generators when:**\n",
    "- Processing large datasets that don't fit in memory\n",
    "- Streaming results from long-running computations\n",
    "- Building data pipelines with multiple stages\n",
    "- You want incremental results (don't wait for everything)\n",
    "\n",
    "**â Don't use Ray Generators when:**\n",
    "- You need random access to results\n",
    "- You need the full result set at once"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "ray-core-deep-dive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
