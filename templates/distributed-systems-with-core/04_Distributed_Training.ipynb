{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f0b07d0",
   "metadata": {},
   "source": [
    "# Distributed Training with Ray Actors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf1be2d",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates distributed training using Ray Actors and PyTorch Distributed. You'll build a worker group pattern step-by-step, learning each concept as you go.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b> Here is the roadmap for this notebook </b>\n",
    "\n",
    "<ol>\n",
    "  <li>Architecture overview</li>\n",
    "  <li>Part 1: Setup and imports</li>\n",
    "  <li>Part 2: Building workers and orchestration</li>\n",
    "  <li>Part 3: Putting it all together</li>\n",
    "  <li>Part 4: Connection to Ray Train</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c3d2d7",
   "metadata": {},
   "source": [
    "**What We'll Build:** Multiple Ray Actors that coordinate to perform distributed collective operations (like broadcasting tensors), simulating a distributed training setup.\n",
    "\n",
    "**Key Learning Goals:**\n",
    "- Understand how Ray Actors enable stateful distributed computation\n",
    "- Learn the worker group pattern for distributed training\n",
    "- See how Ray integrates with PyTorch Distributed\n",
    "- Master coordination patterns for multi-actor workflows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619c5c10",
   "metadata": {},
   "source": [
    "## Architecture Overview: What We're Building\n",
    "\n",
    "Before we start coding, let's understand what we're building and why.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78690bfd",
   "metadata": {},
   "source": [
    "### The Challenge: Distributed Training\n",
    "\n",
    "In distributed training, we need:\n",
    "1. **Multiple workers** that can train in parallel\n",
    "2. **Stateful processes** that maintain model parameters and optimizer state\n",
    "3. **Communication** between workers to synchronize gradients\n",
    "4. **Coordination** to ensure all workers stay in sync\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb361c8b",
   "metadata": {},
   "source": [
    "### The Solution: Worker Group Pattern\n",
    "\n",
    "We'll build a **worker group** by orchestrating Ray Actors that communicate using PyTorch Distributed to perform collective operations necessary for distributed training.\n",
    "\n",
    "Here's a high-level architecture diagram:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-core/ray-core-ray-summit-2025-distributed-training-with-actors.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887960ff",
   "metadata": {},
   "source": [
    "### Communication Layers\n",
    "\n",
    "We'll use **two communication systems** that work together:\n",
    "\n",
    "**Layer 1: Ray RPC (Driver ↔ Workers)**\n",
    "- Driver creates actors\n",
    "- Driver calls methods on actors (setup, execute, cleanup)\n",
    "- Used for orchestration and control\n",
    "\n",
    "**Layer 2: PyTorch Distributed (Worker ↔ Worker)**\n",
    "- Workers communicate directly with each other\n",
    "- Used for collective operations (broadcast, all-reduce, etc.)\n",
    "- High-performance communication (NCCL for GPU, Gloo for CPU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da0a8da",
   "metadata": {},
   "source": [
    "### Execution Flow\n",
    "\n",
    "Here's what happens when we run distributed training:\n",
    "\n",
    "1. Setup Phase\n",
    "   1. Driver creates N worker actors\n",
    "   1. Each worker gets unique rank (0 to N-1)\n",
    "   1. Driver gets master address from rank 0\n",
    "   1. All workers initialize PyTorch distributed\n",
    "\n",
    "2. Training Phase\n",
    "   1. Workers perform collective operations\n",
    "   1. Example: Broadcast model parameters\n",
    "   1. Example: All-reduce gradients\n",
    "   1. All workers stay synchronized\n",
    "\n",
    "3. Cleanup Phase\n",
    "   1. Workers destroy process groups\n",
    "   1. Actors are terminated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654f5978",
   "metadata": {},
   "source": [
    "### Why This Pattern Matters\n",
    "\n",
    "This pattern is **fundamental to Ray Train** and distributed training in general:\n",
    "- **Scalable**: Works from 2 workers to hundreds\n",
    "- **Flexible**: Supports CPU and GPU training\n",
    "- **Efficient**: High-performance communication with NCCL/Gloo\n",
    "- **Production-ready**: Used by Ray Train for real workloads\n",
    "\n",
    "Now let's start building!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2490ee7e",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Imports\n",
    "\n",
    "Let's start by importing all necessary libraries and initializing Ray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5602a1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import socket\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "\n",
    "import ray\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from datetime import timedelta\n",
    "\n",
    "# Import utility functions for the worker actor\n",
    "from scripts.utils import (\n",
    "    setup_torch_process_group_impl,\n",
    "    cleanup_impl,\n",
    ")\n",
    "\n",
    "# Initialize Ray\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8653acd7",
   "metadata": {},
   "source": [
    "## Part 2: Building Workers and Orchestration Together\n",
    "\n",
    "Let's get started by building the actor and orchestration code to create our distributed training workers. We'll build this step-by-step, adding one capability at a time. For each step, you'll see both the actor code (what runs on each worker) and the orchestration code (how we coordinate all workers from the driver).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1d0582",
   "metadata": {},
   "source": [
    "### Step 2.1: Actor Skeleton with Core Methods\n",
    "\n",
    "First, let's look at the overall structure of our worker actor. Don't worry about understanding every detail yet - we'll implement each method step by step. This skeleton shows you the four core methods our actor will have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103c1f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class DistributedWorker:\n",
    "    \"\"\"A minimal Ray actor for distributed operations.\n",
    "    \n",
    "    This actor manages the worker lifecycle but delegates actual work\n",
    "    to functions passed via the execute() method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rank: int, world_size: int):\n",
    "        \"\"\"Initialize the distributed worker with rank and world_size.\"\"\"\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "    \n",
    "    def setup_torch_process_group(\n",
    "        self,\n",
    "        backend: str,\n",
    "        master_addr: str,\n",
    "        master_port: int,\n",
    "        timeout_s: int = 1800,\n",
    "    ):\n",
    "        \"\"\"Initialize torch distributed process group on this worker.\"\"\"\n",
    "        return setup_torch_process_group_impl(\n",
    "            self.rank, self.world_size, backend, master_addr, master_port, timeout_s\n",
    "        )\n",
    "    \n",
    "    def execute(self, func, *args, **kwargs):\n",
    "        \"\"\"Execute a function on this worker that leverages torch.distributed.\"\"\"\n",
    "        return func(*args, **kwargs)\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up the distributed process group.\"\"\"\n",
    "        return cleanup_impl(self.rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9889e0",
   "metadata": {},
   "source": [
    "**Why This Design?** Notice how the actor is very simple - it just manages basic information (rank and world_size) and provides an `execute()` method to run functions. This keeps the actor small and flexible. All the actual work will be done by functions we pass to `execute()`.\n",
    "\n",
    "Now let's write the code to create multiple workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13daafd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_worker_group(num_workers: int, resources_per_worker: dict):\n",
    "    return [\n",
    "        DistributedWorker.options(**resources_per_worker).remote(\n",
    "            rank=rank,\n",
    "            world_size=num_workers,\n",
    "        )\n",
    "        for rank in range(num_workers)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c67d1b0",
   "metadata": {},
   "source": [
    "Let's test creating some workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887fed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 test workers, each using 1 GPU\n",
    "test_workers = create_worker_group(\n",
    "    num_workers=2,\n",
    "    resources_per_worker={\"num_gpus\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc4b4ba",
   "metadata": {},
   "source": [
    "**What just happened?** The `@ray.remote` decorator turns our regular Python class into a distributed actor that can run on any machine in our cluster. When we call `create_worker_group()`, Ray creates multiple workers in parallel, giving each one a unique rank (0, 1, 2, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9e5810",
   "metadata": {},
   "source": [
    "### Step 2.2: GPU Setup and Coordination\n",
    "\n",
    "**The Problem:** When using NCCL (NVIDIA's collective communication library) for GPU training, each worker needs visibility to **all GPUs on its node**, not just its own GPU. By default, Ray isolates each actor to see only its assigned GPU.\n",
    "\n",
    "**Why This Matters:**\n",
    "- NCCL uses peer-to-peer GPU communication for efficiency\n",
    "- Workers on the same node can use fast NVLink/PCIe instead of going through the network\n",
    "- Without visibility to other GPUs, NCCL falls back to slower communication paths\n",
    "\n",
    "**The Solution:** We gather GPU information from all workers, group them by node, and set `CUDA_VISIBLE_DEVICES` to include all GPUs on each node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a224ab8",
   "metadata": {},
   "source": [
    "Now let's write the orchestration code that solves this problem. This function will:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10689804",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def share_cuda_visible_devices(workers: List):\n",
    "    \"\"\"Share CUDA_VISIBLE_DEVICES across workers on the same node.\"\"\"\n",
    "\n",
    "    # Step 1: Collect metadata from all workers using execute()\n",
    "    metadata_list = ray.get([\n",
    "        worker.execute.remote(get_worker_metadata)\n",
    "        for worker in workers\n",
    "    ])\n",
    "    \n",
    "    # Step 2: Group workers by node\n",
    "    node_to_workers = defaultdict(list)\n",
    "    for worker_idx, (node_id, gpu_ids) in enumerate(metadata_list):\n",
    "        node_to_workers[node_id].append(worker_idx)\n",
    "\n",
    "    node_to_gpu_ids = defaultdict(set)\n",
    "    for worker_idx, (node_id, gpu_ids) in enumerate(metadata_list):\n",
    "        for gpu_id in gpu_ids:\n",
    "            node_to_gpu_ids[node_id].add(str(gpu_id))\n",
    "    \n",
    "    # Step 3: Set CUDA_VISIBLE_DEVICES on each worker using execute()\n",
    "    set_refs = []\n",
    "    for node_id, worker_indices in node_to_workers.items():\n",
    "        gpu_ids_str = \",\".join(sorted(node_to_gpu_ids[node_id]))\n",
    "        \n",
    "        for worker_idx in worker_indices:\n",
    "            set_ref = workers[worker_idx].execute.remote(\n",
    "                set_worker_cuda_devices,\n",
    "                rank=worker_idx,\n",
    "                gpu_ids_str=gpu_ids_str\n",
    "            )\n",
    "            set_refs.append(set_ref)\n",
    "    \n",
    "    # Wait for all workers to complete setting CUDA_VISIBLE_DEVICES\n",
    "    ray.get(set_refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0030a8",
   "metadata": {},
   "source": [
    "The orchestration function above uses two helper functions. Here they are - notice these are regular Python functions, not part of the actor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc670fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_worker_metadata():\n",
    "    \"\"\"Get metadata about this worker (node_id and GPU IDs).\"\"\"\n",
    "    node_id = ray.get_runtime_context().get_node_id()\n",
    "    gpu_ids = ray.get_gpu_ids()\n",
    "    return node_id, gpu_ids\n",
    "\n",
    "def set_worker_cuda_devices(rank: int, gpu_ids_str: str):\n",
    "    \"\"\"Set CUDA_VISIBLE_DEVICES for this worker.\"\"\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_ids_str\n",
    "    print(f\"[Rank {rank}] Set CUDA_VISIBLE_DEVICES={gpu_ids_str}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27104798",
   "metadata": {},
   "source": [
    "Let's explore what these helper functions return. Start by getting a reference to worker 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94b314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_worker = test_workers[0]\n",
    "test_worker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b983c87c",
   "metadata": {},
   "source": [
    "This is an actor handle - a reference to the remote worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb6e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call execute.remote() to run the function on the worker\n",
    "metadata_ref = test_worker.execute.remote(get_worker_metadata)\n",
    "metadata_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd94efbd",
   "metadata": {},
   "source": [
    "This is an ObjectRef - a future/promise that will contain the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e40b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the actual result\n",
    "ray.get(metadata_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a76d99",
   "metadata": {},
   "source": [
    "Returns: (node_id, [gpu_id]). By default, each worker only sees its assigned GPU!\n",
    "\n",
    "Now check worker 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a333eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.get(test_workers[1].execute.remote(get_worker_metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b1823c",
   "metadata": {},
   "source": [
    "Same node_id? Then both workers are on the same machine.\n",
    "\n",
    "Gather from all workers at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e80574",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_refs = [\n",
    "    worker.execute.remote(get_worker_metadata)\n",
    "    for worker in test_workers\n",
    "]\n",
    "metadata_refs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8bd032",
   "metadata": {},
   "source": [
    "A list of ObjectRefs - one per worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edb6a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_list = ray.get(metadata_refs)\n",
    "metadata_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966edd60",
   "metadata": {},
   "source": [
    "This is what the orchestration function processes!\n",
    "\n",
    "Now run the full GPU sharing orchestration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02969c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "share_cuda_visible_devices(test_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfd1d5d",
   "metadata": {},
   "source": [
    "Watch the workers print their expanded GPU visibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcf39e4",
   "metadata": {},
   "source": [
    "### Step 2.3: PyTorch Distributed Initialization\n",
    "\n",
    "**What We're Doing:** Now we need to create a communication channel between workers so they can perform collective operations (broadcast, all-reduce, etc.). PyTorch Distributed provides this through \"process groups.\"\n",
    "\n",
    "**The Challenge:** PyTorch Distributed workers need to find each other on the network. This requires:\n",
    "1. One worker (rank 0) to act as the \"rendezvous point\" and share its address\n",
    "2. All workers to connect to this address and form a process group\n",
    "3. Each worker to know its unique rank and the total world size\n",
    "\n",
    "Now let's implement the function that `setup_torch_process_group()` delegates to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c5fa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_torch_process_group_impl(\n",
    "    rank: int,\n",
    "    world_size: int,\n",
    "    backend: str,\n",
    "    master_addr: str,\n",
    "    master_port: int,\n",
    "    timeout_s: int = 1800,\n",
    "):\n",
    "    \"\"\"Implementation of PyTorch process group initialization.\"\"\"\n",
    "    # Set environment variables for torch distributed\n",
    "    os.environ[\"MASTER_ADDR\"] = master_addr\n",
    "    os.environ[\"MASTER_PORT\"] = str(master_port)\n",
    "    os.environ[\"RANK\"] = str(rank)\n",
    "    os.environ[\"WORLD_SIZE\"] = str(world_size)\n",
    "    \n",
    "    # For NCCL backend, set async error handling\n",
    "    if backend == \"nccl\":\n",
    "        os.environ[\"TORCH_NCCL_ASYNC_ERROR_HANDLING\"] = \"1\"\n",
    "        \n",
    "        # Set CUDA device for this worker\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_ids = ray.get_gpu_ids()\n",
    "            if gpu_ids:\n",
    "                torch.cuda.set_device(gpu_ids[0])\n",
    "    \n",
    "    # Initialize the process group\n",
    "    dist.init_process_group(\n",
    "        backend=backend,\n",
    "        init_method=\"env://\",\n",
    "        rank=rank,\n",
    "        world_size=world_size,\n",
    "        timeout=timedelta(seconds=timeout_s),\n",
    "    )\n",
    "    \n",
    "    print(f\"[Rank {rank}] Process group initialized successfully!\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac99d7d",
   "metadata": {},
   "source": [
    "We also need a helper function to get the network address for the rendezvous point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a7291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_address_and_port():\n",
    "    \"\"\"Get the IP address and an available port.\"\"\"\n",
    "    ip_address = ray.util.get_node_ip_address()\n",
    "    \n",
    "    # Find an available port\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "        s.bind((\"\", 0))\n",
    "        s.listen(1)\n",
    "        port = s.getsockname()[1]\n",
    "    \n",
    "    return ip_address, port"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3811345",
   "metadata": {},
   "source": [
    "Inspect what rank 0 will use as the rendezvous point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32153092",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_0 = test_workers[0]\n",
    "rank_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce834d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "address_ref = rank_0.execute.remote(get_address_and_port)\n",
    "address_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28ba2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.get(address_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb8217b",
   "metadata": {},
   "source": [
    "Returns: (IP_address, port_number) — all workers will connect here.\n",
    "\n",
    "Now here's the orchestration function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b0a924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_torch_distributed(workers: List, backend: str = \"gloo\", timeout_s: int = 1800):\n",
    "    \"\"\"Set up torch distributed process group across all workers.\"\"\"\n",
    "    # Step 1: Get master address and port from rank 0 worker using execute()\n",
    "    master_addr, master_port = ray.get(workers[0].execute.remote(get_address_and_port))\n",
    "    \n",
    "    # Step 2: Initialize process group on all workers in parallel\n",
    "    setup_refs = [\n",
    "         worker.setup_torch_process_group.remote(\n",
    "            backend=backend,\n",
    "            master_addr=master_addr,\n",
    "            master_port=master_port,\n",
    "            timeout_s=timeout_s,\n",
    "        )\n",
    "        for worker in workers\n",
    "    ]\n",
    "    \n",
    "    # Step 3: Wait for all to complete (synchronization barrier)\n",
    "    ray.get(setup_refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a27e957",
   "metadata": {},
   "source": [
    "PyTorch Distributed uses environment variables (`MASTER_ADDR`, `MASTER_PORT`, `RANK`, `WORLD_SIZE`) to coordinate.\n",
    "- **\"gloo\"** for CPU\n",
    "- **\"nccl\"** for GPU (faster!)\n",
    "\n",
    "Run the initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339b9dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_torch_distributed(test_workers, backend=\"nccl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fcd20e",
   "metadata": {},
   "source": [
    "Each worker prints when it joins the process group. Now they can communicate!\n",
    "\n",
    "Let's verify the process group is initialized on one worker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a63b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if distributed is initialized on worker 0\n",
    "check_ref = test_workers[0].execute.remote(lambda: dist.is_initialized())\n",
    "check_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca15a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.get(check_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458d32a0",
   "metadata": {},
   "source": [
    "Should return `True` - the process group is ready!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6679e5",
   "metadata": {},
   "source": [
    "### Step 2.4: Collective Operations\n",
    "\n",
    "Now workers can perform **collective operations** - all workers participate simultaneously.\n",
    "\n",
    "**Broadcast:** One worker (source) sends data to all others at once. Used in distributed training to share model weights!\n",
    "\n",
    "**Orchestration Side:** Trigger torch.distributed.broadcast using execute():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5581e327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_distributed_broadcast(workers: List, device: str, src_rank: int = 0):\n",
    "    \"\"\"Run a broadcast operation across all workers.\"\"\"\n",
    "    world_size = len(workers)\n",
    "    \n",
    "    # Execute broadcast on all workers in parallel using execute()\n",
    "    broadcast_refs = [\n",
    "        worker.execute.remote(\n",
    "            broadcast_tensor,\n",
    "            rank=rank,\n",
    "            world_size=world_size,\n",
    "            device=device,\n",
    "            src_rank=src_rank\n",
    "        )\n",
    "        for rank, worker in enumerate(workers)\n",
    "    ]\n",
    "    \n",
    "    # Wait for completion\n",
    "    tensors = ray.get(broadcast_refs)\n",
    "    \n",
    "    # Display results\n",
    "    for rank, tensor in enumerate(tensors):\n",
    "        print(f\"  Rank {rank}: {tensor.tolist()}\")\n",
    "    \n",
    "    # Verify all tensors match\n",
    "    all_same = all(torch.equal(tensors[0], t) for t in tensors)\n",
    "    assert all_same, \"Broadcast failed: tensors do not match!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303c25de",
   "metadata": {},
   "source": [
    "**Helper Function:** Define broadcast as a standalone function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c281f24d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def broadcast_tensor(rank: int, world_size: int, device: str, src_rank: int = 0):\n",
    "    \"\"\"Participate in a broadcast operation from src_rank to all workers.\"\"\"\n",
    "    if not dist.is_initialized():\n",
    "        raise RuntimeError(\"Process group not initialized!\")\n",
    "    \n",
    "    # Create tensor\n",
    "    if rank == src_rank:\n",
    "        tensor = torch.tensor([100.0, 200.0, 300.0, 400.0, 500.0], device=device)\n",
    "        print(f\"[Rank {rank}] Broadcasting tensor: {tensor.tolist()}\")\n",
    "    else:\n",
    "        tensor = torch.zeros(5, device=device)\n",
    "        print(f\"[Rank {rank}] Before broadcast: {tensor.tolist()}\")\n",
    "    \n",
    "    # Perform the broadcast\n",
    "    dist.broadcast(tensor, src=src_rank)\n",
    "    \n",
    "    print(f\"[Rank {rank}] After broadcast: {tensor.tolist()}\")\n",
    "    return tensor.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315969cb",
   "metadata": {},
   "source": [
    "Run the broadcast operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4bad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_distributed_broadcast(test_workers, device=\"cuda\", src_rank=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725410a7",
   "metadata": {},
   "source": [
    "Watch the output:\n",
    "- Rank 0: creates `[100, 200, 300, 400, 500]`\n",
    "- Rank 1: starts with zeros\n",
    "- After broadcast: both have the same values!\n",
    "\n",
    "This is how model weights get shared in distributed training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c614ff",
   "metadata": {},
   "source": [
    "### Step 2.5: Cleanup\n",
    "\n",
    "Now let's implement the cleanup function that our actor's `cleanup()` method delegates to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e672dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_impl(rank: int) -> bool:\n",
    "    \"\"\"Implementation of process group cleanup.\"\"\"\n",
    "    if dist.is_initialized():\n",
    "        print(f\"[Rank {rank}] Destroying process group\")\n",
    "        dist.destroy_process_group()\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9c5fc0",
   "metadata": {},
   "source": [
    "Now let's define the cleanup orchestration function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a6cd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_workers(workers: List):\n",
    "    \"\"\"Clean up worker actors.\"\"\"\n",
    "    cleanup_refs = [worker.cleanup.remote() for worker in workers]\n",
    "    ray.get(cleanup_refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8400abb",
   "metadata": {},
   "source": [
    "Clean up the test workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a125903",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_workers(test_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba6856c",
   "metadata": {},
   "source": [
    "Each worker destroys its PyTorch process group. Always clean up!\n",
    "\n",
    "Great! Now we have the complete actor and all orchestration functions, built side-by-side.\n",
    "\n",
    "**Why This Design?**\n",
    "- **Minimal Actor**: Only 4 methods manage lifecycle and state\n",
    "- **Flexible**: Any function can be executed via `execute()` without modifying the actor\n",
    "- **Testable**: Helper functions can be tested independently\n",
    "- **Reusable**: Functions like `broadcast_tensor` work across different projects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90797bd0",
   "metadata": {},
   "source": [
    "## Part 3: Complete Example\n",
    "\n",
    "Here's the complete workflow to run distributed training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd37c33",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "num_workers = 4\n",
    "use_gpu = False  # Set to True if you have GPUs\n",
    "\n",
    "# Determine backend, resources, and device\n",
    "if use_gpu and torch.cuda.is_available():\n",
    "    backend = \"nccl\"\n",
    "    resources_per_worker = {\"num_gpus\": 1}\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    backend = \"gloo\"\n",
    "    resources_per_worker = {\"num_cpus\": 1}\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Create workers\n",
    "workers = create_worker_group(\n",
    "    num_workers=num_workers,\n",
    "    resources_per_worker=resources_per_worker,\n",
    ")\n",
    "\n",
    "# Setup GPU visibility (if using GPUs)\n",
    "if use_gpu and torch.cuda.is_available():\n",
    "    share_cuda_visible_devices(workers)\n",
    "\n",
    "# Initialize PyTorch Distributed\n",
    "setup_torch_distributed(workers, backend=backend)\n",
    "\n",
    "# Run distributed operation\n",
    "run_distributed_broadcast(workers, device=device, src_rank=0)\n",
    "\n",
    "# Cleanup\n",
    "cleanup_workers(workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1d1f02",
   "metadata": {},
   "source": [
    "## Part 4: Hands-On Exercise\n",
    "\n",
    "Now it's your turn! Add an all-reduce operation as a standalone function.\n",
    "\n",
    "**Task:** Implement `allreduce_tensor()` function where each worker contributes its rank value, and all workers receive the sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a79f2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this function\n",
    "# Hint: Use dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n",
    "\n",
    "def allreduce_tensor(rank: int, world_size: int, device: str):\n",
    "    \"\"\"Perform an all-reduce (sum) operation.\"\"\"\n",
    "    # Create tensor with this worker's rank\n",
    "    tensor = torch.tensor([float(rank)], device=device)\n",
    "    print(f\"[Rank {rank}] Before all-reduce: {tensor.item()}\")\n",
    "    \n",
    "    # TODO: Perform all-reduce with SUM operation\n",
    "    # Your code here\n",
    "    \n",
    "    print(f\"[Rank {rank}] After all-reduce: {tensor.item()}\")\n",
    "    return tensor.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52800f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d7c6b",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>Click to see the solution</summary>\n",
    "\n",
    "```python\n",
    "def allreduce_tensor(rank: int, world_size: int, device: str):\n",
    "    \"\"\"Perform an all-reduce (sum) operation.\"\"\"\n",
    "    tensor = torch.tensor([float(rank)], device=device)\n",
    "    print(f\"[Rank {rank}] Before all-reduce: {tensor.item()}\")\n",
    "    \n",
    "    # All-reduce sums values from all workers\n",
    "    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n",
    "    \n",
    "    print(f\"[Rank {rank}] After all-reduce: {tensor.item()}\")\n",
    "    return tensor.cpu()\n",
    "\n",
    "Test it using execute():\n",
    "world_size = len(workers)\n",
    "allreduce_refs = [\n",
    "    worker.execute.remote(allreduce_tensor, rank, world_size, device)\n",
    "    for rank, worker in enumerate(workers)\n",
    "]\n",
    "tensors = ray.get(allreduce_refs)\n",
    "# For 4 workers with ranks 0,1,2,3: sum = 0+1+2+3 = 6\n",
    "# All workers should receive 6\n",
    "```\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef49c6e",
   "metadata": {},
   "source": [
    "## Part 5: Connection to Ray Train\n",
    "\n",
    "This worker group pattern is the foundation of **Ray Train**!\n",
    "\n",
    "Ray Train provides a high-level API that handles all this complexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883538b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train import ScalingConfig\n",
    "\n",
    "# Ray Train abstracts away all the worker management\n",
    "# trainer = TorchTrainer(\n",
    "#     train_loop_per_worker=your_training_function,\n",
    "#     scaling_config=ScalingConfig(\n",
    "#         num_workers=4,\n",
    "#         use_gpu=True\n",
    "#     )\n",
    "# )\n",
    "# result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6d2e0c",
   "metadata": {},
   "source": [
    "Under the hood, Ray Train:\n",
    "- Creates worker actors (like we did)\n",
    "- Sets up PyTorch distributed (like we did)\n",
    "- Handles checkpointing, fault tolerance, and more\n",
    "- Integrates with popular ML frameworks\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "split_at_heading": true
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
