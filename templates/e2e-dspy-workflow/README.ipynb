{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end DSPy Workflows Guide \n",
    "\n",
    "Time to complete: X Hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "You are a bank, and you want to categorize customer support queries into one of 25 categories. You have hand labeled 100 examples and have collected 4,000 unlabeled examples.\n",
    "\n",
    "You want to use an LLM to solve this problem, because you don't have enough labeled data to train a traditional classifier.\n",
    "\n",
    "You also don't want to spend a lot of money on inference. \n",
    "\n",
    "## Motivation\n",
    "\n",
    "You decide to use DSPy to solve this problem because the following flow is fairly difficult to orchestrate manually:\n",
    "Data Collection/Labeling -> Fine-tuning -> Prompt Optimization -> Evaluation -> Deployment\n",
    "\n",
    "By using DSPy on Anyscale, you can easily orchestrate this flow and solve the problem.\n",
    "\n",
    "## Solution\n",
    "\n",
    "You don't want pay to host a 70B model, so you will instead finetune a 1B model, which you can host and serve at a low cost using Anyscale's RayLLM offering.\n",
    "\n",
    "In order to help the 1B model understand the reasoning behind why the 70B model makes certain classifications, you will use Chains of Thought to distill knowledge from the 70B model.\n",
    "\n",
    "So what will this look like?\n",
    "\n",
    "1. Collect 4,000 unlabeled examples\n",
    "2. Label all of them with your 70B oracle model running locally\n",
    "3. Use the new DSPy finetuning tools to finetune a 1B model\n",
    "- This takes about 20 minutes on 4xA100-80GB GPUs, and uses Anyscale's LLMForge in the background to finetune the model\n",
    "4. Evaluate and prompt optimize your 1B model checkpoints against the labeled dataset\n",
    "5. Take the best performing 1B checkpoint and compare it to the un-finetuned 1B model on the true test set\n",
    "6. Deploy the optimized 1B model/DSPy pipeline to production using Anyscale's RayLLM # TODO\n",
    "\n",
    "Note(isaac): we arent doing anything with the 100 labeled examples yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "## Set Up\n",
    "1. Installing DSPy\n",
    "2. Setting up the environment\n",
    "3. Loading the dataset\n",
    "4. Setting up the program, metric, and evaluator\n",
    "\n",
    "## Data Collection\n",
    "\n",
    "1. Collect 4,000 unlabeled examples\n",
    "2. Label all of them with your 70B oracle model running locally\n",
    "\n",
    "## Fine-tuning\n",
    "\n",
    "1. Use the new DSPy finetuning tools to finetune a 1B model\n",
    "- This takes about 20 minutes on 4xA100-80GB GPUs, and uses Anyscale's LLMForge in the background to finetune the model\n",
    "\n",
    "## Evaluation\n",
    "1. Evaluate and prompt optimize your 1B model checkpoints against the labeled dataset\n",
    "2. Find the best performing 1B checkpoint and compare it to the un-finetuned 1B model on the true test set\n",
    "\n",
    "## Serving\n",
    "1. Deploy the optimized 1B model/DSPy pipeline to production using Anyscale's RayLLM # TODO\n",
    "\n",
    "## Future Work and Open Questions\n",
    "- Efficient batch inference with a DSPy pipeline\n",
    "- Exploring different fine-tuning methods and hyperparameter sweeps\n",
    "\n",
    "This guide aims to provide a comprehensive overview of building, optimizing, and deploying LLM pipelines using DSPy and Anyscale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node Set up:\n",
    "\n",
    "We will be running everything on a head node that uses 4xA100-80GB GPUs. I find that L4s are usually available and suitable for this usecase. You can also use any more powerful node.\n",
    "\n",
    "To change to use A100 GPUs, click the \"1 active node\" in the top right corner, then for workspace node, click the pencil icon and navigate to the A100 tab and select the 4xA100 option. If you do not see A100 in the list of GPUs, they may not be available on your cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Pin to a certain branch of DSPy until merged into main\n",
    "\n",
    "# !pip install -e dspy\n",
    "\n",
    "# ignore future warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import dsp\n",
    "import os\n",
    "import ujson\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "cache_dir = \"/home/ray/default/dspy/cache\"\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "# I have included a .env.example with the necessary environment variables to be set\n",
    "# You can also set them manually if you prefer\n",
    "\n",
    "os.environ[\"DSP_CACHEDIR\"] = cache_dir\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "dspy.settings.configure(experimental=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "\n",
    "litellm.set_verbose=False\n",
    "litellm.suppress_debug_info=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_env_vars = [\n",
    "    \"DSP_CACHEDIR\",\n",
    "    \"HF_TOKEN\",\n",
    "    \"HF_HOME\"\n",
    "]\n",
    "\n",
    "for var in necessary_env_vars:\n",
    "    assert os.environ[var], f\"{var} is not set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 19:30:01,532\tINFO worker.py:1601 -- Connecting to existing Ray cluster at address: 10.0.0.20:6379...\n",
      "2024-10-15 19:30:01,540\tINFO worker.py:1777 -- Connected to Ray cluster. View the dashboard at https://session-fkvdirx4bzefi53sjl55m7asad.i.anyscaleuserdata.com \n",
      "2024-10-15 19:30:01,601\tINFO packaging.py:359 -- Pushing file package 'gcs://_ray_pkg_3239e0f6dadc40c2516b2346f6f530431c921312.zip' (10.01MiB) to Ray cluster...\n",
      "2024-10-15 19:30:01,716\tINFO packaging.py:372 -- Successfully pushed file package 'gcs://_ray_pkg_3239e0f6dadc40c2516b2346f6f530431c921312.zip'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b96f728a9ba4de687da2572d21b5f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<div class=\"lm-Widget p-Widget lm-Panel p-Panel jp-Cell-outputWrapper\">\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <div class=\"jp-RenderedHTMLCommon\" style=\"display: flex; flex-direction: row;\">\n",
       "  <svg viewBox=\"0 0 567 224\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" style=\"height: 3em;\">\n",
       "    <g clip-path=\"url(#clip0_4338_178347)\">\n",
       "        <path d=\"M341.29 165.561H355.29L330.13 129.051C345.63 123.991 354.21 112.051 354.21 94.2307C354.21 71.3707 338.72 58.1807 311.88 58.1807H271V165.561H283.27V131.661H311.8C314.25 131.661 316.71 131.501 319.01 131.351L341.25 165.561H341.29ZM283.29 119.851V70.0007H311.82C331.3 70.0007 342.34 78.2907 342.34 94.5507C342.34 111.271 331.34 119.861 311.82 119.861L283.29 119.851ZM451.4 138.411L463.4 165.561H476.74L428.74 58.1807H416L367.83 165.561H380.83L392.83 138.411H451.4ZM446.19 126.601H398L422 72.1407L446.24 126.601H446.19ZM526.11 128.741L566.91 58.1807H554.35L519.99 114.181L485.17 58.1807H472.44L514.01 129.181V165.541H526.13V128.741H526.11Z\" fill=\"var(--jp-ui-font-color0)\"/>\n",
       "        <path d=\"M82.35 104.44C84.0187 97.8827 87.8248 92.0678 93.1671 87.9146C98.5094 83.7614 105.083 81.5067 111.85 81.5067C118.617 81.5067 125.191 83.7614 130.533 87.9146C135.875 92.0678 139.681 97.8827 141.35 104.44H163.75C164.476 101.562 165.622 98.8057 167.15 96.2605L127.45 56.5605C121.071 60.3522 113.526 61.6823 106.235 60.3005C98.9443 58.9187 92.4094 54.9203 87.8602 49.0574C83.3109 43.1946 81.0609 35.8714 81.5332 28.4656C82.0056 21.0599 85.1679 14.0819 90.4252 8.8446C95.6824 3.60726 102.672 0.471508 110.08 0.0272655C117.487 -0.416977 124.802 1.86091 130.647 6.4324C136.493 11.0039 140.467 17.5539 141.821 24.8501C143.175 32.1463 141.816 39.6859 138 46.0505L177.69 85.7505C182.31 82.9877 187.58 81.4995 192.962 81.4375C198.345 81.3755 203.648 82.742 208.33 85.3976C213.012 88.0532 216.907 91.9029 219.616 96.5544C222.326 101.206 223.753 106.492 223.753 111.875C223.753 117.258 222.326 122.545 219.616 127.197C216.907 131.848 213.012 135.698 208.33 138.353C203.648 141.009 198.345 142.375 192.962 142.313C187.58 142.251 182.31 140.763 177.69 138L138 177.7C141.808 184.071 143.155 191.614 141.79 198.91C140.424 206.205 136.44 212.75 130.585 217.313C124.731 221.875 117.412 224.141 110.004 223.683C102.596 223.226 95.6103 220.077 90.3621 214.828C85.1139 209.58 81.9647 202.595 81.5072 195.187C81.0497 187.779 83.3154 180.459 87.878 174.605C92.4405 168.751 98.9853 164.766 106.281 163.401C113.576 162.035 121.119 163.383 127.49 167.19L167.19 127.49C165.664 124.941 164.518 122.182 163.79 119.3H141.39C139.721 125.858 135.915 131.673 130.573 135.826C125.231 139.98 118.657 142.234 111.89 142.234C105.123 142.234 98.5494 139.98 93.2071 135.826C87.8648 131.673 84.0587 125.858 82.39 119.3H60C58.1878 126.495 53.8086 132.78 47.6863 136.971C41.5641 141.163 34.1211 142.972 26.7579 142.059C19.3947 141.146 12.6191 137.574 7.70605 132.014C2.79302 126.454 0.0813599 119.29 0.0813599 111.87C0.0813599 104.451 2.79302 97.2871 7.70605 91.7272C12.6191 86.1673 19.3947 82.5947 26.7579 81.6817C34.1211 80.7686 41.5641 82.5781 47.6863 86.7696C53.8086 90.9611 58.1878 97.2456 60 104.44H82.35ZM100.86 204.32C103.407 206.868 106.759 208.453 110.345 208.806C113.93 209.159 117.527 208.258 120.522 206.256C123.517 204.254 125.725 201.276 126.771 197.828C127.816 194.38 127.633 190.677 126.253 187.349C124.874 184.021 122.383 181.274 119.205 179.577C116.027 177.88 112.359 177.337 108.826 178.042C105.293 178.746 102.113 180.654 99.8291 183.44C97.5451 186.226 96.2979 189.718 96.3 193.32C96.2985 195.364 96.7006 197.388 97.4831 199.275C98.2656 201.163 99.4132 202.877 100.86 204.32ZM204.32 122.88C206.868 120.333 208.453 116.981 208.806 113.396C209.159 109.811 208.258 106.214 206.256 103.219C204.254 100.223 201.275 98.0151 197.827 96.97C194.38 95.9249 190.676 96.1077 187.348 97.4873C184.02 98.8669 181.274 101.358 179.577 104.536C177.879 107.714 177.337 111.382 178.041 114.915C178.746 118.448 180.653 121.627 183.439 123.911C186.226 126.195 189.717 127.443 193.32 127.44C195.364 127.443 197.388 127.042 199.275 126.259C201.163 125.476 202.878 124.328 204.32 122.88ZM122.88 19.4205C120.333 16.8729 116.981 15.2876 113.395 14.9347C109.81 14.5817 106.213 15.483 103.218 17.4849C100.223 19.4868 98.0146 22.4654 96.9696 25.9131C95.9245 29.3608 96.1073 33.0642 97.4869 36.3922C98.8665 39.7202 101.358 42.4668 104.535 44.1639C107.713 45.861 111.381 46.4036 114.914 45.6992C118.447 44.9949 121.627 43.0871 123.911 40.301C126.195 37.515 127.442 34.0231 127.44 30.4205C127.44 28.3772 127.038 26.3539 126.255 24.4664C125.473 22.5788 124.326 20.8642 122.88 19.4205ZM19.42 100.86C16.8725 103.408 15.2872 106.76 14.9342 110.345C14.5813 113.93 15.4826 117.527 17.4844 120.522C19.4863 123.518 22.4649 125.726 25.9127 126.771C29.3604 127.816 33.0638 127.633 36.3918 126.254C39.7198 124.874 42.4664 122.383 44.1635 119.205C45.8606 116.027 46.4032 112.359 45.6988 108.826C44.9944 105.293 43.0866 102.114 40.3006 99.8296C37.5145 97.5455 34.0227 96.2983 30.42 96.3005C26.2938 96.3018 22.337 97.9421 19.42 100.86ZM100.86 100.86C98.3125 103.408 96.7272 106.76 96.3742 110.345C96.0213 113.93 96.9226 117.527 98.9244 120.522C100.926 123.518 103.905 125.726 107.353 126.771C110.8 127.816 114.504 127.633 117.832 126.254C121.16 124.874 123.906 122.383 125.604 119.205C127.301 116.027 127.843 112.359 127.139 108.826C126.434 105.293 124.527 102.114 121.741 99.8296C118.955 97.5455 115.463 96.2983 111.86 96.3005C109.817 96.299 107.793 96.701 105.905 97.4835C104.018 98.2661 102.303 99.4136 100.86 100.86Z\" fill=\"#00AEEF\"/>\n",
       "    </g>\n",
       "    <defs>\n",
       "        <clipPath id=\"clip0_4338_178347\">\n",
       "            <rect width=\"566.93\" height=\"223.75\" fill=\"white\"/>\n",
       "        </clipPath>\n",
       "    </defs>\n",
       "  </svg>\n",
       "</div>\n",
       "\n",
       "        <table class=\"jp-RenderedHTMLCommon\" style=\"border-collapse: collapse;color: var(--jp-ui-font-color1);font-size: var(--jp-ui-font-size1);\">\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>3.9.19</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>2.36.0</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://session-fkvdirx4bzefi53sjl55m7asad.i.anyscaleuserdata.com\" target=\"_blank\">http://session-fkvdirx4bzefi53sjl55m7asad.i.anyscaleuserdata.com</a></b></td>\n",
       "</tr>\n",
       "\n",
       "</table>\n",
       "\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='session-fkvdirx4bzefi53sjl55m7asad.i.anyscaleuserdata.com', python_version='3.9.19', ray_version='2.36.0', ray_commit='115e0fd7d642480f0262e3cea9596a7f37acb518')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "ray.init(runtime_env={\"env_vars\": os.environ, \"py_modules\": [dspy, dsp]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make use of a random number generator in this notebook. We are creating a Random object here to ensure that our notebook is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "rng = random.Random(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the PolyAI/banking77 dataset for this tutorial. We use the built in dspy DataLoader to load the dataset from Huggingface as a list of dspy.Example objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b4074824a64661951202d2dee52edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e60b5e8339e456386ec63cec5dbb4cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/5.89k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c07c0d116c44058316b8dfaa46e5aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/9.78k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368f8bb1ad3f4e8da076b9b6db3efbe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/158k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4668cced72ec4635a8a0d87d56d69738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/51.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f27c5371ce9648d480640bfc04cbbf45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756a7351be744e888cb51a27f6520318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare the dataset\n",
    "from dspy.datasets import DataLoader\n",
    "\n",
    "dl = DataLoader()\n",
    "full_trainset = dl.from_huggingface(\n",
    "    dataset_name=\"PolyAI/banking77\", # Dataset name from Huggingface\n",
    "    fields=(\"label\", \"text\"), # Fields needed\n",
    "    input_keys=(\"text\",), # What our model expects to recieve to generate an output\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "full_testset = dl.from_huggingface(\n",
    "    dataset_name=\"PolyAI/banking77\", # Dataset name from Huggingface\n",
    "    fields=(\"label\", \"text\"), # Fields needed\n",
    "    input_keys=(\"text\",), # What our model expects to recieve to generate an output\n",
    "    split=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of the label string to an integer\n",
    "int_to_label_dict = {\n",
    "    0: \"activate_my_card\",\n",
    "    1: \"age_limit\",\n",
    "    2: \"apple_pay_or_google_pay\",\n",
    "    3: \"atm_support\",\n",
    "    4: \"automatic_top_up\",\n",
    "    5: \"balance_not_updated_after_bank_transfer\",\n",
    "    6: \"balance_not_updated_after_cheque_or_cash_deposit\",\n",
    "    7: \"beneficiary_not_allowed\",\n",
    "    8: \"cancel_transfer\",\n",
    "    9: \"card_about_to_expire\",\n",
    "    10: \"card_acceptance\",\n",
    "    11: \"card_arrival\",\n",
    "    12: \"card_delivery_estimate\",\n",
    "    13: \"card_linking\",\n",
    "    14: \"card_not_working\",\n",
    "    15: \"card_payment_fee_charged\",\n",
    "    16: \"card_payment_not_recognised\",\n",
    "    17: \"card_payment_wrong_exchange_rate\",\n",
    "    18: \"card_swallowed\",\n",
    "    19: \"cash_withdrawal_charge\",\n",
    "    20: \"cash_withdrawal_not_recognised\",\n",
    "    21: \"change_pin\",\n",
    "    22: \"compromised_card\",\n",
    "    23: \"contactless_not_working\",\n",
    "    24: \"country_support\",\n",
    "    25: \"declined_card_payment\",\n",
    "    26: \"declined_cash_withdrawal\",\n",
    "    27: \"declined_transfer\",\n",
    "    28: \"direct_debit_payment_not_recognised\",\n",
    "    29: \"disposable_card_limits\",\n",
    "    30: \"edit_personal_details\",\n",
    "    31: \"exchange_charge\",\n",
    "    32: \"exchange_rate\",\n",
    "    33: \"exchange_via_app\",\n",
    "    34: \"extra_charge_on_statement\",\n",
    "    35: \"failed_transfer\",\n",
    "    36: \"fiat_currency_support\",\n",
    "    37: \"get_disposable_virtual_card\",\n",
    "    38: \"get_physical_card\",\n",
    "    39: \"getting_spare_card\",\n",
    "    40: \"getting_virtual_card\",\n",
    "    41: \"lost_or_stolen_card\",\n",
    "    42: \"lost_or_stolen_phone\",\n",
    "    43: \"order_physical_card\",\n",
    "    44: \"passcode_forgotten\",\n",
    "    45: \"pending_card_payment\",\n",
    "    46: \"pending_cash_withdrawal\",\n",
    "    47: \"pending_top_up\",\n",
    "    48: \"pending_transfer\",\n",
    "    49: \"pin_blocked\",\n",
    "    50: \"receiving_money\",\n",
    "    51: \"refund_not_showing_up\",\n",
    "    52: \"request_refund\",\n",
    "    53: \"reverted_card_payment\",\n",
    "    54: \"supported_cards_and_currencies\",\n",
    "    55: \"terminate_account\",\n",
    "    56: \"top_up_by_bank_transfer_charge\",\n",
    "    57: \"top_up_by_card_charge\",\n",
    "    58: \"top_up_by_cash_or_cheque\",\n",
    "    59: \"top_up_failed\",\n",
    "    60: \"top_up_limits\",\n",
    "    61: \"top_up_reverted\",\n",
    "    62: \"topping_up_by_card\",\n",
    "    63: \"transaction_charged_twice\",\n",
    "    64: \"transfer_fee_charged\",\n",
    "    65: \"transfer_into_account\",\n",
    "    66: \"transfer_not_received_by_recipient\",\n",
    "    67: \"transfer_timing\",\n",
    "    68: \"unable_to_verify_identity\",\n",
    "    69: \"verify_my_identity\",\n",
    "    70: \"verify_source_of_funds\",\n",
    "    71: \"verify_top_up\",\n",
    "    72: \"virtual_card_not_working\",\n",
    "    73: \"visa_or_mastercard\",\n",
    "    74: \"why_verify_identity\",\n",
    "    75: \"wrong_amount_of_cash_received\",\n",
    "    76: \"wrong_exchange_rate_for_cash_withdrawal\"\n",
    "}\n",
    "\n",
    "label_to_int_dict = {v: k for k, v in int_to_label_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example training set:  Example({'label': 'card_arrival', 'text': 'I am still waiting on my card?'}) (input_keys={'text'})\n"
     ]
    }
   ],
   "source": [
    "def convert_int_to_label(example):\n",
    "    example[\"label\"] = int_to_label_dict[example[\"label\"]]\n",
    "    return example\n",
    "\n",
    "full_trainset = [convert_int_to_label(example) for example in full_trainset]\n",
    "full_testset = [convert_int_to_label(example) for example in full_testset]\n",
    "\n",
    "print(\"Example training set: \", full_trainset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is originally called \"banking77\" because there are 77 labels. We will be reducing this to the top 25 most frequent labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset filtered to top 25 labels. New sizes:\n",
      "Training set size: 4171\n",
      "Test set size: 1000\n",
      "Top 25 labels: card_payment_fee_charged, direct_debit_payment_not_recognised, balance_not_updated_after_cheque_or_cash_deposit, wrong_amount_of_cash_received, cash_withdrawal_charge, transaction_charged_twice, declined_cash_withdrawal, transfer_fee_charged, balance_not_updated_after_bank_transfer, transfer_not_received_by_recipient, request_refund, card_payment_not_recognised, card_payment_wrong_exchange_rate, extra_charge_on_statement, wrong_exchange_rate_for_cash_withdrawal, refund_not_showing_up, reverted_card_payment, cash_withdrawal_not_recognised, activate_my_card, pending_card_payment, cancel_transfer, beneficiary_not_allowed, card_arrival, declined_card_payment, pending_top_up\n",
      "Example({'label': 'card_arrival', 'text': 'I am still waiting on my card?'}) (input_keys={'text'})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter(example['label'] for example in full_trainset)\n",
    "\n",
    "# get the top 25 labels sorted by name for any ties\n",
    "top_25_labels = sorted(label_counts.keys(), key=lambda x: (-label_counts[x], x))[:25]\n",
    "\n",
    "# Filter the datasets to only include examples with the top 25 labels\n",
    "full_trainset_filtered = [example for example in full_trainset if example['label'] in top_25_labels]\n",
    "full_testset_filtered = [example for example in full_testset if example['label'] in top_25_labels]\n",
    "\n",
    "print(f\"Dataset filtered to top 25 labels. New sizes:\")\n",
    "print(f\"Training set size: {len(full_trainset_filtered)}\")\n",
    "print(f\"Test set size: {len(full_testset_filtered)}\")\n",
    "print(f\"Top 25 labels: {', '.join(str(label) for label in top_25_labels)}\")\n",
    "\n",
    "print(full_trainset_filtered[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to pass the labels to the LLM somehow.\n",
    "\n",
    "In DSPy, we can do this by either including it in the docstring of the program or by adding it as an input field to the Signature.\n",
    "\n",
    "Here, we will add it to the docstring, because the set of labels is fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['card_payment_fee_charged', 'direct_debit_payment_not_recognised', 'balance_not_updated_after_cheque_or_cash_deposit', 'wrong_amount_of_cash_received', 'cash_withdrawal_charge', 'transaction_charged_twice', 'declined_cash_withdrawal', 'transfer_fee_charged', 'balance_not_updated_after_bank_transfer', 'transfer_not_received_by_recipient', 'request_refund', 'card_payment_not_recognised', 'card_payment_wrong_exchange_rate', 'extra_charge_on_statement', 'wrong_exchange_rate_for_cash_withdrawal', 'refund_not_showing_up', 'reverted_card_payment', 'cash_withdrawal_not_recognised', 'activate_my_card', 'pending_card_payment', 'cancel_transfer', 'beneficiary_not_allowed', 'card_arrival', 'declined_card_payment', 'pending_top_up']\n"
     ]
    }
   ],
   "source": [
    "labels_in_use = top_25_labels\n",
    "print(labels_in_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will shuffle our training set and split it into a training and labeled set.\n",
    "\n",
    "The scenario we are emulating is that we only have 100 labeled examples to train on. We are saying that we have 4K (length of the training set) unlabeled examples we can then label using an oracle model, and then distill the knowledge from the oracle model into our 1B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_trainset = [d for d in full_trainset_filtered]\n",
    "rng.shuffle(shuffled_trainset)\n",
    "\n",
    "# The devset shouldn't overlap\n",
    "ft_trainset = shuffled_trainset[:-100]\n",
    "labeled_trainset = shuffled_trainset[-100:]\n",
    "\n",
    "testset = full_testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple, 1 step Chain of Thought program.\n",
    "\n",
    "In DSPy, you define a Signature to show your inputs and outputs. You define a module to run the different steps of your program.\n",
    "\n",
    "Our signature has a note at the top containing a simple prompt along with the list of valid outputs.\n",
    "\n",
    "We then have an `intent` field which is the input to the program.\n",
    "\n",
    "Finally we have a `label` field which is the output of the program.\n",
    "\n",
    "We give both of these fields a short description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "# We are setting the experimental flag to True to make use of the fine-tuning\n",
    "# features that are still in development.\n",
    "dspy.settings.configure(experimental=True)\n",
    "\n",
    "class IntentClassification(dspy.Signature):\n",
    "    \"\"\"As a part of a banking issue traiging system, classify the intent of a natural language query into one of the 25 labels.\n",
    "    The intent should exactly match one of the following:\n",
    "    ['card_payment_fee_charged', 'direct_debit_payment_not_recognised', 'balance_not_updated_after_cheque_or_cash_deposit', 'wrong_amount_of_cash_received', 'cash_withdrawal_charge', 'transaction_charged_twice', 'declined_cash_withdrawal', 'transfer_fee_charged', 'balance_not_updated_after_bank_transfer', 'transfer_not_received_by_recipient', 'request_refund', 'card_payment_not_recognised', 'card_payment_wrong_exchange_rate', 'extra_charge_on_statement', 'wrong_exchange_rate_for_cash_withdrawal', 'refund_not_showing_up', 'reverted_card_payment', 'cash_withdrawal_not_recognised', 'activate_my_card', 'pending_card_payment', 'cancel_transfer', 'beneficiary_not_allowed', 'card_arrival', 'declined_card_payment', 'pending_top_up']\n",
    "    \"\"\"\n",
    "\n",
    "    intent = dspy.InputField(desc=\"Intent of the query\")\n",
    "    label = dspy.OutputField(desc=\"Type of the intent; Should just be one of the 25 labels with no other text\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the module, we create a dspy.Module class that contains the Chain of Thought predictor using the signature we defined above.\n",
    "We also pass in the valid labels to the module.\n",
    "\n",
    "Inside the forward method, we pass the text to the predictor, do a little cleaning, and return the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentClassificationModule(dspy.Module):\n",
    "    def __init__(self):\n",
    "        self.intent_classifier = dspy.ChainOfThought(IntentClassification)\n",
    "        self.valid_labels = set(labels_in_use)\n",
    "\n",
    "    def forward(self, text):\n",
    "        prediction = self.intent_classifier(intent=text)\n",
    "        sanitized_prediction = dspy.Prediction(label=prediction.label.lower().strip().replace(\" \", \"_\"), reasoning=prediction.reasoning)\n",
    "        return sanitized_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set up the metric and evaluator. We will be using the answer exact match metric.\n",
    "\n",
    "The evaluator is what we will consider as our test set.\n",
    "\n",
    "We choose `num_threads=300` because a single VLLM instance can anecdotally handle ~256 concurrent requests and it will have 50 requests pending.\n",
    "\n",
    "The `adjusted_exact_match` is because the default `answer_exact_match` metric checks that example.answer and pred.answer are equal rather than checking the label itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the metric and evaluator\n",
    "from dspy.evaluate import answer_exact_match\n",
    "\n",
    "NUM_THREADS = 300\n",
    "\n",
    "def adjusted_exact_match(example, pred, trace=None, frac=1.0):\n",
    "    example.answer = example.label\n",
    "    pred.answer = pred.label\n",
    "    return answer_exact_match(example, pred, trace, frac)\n",
    "\n",
    "metric = adjusted_exact_match\n",
    "common_kwargs = dict(metric=metric, num_threads=NUM_THREADS, display_progress=True, max_errors=10000)\n",
    "\n",
    "evaluate_testset = Evaluate(devset=testset, **common_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        return [ujson.loads(line) for line in f]\n",
    "\n",
    "def write_jsonl(filename, data):\n",
    "    with open(filename, \"w\") as f:\n",
    "        for item in data:\n",
    "            f.write(ujson.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we set up some parameters we will use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 1000\n",
    "MODEL_PARAMETERS = {\n",
    "  \"max_tokens\": MAX_TOKENS,\n",
    "  \"temperature\": 0,\n",
    "}\n",
    "\n",
    "LOCAL_API_PARAMETERS = {\n",
    "  \"api_base\": \"http://localhost:8000/v1\",\n",
    "  \"api_key\": \"fake-key-doesnt-matter\"\n",
    "}\n",
    "vanilla_program = IntentClassificationModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Run above this to do all setup without launching any models\n",
    "# This is useful if you have already collected data and want to start from finetuning or from evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a local VLLM instance to run the initial benchmarks and data collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering training data and running the 70B Model\n",
    "\n",
    "\n",
    "## Preparation\n",
    "\n",
    "Before running the 70B model:\n",
    "1. Remember to set your HF_TOKEN and HF_HOME environment variables\n",
    "2. Use the following command to start the 70B server:\n",
    "\n",
    "   ```\n",
    "   vllm serve meta-llama/Meta-Llama-3.1-70B-Instruct --port 8000 --pipeline_parallel_size 2 --enable_prefix_caching --tensor_parallel_size 2\n",
    "   ```\n",
    "\n",
    "## Parallelism Configuration\n",
    "\n",
    "We've chosen pipeline parallelism and tensor parallelism of 2 for the 70B model based on our current setup. Here's the reasoning:\n",
    "\n",
    "1. Model size: The 70B model has 30 parts of ~5 GB each (based on [HuggingFace documentation](https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct/tree/main)).\n",
    "   - Total size: 30 * 5 GB = 150 GB\n",
    "\n",
    "2. Available VRAM:\n",
    "   - Our GPUs: 80 GB VRAM x 4 = 320 GB\n",
    "   - Tensor parallelism: floor(320/150) = 2\n",
    "   - Pipeline parallelism: floor(num_gpus/2) = 2\n",
    "   - To use all 4 GPUs efficiently:\n",
    "     - Pipeline parallel size: 2\n",
    "     - Tensor parallelism: 2\n",
    "\n",
    "3. Alternative setup (8x24GB GPUs):\n",
    "   - Pipeline parallel size: 1\n",
    "   - Tensor parallelism: ceil(150/24) = 7\n",
    "\n",
    "This configuration allows us to run the 70B model efficiently across our available GPU resources.\n",
    "\n",
    "Note that on Anyscale, you CANNOT download a model without changing HF_HOME on most machines. The folder `/mnt/local_storage/' has enough space for a model download. It is not persisted across cluster restarts, but that is fine for model weights we don't need to save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command for easy copying: \n",
    "# `export HF_HOME=/mnt/local_storage/huggingface; vllm serve meta-llama/Meta-Llama-3.1-70B-Instruct --port 8000 --pipeline_parallel_size 2 --enable_prefix_caching --tensor_parallel_size 2`\n",
    "\n",
    "input(\"Press Enter once you have the vllm server running...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_70b = dspy.LM(model=\"openai/meta-llama/Meta-Llama-3.1-70B-Instruct\", **MODEL_PARAMETERS, **LOCAL_API_PARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "test_predictor = IntentClassificationModule()\n",
    "with dspy.context(lm=llama_70b):\n",
    "    sample_input = ft_trainset[15]\n",
    "    print(sample_input)\n",
    "    print(test_predictor(**sample_input.inputs()).label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see how the vanilla program performs on our small labeled dataset\n",
    "# vanilla_program = IntentClassificationModule()\n",
    "# with dspy.context(lm=llama_70b):\n",
    "#     print(\"Evaluating the vanilla program on the devset using llama 70B...\")\n",
    "#     true_labeled_eval = evaluate_devset(vanilla_program)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we are ready to optimize the pipeline. We want to optimize the 70B pipeline in order to get the best possible data to then train our 8B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization hyperparameters\n",
    "from dspy.teleprompt.random_search import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "# Define the hyperparameters for prompt optimization\n",
    "MAX_BOOTSTRAPPED_DEMOS = 3\n",
    "MAX_LABELED_DEMOS = 3\n",
    "NUM_CANDIDATE_PROGRAMS = 6\n",
    "OPTIMIZER_NUM_TRAIN = 100\n",
    "OPTIMIZER_NUM_VAL = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have added this flag to save you some compute and time while running the notebook\n",
    "# TODO: do prompt optimization on the 100 labeled examples\n",
    "# COMPILE_PROGRAM = False\n",
    "# EVAL_PROGRAM = True\n",
    "\n",
    "# # Compile the optimizer and evaluate\n",
    "# with dspy.context(lm=llama_70b):\n",
    "#     vanilla_program = IntentClassificationModule()\n",
    "#     if COMPILE_PROGRAM:\n",
    "#         bfrs_base_program = bfrs_optimizer.compile(vanilla_program, trainset=po_trainset, valset=po_devset)\n",
    "#         bfrs_base_program.save(f\"b25_70b_31_bfrs_{MAX_BOOTSTRAPPED_DEMOS}_{MAX_LABELED_DEMOS}_{NUM_CANDIDATE_PROGRAMS}.json\")\n",
    "#     else:\n",
    "#         bfrs_base_program = IntentClassificationModule()\n",
    "#         bfrs_base_program.load(f\"b25_70b_31_bfrs_{MAX_BOOTSTRAPPED_DEMOS}_{MAX_LABELED_DEMOS}_{NUM_CANDIDATE_PROGRAMS}.json\")\n",
    "#     if EVAL_PROGRAM:\n",
    "#         llama_70b_bfrs_eval = evaluate_devset(bfrs_base_program)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap Data\n",
    "\n",
    "\n",
    "In this section, we bootstrap data for fine-tuning. In the code block below, we are deciding which program should be used to collect the bootstraps. We are setting this to the prompt optimized program, but one could also set this to the vanilla program, though doing so would lead to lower quality bootstraps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do some data analysis and cleaning\n",
    "# First we will convert the data to a pandas dataframe\n",
    "import pandas as pd\n",
    "from dspy.teleprompt.finetune_teleprompter import bootstrap_data, convert_to_module_level_message_data\n",
    "\n",
    "# TODO: WORKING HERE\n",
    "\n",
    "# For realism of this scenario, we are going to delete all our labels except for our test set(which is cheating and we wouldn't have in production) and our 100 true labeled examples\n",
    "def delete_labels(dataset):\n",
    "    for example in dataset:\n",
    "        if \"label\" in example:\n",
    "            del example[\"label\"]\n",
    "    return dataset\n",
    "\n",
    "ft_trainset_to_label = delete_labels(ft_trainset)\n",
    "with dspy.context(lm=llama_70b):\n",
    "    collected_data = bootstrap_data(vanilla_program, ft_trainset_to_label, num_threads=NUM_THREADS, max_errors=10000)\n",
    "    print(collected_data[0])\n",
    "    collected_data_filtered = [x for x in collected_data if x[\"prediction\"][\"label\"] in labels_in_use]\n",
    "\n",
    "    # Convert collected_data to a pandas DataFrame\n",
    "    dataset = convert_to_module_level_message_data(collected_data_filtered, program=vanilla_program, exclude_demos=True)\n",
    "    \n",
    "print(dataset[0])\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data points removed:\", len(collected_data) - len(collected_data_filtered))\n",
    "# look at all the data that was removed to see if any can be cleaned\n",
    "filtered_data = [x for x in collected_data if x[\"prediction\"][\"label\"] not in labels_in_use]\n",
    "bad_labels = [x[\"prediction\"][\"label\"] for x in filtered_data]\n",
    "print(bad_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nice utility to save the data in case you do not run the notebook all the way through\n",
    "if True:\n",
    "    with open(\"collected_data_filtered.jsonl\", \"w\") as f:\n",
    "        for item in collected_data_filtered:\n",
    "            f.write(ujson.dumps({\"example\": item[\"example\"], \"prediction\": item[\"prediction\"]}) + \"\\n\")\n",
    "else:\n",
    "    with open(\"collected_data_filtered.jsonl\", \"r\") as f:\n",
    "        collected_data_filtered = [ujson.loads(line) for line in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we had an eval set, we would add it here, but we are using all of our examples for finetuning\n",
    "dataset_filenames = {f\"trainset_data_banking.jsonl\": dataset}\n",
    "\n",
    "for filename, data in dataset_filenames.items():\n",
    "    messages_format = [{\"messages\": item} for item in data]\n",
    "    write_jsonl(filename, messages_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning\n",
    "\n",
    "We will use LLM Forge to fine-tune the 1B model.\n",
    "\n",
    "In order to do this, we need to format our data into the correct format (Follows OpenAI messaging format placed in a jsonl file).\n",
    "\n",
    "We initially saved the data into a json file in prompt-completion format.\n",
    "\n",
    "In order to prepare for finetuning, we need to do three steps:\n",
    "1. Format the data into the correct format and verify that the data is valid\n",
    "2. Upload the data to GCP\n",
    "3. Generate the compute configuration file\n",
    "\n",
    "After the compute configuration file is generated, we can submit the job to LLM Forge, using either the command line or using the anyscale jobs sdk.\n",
    "TODO: Add the anyscale jobs sdk submit method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to checkout the fine-tuning documentation for the latest on how to use our [API](https://docs.anyscale.com/llms/finetuning/intro) and additional [capabilities](https://docs.anyscale.com/category/fine-tuning-beta/).\n",
    "\n",
    "We'll fine-tune our LLM by choosing a set of configurations. We have created recipes for different LLMs in the [`training configs`](configs/training/lora/llama-3-8b.yaml) folder which can be used as is or modified for experiments. These configurations provide flexibility over a broad range of parameters such as model, data paths, compute to use for training, number of training epochs, how often to save checkpoints, padding, loss, etc. We also include several [DeepSpeed](https://github.com/microsoft/DeepSpeed) [configurations](configs/deepspeed/zero_3_offload_optim+param.json) to choose from for further optimizations around data/model parallelism, mixed precision, checkpointing, etc.\n",
    "\n",
    "We also have recipes for [LoRA](https://arxiv.org/abs/2106.09685) (where we train a set of small low ranked matrices instead of the original attention and feed forward layers) or full parameter fine-tuning. We recommend starting with LoRA as it's less resource intensive and quicker to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TrainingMethod' from 'dsp.modules.lm' (/home/ray/default/dspy-1/dsp/modules/lm.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdsp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainingMethod\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdspy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclients\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LM\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdspy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclients\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manyscale\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FinetuneJobAnyScale\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TrainingMethod' from 'dsp.modules.lm' (/home/ray/default/dspy-1/dsp/modules/lm.py)"
     ]
    }
   ],
   "source": [
    "from dsp.modules.lm import TrainingMethod\n",
    "from dspy.clients.lm import LM\n",
    "from dspy.clients.anyscale import FinetuneJobAnyScale\n",
    "\n",
    "train_path = f\"trainset_data_banking.jsonl\"\n",
    "eval_path = None\n",
    "method = TrainingMethod.SFT\n",
    "\n",
    "kwargs = {\n",
    "    \"hyperparameters\": {\n",
    "        \"num_devices\": 4,\n",
    "        \"trainer_resources\": None,\n",
    "        \"worker_resources\": None,\n",
    "        \"generation_config\": {\n",
    "            \"prompt_format\": {\n",
    "                \"system\": \"<|start_header_id|>system<|end_header_id|>\\n\\n{instruction}<|eot_id|>\",\n",
    "                \"user\": \"<|start_header_id|>user<|end_header_id|>\\n{instruction}<|eot_id|>\",\n",
    "                \"assistant\": \"<|start_header_id|>assistant<|end_header_id|>\\n\\n{instruction}<|eot_id|>\",\n",
    "                \"trailing_assistant\": \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "                \"bos\": \"<|begin_of_text|>\",\n",
    "                \"system_in_user\": False,\n",
    "                \"default_system_message\": \"\"\n",
    "            },\n",
    "        },\n",
    "        \"learning_rate\": 3e-5,\n",
    "        \"num_epochs\": 6,\n",
    "        \"train_batch_size_per_device\": 32\n",
    "    },\n",
    "    \"use_lora\": True,\n",
    "    # TODO: I think this needs to be set dynamically\n",
    "    # \"lora_dynamic_folder\": \"dspy/lora_weights/prodjob_qmulcjw4x8z599m8hkyja8tbmi/meta-llama/Llama-3.2-1B-Instruct\"\n",
    "}\n",
    "\n",
    "finetuneable_lm = dspy.LM(model=\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "finetuning_job = await finetuneable_lm.finetune(method, train_path, eval_path, provider=\"anyscale\", train_kwargs=kwargs)\n",
    "model_names = []\n",
    "async for model_name in finetuning_job:\n",
    "    model_names.append(model_name)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Currently I am manually updating HF_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Throughout this section, anything using 8B model (or technically 70B too) should use the new evaluate with ray data batch offline(or technically online) inference.\n",
    "\n",
    "Probably worth testing offline with 8x8 threads vs just 64 threads to see if it makes a meaningful difference.\n",
    "\n",
    "## Performance comparisons\n",
    "\n",
    "Synthetic Devset:\n",
    "- 1B Non-finetuned\n",
    "- 1B Non-finetuned + Prompt Optimization\n",
    "- 1B Finetuned (all checkpoints)\n",
    "- 1B Finetuned (all checkpoints) + Prompt Optimization\n",
    "\n",
    "Note that for this task, where the eval loss of a checkpoint isn't necessarily informative of the downstream performance of the program, because there are chains of though inside output, we need to test all possible checkpoints to see which one performs best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is nice to save your model names in case something goes wrong and you want to reload them.\n",
    "import json\n",
    "if True:\n",
    "    if model_names is not None:\n",
    "        with open(\"model_names.json\", \"w\") as f:\n",
    "            json.dump(model_names, f)\n",
    "else:\n",
    "    with open(\"model_names.json\", \"r\") as f:\n",
    "        model_names = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run a local RayLLM instance that serves the model.\n",
    "\n",
    "Provided with this template is are two files, `serve_1B.yaml` and `\\model_configs\\meta-llama--Llama-3_2-1B-Instruct.yaml`. \n",
    "\n",
    "The first file, `serve_1B.yaml`, contains the serve configuration to load the model with RayLLM.\n",
    "\n",
    "The second file, `\\model_configs\\meta-llama--Llama-3_2-1B-Instruct.yaml`, contains the necessary configurations to run the 1B model.\n",
    "\n",
    "The important part of the second file is the \"dynamic_lora_loading_path\" field. This is the path to the folder where the LoRA weights are stored.\n",
    "\n",
    "DSPy will automatically save the LoRA weights to a folder in your cloud environment at $ANYSCALE_\n",
    "\n",
    "Make sure to set your HF_TOKEN and HF_HOME environment variables, and run the following command to start the server:\n",
    "\n",
    "`serve run serve_1B.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command for easy copying: \n",
    "llama_1b = dspy.LM(model=\"openai/meta-llama/Llama-3.2-1B-Instruct\", **LOCAL_API_PARAMETERS, **MODEL_PARAMETERS)\n",
    "finetuned_llamas_1b = {f: dspy.LM(model=\"openai/\" + f, **LOCAL_API_PARAMETERS, **MODEL_PARAMETERS) for f in model_names}\n",
    "all_llamas = {**finetuned_llamas_1b, \"base\": llama_1b}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check that the finetuned models are working\n",
    "finetuned_llama = list(finetuned_llamas_1b.values())[0]\n",
    "with dspy.context(lm=finetuned_llama):\n",
    "    print(finetuned_llama.model)\n",
    "    test_predictor = IntentClassificationModule()\n",
    "    sample_input = ft_trainset[11]\n",
    "    print(sample_input)\n",
    "    print(test_predictor(**sample_input.inputs()).label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try optimizing the program with the finetuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know how well the base pipeline performs, let's run prompt optimization on the pipeline in order to juice up the performance.\n",
    "\n",
    "Let's go over what the hyperparameters mean:\n",
    "- MAX_BOOTSTRAPPED_DEMOS: DSPy will \"bootstrap\" the program by collecting examples at each step that are successful and reusing those in the pipeline. This means that it will automatically collect and add chains of thought to the pipeline.\n",
    "- MAX_LABELED_DEMOS: DSPy will also insert some labeled demonstrations from the training set. These would be unmodified examples from the training set that are just using the gold answer.\n",
    "- NUM_CANDIDATE_PROGRAMS: This is the number of candidate programs that the optimizer will generate. The actual number of programs that are created is this plus three, as DSPy will also try a program with no examples, a program with TODO (check)\n",
    "- OPTIMIZER_NUM_TRAIN and OPTIMIZER_NUM_VAL: These are the number of examples that the optimizer will use for training and validation. Note that we will be taking the \"validation\" set from the trainset so as the actual validation set is untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the optimizer\n",
    "bfrs_optimizer = BootstrapFewShotWithRandomSearch(\n",
    "    metric=metric,\n",
    "    max_bootstrapped_demos=MAX_BOOTSTRAPPED_DEMOS,\n",
    "    max_labeled_demos=MAX_LABELED_DEMOS,\n",
    "    num_candidate_programs=NUM_CANDIDATE_PROGRAMS,\n",
    "    num_threads=NUM_THREADS,\n",
    "    max_errors=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collected_data_to_example(data):\n",
    "    return dspy.Example(text=data[\"example\"][\"text\"], label=data[\"prediction\"][\"label\"]).with_inputs(\"text\")\n",
    "\n",
    "collected_data_examples = [collected_data_to_example(x) for x in collected_data_filtered]\n",
    "\n",
    "devset_synthetic = collected_data_examples[:DEV_SIZE]\n",
    "ft_optimizer_devset = collected_data_examples[DEV_SIZE:DEV_SIZE+OPTIMIZER_NUM_VAL]\n",
    "ft_optimizer_trainset = collected_data_examples[DEV_SIZE+OPTIMIZER_NUM_VAL:]\n",
    "\n",
    "\n",
    "print(len(devset_synthetic), len(ft_optimizer_trainset), len(ft_optimizer_devset))\n",
    "print(devset_synthetic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPILE_PROGRAM = True\n",
    "\n",
    "ft_results = {}\n",
    "for folder, llama in all_llamas.items():\n",
    "    print(\"Evaluating\", llama.model)\n",
    "    ft_results[folder] = {}\n",
    "    with dspy.context(lm=llama):\n",
    "        evaluate_devset = Evaluate(devset=devset_synthetic, metric=metric, num_threads=NUM_THREADS, display_progress=True, max_errors=10000)\n",
    "        \n",
    "        vanilla_program = IntentClassificationModule()\n",
    "        devset_result = evaluate_devset(vanilla_program)\n",
    "        ft_results[folder][\"vanilla\"] = {\"devset\": devset_result}\n",
    "\n",
    "        if COMPILE_PROGRAM:\n",
    "            bfrs_finetuned_program = bfrs_optimizer.compile(vanilla_program, trainset=ft_optimizer_trainset, valset=ft_optimizer_devset)\n",
    "            bfrs_finetuned_program.save(f\"simpleintent_1b_32_ft_bfrs_{MAX_BOOTSTRAPPED_DEMOS}_{MAX_LABELED_DEMOS}_{NUM_CANDIDATE_PROGRAMS}_{folder.split('/')[-1]}.json\")\n",
    "        else:\n",
    "            bfrs_finetuned_program = IntentClassificationModule()\n",
    "            bfrs_finetuned_program.load(f\"simpleintent_1b_32_ft_bfrs_{MAX_BOOTSTRAPPED_DEMOS}_{MAX_LABELED_DEMOS}_{NUM_CANDIDATE_PROGRAMS}_{folder.split('/')[-1]}.json\")\n",
    "        \n",
    "        llama_8b_bfrs_finetuned_eval = evaluate_devset(bfrs_finetuned_program)\n",
    "        ft_results[folder][\"bfrs\"] = {\"devset\": llama_8b_bfrs_finetuned_eval, \"true_labels\": None, \"testset\": None}\n",
    "        print(f\"result for {folder}: {llama_8b_bfrs_finetuned_eval}, None, None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    import json\n",
    "    with open(\"ft_results.json\", \"w\") as f:\n",
    "        json.dump(ft_results, f)\n",
    "else:\n",
    "    ft_results = json.load(open(\"ft_results.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to evaluate the test set\n",
    "best_non_base_model = max([x for x in ft_results.keys() if x != \"base\"], key=lambda x: ft_results[x][\"bfrs\"][\"devset\"])\n",
    "print(\"Best non-base model:\", best_non_base_model)\n",
    "base_and_best = {\"base\": all_llamas[\"base\"], best_non_base_model: all_llamas[best_non_base_model]}\n",
    "\n",
    "for folder, llama in base_and_best.items():\n",
    "    print(\"Evaluating\", folder)\n",
    "    vanilla_program = IntentClassificationModule()\n",
    "    \n",
    "    with dspy.context(lm=llama):\n",
    "        testset_result_vanilla = evaluate_testset(vanilla_program)\n",
    "        ft_results[folder][\"vanilla\"][\"testset\"] = testset_result_vanilla\n",
    "        vanilla_program.load(f\"simpleintent_1b_32_ft_bfrs_{MAX_BOOTSTRAPPED_DEMOS}_{MAX_LABELED_DEMOS}_{NUM_CANDIDATE_PROGRAMS}_{folder.split('/')[-1]}.json\")\n",
    "        testset_result = evaluate_testset(vanilla_program)\n",
    "        ft_results[folder][\"bfrs\"][\"testset\"] = testset_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "if True:\n",
    "    with open(\"ft_results.json\", \"w\") as f:\n",
    "        json.dump(ft_results, f)\n",
    "else:\n",
    "    ft_results = json.load(open(\"ft_results.json\"))\n",
    "    print(ft_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data for plotting\n",
    "models = []\n",
    "vanilla_devset = []\n",
    "bfrs_devset = []\n",
    "vanilla_testset = []\n",
    "bfrs_testset = []\n",
    "\n",
    "for model, results in ft_results.items():\n",
    "    if model == \"base\":\n",
    "        models.append(\"base\")\n",
    "    else:\n",
    "        models.append(\"Epoch \" + model.split(':')[1].split('-')[1])  # Extract epoch information\n",
    "    vanilla_devset.append(results['vanilla']['devset'])\n",
    "    bfrs_devset.append(results['bfrs']['devset'])\n",
    "    vanilla_testset.append(results['vanilla'].get('testset', None))\n",
    "    bfrs_testset.append(results['bfrs'].get('testset', None))\n",
    "\n",
    "# Sort the data by epoch, keeping \"base\" at the beginning\n",
    "sorted_data = sorted(zip(models, vanilla_devset, bfrs_devset, vanilla_testset, bfrs_testset),\n",
    "                     key=lambda x: (x[0] != \"base\", x[0]))\n",
    "models, vanilla_devset, bfrs_devset, vanilla_testset, bfrs_testset = zip(*sorted_data)\n",
    "\n",
    "for i in range(len(models)):\n",
    "    print(models[i], \"vanilla_devset\", vanilla_devset[i], \"bfrs_devset\", bfrs_devset[i], \"vanilla_testset\", vanilla_testset[i], \"bfrs_testset\", bfrs_testset[i])\n",
    "\n",
    "# Set up the plot\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "\n",
    "# Adjust bar positions and width\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "# Plot bars for Dev Set (top graph)\n",
    "ax1.bar(x - width/2, vanilla_devset, width, label='No Prompt Optimization', color='skyblue')\n",
    "ax1.bar(x + width/2, bfrs_devset, width, label='BootstrapFewShotRandomSearch', color='lightgreen')\n",
    "\n",
    "# Add value labels on top of each bar for Dev Set\n",
    "for i, v in enumerate(vanilla_devset):\n",
    "    ax1.text(i - width/2, v, f'{v:.1f}', ha='center', va='bottom', fontsize=8)\n",
    "for i, v in enumerate(bfrs_devset):\n",
    "    ax1.text(i + width/2, v, f'{v:.1f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Customize the Dev Set plot\n",
    "ax1.set_ylabel('Dev Set Scores')\n",
    "ax1.set_title('Model Performance Comparison Across Epochs (Synthetic Dev Set; N=1000)')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(models, rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Find the highest devset score and its corresponding model\n",
    "highest_devset_score = max(bfrs_devset)\n",
    "highest_score_model = models[bfrs_devset.index(highest_devset_score)]\n",
    "\n",
    "print(highest_devset_score, highest_score_model, bfrs_devset[models.index(highest_score_model)])\n",
    "\n",
    "# Prepare data for the bottom graph\n",
    "# Prepare data for the bottom graph (Test Set)\n",
    "base_vanilla_testset = vanilla_testset[models.index(\"base\")]\n",
    "base_bfrs_testset = bfrs_testset[models.index(\"base\")]\n",
    "\n",
    "best_model_index = bfrs_devset.index(highest_devset_score)\n",
    "best_vanilla_testset = vanilla_testset[best_model_index]\n",
    "best_bfrs_testset = bfrs_testset[best_model_index]\n",
    "\n",
    "print(\"best_vanilla_testset\", best_vanilla_testset, \"best_bfrs_testset\", best_bfrs_testset)\n",
    "print(\"base_vanilla_testset\", base_vanilla_testset, \"base_bfrs_testset\", base_bfrs_testset)\n",
    "\n",
    "# Plot bars for Test Set (bottom graph)\n",
    "models_to_plot = [\"Base Model\", f\"Best Model ({highest_score_model})\"]\n",
    "x_test = np.arange(len(models_to_plot))\n",
    "\n",
    "ax2.bar(x_test - width/2, [base_vanilla_testset, best_vanilla_testset], width, label='No Prompt Optimization', color='skyblue')\n",
    "ax2.bar(x_test + width/2, [base_bfrs_testset, best_bfrs_testset], width, label='BootstrapFewShotRandomSearch', color='lightgreen')\n",
    "\n",
    "# Add value labels on top of each bar for Test Set\n",
    "for i, v in enumerate([base_vanilla_testset, best_vanilla_testset]):\n",
    "    ax2.text(i - width/2, v, f'{v:.1f}', ha='center', va='bottom', fontsize=8)\n",
    "for i, v in enumerate([base_bfrs_testset, best_bfrs_testset]):\n",
    "    ax2.text(i + width/2, v, f'{v:.1f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Customize the Test Set plot\n",
    "ax2.set_ylabel('Test Set Scores')\n",
    "ax2.set_title('Model Performance Comparison (Real Test Set; N=1000)')\n",
    "ax2.set_xticks(x_test)\n",
    "ax2.set_xticklabels(models_to_plot)\n",
    "ax2.legend()\n",
    "ax2.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, lets give the base 8B model a fair chance by prompt optimizing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can compare all iterations of this pipeline\n",
    "print(f\"Results for HotPotQA fine-tuning LLaMa 8B with a starting trainset\")\n",
    "print(f\"    70B model (vanilla program): {llama_70b_base_eval}\")\n",
    "print(f\"    70B model (bfrs program): {llama_70b_bfrs_eval}\")\n",
    "print(f\"    8B model (vanilla program): {vanilla_8b_base_eval}\")\n",
    "print(f\"    8B model (bfrs program): {llama_8b_bfrs_eval}\")\n",
    "print(f\"    8B model (finetuned program): {llama_8b_finetuned_eval}\")\n",
    "print(f\"    8B model (finetuned bfrs program): {llama_8b_bfrs_finetuned_eval}\")\n",
    "print(f\"    8B model (finetuned mipro program): {llama_8b_ft_mipro_eval}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO - Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by running the rayllm CLI command below to start the workflow to generate the service yaml configuration:\n",
    "```bash\n",
    "mkdir /home/ray/default/deploy/services\n",
    "cd /home/ray/default/deploy/services\n",
    "rayllm gen-config \n",
    "```\n",
    "\n",
    "<img src=\"assets/cli.png\" width=500 alt=\"todo! get this inage of what I need to serve\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"background-color: yellow;\">&nbsp; IMPORTANT&nbsp;</b>: Please `Terminate` your service from the Service page to avoid depleting your free trial credits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "!python src/clear_cell_nums.py\n",
    "!find . | grep -E \".ipynb_checkpoints\" | xargs rm -rf\n",
    "!find . | grep -E \"(__pycache__|\\.pyc|\\.pyo)\" | xargs rm -rf\n",
    "!rm -rf __pycache__ data .HF_TOKEN deploy/services"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
