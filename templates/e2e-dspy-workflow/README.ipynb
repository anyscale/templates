{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end DSPy Workflows Guide \n",
    "\n",
    "Time to complete: 1 hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an Efficient LLM Pipeline with DSPy and Anyscale\n",
    "\n",
    "In this guide, we'll show how you can build an efficient pipeline covering synthetic data generation, data processing, fine-tuning, evaluation and serving with DSPy and Anyscale. \n",
    "\n",
    "## Why DSPy and Anyscale?\n",
    "DSPy simplifies the complex workflow of:\n",
    "- Data Collection/Labeling\n",
    "- Fine-tuning\n",
    "- Prompt Optimization\n",
    "- Evaluation\n",
    "  \n",
    "We'll use Anyscale for scalable infrastructure for training and serving/deploying models.\n",
    "\n",
    "## Scenario: Cost-Effective Customer Support Query Classification\n",
    "\n",
    "Consider an example of classification with limited labelled data. The specific dataset we'll be working with is the [PolyAI/banking77](https://huggingface.co/datasets/PolyAI/banking77) dataset, which consists of customer support queries for a bank. We'll simulate a scenario with low labelled data: let's say we have a dataset has limited labeled data (100 queries) and 4,000 unlabeled customer queries. We'll build a solution that leverages DSPy on Anyscale to distill knowledge from a 70B model into a more cost-effective 1B model, making it practical for production deployment.\n",
    "\n",
    "- DSPy enables easy creation of a pipeline for knowledge distillation from a 70B model to a 1B model in a low data environment\n",
    "- Anyscale's infrastructure supports efficient fine-tuning and deployment\n",
    "- Result: A cost-effective, accurate classification system for 25 categories\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. Setup\n",
    "2. Data Processing and Labeling\n",
    "3. Model Fine-tuning\n",
    "4. Evaluation and Optimization\n",
    "5. Production Deployment\n",
    "6. Future Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be running everything on A100-80GB GPUs. This is not necessary, especially for running a 1B model. You can edit the serving configuration files used throughout the notebook to use different GPUs if you do not have access to A100s.\n",
    "\n",
    "We use Anyscale's Auto-select worker node feature to launch and manage child nodes that are running our LLM. You can also set your own compute configuration to autoscale different types of GPUs at different ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dspy is already installed\n",
      "Requirement already satisfied: matplotlib in /home/ray/anaconda3/lib/python3.9/site-packages (3.9.2)\n",
      "Requirement already satisfied: python-dotenv in /home/ray/anaconda3/lib/python3.9/site-packages (1.0.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ray/anaconda3/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/ray/anaconda3/lib/python3.9/site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/ray/anaconda3/lib/python3.9/site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ray/anaconda3/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from matplotlib) (6.4.5)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.19.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ray/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "\n",
    "if importlib.util.find_spec(\"dspy\") is None:\n",
    "    print(\"Installing dspy\")\n",
    "    !pip install git+https://github.com/stanfordnlp/dspy.git@anyscale-ft-updates\n",
    "\n",
    "else:\n",
    "    print(\"dspy is already installed\")\n",
    "\n",
    "!pip install matplotlib python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "dspy.settings.configure(experimental=True)\n",
    "\n",
    "import ujson\n",
    "\n",
    "from src import set_dspy_cache_location\n",
    "set_dspy_cache_location(\"/home/ray/default/dspy/cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run this notebook, you need to have the following environment variables set:\n",
    "- HF_HOME=/mnt/local_storage/huggingface (By default, the cache directory used by HuggingFace is in the home directory -`/home/ray` in this workspace. We'll use `/mnt/local_storage` here for downloading large model weight files)\n",
    "- HF_TOKEN\n",
    "- (optional) WANDB_API_KEY\n",
    "\n",
    "You can get a HF_TOKEN [here](https://huggingface.co/settings/tokens). You will need to request access to the Meta-Llama-3.1-70B-Instruct model and the Llama-3.2-1B-Instruct model.\n",
    "\n",
    "You can get a WANDB_API_KEY [here](https://wandb.ai/authorize)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"background-color: yellow;\">&nbsp;ðŸ”„ REPLACE&nbsp;</b>: Set the HF_TOKEN, HF_HOME, and optionally the WANDB_API_KEY environment variables in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import ray \n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/mnt/local_storage/huggingface\"\n",
    "os.environ[\"HF_TOKEN\"] = \"Add your HF token here\"\n",
    "# Optional: Add your Wandb token\n",
    "# os.environ[\"WANDB_API_KEY\"] = \"12345\"\n",
    "\n",
    "# You can also use a .env file to store your HF_TOKEN and WANDB_API_KEY\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import set_random_seed\n",
    "rng = set_random_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import check_env_vars\n",
    "\n",
    "# Check if env vars are set correctly\n",
    "check_env_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +1m49s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "\u001b[36m(autoscaler +1m49s)\u001b[0m [autoscaler] [4xA100-80GB:48CPU-680GB] Upscaling 1 node(s).\n",
      "\u001b[36m(autoscaler +3m26s)\u001b[0m [autoscaler] [4xA100-80GB:48CPU-680GB|a2-ultragpu-4g] [us-east5-b] [on-demand] Launched 1 instances.\n",
      "\u001b[36m(autoscaler +23m55s)\u001b[0m [autoscaler] Downscaling node g-b285261c9a85d0001 (node IP: 10.0.15.193) due to node idle termination.\n"
     ]
    }
   ],
   "source": [
    "from src import init_ray\n",
    "init_ray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make use of a random number generator in this notebook to ensure that our notebook is reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the `PolyAI/banking77` dataset for this tutorial. We use the built in dspy DataLoader to load the dataset from Huggingface as a list of dspy.Example objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Prepare the dataset\n",
    "from src import load_data_from_huggingface, convert_int_label_to_string\n",
    "full_trainset, full_testset = load_data_from_huggingface()\n",
    "\n",
    "full_trainset_processed, full_testset_processed = convert_int_label_to_string(full_trainset, full_testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is originally called \"banking77\" because there are 77 labels. We will be reducing this to the top 25 most frequent labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset filtered to top 25 labels. New sizes:\n",
      "Training set size: 4171; Test set size: 1000\n",
      "Top 25 labels: card_payment_fee_charged, direct_debit_payment_not_recognised, balance_not_updated_after_cheque_or_cash_deposit, wrong_amount_of_cash_received, cash_withdrawal_charge, transaction_charged_twice, declined_cash_withdrawal, transfer_fee_charged, balance_not_updated_after_bank_transfer, transfer_not_received_by_recipient, request_refund, card_payment_not_recognised, card_payment_wrong_exchange_rate, extra_charge_on_statement, wrong_exchange_rate_for_cash_withdrawal, refund_not_showing_up, reverted_card_payment, cash_withdrawal_not_recognised, activate_my_card, pending_card_payment, cancel_transfer, beneficiary_not_allowed, card_arrival, declined_card_payment, pending_top_up\n",
      "Example training set: Example({'label': 'card_arrival', 'text': 'I am still waiting on my card?'}) (input_keys={'text'})\n",
      "Example test set: Example({'label': 'card_arrival', 'text': 'How do I locate my card?'}) (input_keys={'text'})\n"
     ]
    }
   ],
   "source": [
    "from src import filter_to_top_n_labels\n",
    "full_trainset_filtered, full_testset_filtered, top_25_labels = filter_to_top_n_labels(full_trainset_processed, full_testset_processed, n=25)\n",
    "\n",
    "print(f\"Dataset filtered to top 25 labels. New sizes:\")\n",
    "print(f\"Training set size: {len(full_trainset_filtered)}; Test set size: {len(full_testset_filtered)}\")\n",
    "print(f\"Top 25 labels: {', '.join(str(label) for label in top_25_labels)}\")\n",
    "print(f\"Example training set: {full_trainset_filtered[0]}\")\n",
    "print(f\"Example test set: {full_testset_filtered[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['card_payment_fee_charged', 'direct_debit_payment_not_recognised', 'balance_not_updated_after_cheque_or_cash_deposit', 'wrong_amount_of_cash_received', 'cash_withdrawal_charge', 'transaction_charged_twice', 'declined_cash_withdrawal', 'transfer_fee_charged', 'balance_not_updated_after_bank_transfer', 'transfer_not_received_by_recipient', 'request_refund', 'card_payment_not_recognised', 'card_payment_wrong_exchange_rate', 'extra_charge_on_statement', 'wrong_exchange_rate_for_cash_withdrawal', 'refund_not_showing_up', 'reverted_card_payment', 'cash_withdrawal_not_recognised', 'activate_my_card', 'pending_card_payment', 'cancel_transfer', 'beneficiary_not_allowed', 'card_arrival', 'declined_card_payment', 'pending_top_up']\n"
     ]
    }
   ],
   "source": [
    "labels_in_use = top_25_labels\n",
    "print(labels_in_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will shuffle our training set and split it into a training and labeled set.\n",
    "\n",
    "The scenario we are emulating is that we only have 100 labeled examples to train on. We are saying that we have 4K (length of the training set) unlabeled examples we can then label using an oracle model, and then distill the knowledge from the oracle model into our 1B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import adjusted_exact_match, delete_labels, NUM_THREADS\n",
    "\n",
    "common_kwargs = dict(metric=adjusted_exact_match, num_threads=NUM_THREADS, display_progress=True, max_errors=10000)\n",
    "\n",
    "shuffled_trainset = [d for d in full_trainset_filtered]\n",
    "rng.shuffle(shuffled_trainset)\n",
    "\n",
    "# The devset shouldn't overlap\n",
    "ft_trainset = shuffled_trainset[:-100]\n",
    "labeled_trainset = shuffled_trainset[-100:]\n",
    "\n",
    "# For realism of this scenario, we are going to delete all our labels except for our test set(which is cheating and we wouldn't have in production) and our 100 true labeled examples\n",
    "ft_trainset_to_label = delete_labels(ft_trainset)\n",
    "\n",
    "testset = full_testset_filtered\n",
    "evaluate_testset = dspy.Evaluate(devset=testset, **common_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Simple Chain of Thought Program in DSPy\n",
    "\n",
    "## Defining the Signature\n",
    "\n",
    "At the heart of our DSPy program is the `Signature` class. This class serves as a blueprint, outlining the inputs and outputs of our language model task. Here's how we structure it:\n",
    "\n",
    "1. **Docstring for Context**: We utilize the docstring to provide context to the LLM. In this case, we're passing our fixed set of 25 labels directly in the docstring. This approach is ideal when dealing with a static set of options.\n",
    "\n",
    "2. **Input Field**: We define an `intent` field as the input to our program. This will contain the natural language query we want to classify.\n",
    "\n",
    "3. **Output Field**: The `label` field represents our desired output - the classified intent.\n",
    "\n",
    "Both input and output fields are accompanied by concise descriptions, just to help the LLM understand the task.\n",
    "\n",
    "By structuring our program this way, we utilize DSPy's capabilities to create a clear, modular design that's both powerful and easy to maintain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentClassification(dspy.Signature):\n",
    "    \"\"\"As a part of a banking issue traiging system, classify the intent of a natural language query into one of the 25 labels.\n",
    "    The intent should exactly match one of the following:\n",
    "    ['card_payment_fee_charged', 'direct_debit_payment_not_recognised', 'balance_not_updated_after_cheque_or_cash_deposit', 'wrong_amount_of_cash_received', 'cash_withdrawal_charge', 'transaction_charged_twice', 'declined_cash_withdrawal', 'transfer_fee_charged', 'balance_not_updated_after_bank_transfer', 'transfer_not_received_by_recipient', 'request_refund', 'card_payment_not_recognised', 'card_payment_wrong_exchange_rate', 'extra_charge_on_statement', 'wrong_exchange_rate_for_cash_withdrawal', 'refund_not_showing_up', 'reverted_card_payment', 'cash_withdrawal_not_recognised', 'activate_my_card', 'pending_card_payment', 'cancel_transfer', 'beneficiary_not_allowed', 'card_arrival', 'declined_card_payment', 'pending_top_up']\n",
    "    \"\"\"\n",
    "\n",
    "    intent = dspy.InputField(desc=\"Intent of the query\")\n",
    "    label = dspy.OutputField(desc=\"Type of the intent; Should just be one of the 25 labels with no other text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the module, we create a dspy.Module class that contains the Chain of Thought predictor using the signature we defined above.\n",
    "We also pass in the valid labels to the module.\n",
    "\n",
    "Inside the forward method, we pass the text to the predictor, do a little cleaning, and return the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentClassificationModule(dspy.Module):\n",
    "    def __init__(self, labels_in_use):\n",
    "        self.intent_classifier = dspy.ChainOfThought(IntentClassification)\n",
    "        self.valid_labels = set(labels_in_use)\n",
    "\n",
    "    def forward(self, text):\n",
    "        prediction = self.intent_classifier(intent=text)\n",
    "        sanitized_prediction = dspy.Prediction(label=prediction.label.lower().strip().replace(\" \", \"_\"), reasoning=prediction.reasoning)\n",
    "        return sanitized_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we set up some the vanilla program we will use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import MODEL_PARAMETERS, LOCAL_API_PARAMETERS\n",
    "vanilla_program = IntentClassificationModule(labels_in_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Run above this to do all setup without launching any models\n",
    "# This is useful if you have already collected data and want to start from finetuning or from evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying and Utilizing a 70B Language Model\n",
    "\n",
    "This section outlines the process of deploying and utilizing a 70B parameter language model for data gathering and training. Key steps include:\n",
    "\n",
    "1. Infrastructure: Leverage Anyscale's [RayLLM](https://docs.anyscale.com/llms/serving/intro) with \"Auto-select worker nodes\" for dynamically allocating GPUs.\n",
    "2. Configuration: Use a pre-generated serve config file (created via `rayllm gen-config`) to configure the RayLLM instance.\n",
    "\n",
    "Below we show the contents of the serve config and its corresponding model config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import get_serve_and_model_config, update_serve_config_hf_token\n",
    "\n",
    "get_serve_and_model_config(\"serve_70B.yaml\")\n",
    "# update with custom HF_TOKEN\n",
    "update_serve_config_hf_token(\"serve_70B.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-22 16:21:31,283\tINFO scripts.py:489 -- Running config file: 'serve_70B.yaml'.\n",
      "2024-10-22 16:21:31,636\tINFO worker.py:1601 -- Connecting to existing Ray cluster at address: 10.0.0.43:6379...\n",
      "2024-10-22 16:21:31,643\tINFO worker.py:1777 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-14cq64yvhmxmst8dtpzxdpujh5.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2024-10-22 16:21:31,646\tINFO packaging.py:359 -- Pushing file package 'gcs://_ray_pkg_5a355eea28b0272e224cd7d876094fb507ce33f8.zip' (0.49MiB) to Ray cluster...\n",
      "2024-10-22 16:21:31,651\tINFO packaging.py:372 -- Successfully pushed file package 'gcs://_ray_pkg_5a355eea28b0272e224cd7d876094fb507ce33f8.zip'.\n",
      "\u001b[36m(ProxyActor pid=10254)\u001b[0m INFO 2024-10-22 16:21:35,163 proxy 10.0.0.43 proxy.py:1235 - Proxy starting on node 49c58362668f0d85e4a767f866414e0b74c8b30946471a1ca7b270d6 (HTTP port: 8000).\n",
      "INFO 2024-10-22 16:21:35,207 serve 10125 api.py:277 - Started Serve in namespace \"serve\".\n",
      "2024-10-22 16:21:35,214\tSUCC scripts.py:540 -- \u001b[32mSubmitted deploy config successfully.\u001b[39m\n",
      "\u001b[0m\u001b[36m(ServeController pid=10198)\u001b[0m INFO 2024-10-22 16:21:35,210 controller 10198 application_state.py:881 - Deploying new app 'llm-endpoint'.\n",
      "\u001b[36m(ServeController pid=10198)\u001b[0m INFO 2024-10-22 16:21:35,211 controller 10198 application_state.py:457 - Importing and building app 'llm-endpoint'.\n"
     ]
    }
   ],
   "source": [
    "!serve run --non-blocking serve_70B.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_70b = dspy.LM(model=\"openai/meta-llama/Meta-Llama-3.1-70B-Instruct\", **MODEL_PARAMETERS, **LOCAL_API_PARAMETERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can expect the cell below to take around 8-10 minutes to run, as it waits for the cluster to recruit a worker node if necessary and also downloads the 70B model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program input: Example({'text': 'I still have not received an answer as to why I was charged $1.00 in a transaction?'}) (input_keys={'text'})\n",
      "Program output label: extra_charge_on_statement\n"
     ]
    }
   ],
   "source": [
    "from src import sanity_check_program\n",
    "\n",
    "sanity_check_program(llama_70b, vanilla_program, ft_trainset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap Data\n",
    "\n",
    "\n",
    "In this section, we bootstrap data for fine-tuning.\n",
    "\n",
    "We delete all the true labels to be accurate to the scenario, and then collect data from the oracle LLM.\n",
    "\n",
    "We use a metric that checks if the prediction is in the set of labels we are using to get rid of any nonsense labels that the oracle LLM may hallucinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 1  (100.0):   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 50 / 50  (100.0): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:06<00:00,  7.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'role': 'system', 'content': \"Your input fields are:\\n1. `intent` (str): Intent of the query\\n\\nYour output fields are:\\n1. `reasoning` (str): ${produce the output fields}. We ...\\n2. `label` (str): Type of the intent; Should just be one of the 25 labels with no other text\\n\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n[[ ## intent ## ]]\\n{intent}\\n\\n[[ ## reasoning ## ]]\\n{reasoning}\\n\\n[[ ## label ## ]]\\n{label}\\n\\n[[ ## completed ## ]]\\n\\nIn adhering to this structure, your objective is: \\n        As a part of a banking issue traiging system, classify the intent of a natural language query into one of the 25 labels.\\n        The intent should exactly match one of the following:\\n        ['card_payment_fee_charged', 'direct_debit_payment_not_recognised', 'balance_not_updated_after_cheque_or_cash_deposit', 'wrong_amount_of_cash_received', 'cash_withdrawal_charge', 'transaction_charged_twice', 'declined_cash_withdrawal', 'transfer_fee_charged', 'balance_not_updated_after_bank_transfer', 'transfer_not_received_by_recipient', 'request_refund', 'card_payment_not_recognised', 'card_payment_wrong_exchange_rate', 'extra_charge_on_statement', 'wrong_exchange_rate_for_cash_withdrawal', 'refund_not_showing_up', 'reverted_card_payment', 'cash_withdrawal_not_recognised', 'activate_my_card', 'pending_card_payment', 'cancel_transfer', 'beneficiary_not_allowed', 'card_arrival', 'declined_card_payment', 'pending_top_up']\"}, {'role': 'user', 'content': '[[ ## intent ## ]]\\nI still have not received an answer as to why I was charged $1.00 in a transaction?\\n\\nRespond with the corresponding output fields, starting with the field `reasoning`, then `label`, and then ending with the marker for `completed`.'}, {'role': 'assistant', 'content': '[[ ## reasoning ## ]]\\nThe user is inquiring about a $1.00 transaction charge and has not received an explanation for it, indicating a concern about an unexpected fee.\\n\\n[[ ## label ## ]]\\nextra_charge_on_statement\\n\\n[[ ## completed ## ]]'}]}\n",
      "Length of dataset:\t 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt.finetune_teleprompter import bootstrap_data, convert_to_module_level_message_data\n",
    "from src import NUM_THREADS, get_valid_label_metric_fn\n",
    "\n",
    "with dspy.context(lm=llama_70b):\n",
    "    collected_data = bootstrap_data(vanilla_program, ft_trainset_to_label, num_threads=NUM_THREADS, max_errors=10000, metric=get_valid_label_metric_fn(labels_in_use))\n",
    "    # Make sure to only include the labels we are actively using or that arent hallucinated by the oracle\n",
    "    collected_data_filtered = [x for x in collected_data if x[\"prediction\"][\"label\"] in labels_in_use]\n",
    "    \n",
    "    dataset = convert_to_module_level_message_data(collected_data_filtered, program=vanilla_program, exclude_demos=True)\n",
    "\n",
    "    dataset_formatted = [{\"messages\": item} for item in dataset]\n",
    "\n",
    "print(dataset_formatted[0])\n",
    "print(\"Length of dataset:\\t\", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-22 16:39:50,520\tWARN scripts.py:132 -- \u001b[33mThe `RAY_AGENT_ADDRESS` env var has been deprecated in favor of the `RAY_DASHBOARD_ADDRESS` env var. The `RAY_AGENT_ADDRESS` is ignored.\u001b[39m\n",
      "2024-10-22 16:39:50,724\tSUCC scripts.py:747 -- \u001b[32mSent shutdown request; applications will be deleted asynchronously.\u001b[39m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Now we are done with the 70B model, so we can kill the server\n",
    "!serve shutdown -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning\n",
    "\n",
    "We will use LLM Forge to fine-tune the 1B model.\n",
    "\n",
    "In order to do this, we need to format our data into the correct format (Follows OpenAI messaging format).\n",
    "\n",
    "Anyscale now has a first class integration with DSPy for finetuning. Anyscale offers a tool for finetuning called LLMForge, which DSPy will interface with to do the actual finetuning using your own cluster on the task you defined above.\n",
    "\n",
    "We can let DSPy do the rest, where it will properly generate the config and run the finetuning.\n",
    "\n",
    "Be sure to checkout the fine-tuning documentation for the latest on how to use our [API](https://docs.anyscale.com/llms/finetuning/intro) and additional [capabilities](https://docs.anyscale.com/category/fine-tuning-beta/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4802b738d4ac49e7b73786780a95a0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Upload complete!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mUpload complete!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36m(anyscale +1h33m52.8s)\u001b[0m Uploading local dir '.' to cloud storage.\n",
      "\u001b[1m\u001b[36m(anyscale +1h33m56.5s)\u001b[0m Including workspace-managed pip dependencies.\n",
      "\u001b[1m\u001b[36m(anyscale +1h33m56.9s)\u001b[0m Job 'dspy-llmforge-fine-tuning-job' submitted, ID: 'prodjob_pvc4fnjee773klmwk9sj62sjm4'.\n",
      "\u001b[1m\u001b[36m(anyscale +1h33m56.9s)\u001b[0m View the job in the UI: https://console.anyscale.com/jobs/prodjob_pvc4fnjee773klmwk9sj62sjm4\n",
      "\u001b[1m\u001b[36m(anyscale +1h33m57.2s)\u001b[0m Waiting for job 'prodjob_pvc4fnjee773klmwk9sj62sjm4' to reach target state SUCCEEDED, currently in state: STARTING\n",
      "\u001b[1m\u001b[36m(anyscale +1h35m40.8s)\u001b[0m Job 'prodjob_pvc4fnjee773klmwk9sj62sjm4' transitioned from STARTING to RUNNING\n"
     ]
    }
   ],
   "source": [
    "from dspy.clients.lm import TrainingMethod\n",
    "\n",
    "train_data = dataset_formatted\n",
    "method = TrainingMethod.SFT\n",
    "job_path = \"configs/job.yaml\"\n",
    "llmforge_config_path = \"configs/training/lora/llama-3-8b.yaml\"\n",
    "serve_config_path = \"serve_1B.yaml\"\n",
    "\n",
    "finetuneable_lm = dspy.LM(model=\"meta-llama/Llama-3.2-1B-Instruct\", **MODEL_PARAMETERS, **LOCAL_API_PARAMETERS)\n",
    "\n",
    "try:\n",
    "    finetuning_job = finetuneable_lm.finetune(train_data=train_data, train_kwargs={\"job_config_path\": job_path, \"llmforge_config_path\": llmforge_config_path, \"serve_config_path\": serve_config_path}, train_method=method, provider=\"anyscale\")\n",
    "    finetuned_llama = finetuning_job.result()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "## Performance comparisons\n",
    "\n",
    "**Synthetic Devset:**\n",
    "- 1B Non-finetuned\n",
    "- 1B Non-finetuned + Prompt Optimization\n",
    "- 1B Finetuned (last checkpoint)\n",
    "- 1B Finetuned (last checkpoint) + Prompt Optimization\n",
    "\n",
    "**Test set:**\n",
    "- 1B Non-finetuned + Prompt Optimization\n",
    "- 1B Finetuned (last checkpoint) + Prompt Optimization\n",
    "\n",
    "Note that we do not provide an eval set when finetuning, as the eval loss of a checkpoint isn't necessarily predictive of the downstream performance of the program. We use the last checkpoint by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(finetuned_llama.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run a local RayLLM instance that serves the model.\n",
    "\n",
    "Provided with this template is are two files, `serve_1B.yaml` and `\\model_configs\\meta-llama--Llama-3_2-1B-Instruct.yaml`. \n",
    "\n",
    "The first file, `serve_1B.yaml`, contains the serve configuration to load the model with RayLLM.\n",
    "\n",
    "The second file, `\\model_configs\\meta-llama--Llama-3_2-1B-Instruct.yaml`, contains the necessary configurations to run the 1B model.\n",
    "\n",
    "<b style=\"background-color: yellow;\">&nbsp;ðŸ”„ REPLACE&nbsp;</b>:\n",
    "Make sure you set your HF_TOKEN and HF_HOME environment variables in the workspace runtime environment variables, and run the following command to start the server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import update_serve_config_hf_token\n",
    "\n",
    "update_serve_config_hf_token(\"serve_1B.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this command to start the 1B RayLLM server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!serve run --non-blocking serve_1B.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import MODEL_PARAMETERS, LOCAL_API_PARAMETERS\n",
    "\n",
    "non_ft_llama = dspy.LM(model=\"openai/meta-llama/Llama-3.2-1B-Instruct\", **MODEL_PARAMETERS, **LOCAL_API_PARAMETERS)\n",
    "\n",
    "all_llamas = {\"base\": non_ft_llama, \"ft\": finetuned_llama}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check that the finetuned models are working\n",
    "\n",
    "try:\n",
    "    sanity_check_program(finetuned_llama, vanilla_program, ft_trainset[0])\n",
    "    sanity_check_program(non_ft_llama, vanilla_program, ft_trainset[0])\n",
    "except ValueError as e:\n",
    "    # Sometimes the 1B model isn't capable of correctly outputting the label before prompt optimization, so we can just ignore this error.\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to be doing prompt optimization using DSPy's `BootstrapFewShotWithRandomSearch (BFRS)` function.\n",
    "\n",
    "BFRS will:\n",
    "- Collect a set of chains of thought from the oracle\n",
    "- Use these examples that lead to a correct prediction to \"bootstrap\" the program\n",
    "- See which set of examples lead to the most correct predictions across your evaluation metric\n",
    "- Continue this process for a set number of iterations, using the best performing programs to bootstrap the next iteration\n",
    "- Return the best program\n",
    "\n",
    "Let's go over what the hyperparameters mean:\n",
    "- **max_bootstrapped_demos**: DSPy will \"bootstrap\" the program by collecting examples at each step that are successful and reusing those in the pipeline. This means that it will automatically collect and add chains of thought to the pipeline.\n",
    "- **max_labeled_demos**: DSPy will also insert some labeled demonstrations from the training set. These would be unmodified examples from the training set that are just using the given answer.\n",
    "- **num_candidate_programs**: This is the number of candidate programs that the optimizer will generate. The actual number of programs that are created is this plus three, as DSPy will also try a program with no examples, a program with just the labeled demonstrations, and a bootstrapped program with the first few examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import bootstrap_fewshot_random_search_parameters, metric\n",
    "\n",
    "print(\"Parameters:\")\n",
    "for k, v in bootstrap_fewshot_random_search_parameters.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import split_into_devset_and_optimizer_sets\n",
    "\n",
    "def collected_data_to_example(data):\n",
    "    return dspy.Example(text=data[\"example\"][\"text\"], label=data[\"prediction\"][\"label\"]).with_inputs(\"text\")\n",
    "\n",
    "collected_data_examples = [collected_data_to_example(x) for x in collected_data_filtered]\n",
    "\n",
    "devset_synthetic, ft_optimizer_trainset, ft_optimizer_devset = split_into_devset_and_optimizer_sets(collected_data_examples, dev_size=1000, optimizer_num_val=300)\n",
    "print(\"Lengths:\")\n",
    "print(\"Synthetic Devset:\\t\", len(devset_synthetic))\n",
    "print(\"Optimizer Trainset:\\t\", len(ft_optimizer_trainset))\n",
    "print(\"Optimizer Devset:\\t\", len(ft_optimizer_devset))\n",
    "print(\"Example from synthetic devset:\")\n",
    "print(devset_synthetic[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will our finetuned model and the base model, prompt optimize them, and evaluate them on the synthetic devset.\n",
    "\n",
    "Note that there is a `%%capture` below. This is to suppress the output of the evaluation and prompt optimization because it is quite long and will slow down the notebook. We will graph the results in the cell after. You can remove the tag in order to see the output.\n",
    "\n",
    "You can expect this to take around 15 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "from src import evaluate_and_prompt_optimize\n",
    "\n",
    "evaluation_kwargs = {\n",
    "    \"models\": all_llamas,\n",
    "    \"module_class\": IntentClassificationModule,\n",
    "    \"optimizer_trainset\": ft_optimizer_trainset,\n",
    "    \"optimizer_valset\": ft_optimizer_devset,\n",
    "    \"devset\": devset_synthetic,\n",
    "    \"metric\": metric,\n",
    "    \"labels_in_use\": labels_in_use\n",
    "}\n",
    "\n",
    "ft_results = evaluate_and_prompt_optimize(**evaluation_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    import json\n",
    "    with open(\"ft_results.json\", \"w\") as f:\n",
    "        json.dump(ft_results, f)\n",
    "else:\n",
    "    ft_results = json.load(open(\"ft_results.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ft_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import graph_devset_results, graph_testset_results\n",
    "\n",
    "graph_devset_results(ft_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the highest performing model is the final epoch with a score of 50.2 on our synthetic devset.\n",
    "\n",
    "We will now take this best performing model and evaluate it and our prompt optimized base model on the true test set to see if we have improved performance.\n",
    "\n",
    "This should take around 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Now we need to evaluate the test set\n",
    "from src import run_testset_evaluation\n",
    "\n",
    "testset_evaluation_kwargs = {\n",
    "    \"ft_results\": ft_results,\n",
    "    \"all_llamas\": all_llamas,\n",
    "    \"labels_in_use\": labels_in_use,\n",
    "    \"testset\": testset,\n",
    "    \"metric\": metric,\n",
    "    \"module_class\": IntentClassificationModule\n",
    "}\n",
    "\n",
    "ft_results_testset, (best_program_path, best_model, best_score) = run_testset_evaluation(**testset_evaluation_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_testset_results(ft_results_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best testset result: \\n{best_model} with score: {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Add context about what we are doing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"background-color: blue;\">&nbsp;ðŸ”„ RUN (optional)&nbsp;</b>:\n",
    "You can optionally deploy your model to Anyscale in order to use it in production.\n",
    "To do this, run the following command:\n",
    "\n",
    "```\n",
    "!anyscale service deploy -f serve_1B.yaml\n",
    "```\n",
    "\n",
    "Follow the URL in order to find your service URL and API key for your deployed service.\n",
    "\n",
    "If you choose not to deploy your model, you can run the following code to run the model locally.\n",
    "```\n",
    "serve run serve_1B.yaml\n",
    "```\n",
    "\n",
    "If you never took down your service from the previous section, there is no need to rerun the service run command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !anyscale service deploy -f serve_1B.yaml\n",
    "# !serve run serve_1B.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"background-color: yellow;\">&nbsp;ðŸ”„ REPLACE&nbsp;</b>:\n",
    "Replace the following variables with your Anyscale service URL and API key.\n",
    "\n",
    "```\n",
    "ANYSCALE_SERVICE_BASE_URL = None\n",
    "ANYSCALE_API_KEY = None\n",
    "```\n",
    "\n",
    "You can find them by clicking the query button on the Anyscale dashboard for your service.\n",
    "\n",
    "<!-- <img src=\"files/service-query.png\" alt=\"Service Query\" width=\"500\"> -->\n",
    "![Service Query](README_files/service-query.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANYSCALE_SERVICE_BASE_URL = None\n",
    "ANYSCALE_API_KEY = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import MODEL_PARAMETERS, LOCAL_API_PARAMETERS\n",
    "if ANYSCALE_SERVICE_BASE_URL and ANYSCALE_API_KEY:\n",
    "    API_PARAMETERS = {\"api_base\": ANYSCALE_SERVICE_BASE_URL, \"api_key\": ANYSCALE_API_KEY}\n",
    "else:\n",
    "    API_PARAMETERS = LOCAL_API_PARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use ray serve in order to deploy our DSPy program.\n",
    "\n",
    "The RayLLM instance you deployed will autoscale according to the number of requests you make based on the configuration inside of the `serve_1B.yaml` file.\n",
    "\n",
    "Ray serve does all the hard work for you there, so all you need to do is provide the URL and API key to query your model.\n",
    "\n",
    "Now to deploy the DSPy program on top of the RayLLM instance, we can create a FastAPI wrapper around our DSPy program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import serve\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@serve.deployment(\n",
    "    ray_actor_options={\"num_cpus\": 0.1},\n",
    "    autoscaling_config=dict(min_replicas=1, max_replicas=3)\n",
    ")\n",
    "@serve.ingress(app)\n",
    "class LLMClient:\n",
    "    def __init__(self):\n",
    "        self.llm = dspy.LM(model=\"openai/\" + best_model, **MODEL_PARAMETERS, **API_PARAMETERS)\n",
    "        dspy.settings.configure(experimental=True, lm=self.llm)\n",
    "        self.program = IntentClassificationModule(labels_in_use)\n",
    "        self.program.load(best_program_path)\n",
    "\n",
    "    @app.get(\"/\")\n",
    "    async def classify_intent(\n",
    "        self,\n",
    "        query: str,\n",
    "    ):\n",
    "        \"\"\"Answer the given question and provide sources.\"\"\"\n",
    "        retrieval_response = self.program(query)\n",
    "\n",
    "        return retrieval_response.label\n",
    "\n",
    "llm_client = LLMClient.bind()\n",
    "llm_handle = serve.run(llm_client, route_prefix=\"/classify_intent\", name=\"llm_client\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_query = ft_trainset[1][\"text\"]\n",
    "llm_response = await llm_handle.classify_intent.remote(\n",
    "    query=example_query,\n",
    ")\n",
    "print(example_query)\n",
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also query directly using HTTP requests, because we use the `@app` decorator on our FastAPI app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "try:\n",
    "    response = requests.get(f\"http://localhost:8000/classify_intent?query={example_query}\")\n",
    "    print(response.json())\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"background-color: yellow;\">&nbsp;ðŸ›‘ IMPORTANT&nbsp;</b>: Please `Terminate` your service from the Service page to avoid depleting your free trial credits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "!python src/clear_cell_nums.py\n",
    "!find . | grep -E \".ipynb_checkpoints\" | xargs rm -rf\n",
    "!find . | grep -E \"(__pycache__|\\.pyc|\\.pyo)\" | xargs rm -rf\n",
    "!rm -rf __pycache__ data .HF_TOKEN deploy/services"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
