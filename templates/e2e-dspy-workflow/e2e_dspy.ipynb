{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end DSPy Workflows Guide "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Motivation - have this problem and going to solve it with dspy and that is why we believe ti is the right solution\n",
    "\n",
    "This guide will cover the following topics:\n",
    "\n",
    "## Creating a Multi-stage LLM Pipeline\n",
    "- Building a pipeline with an untuned model in DSPy\n",
    "- Implementing batch inference (using Ray data)\n",
    "\n",
    "## Improving the Pipeline\n",
    "1. Prompt optimization\n",
    "2. Fine-tuning\n",
    "    - How to make an 8B model perform almost as well as a 70B model in your pipeline\n",
    "3. Combining fine-tuning with prompt optimization\n",
    "\n",
    "## Deployment\n",
    "- Steps to deploy the optimized pipeline and fine-tuned model to production\n",
    "\n",
    "## Future Work and Open Questions\n",
    "- Efficient batch inference with a DSPy pipeline\n",
    "- Exploring different fine-tuning methods and hyperparameter sweeps\n",
    "\n",
    "This guide aims to provide a comprehensive overview of building, optimizing, and deploying LLM pipelines using DSPy and Anyscale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node Set up:\n",
    "\n",
    "We will be running everything on a head node that uses 4xA100-80GB GPUs. I find that L4s are usually available and suitable for this usecase. You can also use any more powerful node.\n",
    "\n",
    "To change to use A100 GPUs, click the \"1 active node\" in the top right corner, then for workspace node, click the pencil icon and navigate to the A100 tab and select the 4xA100 option. If you do not see A100 in the list of GPUs, they may not be available on your cloud. Choose another kind of GPU (This notebook has been tested on X, and Y as alternatives) (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(work): DSPy installation cell\n",
    "# TODO(decision): are these changes going to be merged into DSPy main\n",
    "\n",
    "# TODO: look at my own init file to see all the stupid extra pip installs\n",
    "\n",
    "# !pip install -e dspy-d\n",
    "# !pip install -r dspy-d/requirements.txt\n",
    "# !pip install vllm\n",
    "\n",
    "# ignore future warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dspy\n",
    "import dsp\n",
    "import os\n",
    "import ujson\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# TODO: include cache in notebook\n",
    "cache_dir = \"/home/ray/default/dspy/cache\"\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "# I have included a .env.example with the necessary environment variables to be set\n",
    "# You can also set them manually if you prefer\n",
    "\n",
    "os.environ[\"DSP_CACHEDIR\"] = cache_dir\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_env_vars = [\n",
    "    \"DSP_CACHEDIR\",\n",
    "    \"HF_TOKEN\",\n",
    "    \"HF_HOME\"\n",
    "]\n",
    "\n",
    "for var in necessary_env_vars:\n",
    "    assert os.environ[var], f\"{var} is not set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 00:49:32,762\tINFO worker.py:1601 -- Connecting to existing Ray cluster at address: 10.0.15.208:6379...\n",
      "2024-09-27 00:49:32,817\tINFO worker.py:1777 -- Connected to Ray cluster. View the dashboard at https://session-fkvdirx4bzefi53sjl55m7asad.i.anyscaleuserdata.com \n",
      "2024-09-27 00:49:32,840\tINFO packaging.py:531 -- Creating a file package for local directory '/home/ray/default/dspy/dspy'.\n",
      "2024-09-27 00:49:32,867\tINFO packaging.py:359 -- Pushing file package 'gcs://_ray_pkg_541b0a69443f2800.zip' (0.68MiB) to Ray cluster...\n",
      "2024-09-27 00:49:32,875\tINFO packaging.py:372 -- Successfully pushed file package 'gcs://_ray_pkg_541b0a69443f2800.zip'.\n",
      "2024-09-27 00:49:32,888\tINFO packaging.py:531 -- Creating a file package for local directory '/home/ray/default/dspy/dsp'.\n",
      "2024-09-27 00:49:32,909\tINFO packaging.py:359 -- Pushing file package 'gcs://_ray_pkg_d6688facdffc71bd.zip' (0.64MiB) to Ray cluster...\n",
      "2024-09-27 00:49:32,916\tINFO packaging.py:372 -- Successfully pushed file package 'gcs://_ray_pkg_d6688facdffc71bd.zip'.\n",
      "2024-09-27 00:49:32,920\tINFO packaging.py:359 -- Pushing file package 'gcs://_ray_pkg_82786fa6f6a2543e41d78f1ea544e3cac1d1df0d.zip' (0.89MiB) to Ray cluster...\n",
      "2024-09-27 00:49:32,930\tINFO packaging.py:372 -- Successfully pushed file package 'gcs://_ray_pkg_82786fa6f6a2543e41d78f1ea544e3cac1d1df0d.zip'.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "if not ray.is_initialized():\n",
    "    ray.init(runtime_env={\"env_vars\": os.environ, \"py_modules\": [dspy, dsp]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make use of a random number generator in this notebook. We are creating a Random object here to ensure that our notebook is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "rng = random.Random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating your multi-stage LLM pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.datasets import HotPotQA\n",
    "from dspy.evaluate import Evaluate\n",
    "from dsp.utils.utils import deduplicate\n",
    "\n",
    "\n",
    "# We are setting the experimental flag to True to make use of the fine-tuning\n",
    "# features that are still in development.\n",
    "dspy.settings.configure(experimental=True)\n",
    "\n",
    "# Define the program\n",
    "class BasicMH(dspy.Module):\n",
    "    def __init__(self, passages_per_hop=3, num_hops=2):\n",
    "        super().__init__()\n",
    "        self.num_hops = num_hops\n",
    "        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n",
    "        # self.generate_query = dspy.ChainOfThought(GenerateSearchQuery)\n",
    "        self.generate_query = [dspy.ChainOfThought(\"context, question -> search_query\") for _ in range(self.num_hops)]\n",
    "        self.generate_answer = dspy.ChainOfThought(\"context, question -> answer\")\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = []\n",
    "        \n",
    "        for hop in range(self.num_hops):\n",
    "            search_query = self.generate_query[hop](context=context, question=question).search_query\n",
    "            passages = self.retrieve(search_query).passages\n",
    "            context = deduplicate(context + passages)\n",
    "\n",
    "        prediction = self.generate_answer(context=context, question=question).copy(context=context)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's break down the BasicMH program\n",
    "\n",
    "## Program Design\n",
    "The BasicMH program is designed to answer complex questions requiring multiple retrieval steps without relying on the model's internal knowledge.\n",
    "\n",
    "For example, if we asked \"What the capital of the country that has the eiffel tower\"\n",
    "\n",
    "The model cannot just throw the original question \"What the capital of the country that has the eiffel tower\" into a retriever, the model needs to generate a query with the right search terms in order to get the correct passages.\n",
    "\n",
    "### Program Flow:\n",
    "1. Iterate through the specified number of hops:\n",
    "\n",
    "   a. Generate a search query using `generate_query` modules.\n",
    "\n",
    "   b. Retrieve passages using the `retrieve` module.\n",
    "\n",
    "   c. Deduplicate and add retrieved passages to the context.\n",
    "2. After completing all hops, generate the final answer using the `generate_answer` module.\n",
    "\n",
    "This multi-step approach enables the program to break down complex questions and gather necessary information incrementally, improving the accuracy of the final answer.\n",
    "\n",
    "## Program Implementation\n",
    "\n",
    "### `BasicMH` class\n",
    "\n",
    "#### `__init__` method\n",
    "\n",
    "This method initializes the class with the specified number of hops and passages per hop. It also sets up the `retrieve` module for passage retrieval and a list of `generate_query` modules for generating search queries at each hop. It should be noted that each hop has a different generate_query module, which means they all get optimized separately. The intuition here is that a good prompt for the first hop may not be good for the second hop, and so on.\n",
    "\n",
    "#### `forward` method\n",
    "\n",
    "This method implements the forward pass of the program. It iterates through the specified number of hops, generating a search query at each hop and retrieving passages. The retrieved passages are then deduplicated and added to the context. After completing all hops, it generates the final answer using the `generate_answer` module.\n",
    "\n",
    "A note here is that you directly return the result of the `generate_answer` module. A common anti-pattern is to return `prediction.answer` instead of `prediction`. This is because `prediction` is an object that contains information that is useful for optimization, and the answer can be extracted later.\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "Here's an example of how to use the `BasicMH` class:\n",
    "\n",
    "```\n",
    "program = BasicMH()\n",
    "prediction = program(question=\"What is the capital of the country that has the Eiffel Tower?\")\n",
    "print(prediction.answer)\n",
    "# Prints: Paris\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we load the dataset using a built in `HotPotQA` dataset class from DSPy.\n",
    "\n",
    "We set the `train_seed` and `eval_seed` to `0` for reproducibility and the `test_size` to `0` because we do not need a test set for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de684f8a870849eca2957aceadec4dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a9dd95cd9c6437e9c8b808fc3b620ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/9.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae371ffd8b24733bdbea0097312a410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/566M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6e0deb532d415097e9a381175d6009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/47.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee626ad498444efa116cfd6422956ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/46.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b670df857df4aeeaaadacac5335b00c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/90447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbcf8e7ad71e429094a6fa110bc1a470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/7405 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff90e7fe82944bcbe51f7bf24c75a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/7405 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare the dataset\n",
    "TRAIN_SIZE = 1500\n",
    "DEV_SIZE = 1500\n",
    "dataset = HotPotQA(train_seed=0, eval_seed=0, test_size=0)\n",
    "trainset = [x.with_inputs('question') for x in dataset.train]\n",
    "devset = [x.with_inputs('question') for x in dataset.dev]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set up the metric and evaluator. We will be using the answer exact match metric.\n",
    "\n",
    "The evaluator is what we will consider as our test set.\n",
    "\n",
    "We choose `num_threads=90` because we are bottlenecked by the retrieval server, and through testing this is the maximum number of concurrent threads that can be run without causing issues for other people using the retrieval server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the metric and evaluator\n",
    "NUM_THREADS = 90\n",
    "metric = dspy.evaluate.answer_exact_match\n",
    "evaluate_devset = Evaluate(devset=devset[:DEV_SIZE], metric=metric, num_threads=NUM_THREADS, display_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO(optional): Implement LLM as judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the retriever model\n",
    "# Note that this is not hosted on Anyscale\n",
    "COLBERT_V2_ENDPOINT = \"http://20.102.90.50:2017/wiki17_abstracts\"\n",
    "retriever = dspy.ColBERTv2(url=COLBERT_V2_ENDPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering baseline performance\n",
    "\n",
    "run evaluate on a base pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 3000\n",
    "MODEL_PARAMETERS = {\n",
    "  \"max_tokens\": MAX_TOKENS,\n",
    "  \"temperature\": 0,\n",
    "}\n",
    "\n",
    "LOCAL_API_PARAMETERS = {\n",
    "  \"api_base\": \"http://localhost:8000/v1\",\n",
    "  \"api_provider\": \"vllm\",\n",
    "  \"api_key\": \"fake-key-doesnt-matter\"\n",
    "}\n",
    "vanilla_program = BasicMH()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Run above this to do all setup without launching any models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a local VLLM instance to run the initial benchmarks and data collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model to run is the 8B model in order to collect a baseline of performance.\n",
    "\n",
    "You can run the local VLLM instance with the following command:\n",
    "\n",
    "Make sure to set your HF_TOKEN and HF_HOME environment variables\n",
    "\n",
    "For Anyscale, putting models into /mnt/local_storage is a typical pattern.\n",
    "\n",
    "\n",
    "`vllm serve meta-llama/Meta-Llama-3.1-8B-Instruct --port 8000 --pipeline_parallel_size 4 --enable_prefix_caching`\n",
    "\n",
    "Lets break down what this command does:\n",
    "- `vllm serve` is the command to run the VLLM server\n",
    "- `meta-llama/Meta-Llama-3.1-8B-Instruct` is the model to run\n",
    "- `--port 8000` is the port to run the server on\n",
    "- `--pipeline_parallel_size 4` is the number of pipeline parallel size to run the server with. We are using 4 because we have 4 GPUs all of which can hold an instance of the model.\n",
    "- `--enable_prefix_caching` is the flag to enable the prefix caching. This will store and reuse the beginnings of prompts to avoid repeating the same computation. This is especially useful for DSPy since we are almost always using prompts with the same beginning parts in the form of few shot demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command for easy copying: \n",
    "# `vllm serve meta-llama/Meta-Llama-3.1-8B-Instruct --port 8000 --pipeline_parallel_size 4 --enable_prefix_caching`\n",
    "input(\"Press Enter once you have the vllm server running...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: switch to local model\n",
    "# llama_8b = dspy.MultiOpenAI(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", **MODEL_PARAMETERS, **LOCAL_API_PARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check to see if the program is working\n",
    "# with dspy.context(lm=llama_8b, rm=retriever):\n",
    "#     test_predictor = BasicMH()\n",
    "#     print(test_predictor(question=\"What is the capital of France?\").answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with dspy.context(lm=llama_8b, rm=retriever):\n",
    "#   print(\"Evaluating the vanilla program on the devset using the model to be trained (llama 8B)...\")\n",
    "#   vanilla_8b_base_eval = evaluate_devset(vanilla_program)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the 70B Model\n",
    "\n",
    "Now that we have a baseline for the 8B model, let's run the 70B model and compare its performance.\n",
    "\n",
    "## Preparation\n",
    "\n",
    "Before running the 70B model:\n",
    "1. Kill the 8B server (use `Ctrl+C`) to free up memory.\n",
    "2. Remember to set your HF_TOKEN and HF_HOME environment variables\n",
    "3. Use the following command to start the 70B server:\n",
    "\n",
    "   ```\n",
    "   vllm serve meta-llama/Meta-Llama-3.1-70B-Instruct --port 8000 --pipeline_parallel_size 2 --enable_prefix_caching --tensor_parallel_size 2\n",
    "   ```\n",
    "\n",
    "## Parallelism Configuration\n",
    "\n",
    "We've chosen pipeline parallelism and tensor parallelism of 2 for the 70B model based on our current setup. Here's the reasoning:\n",
    "\n",
    "1. Model size: The 70B model has 30 parts of ~5 GB each (based on [HuggingFace documentation](https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct/tree/main)).\n",
    "   - Total size: 30 * 5 GB = 150 GB\n",
    "\n",
    "2. Available VRAM:\n",
    "   - Our GPUs: 80 GB VRAM x 4 = 320 GB\n",
    "   - Tensor parallelism: floor(320/150) = 2\n",
    "   - Pipeline parallelism: floor(num_gpus/2) = 2\n",
    "   - To use all 4 GPUs efficiently:\n",
    "     - Pipeline parallel size: 2\n",
    "     - Tensor parallelism: 2\n",
    "\n",
    "3. Alternative setup (8x24GB GPUs):\n",
    "   - Pipeline parallel size: 1\n",
    "   - Tensor parallelism: ceil(150/24) = 7\n",
    "\n",
    "This configuration allows us to run the 70B model efficiently across our available GPU resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I needed to add the HF_HOME var to my serve config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\n",
      "Reducing Torch parallelism from 24 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n"
     ]
    }
   ],
   "source": [
    "# Command for easy copying: \n",
    "import subprocess\n",
    "# `export HF_HOME=/mnt/local_storage/huggingface`\n",
    "# `vllm serve meta-llama/Meta-Llama-3.1-70B-Instruct --port 8000 --pipeline_parallel_size 2 --enable_prefix_caching --tensor_parallel_size 2`\n",
    "# input(\"Press Enter once you have the vllm server running...\")\n",
    "# process = subprocess.Popen([\"python3\", \"-m\", \"http.server\", \"8000\"])\n",
    "process = subprocess.Popen([\"vllm\", \"serve\", \"meta-llama/Meta-Llama-3.1-70B-Instruct\", \"--port\", \"8000\", \"--pipeline_parallel_size\", \"2\", \"--enable_prefix_caching\", \"--tensor_parallel_size\", \"2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in multiopenai 0\n"
     ]
    }
   ],
   "source": [
    "llama_70b = dspy.MultiOpenAI(model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\", **MODEL_PARAMETERS, **LOCAL_API_PARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=177647)\u001b[0;0m /home/ray/anaconda3/lib/python3.9/site-packages/vllm/distributed/parallel_state.py:431: UserWarning: The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:1544.)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=177647)\u001b[0;0m   object_tensor = torch.frombuffer(pickle.dumps(obj), dtype=torch.uint8)\n",
      "[rank1]:[W926 23:08:05.603112934 ProcessGroupNCCL.cpp:2892] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())\n",
      "[rank3]:[W926 23:08:05.603192234 ProcessGroupNCCL.cpp:2892] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())\n",
      "/home/ray/anaconda3/lib/python3.9/site-packages/vllm/distributed/parallel_state.py:431: UserWarning: The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:1544.)\n",
      "  object_tensor = torch.frombuffer(pickle.dumps(obj), dtype=torch.uint8)\n",
      "[rank2]:[W926 23:08:05.841927292 ProcessGroupNCCL.cpp:2892] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())\n",
      "[rank0]:[W926 23:08:05.842452884 ProcessGroupNCCL.cpp:2892] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:46818 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:46818 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:46818 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "Moscow\n"
     ]
    }
   ],
   "source": [
    "# Another sanity check\n",
    "with dspy.context(lm=llama_70b, rm=retriever):\n",
    "    test_predictor = BasicMH()\n",
    "    print(test_predictor(question=\"What is the capital of Russia?\").answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the vanilla program on the devset using llama 70B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1500 [00:00<?, ?it/s]\n",
      "WARNING:dspy.evaluate.evaluate:2024-09-26T23:08:43.588631Z [warning  ] Evaluation was cancelled. The results may be incomplete. [dspy.evaluate.evaluate] filename=evaluate.py lineno=126\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dspy\u001b[38;5;241m.\u001b[39mcontext(lm\u001b[38;5;241m=\u001b[39mllama_70b, rm\u001b[38;5;241m=\u001b[39mretriever):\n\u001b[1;32m      2\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating the vanilla program on the devset using llama 70B...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m   llama_70b_base_eval \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_devset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvanilla_program\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/default/dspy/dspy/evaluate/evaluate.py:196\u001b[0m, in \u001b[0;36mEvaluate.__call__\u001b[0;34m(self, program, metric, devset, num_threads, display_progress, display_table, return_all_scores, return_outputs)\u001b[0m\n\u001b[1;32m    194\u001b[0m     reordered_devset, ncorrect, ntotal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_single_thread(wrapped_program, devset, display_progress)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m     reordered_devset, ncorrect, ntotal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_multi_thread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrapped_program\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplay_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m dspy\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Metric: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mncorrect\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mntotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(\u001b[38;5;241m100\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mncorrect\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mntotal,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    205\u001b[0m predicted_devset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(reordered_devset)\n",
      "File \u001b[0;32m~/default/dspy/dspy/evaluate/evaluate.py:127\u001b[0m, in \u001b[0;36mEvaluate._execute_multi_thread\u001b[0;34m(self, wrapped_program, devset, num_threads, display_progress)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_jobs\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[1;32m    126\u001b[0m     dspy\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation was cancelled. The results may be incomplete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reordered_devset, ncorrect, ntotal\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with dspy.context(lm=llama_70b, rm=retriever):\n",
    "  print(\"Evaluating the vanilla program on the devset using llama 70B...\")\n",
    "  llama_70b_base_eval = evaluate_devset(vanilla_program)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope to bring the 8B performance up to at least 70B level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the LLaMa 70B pipeline\n",
    "\n",
    "Now we are ready to optimize the pipeline. We want to optimize the 70B pipeline in order to get the best possible data to then train our 8B model.\n",
    "\n",
    "We will use Bootstrap Few Shot with Random Search (BFRS) to optimize the pipeline.\n",
    "\n",
    "The essence of BFRS is to try out different configurations of few shot demonstrations per step and see which one works best on the validation set.\n",
    "\n",
    "The cool part about BFRs is that it will automatically collect the \"good\" chains of thought for us and add them to the examples at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know how well the base pipeline performs, let's run prompt optimization on the pipeline in order to juice up the performance.\n",
    "\n",
    "Let's go over what the hyperparameters mean:\n",
    "- MAX_BOOTSTRAPPED_DEMOS: DSPy will \"bootstrap\" the program by collecting examples at each step that are successful and reusing those in the pipeline. This means that it will automatically collect and add chains of thought to the pipeline.\n",
    "- MAX_LABELED_DEMOS: DSPy will also insert some labeled demonstrations from the training set. These would be unmodified examples from the training set that are just using the gold answer.\n",
    "- NUM_CANDIDATE_PROGRAMS: This is the number of candidate programs that the optimizer will generate. The actual number of programs that are created is this plus three, as DSPy will also try a program with no examples, a program with TODO (check)\n",
    "- OPTIMIZER_NUM_TRAIN and OPTIMIZER_NUM_VAL: These are the number of examples that the optimizer will use for training and validation. Note that we will be taking the \"validation\" set from the trainset so as the actual validation set is untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization hyperparameters\n",
    "from dspy.teleprompt.random_search import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "# Define the hyperparameters for prompt optimization\n",
    "MAX_BOOTSTRAPPED_DEMOS = 3\n",
    "MAX_LABELED_DEMOS = 3\n",
    "NUM_CANDIDATE_PROGRAMS = 6\n",
    "OPTIMIZER_NUM_TRAIN = 100\n",
    "OPTIMIZER_NUM_VAL = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training and validation sets for the optimizer using the original\n",
    "# trainset. This ensures that our actual devset is left untouched.\n",
    "shuffled_trainset = [d for d in trainset]\n",
    "rng.shuffle(shuffled_trainset)\n",
    "optimizer_trainset = shuffled_trainset[:OPTIMIZER_NUM_TRAIN]\n",
    "optimizer_valset = shuffled_trainset[OPTIMIZER_NUM_TRAIN:OPTIMIZER_NUM_TRAIN+OPTIMIZER_NUM_VAL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 3 traces per predictor.\n",
      "Will attempt to bootstrap 6 candidate sets.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the optimizer\n",
    "bfrs_optimizer = BootstrapFewShotWithRandomSearch(\n",
    "    metric=metric,\n",
    "    max_bootstrapped_demos=MAX_BOOTSTRAPPED_DEMOS,\n",
    "    max_labeled_demos=MAX_LABELED_DEMOS,\n",
    "    num_candidate_programs=NUM_CANDIDATE_PROGRAMS,\n",
    "    num_threads=NUM_THREADS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have added this flag to save you some compute and time while running the notebook\n",
    "COMPILE_PROGRAM = False\n",
    "\n",
    "# Compile the optimizer and evaluate\n",
    "with dspy.context(lm=llama_70b, rm=retriever):\n",
    "    vanilla_program = BasicMH()\n",
    "    if COMPILE_PROGRAM:\n",
    "        bfrs_base_program = bfrs_optimizer.compile(vanilla_program, trainset=optimizer_trainset, valset=optimizer_valset)\n",
    "        bfrs_base_program.save(f\"basicmh_70b_31_bfrs_{MAX_BOOTSTRAPPED_DEMOS}_{MAX_LABELED_DEMOS}_{NUM_CANDIDATE_PROGRAMS}.json\")\n",
    "    else:\n",
    "        bfrs_base_program = BasicMH()\n",
    "        bfrs_base_program.load(f\"basicmh_70b_31_bfrs_{MAX_BOOTSTRAPPED_DEMOS}_{MAX_LABELED_DEMOS}_{NUM_CANDIDATE_PROGRAMS}.json\")\n",
    "    \n",
    "    llama_70b_bfrs_eval = evaluate_devset(bfrs_base_program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import MIPROv2\n",
    "\n",
    "eval_kwargs = dict(display_progress=True, display_table=0, num_threads=NUM_THREADS)\n",
    "teleprompter = MIPROv2(prompt_model=llama_70b, task_model=llama_70b, metric=metric, num_candidates=10, init_temperature=0.9, verbose=True)\n",
    "\n",
    "COMPILE_PROGRAM = False\n",
    "if COMPILE_PROGRAM:\n",
    "    with dspy.context(lm=llama_70b, rm=retriever):\n",
    "        compiled_program = teleprompter.compile(vanilla_program, trainset=optimizer_trainset, valset=optimizer_valset, num_batches=30, max_bootstrapped_demos=MAX_BOOTSTRAPPED_DEMOS,max_labeled_demos=MAX_LABELED_DEMOS, eval_kwargs=eval_kwargs, requires_permission_to_run=False)\n",
    "        compiled_program.save(f\"basicmh_70b_31_MIPROv2_{MAX_BOOTSTRAPPED_DEMOS}_{MAX_LABELED_DEMOS}.json\")\n",
    "else:\n",
    "    compiled_program = BasicMH()\n",
    "    compiled_program.load(f\"basicmh_70b_31_MIPROv2_{MAX_BOOTSTRAPPED_DEMOS}_{MAX_LABELED_DEMOS}.json\")\n",
    "\n",
    "# with dspy.context(lm=llama_70b, rm=retriever):\n",
    "#     llama_70b_mipro_eval = evaluate_devset(compiled_program)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap Data\n",
    "\n",
    "\n",
    "In this section, we bootstrap data for fine-tuning. In the code block below, we are deciding which program should be used to collect the bootstraps. We are setting this to the prompt optimized program, but one could also set this to the vanilla program, though doing so would lead to lower quality bootstraps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_program = bfrs_base_program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do something that kind of looks like rejection sampling here. For 3 rounds, we will run the model on the dataset and collect any \n",
    "examples that are solved. We then remove the solved examples from the dataset and repeat.\n",
    "\n",
    "In the [Large Language Monkeys paper](https://arxiv.org/pdf/2407.21787), they show that sampling up to 100 times can still get you a single correct answe in domains with ground truth or a verifier, so we can get away with this form of rejection sampling. Three rounds puts us at a decent spot on the curve of sampling up to N times. If we were in a domain where getting any correct answers was extremely hard, we may consider doing more rounds of sampling, but for now 3 rounds works.\n",
    "\n",
    "We did some experiments to see what the sampling curve looks like:\n",
    "\n",
    "<!-- ![Sampling Curve](./sampling_curve.png) -->\n",
    "<img src=\"./sampling_curve.png\" alt=\"Sampling Curve\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(autoscaler +1m21s)"
     ]
    }
   ],
   "source": [
    "TRAIN_SIZE = 1500\n",
    "EVAL_SIZE = int(TRAIN_SIZE/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llama_70b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 38\u001b[0m\n\u001b[1;32m     34\u001b[0m EVAL_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(TRAIN_SIZE\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     36\u001b[0m dataset_filenames \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrainset_data_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTRAIN_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m: shuffled_trainset[:TRAIN_SIZE], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrainset_val_data_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEVAL_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m: shuffled_trainset[TRAIN_SIZE:TRAIN_SIZE\u001b[38;5;241m+\u001b[39mEVAL_SIZE]}\n\u001b[0;32m---> 38\u001b[0m dspy\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mconfigure(experimental\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, lm\u001b[38;5;241m=\u001b[39m\u001b[43mllama_70b\u001b[49m, rm\u001b[38;5;241m=\u001b[39mretriever)\n\u001b[1;32m     40\u001b[0m WRITE_DATA \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m WRITE_DATA:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llama_70b' is not defined"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt.finetune_teleprompter import bootstrap_data, convert_to_module_level_prompt_completion_data, bootstrap_data_for_round\n",
    "import ujson\n",
    "\n",
    "# This should be moved inside the finetune_teleprompter class\n",
    "def write_data(program, data, filename):\n",
    "    print(\"Bootstrapping and writing data to\", filename)\n",
    "    correct_data = []\n",
    "    unsolved_examples = data.copy()\n",
    "    sampling_temperature = 0.902\n",
    "    sampling_temperature_delta = 0.0001\n",
    "    \n",
    "    for i in range(3):\n",
    "        if len(unsolved_examples) == 0:\n",
    "            break\n",
    "        data = bootstrap_data_for_round(program, unsolved_examples, metric=metric, num_threads=NUM_THREADS, sampling_round=i, sampling_temperature=sampling_temperature, sampling_temperature_delta=sampling_temperature_delta)\n",
    "        correct_data_round = [x for x in data if x[\"score\"]]\n",
    "        correct_examples_round = set([x[\"example\"] for x in correct_data_round])\n",
    "        print(f\"Round {i} complete. Solved {len(correct_data_round)} of {len(unsolved_examples)} examples. {len(unsolved_examples) - len(correct_examples_round)} examples remain unsolved.\")\n",
    "        unsolved_examples = [x for x in unsolved_examples if x not in correct_examples_round]\n",
    "\n",
    "        correct_data.extend(correct_data_round)\n",
    "        sampling_temperature += sampling_temperature_delta\n",
    "    \n",
    "    # Convert the data to prompt completion format\n",
    "    dataset = convert_to_module_level_prompt_completion_data(correct_data, program=program, exclude_demos=True)\n",
    "    \n",
    "    # Format the data for finetuning using the LM\n",
    "    print(\"Writing dataset with length\", len(dataset), \"to\", filename)\n",
    "    with open(filename, \"w\") as f:\n",
    "        ujson.dump(dataset, f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset_filenames = {f\"trainset_data_{TRAIN_SIZE}.json\": shuffled_trainset[:TRAIN_SIZE], f\"trainset_val_data_{EVAL_SIZE}.json\": shuffled_trainset[TRAIN_SIZE:TRAIN_SIZE+EVAL_SIZE]}\n",
    "\n",
    "dspy.settings.configure(experimental=True, lm=llama_70b, rm=retriever)\n",
    "\n",
    "WRITE_DATA = False\n",
    "if WRITE_DATA:\n",
    "    for filename, data in dataset_filenames.items():\n",
    "        write_data(bootstrap_program, data, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at an example prompt completion pair!\n",
    "with open(f\"trainset_data_{TRAIN_SIZE}.json\", \"r\") as f:\n",
    "    data_example = ujson.load(f)\n",
    "print(\"Example prompt:\")\n",
    "print(data_example[0]['prompt'])\n",
    "print(\"-\"*50,\"\\n\")\n",
    "print(\"Example completion:\")\n",
    "print(data_example[0]['completion'])\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should kill your 70B vllm server so that you can use your GPUs for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press enter once you have killed the 70B vllm server\n",
    "input(\"Press Enter once you have killed the 70B vllm server (press Ctrl+C to kill)...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning\n",
    "\n",
    "We will use LLM Forge to fine-tune the 8B model.\n",
    "\n",
    "In order to do this, we need to format our data into the correct format (Follows OpenAI messaging format placed in a jsonl file).\n",
    "\n",
    "We initially saved the data into a json file in prompt-completion format.\n",
    "\n",
    "In order to prepare for finetuning, we need to do three steps:\n",
    "1. Format the data into the correct format and verify that the data is valid\n",
    "2. Upload the data to GCP\n",
    "3. Generate the compute configuration file\n",
    "\n",
    "After the compute configuration file is generated, we can submit the job to LLM Forge, using either the command line or using the anyscale jobs sdk.\n",
    "TODO: Add the anyscale jobs sdk submit method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to checkout the fine-tuning documentation for the latest on how to use our [API](https://docs.anyscale.com/llms/finetuning/intro) and additional [capabilities](https://docs.anyscale.com/category/fine-tuning-beta/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in multiopenai 0.0\n"
     ]
    }
   ],
   "source": [
    "student_llama_8b = dspy.TrainableAnyscale(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: All this should be moved into the TrainableAnyscaleLM class. You should instead just call a finetune method with your datasets, hparams, compute config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found in the dataset format using the OpenAI API.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2358 examples that are missing a system message.\n",
      "The charge for finetuning is determined by the number of epochs multiplied by the number of billing tokens in the dataset. Here are the stats for this training dataset:\n",
      "    num_billing_tokens: 1171888\n",
      "    n_epochs: 3\n",
      "    num_total_charge_tokens: 3515664\n",
      "No errors found in the dataset format using the OpenAI API.\n",
      "Number of items in train data: 2358\n",
      "Uploading train data to S3 at gs://storage-bucket-cld-tffbxe9ia5phqr1unxhz4f7e1e/org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/trainset_data_1500_formatted.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://trainset_data_1500_formatted.jsonl to gs://storage-bucket-cld-tffbxe9ia5phqr1unxhz4f7e1e/org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/trainset_data_1500_formatted.jsonl\n",
      "  \n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in val data: 609\n",
      "Uploading val data to S3 at gs://storage-bucket-cld-tffbxe9ia5phqr1unxhz4f7e1e/org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/trainset_val_data_375_formatted.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://trainset_val_data_375_formatted.jsonl to gs://storage-bucket-cld-tffbxe9ia5phqr1unxhz4f7e1e/org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/trainset_val_data_375_formatted.jsonl\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default yaml template for model: configs/training/lora/llama-3-8b.yaml\n",
      "{'model_id': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'train_path': 'gs://storage-bucket-cld-tffbxe9ia5phqr1unxhz4f7e1e/org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/trainset_data_1500_formatted.jsonl', 'valid_path': 'gs://storage-bucket-cld-tffbxe9ia5phqr1unxhz4f7e1e/org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/trainset_val_data_375_formatted.jsonl', 'context_length': 512, 'num_devices': 4, 'num_epochs': 4, 'train_batch_size_per_device': 16, 'eval_batch_size_per_device': 16, 'learning_rate': '1e-4', 'padding': 'longest', 'num_checkpoints_to_keep': 10, 'dataset_size_scaling_factor': 10000, 'output_dir': '/mnt/local_storage', 'deepspeed': {'config_path': 'configs/deepspeed/zero_3_offload_optim+param.json'}, 'flash_attention_2': True, 'lora_config': {'r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'target_modules': ['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'embed_tokens', 'lm_head'], 'task_type': 'CAUSAL_LM', 'modules_to_save': [], 'bias': 'none', 'fan_in_fan_out': False, 'init_lora_weights': True}, 'logger': {'provider': 'wandb'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dsp.modules.lm import TrainingMethod\n",
    "\n",
    "train_path = f\"trainset_data_{TRAIN_SIZE}.json\"\n",
    "eval_path = f\"trainset_val_data_{EVAL_SIZE}.json\"\n",
    "method = TrainingMethod.SFT\n",
    "kwargs = {\n",
    "    \"hyperparameters\": {\n",
    "        \"num_devices\": 4,\n",
    "        \"trainer_resources\": None,\n",
    "        \"worker_resources\": None\n",
    "    },\n",
    "    \"use_lora\": True\n",
    "}\n",
    "\n",
    "if method != TrainingMethod.SFT:\n",
    "    raise NotImplementedError(\"Only SFT training is supported at the moment.\")\n",
    "\n",
    "train_dataset = student_llama_8b._format_data_for_vanilla_finetuning(train_path)\n",
    "val_dataset = student_llama_8b._format_data_for_vanilla_finetuning(eval_path) if eval_path else None\n",
    "\n",
    "if not student_llama_8b._verify_datasets(train_dataset, val_dataset):\n",
    "    print(\"Unable to verify arguments\")\n",
    "    raise RuntimeError(\"Unable to verify argument\")\n",
    "\n",
    "# TODO: This should be a function inside the TrainableAnyscaleLM class\n",
    "formatted_paths = {}\n",
    "for path, dataset in [(train_path, train_dataset), (eval_path, val_dataset)]:\n",
    "    if not (path and dataset):\n",
    "        continue\n",
    "    formatted_path = path.split(\".\")[0] + \"_formatted.jsonl\"\n",
    "    with open(formatted_path, \"w\") as f:\n",
    "        for item in dataset:\n",
    "            f.write(ujson.dumps(item) + \"\\n\")\n",
    "\n",
    "    formatted_paths[path] = formatted_path\n",
    "\n",
    "# print(formatted_paths[train_path])\n",
    "remote_train_path, remote_eval_path = student_llama_8b._submit_data(train_path=formatted_paths[train_path], eval_path=formatted_paths[eval_path])\n",
    "compute_config_path, compute_config = student_llama_8b._generate_config_files(train_path=remote_train_path, eval_path=remote_eval_path, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll fine-tune our LLM by choosing a set of configurations. We have created recipes for different LLMs in the [`training configs`](configs/training/lora/llama-3-8b.yaml) folder which can be used as is or modified for experiments. These configurations provide flexibility over a broad range of parameters such as model, data paths, compute to use for training, number of training epochs, how often to save checkpoints, padding, loss, etc. We also include several [DeepSpeed](https://github.com/microsoft/DeepSpeed) [configurations](configs/deepspeed/zero_3_offload_optim+param.json) to choose from for further optimizations around data/model parallelism, mixed precision, checkpointing, etc.\n",
    "\n",
    "We also have recipes for [LoRA](https://arxiv.org/abs/2106.09685) (where we train a set of small low ranked matrices instead of the original attention and feed forward layers) or full parameter fine-tuning. We recommend starting with LoRA as it's less resource intensive and quicker to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute config: JobConfig(name='dspy-llmforge-fine-tuning-job', image_uri='localhost:5555/anyscale/llm-forge:0.5.6', compute_config=None, env_vars={'WANDB_API_KEY': 'c75a837e8271ce763121d06742fb9fc3fd2cc7f0'}, py_modules=None, cloud=None, project=None, ray_version=None, job_queue_config=None)\n"
     ]
    }
   ],
   "source": [
    "# View the compute config\n",
    "print(\"Compute config:\", compute_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(anyscale +1m36.3s) Uploading local dir '.' to cloud storage.\n",
      "(anyscale +1m38.1s) Job 'dspy-llmforge-fine-tuning-job' submitted, ID: 'prodjob_lqhl71xveu12xtke1xuy6nth65'.\n",
      "(anyscale +1m38.1s) View the job in the UI: https://console.anyscale.com/jobs/prodjob_lqhl71xveu12xtke1xuy6nth65\n",
      "(anyscale +1m38.2s) Waiting for job 'prodjob_lqhl71xveu12xtke1xuy6nth65' to reach target state SUCCEEDED, currently in state: STARTING\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SKIP_FT:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# TODO: Get job working with LLMForge\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# prodjob_idbtjgp6lrggxsjas5snj1jiv2\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     job_id: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m anyscale\u001b[38;5;241m.\u001b[39mjob\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m      8\u001b[0m         compute_config\n\u001b[1;32m      9\u001b[0m     )\n\u001b[0;32m---> 10\u001b[0m     \u001b[43manyscale\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_s\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m18000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m succeeded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# command = compute_config.entrypoint\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# print(command)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# os.system(command)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/anyscale/_private/sdk/__init__.py:44\u001b[0m, in \u001b[0;36msdk_command.<locals>._inject_typed_sdk_singleton.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m         _LAZY_SDK_SINGLETONS[key] \u001b[38;5;241m=\u001b[39m sdk_cls()\n\u001b[1;32m     42\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_sdk\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _LAZY_SDK_SINGLETONS[key]  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/anyscale/job/commands.py:212\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(name, id, cloud, project, state, timeout_s, _sdk, **_kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\"Wait for a job to enter a specific state.\"\"\"\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m _resolve_id_from_args(\u001b[38;5;28mid\u001b[39m, _kwargs)  \u001b[38;5;66;03m# noqa: A001\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m \u001b[43m_sdk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloud\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcloud\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_s\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/anyscale/job/_private/job_sdk.py:413\u001b[0m, in \u001b[0;36mPrivateJobSDK.wait\u001b[0;34m(self, name, job_id, cloud, project, state, timeout_s, interval_s)\u001b[0m\n\u001b[1;32m    408\u001b[0m curr_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_job_state_from_job_model(job_model)\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting for job \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob_id_or_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to reach target state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, currently in state: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurr_state\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    412\u001b[0m )\n\u001b[0;32m--> 413\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer\u001b[38;5;241m.\u001b[39mpoll(timeout_s\u001b[38;5;241m=\u001b[39mtimeout_s, interval_s\u001b[38;5;241m=\u001b[39minterval_s):\n\u001b[1;32m    414\u001b[0m     job_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resolve_to_job_model(\n\u001b[1;32m    415\u001b[0m         name\u001b[38;5;241m=\u001b[39mname, job_id\u001b[38;5;241m=\u001b[39mjob_id, cloud\u001b[38;5;241m=\u001b[39mcloud, project\u001b[38;5;241m=\u001b[39mproject\n\u001b[1;32m    416\u001b[0m     )\n\u001b[1;32m    417\u001b[0m     new_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_job_state_from_job_model(job_model)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/anyscale/_private/sdk/timer.py:24\u001b[0m, in \u001b[0;36mRealTimer.poll\u001b[0;34m(self, timeout_s, interval_s)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterval_s\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import anyscale\n",
    "\n",
    "SKIP_FT = False\n",
    "if not SKIP_FT:\n",
    "    # TODO: Get job working with LLMForge\n",
    "    # prodjob_idbtjgp6lrggxsjas5snj1jiv2\n",
    "    job_id: str = anyscale.job.submit(\n",
    "        compute_config\n",
    "    )\n",
    "    anyscale.job.wait(id=job_id, timeout_s=18000)\n",
    "    print(f\"Job {job_id} succeeded!\")\n",
    "\n",
    "\n",
    "    # command = compute_config.entrypoint\n",
    "    # print(command)\n",
    "    # os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input(\"Press Enter once you have copied the path from the logs...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_gcp_bucket(bucket_name, source_folder, destination_folder):\n",
    "    \"\"\"Downloads a folder from a GCP bucket to a local folder.\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): The name of the GCP bucket.\n",
    "        source_folder (str): The path to the folder in the bucket.\n",
    "        destination_folder (str): The local path where files should be saved.\n",
    "\n",
    "    Returns:\n",
    "        str: The path to the downloaded folder.\n",
    "    \"\"\"\n",
    "    import google.cloud.storage as storage\n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs(prefix=source_folder)\n",
    "\n",
    "    for blob in blobs:\n",
    "        if blob.name.endswith('/'):\n",
    "            continue  # Skip directories\n",
    "        \n",
    "        relative_path = os.path.relpath(blob.name, source_folder)\n",
    "        local_file_path = os.path.join(destination_folder, relative_path)\n",
    "        \n",
    "        os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "        # only download if the file doesn't exist\n",
    "        if not os.path.exists(local_file_path):\n",
    "            blob.download_to_filename(local_file_path)\n",
    "            print(f\"Downloaded {blob.name} to {local_file_path}\")\n",
    "\n",
    "    print(f\"Folder {source_folder} downloaded to {destination_folder}.\")\n",
    "    return destination_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/.validate_storage_marker to llama_8b_checkpoints_torchtrainer/.validate_storage_marker\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/TorchTrainer_2948f_00000_0_2024-09-26_17-26-38/events.out.tfevents.1727396806.g-a13b16472ed770001 to llama_8b_checkpoints_torchtrainer/TorchTrainer_2948f_00000_0_2024-09-26_17-26-38/events.out.tfevents.1727396806.g-a13b16472ed770001\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/TorchTrainer_2948f_00000_0_2024-09-26_17-26-38/params.json to llama_8b_checkpoints_torchtrainer/TorchTrainer_2948f_00000_0_2024-09-26_17-26-38/params.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/TorchTrainer_2948f_00000_0_2024-09-26_17-26-38/params.pkl to llama_8b_checkpoints_torchtrainer/TorchTrainer_2948f_00000_0_2024-09-26_17-26-38/params.pkl\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/TorchTrainer_2948f_00000_0_2024-09-26_17-26-38/progress.csv to llama_8b_checkpoints_torchtrainer/TorchTrainer_2948f_00000_0_2024-09-26_17-26-38/progress.csv\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/TorchTrainer_2948f_00000_0_2024-09-26_17-26-38/result.json to llama_8b_checkpoints_torchtrainer/TorchTrainer_2948f_00000_0_2024-09-26_17-26-38/result.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/basic-variant-state-2024-09-26_17-26-38.json to llama_8b_checkpoints_torchtrainer/basic-variant-state-2024-09-26_17-26-38.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-0-total-trained-steps-37/README.md to llama_8b_checkpoints_torchtrainer/epochs-0-total-trained-steps-37/README.md\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-0-total-trained-steps-37/adapter_config.json to llama_8b_checkpoints_torchtrainer/epochs-0-total-trained-steps-37/adapter_config.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-0-total-trained-steps-37/adapter_model.safetensors to llama_8b_checkpoints_torchtrainer/epochs-0-total-trained-steps-37/adapter_model.safetensors\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-0-total-trained-steps-37/config.json to llama_8b_checkpoints_torchtrainer/epochs-0-total-trained-steps-37/config.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-0-total-trained-steps-37/new_embeddings.safetensors to llama_8b_checkpoints_torchtrainer/epochs-0-total-trained-steps-37/new_embeddings.safetensors\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-0-total-trained-steps-37/rayllm_generation_config.json to llama_8b_checkpoints_torchtrainer/epochs-0-total-trained-steps-37/rayllm_generation_config.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-0-total-trained-steps-37/special_tokens_map.json to llama_8b_checkpoints_torchtrainer/epochs-0-total-trained-steps-37/special_tokens_map.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-0-total-trained-steps-37/tokenizer.json to llama_8b_checkpoints_torchtrainer/epochs-0-total-trained-steps-37/tokenizer.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-0-total-trained-steps-37/tokenizer_config.json to llama_8b_checkpoints_torchtrainer/epochs-0-total-trained-steps-37/tokenizer_config.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-1-total-trained-steps-74/README.md to llama_8b_checkpoints_torchtrainer/epochs-1-total-trained-steps-74/README.md\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-1-total-trained-steps-74/adapter_config.json to llama_8b_checkpoints_torchtrainer/epochs-1-total-trained-steps-74/adapter_config.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-1-total-trained-steps-74/adapter_model.safetensors to llama_8b_checkpoints_torchtrainer/epochs-1-total-trained-steps-74/adapter_model.safetensors\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-1-total-trained-steps-74/config.json to llama_8b_checkpoints_torchtrainer/epochs-1-total-trained-steps-74/config.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-1-total-trained-steps-74/new_embeddings.safetensors to llama_8b_checkpoints_torchtrainer/epochs-1-total-trained-steps-74/new_embeddings.safetensors\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-1-total-trained-steps-74/rayllm_generation_config.json to llama_8b_checkpoints_torchtrainer/epochs-1-total-trained-steps-74/rayllm_generation_config.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-1-total-trained-steps-74/special_tokens_map.json to llama_8b_checkpoints_torchtrainer/epochs-1-total-trained-steps-74/special_tokens_map.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-1-total-trained-steps-74/tokenizer.json to llama_8b_checkpoints_torchtrainer/epochs-1-total-trained-steps-74/tokenizer.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-1-total-trained-steps-74/tokenizer_config.json to llama_8b_checkpoints_torchtrainer/epochs-1-total-trained-steps-74/tokenizer_config.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-2-total-trained-steps-111/README.md to llama_8b_checkpoints_torchtrainer/epochs-2-total-trained-steps-111/README.md\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-2-total-trained-steps-111/adapter_config.json to llama_8b_checkpoints_torchtrainer/epochs-2-total-trained-steps-111/adapter_config.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-2-total-trained-steps-111/adapter_model.safetensors to llama_8b_checkpoints_torchtrainer/epochs-2-total-trained-steps-111/adapter_model.safetensors\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-2-total-trained-steps-111/config.json to llama_8b_checkpoints_torchtrainer/epochs-2-total-trained-steps-111/config.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-2-total-trained-steps-111/new_embeddings.safetensors to llama_8b_checkpoints_torchtrainer/epochs-2-total-trained-steps-111/new_embeddings.safetensors\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-2-total-trained-steps-111/rayllm_generation_config.json to llama_8b_checkpoints_torchtrainer/epochs-2-total-trained-steps-111/rayllm_generation_config.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-2-total-trained-steps-111/special_tokens_map.json to llama_8b_checkpoints_torchtrainer/epochs-2-total-trained-steps-111/special_tokens_map.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-2-total-trained-steps-111/tokenizer.json to llama_8b_checkpoints_torchtrainer/epochs-2-total-trained-steps-111/tokenizer.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-2-total-trained-steps-111/tokenizer_config.json to llama_8b_checkpoints_torchtrainer/epochs-2-total-trained-steps-111/tokenizer_config.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-3-total-trained-steps-148/README.md to llama_8b_checkpoints_torchtrainer/epochs-3-total-trained-steps-148/README.md\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-3-total-trained-steps-148/adapter_config.json to llama_8b_checkpoints_torchtrainer/epochs-3-total-trained-steps-148/adapter_config.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-3-total-trained-steps-148/adapter_model.safetensors to llama_8b_checkpoints_torchtrainer/epochs-3-total-trained-steps-148/adapter_model.safetensors\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-3-total-trained-steps-148/config.json to llama_8b_checkpoints_torchtrainer/epochs-3-total-trained-steps-148/config.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-3-total-trained-steps-148/new_embeddings.safetensors to llama_8b_checkpoints_torchtrainer/epochs-3-total-trained-steps-148/new_embeddings.safetensors\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-3-total-trained-steps-148/rayllm_generation_config.json to llama_8b_checkpoints_torchtrainer/epochs-3-total-trained-steps-148/rayllm_generation_config.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-3-total-trained-steps-148/special_tokens_map.json to llama_8b_checkpoints_torchtrainer/epochs-3-total-trained-steps-148/special_tokens_map.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-3-total-trained-steps-148/tokenizer.json to llama_8b_checkpoints_torchtrainer/epochs-3-total-trained-steps-148/tokenizer.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/epochs-3-total-trained-steps-148/tokenizer_config.json to llama_8b_checkpoints_torchtrainer/epochs-3-total-trained-steps-148/tokenizer_config.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/experiment_state-2024-09-26_17-26-38.json to llama_8b_checkpoints_torchtrainer/experiment_state-2024-09-26_17-26-38.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/trainer.pkl to llama_8b_checkpoints_torchtrainer/trainer.pkl\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38/tuner.pkl to llama_8b_checkpoints_torchtrainer/tuner.pkl\n",
      "Folder org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38 downloaded to llama_8b_checkpoints_torchtrainer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'llama_8b_checkpoints_torchtrainer'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I need to get all the checkpoints somehow\n",
    "# bucket: storage-bucket-cld-tffbxe9ia5phqr1unxhz4f7e1e and path: org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/lora_fine_tuning/meta-llama/Meta-Llama-3.1-8B-Instruct:isaac:vblcs\n",
    "\n",
    "# download_from_gcp_bucket(\"storage-bucket-cld-tffbxe9ia5phqr1unxhz4f7e1e\", \"org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/lora_fine_tuning/meta-llama/Meta-Llama-3.1-8B-Instruct:isaac:vblcs\", \"llama_8b_checkpoints\")\n",
    "\n",
    "# storage-bucket-cld-tffbxe9ia5phqr1unxhz4f7e1e/org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38\n",
    "\n",
    "download_from_gcp_bucket(\"storage-bucket-cld-tffbxe9ia5phqr1unxhz4f7e1e\", \"org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_17-26-38\", \"llama_8b_checkpoints_torchtrainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets sanity check the finetuned model. We need to download it first.\n",
    "# After the finetuning is complete, the logs will say something like \"Note: Best LoRA weights forwarded to gs://storage-bucket...\"\n",
    "# Update that link below with the correct path.\n",
    "if job_id:\n",
    "    model_info = anyscale.llm.model.get(job_id=job_id).to_dict()\n",
    "\n",
    "model_id = model_info[\"base_model_id\"] if job_id else \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "lora_source_path = model_info['storage_uri'] if job_id else \"\"\n",
    "\n",
    "# lora_source_path = \"gs://storage-bucket-cld-tffbxe9ia5phqr1unxhz4f7e1e/org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/lora_fine_tuning/meta-llama/Meta-Llama-3.1-8B-Instruct:isaac:yuxwd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_12-25-15/epochs-1-total-trained-steps-74/README.md to /mnt/local_storage/dspy/mhqa-lora/README.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_12-25-15/epochs-1-total-trained-steps-74/adapter_config.json to /mnt/local_storage/dspy/mhqa-lora/adapter_config.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_12-25-15/epochs-1-total-trained-steps-74/adapter_model.safetensors to /mnt/local_storage/dspy/mhqa-lora/adapter_model.safetensors\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_12-25-15/epochs-1-total-trained-steps-74/config.json to /mnt/local_storage/dspy/mhqa-lora/config.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_12-25-15/epochs-1-total-trained-steps-74/new_embeddings.safetensors to /mnt/local_storage/dspy/mhqa-lora/new_embeddings.safetensors\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_12-25-15/epochs-1-total-trained-steps-74/rayllm_generation_config.json to /mnt/local_storage/dspy/mhqa-lora/rayllm_generation_config.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_12-25-15/epochs-1-total-trained-steps-74/special_tokens_map.json to /mnt/local_storage/dspy/mhqa-lora/special_tokens_map.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_12-25-15/epochs-1-total-trained-steps-74/tokenizer.json to /mnt/local_storage/dspy/mhqa-lora/tokenizer.json\n",
      "Downloaded org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_12-25-15/epochs-1-total-trained-steps-74/tokenizer_config.json to /mnt/local_storage/dspy/mhqa-lora/tokenizer_config.json\n",
      "Folder org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3.1-8B-Instruct/TorchTrainer_2024-09-26_12-25-15/epochs-1-total-trained-steps-74 downloaded to /mnt/local_storage/dspy/mhqa-lora.\n"
     ]
    }
   ],
   "source": [
    "# # Parse the GCS path\n",
    "bucket_name = lora_source_path.split('/')[2]\n",
    "source_folder = '/'.join(lora_source_path.split('/')[3:])\n",
    "\n",
    "# Download the LoRA model folder locally\n",
    "local_lora_path = download_from_gcp_bucket(bucket_name, source_folder, \"/mnt/local_storage/dspy/mhqa-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Throughout this section, anything using 8B model (or technically 70B too) should use the new evaluate with ray data batch offline(or technically online) inference.\n",
    "\n",
    "Probably worth testing offline with 8x8 threads vs just 64 threads to see if it makes a meaningful difference.\n",
    "\n",
    "## Performance comparisons\n",
    "\n",
    "- 70B\n",
    "- 70B BSFS\n",
    "- 8B\n",
    "- 8B BSFT\n",
    "- 8B BSFT + BSFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to serve the base model and tell VLLM where to find the LoRA weights\n",
    "\n",
    "Run the following command:\n",
    "\n",
    "```\n",
    "vllm serve meta-llama/Meta-Llama-3.1-8B-Instruct --port 8000 --pipeline_parallel_size 4 --enable_prefix_caching --enable_lora --lora_modules mhqa-lora=/mnt/local_storage/dspy/mhqa-lora\n",
    "```\n",
    "\n",
    "# Explanation:\n",
    "This command starts a VLLM server to serve the Meta-Llama-3-8B-Instruct model with LoRA fine-tuning.\n",
    "Here's a breakdown of the command:\n",
    "- 'vllm serve': Starts the VLLM server\n",
    "- 'meta-llama/Meta-Llama-3.1-8B-Instruct': Specifies the base model to use\n",
    "- '--port 8000': Sets the server port to 8000\n",
    "- '--pipeline_parallel_size 4': Enables pipeline parallelism with 4 stages\n",
    "- '--enable_prefix_caching': Enables caching of prefixes for faster inference\n",
    "- '--enable_lora': Enables LoRA (Low-Rank Adaptation) for fine-tuning\n",
    "- '--lora_modules mhqa-lora=/mnt/local_storage/dspy/mhqa-lora': Specifies the name of the LoRA module and the path to the LoRA weights. We use the name instead of the base model name when trying to use the LoRA weights. If we just use the base model name, the server will ignore the LoRA weights.\n",
    "\n",
    "This setup allows us to serve a fine-tuned version of the 8B model, which we'll use for subsequent evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs-3-total-trained-steps-148': '/home/ray/default/templates/templates/e2e-dspy-workflow/llama_8b_checkpoints_torchtrainer/epochs-3-total-trained-steps-148',\n",
       " 'epochs-2-total-trained-steps-111': '/home/ray/default/templates/templates/e2e-dspy-workflow/llama_8b_checkpoints_torchtrainer/epochs-2-total-trained-steps-111',\n",
       " 'epochs-0-total-trained-steps-37': '/home/ray/default/templates/templates/e2e-dspy-workflow/llama_8b_checkpoints_torchtrainer/epochs-0-total-trained-steps-37',\n",
       " 'epochs-1-total-trained-steps-74': '/home/ray/default/templates/templates/e2e-dspy-workflow/llama_8b_checkpoints_torchtrainer/epochs-1-total-trained-steps-74'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# All checkpoints are inside llama_8b_checkpoints_torchtrainer\n",
    "# get all folders inside llama_8b_checkpoints_torchtrainer\n",
    "folders = os.listdir(\"llama_8b_checkpoints_torchtrainer\")\n",
    "folders = [f for f in folders if os.path.isdir(os.path.join(\"llama_8b_checkpoints_torchtrainer\", f)) and f.startswith(\"epoch\")]\n",
    "\n",
    "# Get the current working directory\n",
    "current_working_path = os.getcwd()\n",
    "\n",
    "folder_lora_location = {f: os.path.join(current_working_path, \"llama_8b_checkpoints_torchtrainer\", f) for f in folders}\n",
    "folder_lora_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in multiopenai 0\n",
      "in multiopenai 0\n"
     ]
    }
   ],
   "source": [
    "# Command for easy copying: \n",
    "\n",
    "# `vllm serve meta-llama/Meta-Llama-3.1-8B-Instruct --port 8000 --pipeline_parallel_size 4 --enable_prefix_caching --enable_lora --lora_modules mhqa-lora=/mnt/local_storage/dspy/mhqa-lora`\n",
    "# LOCAL_API_PARAMETERS[\"api_base\"] = \"http://localhost:6942/v1\"\n",
    "llama_8b = dspy.MultiOpenAI(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", **LOCAL_API_PARAMETERS, **MODEL_PARAMETERS)\n",
    "# mhqa_llama_8b = dspy.MultiOpenAI(model=\"mhqa-lora\", **LOCAL_API_PARAMETERS, **MODEL_PARAMETERS)\n",
    "mhqa_finetuned_llamas_8b = {f: dspy.MultiOpenAI(model=f, **LOCAL_API_PARAMETERS, **MODEL_PARAMETERS) for f in folder_lora_location.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets dynamically generate a vllm command that will load all the models\n",
    "base = \"vllm serve meta-llama/Meta-Llama-3.1-8B-Instruct --port 8000 --pipeline_parallel_size 4 --enable_prefix_caching --enable_lora --lora_modules\"\n",
    "for folder, path in folder_lora_location.items():\n",
    "    base += f\" {folder}={path}\"\n",
    "print(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 549 / 1500  (36.6): 100%|██████████| 1500/1500 [07:43<00:00,  3.23it/s]\n"
     ]
    }
   ],
   "source": [
    "for folder, llama in mhqa_finetuned_llamas_8b.items():\n",
    "    with dspy.context(lm=llama, rm=retriever):\n",
    "        print(f\"Evaluating the vanilla program on the devset using the model to be trained ({folder})...\")\n",
    "        eval_result = evaluate_devset(vanilla_program)\n",
    "        print(f\"result for {folder}: {eval_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try optimizing the program with the finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 56 / 150  (37.3): 100%|██████████| 150/150 [00:51<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 37.33 for set: [0, 0, 0]\n",
      "New best sscore: 37.33 for seed -3\n",
      "Scores so far: [37.33]\n",
      "Best score: 37.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 52 / 150  (34.7): 100%|██████████| 150/150 [00:08<00:00, 16.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 34.67 for set: [3, 3, 3]\n",
      "Scores so far: [37.33, 34.67]\n",
      "Best score: 37.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [00:43<04:07,  2.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 16 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 57 / 150  (38.0): 100%|██████████| 150/150 [02:18<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 38.0 for set: [3, 3, 3]\n",
      "New best sscore: 38.0 for seed -1\n",
      "Scores so far: [37.33, 34.67, 38.0]\n",
      "Best score: 38.0\n",
      "Average of max per entry across top 1 scores: 0.38\n",
      "Average of max per entry across top 2 scores: 0.48\n",
      "Average of max per entry across top 3 scores: 0.5\n",
      "Average of max per entry across top 5 scores: 0.5\n",
      "Average of max per entry across top 8 scores: 0.5\n",
      "Average of max per entry across top 9999 scores: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:09<02:27,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 7 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 64 / 150  (42.7): 100%|██████████| 150/150 [01:07<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 42.67 for set: [3, 3, 3]\n",
      "New best sscore: 42.67 for seed 0\n",
      "Scores so far: [37.33, 34.67, 38.0, 42.67]\n",
      "Best score: 42.67\n",
      "Average of max per entry across top 1 scores: 0.4266666666666667\n",
      "Average of max per entry across top 2 scores: 0.4666666666666667\n",
      "Average of max per entry across top 3 scores: 0.5466666666666666\n",
      "Average of max per entry across top 5 scores: 0.56\n",
      "Average of max per entry across top 8 scores: 0.56\n",
      "Average of max per entry across top 9999 scores: 0.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:15<04:09,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 7 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 55 / 150  (36.7): 100%|██████████| 150/150 [00:56<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 36.67 for set: [3, 3, 3]\n",
      "Scores so far: [37.33, 34.67, 38.0, 42.67, 36.67]\n",
      "Best score: 42.67\n",
      "Average of max per entry across top 1 scores: 0.4266666666666667\n",
      "Average of max per entry across top 2 scores: 0.4666666666666667\n",
      "Average of max per entry across top 3 scores: 0.5466666666666666\n",
      "Average of max per entry across top 5 scores: 0.58\n",
      "Average of max per entry across top 8 scores: 0.58\n",
      "Average of max per entry across top 9999 scores: 0.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:12<03:19,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 7 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 56 / 150  (37.3): 100%|██████████| 150/150 [00:43<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 37.33 for set: [3, 3, 3]\n",
      "Scores so far: [37.33, 34.67, 38.0, 42.67, 36.67, 37.33]\n",
      "Best score: 42.67\n",
      "Average of max per entry across top 1 scores: 0.4266666666666667\n",
      "Average of max per entry across top 2 scores: 0.4666666666666667\n",
      "Average of max per entry across top 3 scores: 0.5466666666666666\n",
      "Average of max per entry across top 5 scores: 0.5733333333333334\n",
      "Average of max per entry across top 8 scores: 0.5866666666666667\n",
      "Average of max per entry across top 9999 scores: 0.5866666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:02<04:31,  2.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 50 / 150  (33.3): 100%|██████████| 150/150 [00:45<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 33.33 for set: [3, 3, 3]\n",
      "Scores so far: [37.33, 34.67, 38.0, 42.67, 36.67, 37.33, 33.33]\n",
      "Best score: 42.67\n",
      "Average of max per entry across top 1 scores: 0.4266666666666667\n",
      "Average of max per entry across top 2 scores: 0.4666666666666667\n",
      "Average of max per entry across top 3 scores: 0.5466666666666666\n",
      "Average of max per entry across top 5 scores: 0.5733333333333334\n",
      "Average of max per entry across top 8 scores: 0.5866666666666667\n",
      "Average of max per entry across top 9999 scores: 0.5866666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:05<04:32,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 53 / 150  (35.3): 100%|██████████| 150/150 [00:42<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 35.33 for set: [3, 3, 3]\n",
      "Scores so far: [37.33, 34.67, 38.0, 42.67, 36.67, 37.33, 33.33, 35.33]\n",
      "Best score: 42.67\n",
      "Average of max per entry across top 1 scores: 0.4266666666666667\n",
      "Average of max per entry across top 2 scores: 0.4666666666666667\n",
      "Average of max per entry across top 3 scores: 0.5466666666666666\n",
      "Average of max per entry across top 5 scores: 0.5733333333333334\n",
      "Average of max per entry across top 8 scores: 0.5933333333333334\n",
      "Average of max per entry across top 9999 scores: 0.5933333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:05<03:12,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 61 / 150  (40.7): 100%|██████████| 150/150 [01:07<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 40.67 for set: [3, 3, 3]\n",
      "Scores so far: [37.33, 34.67, 38.0, 42.67, 36.67, 37.33, 33.33, 35.33, 40.67]\n",
      "Best score: 42.67\n",
      "Average of max per entry across top 1 scores: 0.4266666666666667\n",
      "Average of max per entry across top 2 scores: 0.47333333333333333\n",
      "Average of max per entry across top 3 scores: 0.5\n",
      "Average of max per entry across top 5 scores: 0.5666666666666667\n",
      "Average of max per entry across top 8 scores: 0.6\n",
      "Average of max per entry across top 9999 scores: 0.6\n",
      "9 candidate programs found.\n",
      "[('retrieve', <dspy.retrieve.retrieve.Retrieve object at 0x74b1b5f7e220>), ('generate_query[0]', Predict(StringSignature(context, question -> rationale, search_query\n",
      "    instructions='Given the fields `context`, `question`, produce the fields `search_query`.'\n",
      "    context = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Context:', 'desc': '${context}'})\n",
      "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
      "    rationale = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${produce the output fields}. We ...', '__dspy_field_type': 'output'})\n",
      "    search_query = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Search Query:', 'desc': '${search_query}'})\n",
      "))), ('generate_query[1]', Predict(StringSignature(context, question -> rationale, search_query\n",
      "    instructions='Given the fields `context`, `question`, produce the fields `search_query`.'\n",
      "    context = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Context:', 'desc': '${context}'})\n",
      "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
      "    rationale = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${produce the output fields}. We ...', '__dspy_field_type': 'output'})\n",
      "    search_query = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Search Query:', 'desc': '${search_query}'})\n",
      "))), ('generate_answer', Predict(StringSignature(context, question -> rationale, answer\n",
      "    instructions='Given the fields `context`, `question`, produce the fields `answer`.'\n",
      "    context = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Context:', 'desc': '${context}'})\n",
      "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
      "    rationale = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${produce the output fields}. We ...', '__dspy_field_type': 'output'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Answer:', 'desc': '${answer}'})\n",
      ")))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 646 / 1500  (43.1): 100%|██████████| 1500/1500 [13:00<00:00,  1.92it/s]\n"
     ]
    }
   ],
   "source": [
    "COMPILE_PROGRAM = True\n",
    "\n",
    "with dspy.context(lm=mhqa_llama_8b, rm=retriever):\n",
    "    vanilla_program = BasicMH()\n",
    "    if COMPILE_PROGRAM:\n",
    "        bfrs_finetuned_program = bfrs_optimizer.compile(vanilla_program, trainset=optimizer_trainset, valset=optimizer_valset)\n",
    "        bfrs_finetuned_program.save(f\"basicmh_8b_32_ft_bfrs_{MAX_BOOTSTRAPPED_DEMOS}_{MAX_LABELED_DEMOS}_{NUM_CANDIDATE_PROGRAMS}.json\")\n",
    "    else:\n",
    "        bfrs_finetuned_program = BasicMH()\n",
    "        bfrs_finetuned_program.load(f\"basicmh_8b_32_ft_bfrs_{MAX_BOOTSTRAPPED_DEMOS}_{MAX_LABELED_DEMOS}_{NUM_CANDIDATE_PROGRAMS}.json\")\n",
    "    llama_8b_bfrs_finetuned_eval = evaluate_devset(bfrs_finetuned_program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPILE_PROGRAM = True\n",
    "with dspy.context(lm=mhqa_llama_8b, rm=retriever):\n",
    "    vanilla_program = BasicMH()\n",
    "    if COMPILE_PROGRAM:\n",
    "        teleprompter = MIPROv2(prompt_model=llama_8b, task_model=mhqa_llama_8b, metric=metric, num_candidates=10, init_temperature=0.9, verbose=True)\n",
    "        compiled_program = teleprompter.compile(vanilla_program, trainset=optimizer_trainset, valset=optimizer_valset, num_batches=30, max_bootstrapped_demos=MAX_BOOTSTRAPPED_DEMOS,max_labeled_demos=MAX_LABELED_DEMOS, eval_kwargs=eval_kwargs, requires_permission_to_run=False)\n",
    "        compiled_program.save(f\"basicmh_8b_ft_MIPROv2_{MAX_BOOTSTRAPPED_DEMOS}_{MAX_LABELED_DEMOS}.json\")\n",
    "    else:\n",
    "        compiled_program = BasicMH()\n",
    "        compiled_program.load(f\"basicmh_8b_ft_MIPROv2_{MAX_BOOTSTRAPPED_DEMOS}_{MAX_LABELED_DEMOS}.json\")\n",
    "    llama_8b_ft_mipro_eval = evaluate_devset(compiled_program)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, lets give the base 8B model a fair chance by prompt optimizing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 53 / 150  (35.3): 100%|██████████| 150/150 [00:00<00:00, 557.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 35.33 for set: [0, 0, 0]\n",
      "New best sscore: 35.33 for seed -3\n",
      "Scores so far: [35.33]\n",
      "Best score: 35.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 65 / 150  (43.3): 100%|██████████| 150/150 [00:00<00:00, 566.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 43.33 for set: [3, 3, 3]\n",
      "New best sscore: 43.33 for seed -2\n",
      "Scores so far: [35.33, 43.33]\n",
      "Best score: 43.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [00:00<00:00, 178.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 13 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 77 / 150  (51.3): 100%|██████████| 150/150 [00:00<00:00, 544.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 51.33 for set: [3, 3, 3]\n",
      "New best sscore: 51.33 for seed -1\n",
      "Scores so far: [35.33, 43.33, 51.33]\n",
      "Best score: 51.33\n",
      "Average of max per entry across top 1 scores: 0.5133333333333333\n",
      "Average of max per entry across top 2 scores: 0.5866666666666667\n",
      "Average of max per entry across top 3 scores: 0.6066666666666667\n",
      "Average of max per entry across top 5 scores: 0.6066666666666667\n",
      "Average of max per entry across top 8 scores: 0.6066666666666667\n",
      "Average of max per entry across top 9999 scores: 0.6066666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:00<00:00, 197.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 69 / 150  (46.0): 100%|██████████| 150/150 [00:08<00:00, 18.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 46.0 for set: [3, 3, 3]\n",
      "Scores so far: [35.33, 43.33, 51.33, 46.0]\n",
      "Best score: 51.33\n",
      "Average of max per entry across top 1 scores: 0.5133333333333333\n",
      "Average of max per entry across top 2 scores: 0.5866666666666667\n",
      "Average of max per entry across top 3 scores: 0.6266666666666667\n",
      "Average of max per entry across top 5 scores: 0.6466666666666666\n",
      "Average of max per entry across top 8 scores: 0.6466666666666666\n",
      "Average of max per entry across top 9999 scores: 0.6466666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:00<00:00, 190.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 9  (0.0):   6%|▌         | 9/150 [07:07<1:51:37, 47.50s/it]\n",
      "Average Metric: 74 / 150  (49.3): 100%|██████████| 150/150 [01:09<00:00,  2.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 49.33 for set: [3, 3, 3]\n",
      "Scores so far: [35.33, 43.33, 51.33, 46.0, 49.33]\n",
      "Best score: 51.33\n",
      "Average of max per entry across top 1 scores: 0.5133333333333333\n",
      "Average of max per entry across top 2 scores: 0.5866666666666667\n",
      "Average of max per entry across top 3 scores: 0.6266666666666667\n",
      "Average of max per entry across top 5 scores: 0.6666666666666666\n",
      "Average of max per entry across top 8 scores: 0.6666666666666666\n",
      "Average of max per entry across top 9999 scores: 0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:01<02:33,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0 / 2  (0.0):   1%|▏         | 2/150 [00:13<14:15,  5.78s/it]ERROR:dspy.evaluate.evaluate:2024-09-26T21:01:07.853367Z [error    ] Error for example in dev set: \t\t HTTPConnectionPool(host='20.102.90.50', port=2017): Read timed out. (read timeout=10) [dspy.evaluate.evaluate] filename=evaluate.py lineno=183\n",
      "Average Metric: 0.0 / 3  (0.0):   2%|▏         | 3/150 [00:14<08:05,  3.30s/it]error    6T21:01:07.861729Z [] Error for example in dev set: \t\t HTTPConnectionPool(host='20.102.90.50', port=2017): Read timed out. (read timeout=10) [dspy.evaluate.evaluate] filename=evaluate.py lineno=183\n",
      "Average Metric: 0.0 / 4  (0.0):   2%|▏         | 3/150 [00:14<08:05,  3.30s/it]ERROR:dspy.evaluate.evaluate:2024-09-26T21:01:07.951199Z [error    ] Error for example in dev set: \t\t HTTPConnectionPool(host='20.102.90.50', port=2017): Read timed out. (read timeout=10) [dspy.evaluate.evaluate] filename=evaluate.py lineno=183\n",
      "Average Metric: 0.0 / 5  (0.0):   3%|▎         | 5/150 [00:14<03:31,  1.46s/it]ERROR:dspy.evaluate.evaluate:2024-09-26T21:01:07.973872Z [error    ] Error for example in dev set: \t\t HTTPConnectionPool(host='20.102.90.50', port=2017): Read timed out. (read timeout=10) [dspy.evaluate.evaluate] filename=evaluate.py lineno=183\n",
      "Average Metric: 0.0 / 6  (0.0):   3%|▎         | 5/150 [00:14<03:31,  1.46s/it]2024-09-26T21:01:07.976639Z [error    ] Error for example in dev set: \t\t HTTPConnectionPool(host='20.102.90.50', port=2017): Read timed out. (read timeout=10) [dspy.evaluate.evaluate] filename=evaluate.py lineno=183\n",
      "Average Metric: 0.0 / 7  (0.0):   4%|▍         | 6/150 [00:14<03:30,  1.46s/it]2024-09-26T21:01:07.977441Z [error    ] Error for example in dev set: \t\t HTTPConnectionPool(host='20.102.90.50', port=2017): Read timed out. (read timeout=10) [dspy.evaluate.evaluate] filename=evaluate.py lineno=183\n",
      "Average Metric: 0.0 / 8  (0.0):   5%|▍         | 7/150 [00:14<03:28,  1.46s/it]ERROR:dspy.evaluate.evaluate:2024-09-26T21:01:08.092028Z [error    ] Error for example in dev set: \t\t HTTPConnectionPool(host='20.102.90.50', port=2017): Read timed out. (read timeout=10) [dspy.evaluate.evaluate] filename=evaluate.py lineno=183\n",
      "Average Metric: 0.0 / 9  (0.0):   6%|▌         | 9/150 [00:14<01:21,  1.72it/s]ERROR:dspy.evaluate.evaluate:2024-09-26T21:01:08.257331Z [error    ] Error for example in dev set: \t\t HTTPConnectionPool(host='20.102.90.50', port=2017): Read timed out. (read timeout=10) [dspy.evaluate.evaluate] filename=evaluate.py lineno=183\n",
      "Average Metric: 0.0 / 10  (0.0):   6%|▌         | 9/150 [00:14<01:21,  1.72it/s]ERROR:dspy.evaluate.evaluate:2024-09-26T21:01:08.314044Z [error    ] Error for example in dev set: \t\t HTTPConnectionPool(host='20.102.90.50', port=2017): Read timed out. (read timeout=10) [dspy.evaluate.evaluate] filename=evaluate.py lineno=183\n",
      "Average Metric: 0.0 / 11  (0.0):   7%|▋         | 11/150 [00:26<01:00,  2.29it/s]"
     ]
    },
    {
     "ename": "ReadTimeout",
     "evalue": "HTTPConnectionPool(host='20.102.90.50', port=2017): Read timed out. (read timeout=10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mtimeout\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:462\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 462\u001b[0m     httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 281\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[0;31mtimeout\u001b[0m: timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:801\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    799\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 801\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/urllib3/util/retry.py:552\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[0;32m--> 552\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/urllib3/packages/six.py:770\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 770\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:469\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 469\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:358\u001b[0m, in \u001b[0;36mHTTPConnectionPool._raise_timeout\u001b[0;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[0;32m--> 358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m timeout_value\n\u001b[1;32m    360\u001b[0m     )\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3. In Python 2 we have\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# to specifically catch it and throw the timeout error\u001b[39;00m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPConnectionPool(host='20.102.90.50', port=2017): Read timed out. (read timeout=10)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m vanilla_program \u001b[38;5;241m=\u001b[39m BasicMH()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m COMPILE_PROGRAM:\n\u001b[0;32m----> 6\u001b[0m     bfrs_program \u001b[38;5;241m=\u001b[39m \u001b[43mbfrs_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvanilla_program\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_trainset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_valset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     bfrs_program\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbasicmh_8b_31_bfrs_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_BOOTSTRAPPED_DEMOS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_LABELED_DEMOS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_CANDIDATE_PROGRAMS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/default/dspy/dspy/teleprompt/random_search.py:123\u001b[0m, in \u001b[0;36mBootstrapFewShotWithRandomSearch.compile\u001b[0;34m(self, student, teacher, trainset, valset, restrict, labeled_sample)\u001b[0m\n\u001b[1;32m    112\u001b[0m     program2 \u001b[38;5;241m=\u001b[39m teleprompter\u001b[38;5;241m.\u001b[39mcompile(student, teacher\u001b[38;5;241m=\u001b[39mteacher, trainset\u001b[38;5;241m=\u001b[39mtrainset2)\n\u001b[1;32m    114\u001b[0m evaluate \u001b[38;5;241m=\u001b[39m Evaluate(\n\u001b[1;32m    115\u001b[0m     devset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalset,\n\u001b[1;32m    116\u001b[0m     metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m     display_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    121\u001b[0m )\n\u001b[0;32m--> 123\u001b[0m score, subscores \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogram2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_all_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m all_subscores\u001b[38;5;241m.\u001b[39mappend(subscores)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m############ Assertion-aware Optimization ############\u001b[39;00m\n",
      "File \u001b[0;32m~/default/dspy/dspy/evaluate/evaluate.py:196\u001b[0m, in \u001b[0;36mEvaluate.__call__\u001b[0;34m(self, program, metric, devset, num_threads, display_progress, display_table, return_all_scores, return_outputs)\u001b[0m\n\u001b[1;32m    194\u001b[0m     reordered_devset, ncorrect, ntotal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_single_thread(wrapped_program, devset, display_progress)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m     reordered_devset, ncorrect, ntotal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_multi_thread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrapped_program\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplay_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m dspy\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Metric: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mncorrect\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mntotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(\u001b[38;5;241m100\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mncorrect\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mntotal,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    205\u001b[0m predicted_devset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(reordered_devset)\n",
      "File \u001b[0;32m~/default/dspy/dspy/evaluate/evaluate.py:112\u001b[0m, in \u001b[0;36mEvaluate._execute_multi_thread\u001b[0;34m(self, wrapped_program, devset, num_threads, display_progress)\u001b[0m\n\u001b[1;32m    109\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm\u001b[38;5;241m.\u001b[39mtqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(devset), dynamic_ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m display_progress)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m as_completed(futures):\n\u001b[0;32m--> 112\u001b[0m     example_idx, example, prediction, score \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# use the cancelled_job literal to check if the job was cancelled - use \"is\" not \"==\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# in case the prediction is \"cancelled\" for some reason.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prediction \u001b[38;5;129;01mis\u001b[39;00m job_cancelled:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/concurrent/futures/_base.py:439\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/concurrent/futures/_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/default/dspy/dspy/evaluate/evaluate.py:105\u001b[0m, in \u001b[0;36mEvaluate._execute_multi_thread.<locals>.cancellable_wrapped_program\u001b[0;34m(idx, arg)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_jobs\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, job_cancelled, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_program\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/default/dspy/dspy/evaluate/evaluate.py:181\u001b[0m, in \u001b[0;36mEvaluate.__call__.<locals>.wrapped_program\u001b[0;34m(example_idx, example)\u001b[0m\n\u001b[1;32m    179\u001b[0m     current_error_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_count\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_error_count \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_errors:\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    183\u001b[0m dspy\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError for example in dev set: \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m example_idx, example, {}, \u001b[38;5;241m0.0\u001b[39m\n",
      "File \u001b[0;32m~/default/dspy/dspy/evaluate/evaluate.py:163\u001b[0m, in \u001b[0;36mEvaluate.__call__.<locals>.wrapped_program\u001b[0;34m(example_idx, example)\u001b[0m\n\u001b[1;32m    160\u001b[0m     thread_stacks[threading\u001b[38;5;241m.\u001b[39mget_ident()] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(dspy\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mmain_stack)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mprogram\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     score \u001b[38;5;241m=\u001b[39m metric(\n\u001b[1;32m    165\u001b[0m         example,\n\u001b[1;32m    166\u001b[0m         prediction,\n\u001b[1;32m    167\u001b[0m     )  \u001b[38;5;66;03m# FIXME: TODO: What's the right order? Maybe force name-based kwargs!\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# increment assert and suggest failures to program's attributes\u001b[39;00m\n",
      "File \u001b[0;32m~/default/dspy/dspy/primitives/program.py:26\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 25\u001b[0m, in \u001b[0;36mBasicMH.forward\u001b[0;34m(self, question)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hop \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_hops):\n\u001b[1;32m     24\u001b[0m     search_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_query[hop](context\u001b[38;5;241m=\u001b[39mcontext, question\u001b[38;5;241m=\u001b[39mquestion)\u001b[38;5;241m.\u001b[39msearch_query\n\u001b[0;32m---> 25\u001b[0m     passages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_query\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpassages\n\u001b[1;32m     26\u001b[0m     context \u001b[38;5;241m=\u001b[39m deduplicate(context \u001b[38;5;241m+\u001b[39m passages)\n\u001b[1;32m     28\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_answer(context\u001b[38;5;241m=\u001b[39mcontext, question\u001b[38;5;241m=\u001b[39mquestion)\u001b[38;5;241m.\u001b[39mcopy(context\u001b[38;5;241m=\u001b[39mcontext)\n",
      "File \u001b[0;32m~/default/dspy/dspy/retrieve/retrieve.py:41\u001b[0m, in \u001b[0;36mRetrieve.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/default/dspy/dspy/retrieve/retrieve.py:70\u001b[0m, in \u001b[0;36mRetrieve.forward\u001b[0;34m(self, query_or_queries, k, by_prob, with_metadata, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m k \u001b[38;5;241m=\u001b[39m k \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m with_metadata:\n\u001b[0;32m---> 70\u001b[0m     passages \u001b[38;5;241m=\u001b[39m \u001b[43mdsp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieveEnsemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Prediction(passages\u001b[38;5;241m=\u001b[39mpassages)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/default/dspy/dsp/primitives/search.py:93\u001b[0m, in \u001b[0;36mretrieveEnsemble\u001b[0;34m(queries, k, by_prob, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m queries \u001b[38;5;241m=\u001b[39m [q \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m queries \u001b[38;5;28;01mif\u001b[39;00m q]\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(queries) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m passages \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m queries:\n",
      "File \u001b[0;32m~/default/dspy/dsp/primitives/search.py:14\u001b[0m, in \u001b[0;36mretrieve\u001b[0;34m(query, k, **kwargs)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dsp\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mrm:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo RM is loaded.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m passages \u001b[38;5;241m=\u001b[39m \u001b[43mdsp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(passages, Iterable):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# it's not an iterable yet; make it one.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# TODO: we should unify the type signatures of dspy.Retriever\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     passages \u001b[38;5;241m=\u001b[39m [passages]\n",
      "File \u001b[0;32m~/default/dspy/dsp/modules/colbertv2.py:30\u001b[0m, in \u001b[0;36mColBERTv2.__call__\u001b[0;34m(self, query, k, simplify)\u001b[0m\n\u001b[1;32m     28\u001b[0m     topk: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m colbertv2_post_request(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl, query, k)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     topk: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[43mcolbertv2_get_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m simplify:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [psg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m psg \u001b[38;5;129;01min\u001b[39;00m topk]\n",
      "File \u001b[0;32m~/default/dspy/dsp/modules/cache_utils.py:16\u001b[0m, in \u001b[0;36mnoop_decorator.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/default/dspy/dsp/modules/colbertv2.py:55\u001b[0m, in \u001b[0;36mcolbertv2_get_request_v2_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mcache\n\u001b[1;32m     53\u001b[0m \u001b[38;5;129m@NotebookCacheMemory\u001b[39m\u001b[38;5;241m.\u001b[39mcache\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcolbertv2_get_request_v2_wrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcolbertv2_get_request_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/memory.py:577\u001b[0m, in \u001b[0;36mMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;66;03m# Return the output, without the metadata\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cached_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshelving\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/memory.py:532\u001b[0m, in \u001b[0;36mMemorizedFunc._cached_call\u001b[0;34m(self, args, kwargs, shelving)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing func \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, argument hash \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min location \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    529\u001b[0m     )\n\u001b[1;32m    531\u001b[0m \u001b[38;5;66;03m# Returns the output but not the metadata\u001b[39;00m\n\u001b[0;32m--> 532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshelving\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/memory.py:771\u001b[0m, in \u001b[0;36mMemorizedFunc._call\u001b[0;34m(self, call_id, args, kwargs, shelving)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_before_call(args, kwargs)\n\u001b[1;32m    770\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 771\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_call(call_id, args, kwargs, shelving,\n\u001b[1;32m    773\u001b[0m                         output, start_time)\n",
      "File \u001b[0;32m~/default/dspy/dsp/modules/colbertv2.py:45\u001b[0m, in \u001b[0;36mcolbertv2_get_request_v2\u001b[0;34m(url, query, k)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m     41\u001b[0m     k \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     42\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly k <= 100 is supported for the hosted ColBERTv2 server at the moment.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: query, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m: k}\n\u001b[0;32m---> 45\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m topk \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopk\u001b[39m\u001b[38;5;124m\"\u001b[39m][:k]\n\u001b[1;32m     48\u001b[0m topk \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39md, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]} \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m topk]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/requests/adapters.py:713\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[0;32m--> 713\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidHeader(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mReadTimeout\u001b[0m: HTTPConnectionPool(host='20.102.90.50', port=2017): Read timed out. (read timeout=10)"
     ]
    }
   ],
   "source": [
    "COMPILE_PROGRAM = True\n",
    "\n",
    "with dspy.context(lm=llama_8b, rm=retriever):\n",
    "    vanilla_program = BasicMH()\n",
    "    if COMPILE_PROGRAM:\n",
    "        bfrs_program = bfrs_optimizer.compile(vanilla_program, trainset=optimizer_trainset, valset=optimizer_valset)\n",
    "        bfrs_program.save(f\"basicmh_8b_31_bfrs_{MAX_BOOTSTRAPPED_DEMOS}_{MAX_LABELED_DEMOS}_{NUM_CANDIDATE_PROGRAMS}.json\")\n",
    "    else:\n",
    "        bfrs_program = BasicMH()\n",
    "        bfrs_program.load(f\"basicmh_8b_31_bfrs_{MAX_BOOTSTRAPPED_DEMOS}_{MAX_LABELED_DEMOS}_{NUM_CANDIDATE_PROGRAMS}.json\")\n",
    "    llama_8b_bfrs_eval = evaluate_devset(bfrs_program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can compare all iterations of this pipeline\n",
    "print(f\"Results for HotPotQA fine-tuning LLaMa 8B with a starting trainset\")\n",
    "print(f\"    70B model (vanilla program): {llama_70b_base_eval}\")\n",
    "print(f\"    70B model (bfrs program): {llama_70b_bfrs_eval}\")\n",
    "print(f\"    8B model (vanilla program): {vanilla_8b_base_eval}\")\n",
    "print(f\"    8B model (bfrs program): {llama_8b_bfrs_eval}\")\n",
    "print(f\"    8B model (finetuned program): {llama_8b_finetuned_eval}\")\n",
    "print(f\"    8B model (finetuned bfrs program): {llama_8b_bfrs_finetuned_eval}\")\n",
    "print(f\"    8B model (finetuned mipro program): {llama_8b_ft_mipro_eval}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Let's now use the new offline batch inference to evaluate the finetuned model with optimized program on the entire devset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement once done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"Stop here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving\n",
    "\n",
    "This is the second biggest unknown\n",
    "I imagine it to be easy, but crazier things have happened\n",
    "\n",
    "I need to keep a reference or link to the LLM forge job inside the LM.finetune method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how do I get the ray llm image!\n",
    "\n",
    "We'll start by running the rayllm CLI command below to start the workflow to generate the service yaml configuration:\n",
    "```bash\n",
    "mkdir /home/ray/default/deploy/services\n",
    "cd /home/ray/default/deploy/services\n",
    "rayllm gen-config \n",
    "```\n",
    "\n",
    "<img src=\"assets/cli.png\" width=500 alt=\"todo! get this inage of what I need to serve\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch offline inference\n",
    "- Compare running inference using \n",
    "    - Ray Data \n",
    "    - multithreading on local VLLM thru HTTP\n",
    "    - Multithreading to Ray Serve instance thru HTTP\n",
    "- Dev time estimate: 7 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"background-color: yellow;\">&nbsp;🛑 IMPORTANT&nbsp;</b>: Please `Terminate` your service from the Service page to avoid depleting your free trial credits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "!python src/clear_cell_nums.py\n",
    "!find . | grep -E \".ipynb_checkpoints\" | xargs rm -rf\n",
    "!find . | grep -E \"(__pycache__|\\.pyc|\\.pyo)\" | xargs rm -rf\n",
    "!rm -rf __pycache__ data .HF_TOKEN deploy/services"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
