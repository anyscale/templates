accelerator_type: A100-80G
deployment_config:
  autoscaling_config:
    target_ongoing_requests: 32
  max_ongoing_requests: 64
engine_kwargs:
  enable_chunked_prefill: false
  enable_lora: true
  max_lora_rank: 32
  max_loras: 16
  max_num_batched_tokens: 8192
  max_num_seqs: 64
  tokenizer_pool_extra_config:
    runtime_env:
      pip: null
  tokenizer_pool_size: 2
  trust_remote_code: true
generation_config:
  prompt_format:
    assistant: '<|start_header_id|>assistant<|end_header_id|>


      {instruction}<|eot_id|>'
    bos: <|begin_of_text|>
    default_system_message: ''
    system: '<|start_header_id|>system<|end_header_id|>


      {instruction}<|eot_id|>'
    system_in_user: false
    trailing_assistant: '<|start_header_id|>assistant<|end_header_id|>


      '
    user: '<|start_header_id|>user<|end_header_id|>


      {instruction}<|eot_id|>'
  stopping_sequences: []
  stopping_tokens:
  - 128001
  - 128009
input_modality: text
json_mode:
  enabled: false
llm_engine: VLLMEngine
lora_config:
  dynamic_lora_loading_path: none
  max_num_adapters_per_replica: 64
max_request_context_length: 8192
model_loading_config:
  model_id: meta-llama/Llama-3.2-1B-Instruct
  model_source: meta-llama/Llama-3.2-1B-Instruct
runtime_env:
  env_vars:
    HUGGING_FACE_HUB_TOKEN: hf_1234567890
tensor_parallelism:
  degree: 1
