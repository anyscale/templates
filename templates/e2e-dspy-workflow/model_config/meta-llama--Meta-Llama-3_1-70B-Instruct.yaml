accelerator_type: A100-80G
deployment_config:
  autoscaling_config:
    initial_replicas: 1
    max_replicas: 2
    min_replicas: 0
    target_ongoing_requests: 128
  max_ongoing_requests: 300
engine_kwargs:
  enable_chunked_prefill: true
  max_num_batched_tokens: 8192
  max_num_seqs: 256
  tokenizer_pool_extra_config:
    runtime_env:
      pip: null
  tokenizer_pool_size: 2
  trust_remote_code: true
generation_config:
  prompt_format:
    assistant: "<|start_header_id|>assistant<|end_header_id|>\n\n{instruction}<|eot_id|>"
    bos: <|begin_of_text|>
    default_system_message: ''
    system: "<|start_header_id|>system<|end_header_id|>\n\n{instruction}<|eot_id|>"
    system_in_user: false
    trailing_assistant: "<|start_header_id|>assistant<|end_header_id|>\n\n"
    user: "<|start_header_id|>user<|end_header_id|>\n\n{instruction}<|eot_id|>"
  stopping_sequences: []
  stopping_tokens:
  - 128001
  - 128009
input_modality: text
json_mode:
  enabled: false
llm_engine: VLLMEngine
lora_config: null
max_request_context_length: 8192
model_loading_config:
  model_id: meta-llama/Meta-Llama-3.1-70B-Instruct
  model_source: meta-llama/Meta-Llama-3.1-70B-Instruct
runtime_env:
  env_vars:
    HUGGING_FACE_HUB_TOKEN: hf_1234567890 # replace with your HF_TOKEN
tensor_parallelism:
  degree: 4
