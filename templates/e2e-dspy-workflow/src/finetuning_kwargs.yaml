
hyperparameters:
  num_devices: 4
  trainer_resources: null
  worker_resources:
    accelerator_type:A100-80G: 0.001
  generation_config:
    prompt_format:
      system: "<|start_header_id|>system<|end_header_id|>\n\n{instruction}<|eot_id|>"
      user: "<|start_header_id|>user<|end_header_id|>\n{instruction}<|eot_id|>"
      assistant: "<|start_header_id|>assistant<|end_header_id|>\n\n{instruction}<|eot_id|>"
      trailing_assistant: "<|start_header_id|>assistant<|end_header_id|>\n\n"
      bos: "<|begin_of_text|>"
      system_in_user: false
      default_system_message: ""
  learning_rate: 3.0e-5
  num_epochs: 6
  train_batch_size_per_device: 32

use_lora: true

serve_config_path: "serve_1B.yaml" # Required to update the model config

logging_kwargs:
  provider: "wandb"

compute_config:
  name: "dspy-llmforge-fine-tuning-job"
  entrypoint: "llmforge anyscale finetune {filename}"
  working_dir: "."
  image_uri: "localhost:5555/anyscale/llm-forge:0.5.7"
  requirements: [
    "wandb",
  ]
