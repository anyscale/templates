{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488a6a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install -U anyscale -q\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import ray\n",
    "import json\n",
    "import datasets\n",
    "from datasets import DatasetDict, load_dataset\n",
    "import anyscale\n",
    "from anyscale.llm.dataset import Dataset as AnyscaleDataset\n",
    "import yaml\n",
    "from rich import print\n",
    "from src.utils import SYSTEM_CONTENT, to_llm_schema, get_dataset_file_path, update_datasets_in_fine_tuning_config, download_files_from_remote\n",
    "\n",
    "# Initialize HF token\n",
    "# assert ~/default/.HF_TOKEN exists\n",
    "assert os.path.exists(os.path.expanduser('~/default/.HF_TOKEN')), (\n",
    "    'Please create ~/default/.HF_TOKEN with your Hugging Face token\\n'\n",
    "    'echo \"your_token\" > ~/default/.HF_TOKEN'\n",
    ")\n",
    "HF_TOKEN = open(os.path.expanduser('~/default/.HF_TOKEN')).read().strip()\n",
    "\n",
    "os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "ray.shutdown()\n",
    "ray.init(runtime_env={'env_vars': {'HF_TOKEN': HF_TOKEN}})\n",
    "LLAMA_3_SERVE_CONFIG_PATH = 'deploy/services/model_config/meta-llama--Meta-Llama-3-8B-Instruct.yaml'\n",
    "config = yaml.safe_load(open(LLAMA_3_SERVE_CONFIG_PATH))\n",
    "config['runtime_env']['env_vars']['HUGGING_FACE_HUB_TOKEN'] = HF_TOKEN\n",
    "with open(LLAMA_3_SERVE_CONFIG_PATH, 'w') as f:\n",
    "    yaml.safe_dump(config, f)\n",
    "\n",
    "\n",
    "ray.data.DataContext.get_current().enable_progress_bars = False\n",
    "ray.data.DataContext.get_current().print_on_execution_start = False\n",
    "datasets.disable_progress_bars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0367dfcd",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Deploy Service\n",
    "!anyscale service deploy -f deploy/services/serve.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89a4ef2",
   "metadata": {},
   "source": [
    "# End-to-end LLM Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f28116",
   "metadata": {},
   "source": [
    "In this guide, we'll learn how to run an end-to-end LLM workflow. We separate this into four steps:\n",
    "\n",
    "1. **Data preprocessing**\n",
    "2. **Fine-tuning**\n",
    "3. **Serving**\n",
    "4. **Evaluation**\n",
    "\n",
    "**Objective**: Have an LLM convert unstructured text inputs about video games into structured text outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b5376e",
   "metadata": {},
   "source": [
    "## 0. Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ea51df",
   "metadata": {},
   "source": [
    "Imagine we are trying to convert an unstructured sentence into structured output. Take the problem statement below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876cad2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Given a target sentence, construct the underlying meaning representation of the input sentence as a \n",
       "single function with attributes and attribute values.\n",
       "\n",
       "This function should describe the target string accurately and the function must be one of the following\n",
       "\n",
       "<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'inform'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'request'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'give_opinion'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'confirm'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'verify_attribute'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'suggest'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'request_explanation'</span>,\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'recommend'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'request_attribute'</span><span style=\"font-weight: bold\">]</span>.\n",
       "    \n",
       "The attributes must be one of the following:\n",
       "\n",
       "<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'exp_release_date'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'release_year'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'developer'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'esrb'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'rating'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'genres'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'player_perspective'</span>,\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'has_multiplayer'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'platforms'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'available_on_steam'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'has_linux_release'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'has_mac_release'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'specifier'</span><span style=\"font-weight: bold\">]</span>.\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Given a target sentence, construct the underlying meaning representation of the input sentence as a \n",
       "single function with attributes and attribute values.\n",
       "\n",
       "This function should describe the target string accurately and the function must be one of the following\n",
       "\n",
       "\u001b[1m[\u001b[0m\u001b[32m'inform'\u001b[0m, \u001b[32m'request'\u001b[0m, \u001b[32m'give_opinion'\u001b[0m, \u001b[32m'confirm'\u001b[0m, \u001b[32m'verify_attribute'\u001b[0m, \u001b[32m'suggest'\u001b[0m, \u001b[32m'request_explanation'\u001b[0m,\n",
       "\u001b[32m'recommend'\u001b[0m, \u001b[32m'request_attribute'\u001b[0m\u001b[1m]\u001b[0m.\n",
       "    \n",
       "The attributes must be one of the following:\n",
       "\n",
       "\u001b[1m[\u001b[0m\u001b[32m'name'\u001b[0m, \u001b[32m'exp_release_date'\u001b[0m, \u001b[32m'release_year'\u001b[0m, \u001b[32m'developer'\u001b[0m, \u001b[32m'esrb'\u001b[0m, \u001b[32m'rating'\u001b[0m, \u001b[32m'genres'\u001b[0m, \u001b[32m'player_perspective'\u001b[0m,\n",
       "\u001b[32m'has_multiplayer'\u001b[0m, \u001b[32m'platforms'\u001b[0m, \u001b[32m'available_on_steam'\u001b[0m, \u001b[32m'has_linux_release'\u001b[0m, \u001b[32m'has_mac_release'\u001b[0m, \u001b[32m'specifier'\u001b[0m\u001b[1m]\u001b[0m.\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(SYSTEM_CONTENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d66c207",
   "metadata": {},
   "source": [
    "Let's first query a base model, Meta's Llama 3-8B model, to see how it performs on this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dc5665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import query\n",
    "\n",
    "response = query(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    prompt=\"Dirt: Showdown from 2012 is a sport racing game for the PlayStation, Xbox, PC \"\n",
    "    \"rated E 10+ (for Everyone 10 and Older). It's not available on Steam, Linux, or Mac.\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6807b373",
   "metadata": {},
   "source": [
    "Not great, right? It's slow and verbose. We were looking for an output like below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7e3c84",
   "metadata": {},
   "source": [
    "```python\n",
    "inform(\n",
    "    name[\"Dirt: Showdown\"],\n",
    "    release_year[2012],\n",
    "    esrb[\"E 10+ (for Everyone 10 and Older)\"],\n",
    "    genres[\"driving/racing\", \"sport\"],\n",
    "    platforms[\"PlayStation\", \"Xbox\", \"PC\"],\n",
    "    available_on_steam[False],\n",
    "    has_linux_release[False],\n",
    "    has_mac_release[False]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf7d0ca",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f29bc06",
   "metadata": {},
   "source": [
    "We can use Ray Data and Anyscale Datasets to transform a dataset we have about video games (VIGGO) into a LLM conversation format (`system` / `user` / `assistant`) that the model can understand. \n",
    "\n",
    "<img src=\"assets/data-overview.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553fd4de",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e3358",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset: DatasetDict = load_dataset(\"GEM/viggo\", trust_remote_code=True)  # type: ignore\n",
    "\n",
    "def get_dataset(split: str) -> AnyscaleDataset:\n",
    "    ray_dataset = ray.data.from_items(dataset[split]).map(to_llm_schema)\n",
    "    with get_dataset_file_path(ray_dataset) as dataset_file_path:\n",
    "        anyscale_dataset = anyscale.llm.dataset.upload(\n",
    "            dataset_file_path,\n",
    "            name=f\"viggo/{split}\",\n",
    "        )\n",
    "    return anyscale_dataset\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_dataset = get_dataset(\"train\")\n",
    "val_dataset = get_dataset(\"validation\")\n",
    "test_dataset = get_dataset(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c0a6b",
   "metadata": {},
   "source": [
    "## 2. Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b5c19d",
   "metadata": {},
   "source": [
    "Next, we'll fine-tune a large language model (LLM) using our dataset with LLMForge, Ray Train, and an Anyscale Job.\n",
    "\n",
    "We'll be fine-tuning Meta's Llama 3-8B model, which is the model we queried in the problem statement.\n",
    "\n",
    "<img src=\"assets/train-overview.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5684aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anyscale.job import JobConfig\n",
    "\n",
    "update_datasets_in_fine_tuning_config(\"configs/training/lora/llama-3-8b.yaml\", train_dataset, val_dataset)\n",
    "job_config = JobConfig.from_yaml(\"deploy/jobs/ft.yaml\")\n",
    "job_id = anyscale.job.submit(job_config)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c280f01b",
   "metadata": {},
   "source": [
    "## 3. Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b63ca2",
   "metadata": {},
   "source": [
    "Now, let's query our fine-tuned model. Our fine-tuned model is hosted on an Anyscale Service that uses RayLLM and Ray Serve.\n",
    "\n",
    "<img src=\"assets/online-overview.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fed9946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">inform</span><span style=\"font-weight: bold\">(</span>name<span style=\"font-weight: bold\">[</span>Dirt: Showdown<span style=\"font-weight: bold\">]</span>, release_year<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2012</span><span style=\"font-weight: bold\">]</span>, esrb<span style=\"font-weight: bold\">[</span>E <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>+ <span style=\"font-weight: bold\">(</span>for Everyone <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> and Older<span style=\"font-weight: bold\">)]</span>, genres, \n",
       "platforms<span style=\"font-weight: bold\">[</span>PlayStation, Xbox, PC<span style=\"font-weight: bold\">]</span>, available_on_steam, has_linux_release, has_mac_release<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35minform\u001b[0m\u001b[1m(\u001b[0mname\u001b[1m[\u001b[0mDirt: Showdown\u001b[1m]\u001b[0m, release_year\u001b[1m[\u001b[0m\u001b[1;36m2012\u001b[0m\u001b[1m]\u001b[0m, esrb\u001b[1m[\u001b[0mE \u001b[1;36m10\u001b[0m+ \u001b[1m(\u001b[0mfor Everyone \u001b[1;36m10\u001b[0m and Older\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m, genres, \n",
       "platforms\u001b[1m[\u001b[0mPlayStation, Xbox, PC\u001b[1m]\u001b[0m, available_on_steam, has_linux_release, has_mac_release\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "job_id = \"prodjob_lgcmhahdme45fc4hbyah82m6a7\"  # e2e-llm-workflows\n",
    "\n",
    "fine_tuned_model = anyscale.llm.model.get(job_id=job_id)  # type: ignore\n",
    "response = query(\n",
    "    fine_tuned_model.id,\n",
    "    prompt=\"Dirt: Showdown from 2012 is a sport racing game for the PlayStation, Xbox, PC \"\n",
    "    \"rated E 10+ (for Everyone 10 and Older). It's not available on Steam, Linux, or Mac.\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dea34b2",
   "metadata": {},
   "source": [
    "```python\n",
    "inform(\n",
    "    name[\"Dirt: Showdown\"],\n",
    "    release_year[2012],\n",
    "    esrb[\"E 10+ (for Everyone 10 and Older)\"],\n",
    "    genres[\"driving/racing\", \"sport\"],\n",
    "    platforms[\"PlayStation\", \"Xbox\", \"PC\"],\n",
    "    available_on_steam[False],\n",
    "    has_linux_release[False],\n",
    "    has_mac_release[False]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac2e914",
   "metadata": {},
   "source": [
    "See how much better the output is?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd27090",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3e1127",
   "metadata": {},
   "source": [
    "We can evaluate our fine-tuned LLM to see how well it performs on our task. We'll start by performing offline batch inference where we will use our fine-tuned model to generate the outputs.\n",
    "\n",
    "<img src=\"assets/offline-overview.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811f6613",
   "metadata": {},
   "source": [
    "### Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7dd6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load test set for eval\n",
    "test_data = ray.data.read_json(test_dataset.storage_uri).take_all()\n",
    "test_inputs = [\n",
    "    [message for message in item['messages'] if message['role'] != 'assistant']\n",
    "    for item in test_data\n",
    "]\n",
    "test_outputs = [\n",
    "    [message for message in item['messages'] if message['role'] == 'assistant']\n",
    "    for item in test_data\n",
    "]\n",
    "\n",
    "# Model and tokenizer\n",
    "HF_MODEL = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL)\n",
    "\n",
    "# Download artifacts\n",
    "local_dir = f'/mnt/cluster_storage/{fine_tuned_model.id}'  # Storage accessible by head and worker nodes\n",
    "download_files_from_remote(fine_tuned_model.storage_uri, local_dir)\n",
    "\n",
    "# Extract chat template used during fine-tuning\n",
    "tokenizer_config = json.load(open(os.path.join(local_dir, 'tokenizer_config.json')))\n",
    "chat_template = tokenizer_config['chat_template']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df2e627",
   "metadata": {},
   "source": [
    "### Chat template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646e7a57",
   "metadata": {},
   "source": [
    "When we fine-tuned our model, special tokens (ex. beginning/end of text, etc.) were automatically added to our inputs. We want to apply the same special tokens to our inputs prior to generating outputs using our tuned model. Luckily, the chat template to apply to our inputs (and add those tokens) is readily available inside our tuned model's `tokenizer_config.json` file. We can use our tokenizer to apply this template to our inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba7b2734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-28 01:13:23,840\tINFO dataset.py:2416 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'inputs'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\n\\nGiven a target sentence, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">construct the underlying meaning representation of the input sentence as a \\nsingle function with attributes and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">attribute values.\\n\\nThis function should describe the target string accurately and the function must be one of the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">following\\n\\n['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'request_explanation',\\n'recommend', 'request_attribute'].\\n    \\nThe attributes must be one of the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">following:\\n\\n['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'player_perspective',\\n'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'has_mac_release', 'specifier'].&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nI remember you saying you </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">found Little Big Adventure to be average. Are you not usually that into single-player games on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">PlayStation?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\\n\\n\"</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'outputs'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'verify_attribute(name[Little Big Adventure], rating[average], has_multiplayer[no], </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">platforms[PlayStation])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'inputs'\u001b[0m: \u001b[32m\"\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|begin_of_text|\u001b[0m\u001b[32m><|start_header_id|>system<|end_header_id|>\\n\\nGiven a target sentence, \u001b[0m\n",
       "\u001b[32mconstruct the underlying meaning representation of the input sentence as a \\nsingle function with attributes and \u001b[0m\n",
       "\u001b[32mattribute values.\\n\\nThis function should describe the target string accurately and the function must be one of the\u001b[0m\n",
       "\u001b[32mfollowing\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m'inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', \u001b[0m\n",
       "\u001b[32m'request_explanation',\\n'recommend', 'request_attribute'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\n    \\nThe attributes must be one of the \u001b[0m\n",
       "\u001b[32mfollowing:\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m'name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', \u001b[0m\n",
       "\u001b[32m'player_perspective',\\n'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', \u001b[0m\n",
       "\u001b[32m'has_mac_release', 'specifier'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nI remember you saying you \u001b[0m\n",
       "\u001b[32mfound Little Big Adventure to be average. Are you not usually that into single-player games on \u001b[0m\n",
       "\u001b[32mPlayStation?<|eot_id|><|start_header_id|>assistant<|end_header_id|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m\\n\\n\"\u001b[0m,\n",
       "        \u001b[32m'outputs'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "            \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'content'\u001b[0m: \u001b[32m'verify_attribute\u001b[0m\u001b[32m(\u001b[0m\u001b[32mname\u001b[0m\u001b[32m[\u001b[0m\u001b[32mLittle Big Adventure\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, rating\u001b[0m\u001b[32m[\u001b[0m\u001b[32maverage\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, has_multiplayer\u001b[0m\u001b[32m[\u001b[0m\u001b[32mno\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, \u001b[0m\n",
       "\u001b[32mplatforms\u001b[0m\u001b[32m[\u001b[0m\u001b[32mPlayStation\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m\n",
       "        \u001b[1m]\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply chat template\n",
    "test_input_prompts = [{'inputs': tokenizer.apply_chat_template(\n",
    "    conversation=inputs,\n",
    "    chat_template=chat_template,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    "    return_tensors='np'), 'outputs': outputs} for inputs, outputs in zip(test_inputs, test_outputs)]\n",
    "test_input_prompts_ds = ray.data.from_items(test_input_prompts)\n",
    "print (test_input_prompts_ds.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa87ab4",
   "metadata": {},
   "source": [
    "### Batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f6944d",
   "metadata": {},
   "source": [
    "We will use [vLLM](https://github.com/vllm-project/vllm)'s offline LLM class to load the model and use it for inference. We can easily load our LoRA weights and merge them with the base model (just pass in `lora_path`). And we'll wrap all of this functionality in a class that we can pass to [ray.data.Dataset.map_batches](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html) to apply batch inference at scale.\n",
    "\n",
    "<img src=\"assets/offline-detailed.png\" width=750>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a4c817a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41967d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMPredictor:\n",
    "    def __init__(self, hf_model, sampling_params, lora_path=None):\n",
    "        self.llm = LLM(model=hf_model, enable_lora=bool(lora_path))\n",
    "        self.sampling_params = sampling_params\n",
    "        self.lora_path = lora_path\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        if not self.lora_path:\n",
    "            outputs = self.llm.generate(\n",
    "                prompts=batch['inputs'],\n",
    "                sampling_params=self.sampling_params)\n",
    "        else:\n",
    "            outputs = self.llm.generate(\n",
    "                prompts=batch['inputs'],\n",
    "                sampling_params=self.sampling_params,\n",
    "                lora_request=LoRARequest('lora_adapter', 1, self.lora_path))\n",
    "        inputs = []\n",
    "        generated_outputs = []\n",
    "        for output in outputs:\n",
    "            inputs.append(output.prompt)\n",
    "            generated_outputs.append(' '.join([o.text for o in output.outputs]))\n",
    "        return {\n",
    "            'prompt': inputs,\n",
    "            'expected_output': batch['outputs'],\n",
    "            'generated_text': generated_outputs,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7232f365",
   "metadata": {},
   "source": [
    "During our data preprocessing template, we used the default compute strategy with `map_batches`. But this time we'll specify a custom compute strategy (`concurrency`, `num_gpus`, `batch_size` and `accelerator_type`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91a94445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuned model\n",
    "hf_model = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "sampling_params = SamplingParams(temperature=0, max_tokens=2048)\n",
    "ft_pred_ds = test_input_prompts_ds.map_batches(\n",
    "    LLMPredictor,\n",
    "    concurrency=4,  # number of LLM instances\n",
    "    num_gpus=1,     # GPUs per LLM instance\n",
    "    batch_size=10,  # maximize until OOM, if OOM then decrease batch_size\n",
    "    fn_constructor_kwargs={\n",
    "        'hf_model': hf_model,\n",
    "        'sampling_params': sampling_params,\n",
    "        'lora_path': local_dir,\n",
    "    },\n",
    "    accelerator_type='A10G',  # A10G or L4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9593bb0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'prompt'</span>: \"<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">|begin_of_text|</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Given a target sentence, construct the underlying meaning representation of the input sentence as a </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">single function with attributes and attribute values.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">This function should describe the target string accurately and the function must be one of the following</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'inform'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'request'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'give_opinion'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'confirm'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'verify_attribute'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'suggest'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'request_explanation'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'recommend'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'request_attribute'</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\">.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The attributes must be one of the following:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'name'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'exp_release_date'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'release_year'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'developer'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'esrb'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'rating'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'genres'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'player_perspective'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'has_multiplayer'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'platforms'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'available_on_steam'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'has_linux_release'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'has_mac_release'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'specifier'</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\">.&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Might &amp; Magic: Heroes VI was a barely adequate PC game. From the bird view perspective to the gameplay, everything </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">felt very by the numbers.&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "\",\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'expected_output'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">array</span><span style=\"font-weight: bold\">([{</span><span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'give_opinion(name[Might &amp; Magic: Heroes VI], rating, player_perspective, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">platforms[PC])'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span><span style=\"font-weight: bold\">}]</span>,\n",
       "       <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">object</span><span style=\"font-weight: bold\">)</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'generated_text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'give_opinion(name[Might &amp; Magic: Heroes VI], rating, player_perspective, platforms[PC])'</span><span style=\"font-weight: bold\">}</span>   \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m{\u001b[0m\u001b[32m'prompt'\u001b[0m: \"\u001b[1m<\u001b[0m\u001b[1;95m|begin_of_text|\u001b[0m\u001b[39m><|start_header_id|>system<|end_header_id|>\u001b[0m\n",
       "\n",
       "\u001b[39mGiven a target sentence, construct the underlying meaning representation of the input sentence as a \u001b[0m\n",
       "\u001b[39msingle function with attributes and attribute values.\u001b[0m\n",
       "\n",
       "\u001b[39mThis function should describe the target string accurately and the function must be one of the following\u001b[0m\n",
       "\n",
       "\u001b[1;39m[\u001b[0m\u001b[32m'inform'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'request'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'give_opinion'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'confirm'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'verify_attribute'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'suggest'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'request_explanation'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[32m'recommend'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'request_attribute'\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m.\u001b[0m\n",
       "\u001b[39m    \u001b[0m\n",
       "\u001b[39mThe attributes must be one of the following:\u001b[0m\n",
       "\n",
       "\u001b[1;39m[\u001b[0m\u001b[32m'name'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'exp_release_date'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'release_year'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'developer'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'esrb'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'rating'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'genres'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'player_perspective'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[32m'has_multiplayer'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'platforms'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'available_on_steam'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'has_linux_release'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'has_mac_release'\u001b[0m\u001b[39m, \u001b[0m\n",
       "\u001b[32m'specifier'\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m.<|eot_id|><|start_header_id|>user<|end_header_id|>\u001b[0m\n",
       "\n",
       "\u001b[39mMight & Magic: Heroes VI was a barely adequate PC game. From the bird view perspective to the gameplay, everything \u001b[0m\n",
       "\u001b[39mfelt very by the numbers.<|eot_id|><|start_header_id|>assistant<|end_header_id|\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "\",\n",
       " \u001b[32m'expected_output'\u001b[0m: \u001b[1;35marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'content'\u001b[0m: \u001b[32m'give_opinion\u001b[0m\u001b[32m(\u001b[0m\u001b[32mname\u001b[0m\u001b[32m[\u001b[0m\u001b[32mMight & Magic: Heroes VI\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, rating\u001b[0m\u001b[32m, player_perspective\u001b[0m\u001b[32m, \u001b[0m\n",
       "\u001b[32mplatforms\u001b[0m\u001b[32m[\u001b[0m\u001b[32mPC\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m,\n",
       "       \u001b[33mdtype\u001b[0m=\u001b[35mobject\u001b[0m\u001b[1m)\u001b[0m,\n",
       " \u001b[32m'generated_text'\u001b[0m: \u001b[32m'give_opinion\u001b[0m\u001b[32m(\u001b[0m\u001b[32mname\u001b[0m\u001b[32m[\u001b[0m\u001b[32mMight & Magic: Heroes VI\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, rating\u001b[0m\u001b[32m, player_perspective\u001b[0m\u001b[32m, platforms\u001b[0m\u001b[32m[\u001b[0m\u001b[32mPC\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\u001b[1m}\u001b[0m   \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Batch inference will take ~4 minutes\n",
    "ft_pred = ft_pred_ds.take_all()\n",
    "ft_pred[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81f5658",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f53e9eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Percentage of exact matches: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">77.38</span>%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Percentage of exact matches: \u001b[1;36m77.38\u001b[0m%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exact match (strict!)\n",
    "matches = 0\n",
    "mismatches = []\n",
    "for item in ft_pred:\n",
    "    if item['expected_output'][0]['content'] == item['generated_text'].split('<|eot_id|>')[0]:\n",
    "        matches += 1\n",
    "    else:\n",
    "        mismatches.append(item)\n",
    "print(\"Percentage of exact matches:\", f\"{(matches / float(len(ft_pred)) * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd9ec44",
   "metadata": {},
   "source": [
    "**Note**: you can train for more epochs (`num_epochs: 10`) to further improve the performance.\n",
    "\n",
    "Even our mismatches are not too far off and sometimes it might be worth a closer look because the dataset itself might have a few errors that the model may have identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "685bdfb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'prompt'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\n\\nGiven a target sentence, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">construct the underlying meaning representation of the input sentence as a \\nsingle function with attributes and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">attribute values.\\n\\nThis function should describe the target string accurately and the function must be one of the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">following\\n\\n['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'request_explanation',\\n'recommend', 'request_attribute'].\\n    \\nThe attributes must be one of the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">following:\\n\\n['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'player_perspective',\\n'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'has_mac_release', 'specifier'].&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nWhat would you say was the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">weirdest game you've played?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\\n\\n\"</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'expected_output'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">array</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">([{</span><span style=\"color: #008000; text-decoration-color: #008000\">'content'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'request(specifier[weirdest])'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}]</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">      </span><span style=\"color: #808000; text-decoration-color: #808000\">dtype</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080\">object</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'generated_text'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'request(specifier[weird])'</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'prompt'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\n\\nGiven a target sentence, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">construct the underlying meaning representation of the input sentence as a \\nsingle function with attributes and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">attribute values.\\n\\nThis function should describe the target string accurately and the function must be one of the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">following\\n\\n['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'request_explanation',\\n'recommend', 'request_attribute'].\\n    \\nThe attributes must be one of the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">following:\\n\\n['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'player_perspective',\\n'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'has_mac_release', 'specifier'].&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nWhat's the weirdest game you </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">have ever heard of?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\\n\\n\"</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'expected_output'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">array</span><span style=\"font-weight: bold\">([{</span><span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'request(specifier[weirdest])'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span><span style=\"font-weight: bold\">}]</span>,\n",
       "      <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">object</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'generated_text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'request(specifier[weird])'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'prompt'\u001b[0m: \u001b[32m\"\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|begin_of_text|\u001b[0m\u001b[32m><|start_header_id|>system<|end_header_id|>\\n\\nGiven a target sentence, \u001b[0m\n",
       "\u001b[32mconstruct the underlying meaning representation of the input sentence as a \\nsingle function with attributes and \u001b[0m\n",
       "\u001b[32mattribute values.\\n\\nThis function should describe the target string accurately and the function must be one of the\u001b[0m\n",
       "\u001b[32mfollowing\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m'inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', \u001b[0m\n",
       "\u001b[32m'request_explanation',\\n'recommend', 'request_attribute'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\n    \\nThe attributes must be one of the \u001b[0m\n",
       "\u001b[32mfollowing:\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m'name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', \u001b[0m\n",
       "\u001b[32m'player_perspective',\\n'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', \u001b[0m\n",
       "\u001b[32m'has_mac_release', 'specifier'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat would you say was the \u001b[0m\n",
       "\u001b[32mweirdest game you've played?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[32m'expected_output'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35marray\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m{\u001b[0m\u001b[32m'content'\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'request\u001b[0m\u001b[32m(\u001b[0m\u001b[32mspecifier\u001b[0m\u001b[32m[\u001b[0m\u001b[32mweirdest\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'role'\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'assistant'\u001b[0m\u001b[1;39m}\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m      \u001b[0m\u001b[33mdtype\u001b[0m\u001b[39m=\u001b[0m\u001b[35mobject\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[32m'generated_text'\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'request\u001b[0m\u001b[32m(\u001b[0m\u001b[32mspecifier\u001b[0m\u001b[32m[\u001b[0m\u001b[32mweird\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[1;39m{\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[32m'prompt'\u001b[0m\u001b[39m: \u001b[0m\u001b[32m\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nGiven a target sentence, \u001b[0m\n",
       "\u001b[32mconstruct the underlying meaning representation of the input sentence as a \\nsingle function with attributes and \u001b[0m\n",
       "\u001b[32mattribute values.\\n\\nThis function should describe the target string accurately and the function must be one of the\u001b[0m\n",
       "\u001b[32mfollowing\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m'inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', \u001b[0m\n",
       "\u001b[32m'request_explanation',\\n'recommend', 'request_attribute'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\n    \\nThe attributes must be one of the \u001b[0m\n",
       "\u001b[32mfollowing:\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m'name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', \u001b[0m\n",
       "\u001b[32m'player_perspective',\\n'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', \u001b[0m\n",
       "\u001b[32m'has_mac_release', 'specifier'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat's the weirdest game you \u001b[0m\n",
       "\u001b[32mhave ever heard of?<|eot_id|><|start_header_id|>assistant<|end_header_id|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m\\n\\n\"\u001b[0m,\n",
       "        \u001b[32m'expected_output'\u001b[0m: \u001b[1;35marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'content'\u001b[0m: \u001b[32m'request\u001b[0m\u001b[32m(\u001b[0m\u001b[32mspecifier\u001b[0m\u001b[32m[\u001b[0m\u001b[32mweirdest\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m,\n",
       "      \u001b[33mdtype\u001b[0m=\u001b[35mobject\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[32m'generated_text'\u001b[0m: \u001b[32m'request\u001b[0m\u001b[32m(\u001b[0m\u001b[32mspecifier\u001b[0m\u001b[32m[\u001b[0m\u001b[32mweird\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect a few of the mismatches\n",
    "print(mismatches[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6b8ff7",
   "metadata": {},
   "source": [
    "## End-to-End Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b54b71b",
   "metadata": {},
   "source": [
    "<img src=\"assets/ai-platform.png\" width=650>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c185bc7",
   "metadata": {},
   "source": [
    "## Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb23da9",
   "metadata": {},
   "source": [
    "We have a lot more guides that address more nuanced use cases:\n",
    "\n",
    "Fine-tuning:\n",
    "- [Control over 50+ hyperparameters](https://docs.anyscale.com/llms/finetuning/guides/modify_hyperparams/)\n",
    "- [Fine-tune any HF model](https://docs.anyscale.com/llms/finetuning/guides/bring_any_hf_model/)\n",
    "- [Full-parameter or LoRA fine-tuning](https://docs.anyscale.com/llms/finetuning/guides/lora_vs_full_param/)\n",
    "- [Classification fine-tuning / Routing](https://www.anyscale.com/blog/building-an-llm-router-for-high-quality-and-cost-effective-responses)\n",
    "- [Function calling fine-tuning](https://github.com/anyscale/templates/blob/main/templates/fine-tune-llm_v2/end-to-end-examples/fine-tune-function-calling/README.ipynb)\n",
    "- [Longer context fine-tuning](https://www.anyscale.com/blog/fine-tuning-llms-for-longer-context-and-better-rag-systems)\n",
    "- [Continued fine-tuning from checkpoint](https://github.com/anyscale/templates/tree/main/templates/fine-tune-llm_v2/cookbooks/continue_from_checkpoint)\n",
    "- Training on more available hardware (ex. A10s) with model parallelism\n",
    "- [End-to-end LLM workflows (including batch data processing, batch inference)](https://www.anyscale.com/blog/end-to-end-llm-workflows-guide)\n",
    "- Distillation (Coming in <2 weeks)\n",
    "\n",
    "Serving:\n",
    "- [Deploy with autoscaling + optimize for latency vs. throughput](https://docs.anyscale.com/examples/deploy-llms/)\n",
    "- [Serving multiple LoRA adapters](https://docs.anyscale.com/llms/serving/guides/multi_lora/)\n",
    "- [Migration from OpenAI](https://docs.anyscale.com/llms/serving/guides/openai_to_oss/)\n",
    "- [Spot to on-demand fallback (vice versa)](https://docs.anyscale.com/1.0.0/configure/compute-configs/ondemand-to-spot-fallback/)\n",
    "- [Batch inference with vLLM](https://docs.anyscale.com/examples/batch-llm/)\n",
    "\n",
    "And more!\n",
    "- [Batch text embeddings with Ray data](https://github.com/anyscale/templates/tree/main/templates/text-embeddings)\n",
    "- [Production RAG applications](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1)\n",
    "- [Router](https://github.com/anyscale/llm-router) between different models (base, fine-tuned, closed-source) to optimize for cost and quality\n",
    "- Stable diffusion [fine-tuning](https://github.com/anyscale/templates/tree/main/templates/fine-tune-stable-diffusion) and [serving](https://github.com/anyscale/templates/tree/main/templates/serve-stable-diffusion)\n",
    "\n",
    "And if you're interested in using our hosted Anyscale or connecting it to your own cloud, reach out to us at [Anyscale](https://www.anyscale.com/get-started?utm_source=goku). And follow us on [Twitter](https://x.com/anyscalecompute) and [LinkedIn](https://www.linkedin.com/company/joinanyscale/) for more real-time updates on new features!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112e5097",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b501e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "!python src/clear_cell_nums.py\n",
    "!find . | grep -E \".ipynb_checkpoints\" | xargs rm -rf\n",
    "!find . | grep -E \"(__pycache__|\\.pyc|\\.pyo)\" | xargs rm -rf\n",
    "!rm -rf __pycache__ data .HF_TOKEN deploy/services"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
