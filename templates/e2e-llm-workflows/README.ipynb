{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488a6a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install -U anyscale -q\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import ray\n",
    "import datasets\n",
    "from datasets import DatasetDict, load_dataset\n",
    "import anyscale\n",
    "from anyscale.llm.dataset import Dataset as AnyscaleDataset\n",
    "import yaml\n",
    "from rich import print\n",
    "from src.utils import SYSTEM_CONTENT, to_schema, get_dataset_file_path, update_datasets_in_fine_tuning_config\n",
    "\n",
    "# Initialize HF token\n",
    "# assert ~/default/.HF_TOKEN exists\n",
    "assert os.path.exists(os.path.expanduser('~/default/.HF_TOKEN')), (\n",
    "    'Please create ~/default/.HF_TOKEN with your Hugging Face token\\n'\n",
    "    'echo \"your_token\" > ~/default/.HF_TOKEN'\n",
    ")\n",
    "HF_TOKEN = open(os.path.expanduser('~/default/.HF_TOKEN')).read().strip()\n",
    "\n",
    "os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "ray.shutdown()\n",
    "ray.init(runtime_env={'env_vars': {'HF_TOKEN': HF_TOKEN}})\n",
    "LLAMA_3_SERVE_CONFIG_PATH = 'deploy/services/model_config/meta-llama--Meta-Llama-3-8B-Instruct.yaml'\n",
    "config = yaml.safe_load(open(LLAMA_3_SERVE_CONFIG_PATH))\n",
    "config['runtime_env']['env_vars']['HUGGING_FACE_HUB_TOKEN'] = HF_TOKEN\n",
    "with open(LLAMA_3_SERVE_CONFIG_PATH, 'w') as f:\n",
    "    yaml.safe_dump(config, f)\n",
    "\n",
    "\n",
    "ray.data.DataContext.get_current().enable_progress_bars = False\n",
    "ray.data.DataContext.get_current().print_on_execution_start = False\n",
    "datasets.disable_progress_bars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0367dfcd",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Deploy Service\n",
    "!anyscale service deploy -f deploy/services/serve.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89a4ef2",
   "metadata": {},
   "source": [
    "# End-to-end LLM Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f28116",
   "metadata": {},
   "source": [
    "In this guide, we'll learn how to run an end-to-end LLM workflow. We separate this into four steps:\n",
    "\n",
    "1. **Data preprocessing**\n",
    "2. **Fine-tuning**\n",
    "3. **Serving**\n",
    "4. **(Optional) Evaluation**\n",
    "\n",
    "**Objective**: Have an LLM convert unstructured text inputs about video games into structured text outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf7d0ca",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876cad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SYSTEM_CONTENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dc5665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import query\n",
    "\n",
    "response = query(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    prompt=\"Dirt: Showdown from 2012 is a sport racing game for the PlayStation, Xbox, PC \"\n",
    "    \"rated E 10+ (for Everyone 10 and Older). It's not available on Steam, Linux, or Mac.\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7e3c84",
   "metadata": {},
   "source": [
    "```python\n",
    "inform(\n",
    "    name[\"Dirt: Showdown\"],\n",
    "    release_year[2012],\n",
    "    esrb[\"E 10+ (for Everyone 10 and Older)\"],\n",
    "    genres[\"driving/racing\", \"sport\"],\n",
    "    platforms[\"PlayStation\", \"Xbox\", \"PC\"],\n",
    "    available_on_steam[False],\n",
    "    has_linux_release[False],\n",
    "    has_mac_release[False]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f29bc06",
   "metadata": {},
   "source": [
    "With Ray, we can use batch processing to preprocess our dataset at scale.\n",
    "\n",
    "<img src=\"assets/data-overview.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553fd4de",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e3358",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset: DatasetDict = load_dataset(\"GEM/viggo\", trust_remote_code=True)  # type: ignore\n",
    "\n",
    "def get_dataset(split: str) -> AnyscaleDataset:\n",
    "    ray_dataset = ray.data.from_items(dataset[split]).map(to_schema)\n",
    "    with get_dataset_file_path(ray_dataset) as dataset_file_path:\n",
    "        anyscale_dataset = anyscale.llm.dataset.upload(\n",
    "            dataset_file_path,\n",
    "            name=f\"viggo/{split}\",\n",
    "        )\n",
    "    return anyscale_dataset\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_dataset = get_dataset(\"train\")\n",
    "val_dataset = get_dataset(\"validation\")\n",
    "test_dataset = get_dataset(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5c72cd",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78fd7e6",
   "metadata": {},
   "source": [
    "We'll use [Ray Data](https://docs.ray.io/) to load our dataset and apply preprocessing to batches of our data at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f7df41",
   "metadata": {},
   "source": [
    "We want to preprocess our data by converting it into the `system` / `user` / `assistant` conversation format that our LLM will recognize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1414ef92",
   "metadata": {},
   "source": [
    "To apply our function on our dataset at scale, we can pass it to [`ray.data.Dataset.map`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map.html).\n",
    "\n",
    "<img src=\"assets/data-detailed.png\" width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba079525",
   "metadata": {},
   "source": [
    "### Save and load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46ea17f",
   "metadata": {},
   "source": [
    "Let's save the LLM dataset splits to the cloud for future access. We can do this with an Anyscale Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c0a6b",
   "metadata": {},
   "source": [
    "## 2. Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b5c19d",
   "metadata": {},
   "source": [
    "Next, let's fine-tune a large language model (LLM) using our dataset from the previous data preprocessing step. We'll be fine-tuning Meta's Llama 3, 8B model.\n",
    "\n",
    "<img src=\"assets/train-overview.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3bda2b",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09768304",
   "metadata": {},
   "source": [
    "Anyscale provides a set of configurations for fine-tuning LLMs. Let's choose the default configuration, but load our dataset from the previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86401c83",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584a8f15",
   "metadata": {},
   "source": [
    "Now, let's kick off fine-tuning with an Anyscale job. By fine-tuning with [`llmforge`](https://docs.anyscale.com/llms/finetuning/intro/) and Ray Train, we'll be able to execute fine-tuning in parallel across multiple devices.\n",
    "\n",
    "<img src=\"assets/train-detailed.png\" width=550>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5684aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anyscale.job import JobConfig\n",
    "\n",
    "update_datasets_in_fine_tuning_config(\"configs/training/lora/llama-3-8b.yaml\", train_dataset, val_dataset)\n",
    "job_config = JobConfig.from_yaml(\"deploy/jobs/ft.yaml\")\n",
    "job_id = anyscale.job.submit(job_config)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb1214b",
   "metadata": {},
   "source": [
    "### Load artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c302a0",
   "metadata": {},
   "source": [
    "Once fine-tuning is complete, we can load info about the fine-tuned model, and download its model artifacts locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b25d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import download_files_from_remote\n",
    "\n",
    "job_id = \"prodjob_lgcmhahdme45fc4hbyah82m6a7\"\n",
    "model_info = anyscale.llm.model.get(job_id=job_id)  # type: ignore\n",
    "print(model_info)\n",
    "\n",
    "# Download artifacts\n",
    "local_dir = f'/mnt/cluster_storage/{model_info.id}'  # Storage accessible by head and worker nodes\n",
    "download_files_from_remote(model_info.storage_uri, local_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c280f01b",
   "metadata": {},
   "source": [
    "## 3. Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b63ca2",
   "metadata": {},
   "source": [
    "For model serving, we'll launch an Anyscale service. With `rayllm` and Ray Serve, our service can autoscale to meet any demand.\n",
    "\n",
    "<img src=\"assets/online-overview.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcadbce",
   "metadata": {},
   "source": [
    "Let's also define a function to query our LLM service with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4662c8",
   "metadata": {},
   "source": [
    "### Production service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fed9946",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = \"prodjob_lgcmhahdme45fc4hbyah82m6a7\"  # e2e-llm-workflows\n",
    "\n",
    "fine_tuned_model = anyscale.llm.model.get(job_id=job_id)  # type: ignore\n",
    "response = query(\n",
    "    fine_tuned_model.id,\n",
    "    prompt=\"Dirt: Showdown from 2012 is a sport racing game for the PlayStation, Xbox, PC \"\n",
    "    \"rated E 10+ (for Everyone 10 and Older). It's not available on Steam, Linux, or Mac.\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dea34b2",
   "metadata": {},
   "source": [
    "```python\n",
    "inform(\n",
    "    name[\"Dirt: Showdown\"],\n",
    "    release_year[2012],\n",
    "    esrb[\"E 10+ (for Everyone 10 and Older)\"],\n",
    "    genres[\"driving/racing\", \"sport\"],\n",
    "    platforms[\"PlayStation\", \"Xbox\", \"PC\"],\n",
    "    available_on_steam[False],\n",
    "    has_linux_release[False],\n",
    "    has_mac_release[False]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6b8ff7",
   "metadata": {},
   "source": [
    "## Dev → Prod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b54b71b",
   "metadata": {},
   "source": [
    "We've now served our model into production via [Anyscale Services](https://docs.anyscale.com/examples/intro-services/) but we can just easily productionize our other workloads with [Anyscale Jobs](https://docs.anyscale.com/examples/intro-jobs/) (like we did for fine-tuning above) to execute this entire workflow completely programmatically outside of Workspaces.\n",
    "\n",
    "<img src=\"assets/jobs.png\" width=650>\n",
    "\n",
    "For example, suppose that we want to preprocess batches of new incoming data, fine-tune a model, evaluate it and then compare it to the existing production version. All of this can be productionized by simply launching the workload as a [Job](https://docs.anyscale.com/examples/intro-jobs), which can be triggered manually, periodically (cron) or event-based (via webhooks, etc.). We also provide integrations with your platform/tools to make all of this connect with your existing production workflows.\n",
    "\n",
    "<img src=\"assets/ai-platform.png\" width=650>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd27090",
   "metadata": {},
   "source": [
    "## 4. (Optional) Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3e1127",
   "metadata": {},
   "source": [
    "We can evaluate our fine-tuned LLM to see how well it performs on our task. We'll start by performing offline batch inference where we will use our fine-tuned model to generate the outputs.\n",
    "\n",
    "<img src=\"assets/offline-overview.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811f6613",
   "metadata": {},
   "source": [
    "### Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7dd6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set for eval\n",
    "ft_test_ds = ray.data.read_json(test_dataset.storage_uri)\n",
    "test_data = ft_test_ds.take_all()\n",
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af7c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into inputs/outputs\n",
    "test_inputs = []\n",
    "test_outputs = []\n",
    "for item in test_data:\n",
    "    test_inputs.append([message for message in item['messages'] if message['role'] != 'assistant'])\n",
    "    test_outputs.append([message for message in item['messages'] if message['role'] == 'assistant'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87781ad",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd969dd1",
   "metadata": {},
   "source": [
    "We'll also load the appropriate tokenizer to apply to our input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc8ad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Model and tokenizer\n",
    "HF_MODEL = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df2e627",
   "metadata": {},
   "source": [
    "### Chat template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646e7a57",
   "metadata": {},
   "source": [
    "When we fine-tuned our model, special tokens (ex. beginning/end of text, etc.) were automatically added to our inputs. We want to apply the same special tokens to our inputs prior to generating outputs using our tuned model. Luckily, the chat template to apply to our inputs (and add those tokens) is readily available inside our tuned model's `tokenizer_config.json` file. We can use our tokenizer to apply this template to our inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b812c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252f4d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract chat template used during fine-tuning\n",
    "with open(os.path.join(local_dir, 'tokenizer_config.json')) as file:\n",
    "    tokenizer_config = json.load(file)\n",
    "chat_template = tokenizer_config['chat_template']\n",
    "print (chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7b2734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply chat template\n",
    "test_input_prompts = [{'inputs': tokenizer.apply_chat_template(\n",
    "    conversation=inputs,\n",
    "    chat_template=chat_template,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    "    return_tensors='np'), 'outputs': outputs} for inputs, outputs in zip(test_inputs, test_outputs)]\n",
    "test_input_prompts_ds = ray.data.from_items(test_input_prompts)\n",
    "print (test_input_prompts_ds.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa87ab4",
   "metadata": {},
   "source": [
    "### Batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f6944d",
   "metadata": {},
   "source": [
    "We will use [vLLM](https://github.com/vllm-project/vllm)'s offline LLM class to load the model and use it for inference. We can easily load our LoRA weights and merge them with the base model (just pass in `lora_path`). And we'll wrap all of this functionality in a class that we can pass to [ray.data.Dataset.map_batches](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html) to apply batch inference at scale.\n",
    "\n",
    "<img src=\"assets/offline-detailed.png\" width=750>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4c817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41967d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMPredictor:\n",
    "    def __init__(self, hf_model, sampling_params, lora_path=None):\n",
    "        self.llm = LLM(model=hf_model, enable_lora=bool(lora_path))\n",
    "        self.sampling_params = sampling_params\n",
    "        self.lora_path = lora_path\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        if not self.lora_path:\n",
    "            outputs = self.llm.generate(\n",
    "                prompts=batch['inputs'],\n",
    "                sampling_params=self.sampling_params)\n",
    "        else:\n",
    "            outputs = self.llm.generate(\n",
    "                prompts=batch['inputs'],\n",
    "                sampling_params=self.sampling_params,\n",
    "                lora_request=LoRARequest('lora_adapter', 1, self.lora_path))\n",
    "        inputs = []\n",
    "        generated_outputs = []\n",
    "        for output in outputs:\n",
    "            inputs.append(output.prompt)\n",
    "            generated_outputs.append(' '.join([o.text for o in output.outputs]))\n",
    "        return {\n",
    "            'prompt': inputs,\n",
    "            'expected_output': batch['outputs'],\n",
    "            'generated_text': generated_outputs,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7232f365",
   "metadata": {},
   "source": [
    "During our data preprocessing template, we used the default compute strategy with `map_batches`. But this time we'll specify a custom compute strategy (`concurrency`, `num_gpus`, `batch_size` and `accelerator_type`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a94445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuned model\n",
    "hf_model = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "sampling_params = SamplingParams(temperature=0, max_tokens=2048)\n",
    "ft_pred_ds = test_input_prompts_ds.map_batches(\n",
    "    LLMPredictor,\n",
    "    concurrency=4,  # number of LLM instances\n",
    "    num_gpus=1,     # GPUs per LLM instance\n",
    "    batch_size=10,  # maximize until OOM, if OOM then decrease batch_size\n",
    "    fn_constructor_kwargs={\n",
    "        'hf_model': hf_model,\n",
    "        'sampling_params': sampling_params,\n",
    "        'lora_path': local_dir,\n",
    "    },\n",
    "    accelerator_type='A10G',  # A10G or L4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9593bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch inference will take ~4 minutes\n",
    "ft_pred = ft_pred_ds.take_all()\n",
    "ft_pred[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81f5658",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53e9eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact match (strict!)\n",
    "matches = 0\n",
    "mismatches = []\n",
    "for item in ft_pred:\n",
    "    if item['expected_output'][0]['content'] == item['generated_text'].split('<|eot_id|>')[0]:\n",
    "        matches += 1\n",
    "    else:\n",
    "        mismatches.append(item)\n",
    "matches / float(len(ft_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd9ec44",
   "metadata": {},
   "source": [
    "**Note**: you can train for more epochs (`num_epochs: 10`) to further improve the performance.\n",
    "\n",
    "Even our mismatches are not too far off and sometimes it might be worth a closer look because the dataset itself might have a few errors that the model may have identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685bdfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a few of the mismatches\n",
    "mismatches[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112e5097",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b501e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "!python src/clear_cell_nums.py\n",
    "!find . | grep -E \".ipynb_checkpoints\" | xargs rm -rf\n",
    "!find . | grep -E \"(__pycache__|\\.pyc|\\.pyo)\" | xargs rm -rf\n",
    "!rm -rf __pycache__ data .HF_TOKEN deploy/services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c185bc7",
   "metadata": {},
   "source": [
    "## Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb23da9",
   "metadata": {},
   "source": [
    "We have a lot more guides that address more nuanced use cases:\n",
    "\n",
    "Fine-tuning:\n",
    "- [Control over 50+ hyperparameters](https://docs.anyscale.com/llms/finetuning/guides/modify_hyperparams/)\n",
    "- [Fine-tune any HF model](https://docs.anyscale.com/llms/finetuning/guides/bring_any_hf_model/)\n",
    "- [Full-parameter or LoRA fine-tuning](https://docs.anyscale.com/llms/finetuning/guides/lora_vs_full_param/)\n",
    "- [Classification fine-tuning / Routing](https://www.anyscale.com/blog/building-an-llm-router-for-high-quality-and-cost-effective-responses)\n",
    "- [Function calling fine-tuning](https://github.com/anyscale/templates/blob/main/templates/fine-tune-llm_v2/end-to-end-examples/fine-tune-function-calling/README.ipynb)\n",
    "- [Longer context fine-tuning](https://www.anyscale.com/blog/fine-tuning-llms-for-longer-context-and-better-rag-systems)\n",
    "- [Continued fine-tuning from checkpoint](https://github.com/anyscale/templates/tree/main/templates/fine-tune-llm_v2/cookbooks/continue_from_checkpoint)\n",
    "- Training on more available hardware (ex. A10s) with model parallelism\n",
    "- [End-to-end LLM workflows (including batch data processing, batch inference)](https://www.anyscale.com/blog/end-to-end-llm-workflows-guide)\n",
    "- Distillation (Coming in <2 weeks)\n",
    "\n",
    "Serving:\n",
    "- [Deploy with autoscaling + optimize for latency vs. throughput](https://docs.anyscale.com/examples/deploy-llms/)\n",
    "- [Serving multiple LoRA adapters](https://docs.anyscale.com/llms/serving/guides/multi_lora/)\n",
    "- [Migration from OpenAI](https://docs.anyscale.com/llms/serving/guides/openai_to_oss/)\n",
    "- [Spot to on-demand fallback (vice versa)](https://docs.anyscale.com/1.0.0/configure/compute-configs/ondemand-to-spot-fallback/)\n",
    "- [Batch inference with vLLM](https://docs.anyscale.com/examples/batch-llm/)\n",
    "\n",
    "And more!\n",
    "- [Batch text embeddings with Ray data](https://github.com/anyscale/templates/tree/main/templates/text-embeddings)\n",
    "- [Production RAG applications](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1)\n",
    "- [Router](https://github.com/anyscale/llm-router) between different models (base, fine-tuned, closed-source) to optimize for cost and quality\n",
    "- Stable diffusion [fine-tuning](https://github.com/anyscale/templates/tree/main/templates/fine-tune-stable-diffusion) and [serving](https://github.com/anyscale/templates/tree/main/templates/serve-stable-diffusion)\n",
    "\n",
    "And if you're interested in using our hosted Anyscale or connecting it to your own cloud, reach out to us at [Anyscale](https://www.anyscale.com/get-started?utm_source=goku). And follow us on [Twitter](https://x.com/anyscalecompute) and [LinkedIn](https://www.linkedin.com/company/joinanyscale/) for more real-time updates on new features!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
