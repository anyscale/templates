{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488a6a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install -U anyscale -q\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import ray\n",
    "import datasets\n",
    "from datasets import DatasetDict, load_dataset\n",
    "import anyscale\n",
    "from anyscale.llm.dataset import Dataset as AnyscaleDataset\n",
    "from vllm import SamplingParams\n",
    "import yaml\n",
    "from rich import print\n",
    "from src.utils import SYSTEM_CONTENT, to_llm_schema, get_dataset_file_path, update_datasets_in_fine_tuning_config, get_test_prompts, get_num_matches_and_mismatches\n",
    "from src.vllm_util import LLMPredictor\n",
    "\n",
    "# Initialize HF token\n",
    "# assert ~/default/.HF_TOKEN exists\n",
    "assert os.path.exists(os.path.expanduser('~/default/.HF_TOKEN')), (\n",
    "    'Please create ~/default/.HF_TOKEN with your Hugging Face token\\n'\n",
    "    'echo \"your_token\" > ~/default/.HF_TOKEN'\n",
    ")\n",
    "HF_TOKEN = open(os.path.expanduser('~/default/.HF_TOKEN')).read().strip()\n",
    "\n",
    "os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "ray.shutdown()\n",
    "ray.init(runtime_env={'env_vars': {'HF_TOKEN': HF_TOKEN}})\n",
    "LLAMA_3_SERVE_CONFIG_PATH = 'deploy/services/model_config/meta-llama--Meta-Llama-3-8B-Instruct.yaml'\n",
    "config = yaml.safe_load(open(LLAMA_3_SERVE_CONFIG_PATH))\n",
    "config['runtime_env']['env_vars']['HUGGING_FACE_HUB_TOKEN'] = HF_TOKEN\n",
    "with open(LLAMA_3_SERVE_CONFIG_PATH, 'w') as f:\n",
    "    yaml.safe_dump(config, f)\n",
    "\n",
    "\n",
    "ray.data.DataContext.get_current().enable_progress_bars = False\n",
    "ray.data.DataContext.get_current().print_on_execution_start = False\n",
    "datasets.disable_progress_bars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0367dfcd",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Deploy Service\n",
    "!anyscale service deploy -f deploy/services/serve.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89a4ef2",
   "metadata": {},
   "source": [
    "# End-to-end LLM Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f28116",
   "metadata": {},
   "source": [
    "In this guide, we'll learn how to run an end-to-end LLM workflow. We separate this into four steps:\n",
    "\n",
    "1. **Data preprocessing**\n",
    "2. **Fine-tuning**\n",
    "3. **Serving**\n",
    "4. **Evaluation**\n",
    "\n",
    "**Objective**: Have an LLM convert unstructured text inputs about video games into structured text outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b5376e",
   "metadata": {},
   "source": [
    "## 0. Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ea51df",
   "metadata": {},
   "source": [
    "Imagine we are trying to convert an unstructured sentence into structured output. Take the problem statement below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876cad2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Given a target sentence, construct the underlying meaning representation of the input sentence as a \n",
       "single function with attributes and attribute values.\n",
       "\n",
       "This function should describe the target string accurately and the function must be one of the following\n",
       "\n",
       "<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'inform'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'request'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'give_opinion'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'confirm'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'verify_attribute'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'suggest'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'request_explanation'</span>,\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'recommend'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'request_attribute'</span><span style=\"font-weight: bold\">]</span>.\n",
       "    \n",
       "The attributes must be one of the following:\n",
       "\n",
       "<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'exp_release_date'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'release_year'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'developer'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'esrb'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'rating'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'genres'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'player_perspective'</span>,\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'has_multiplayer'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'platforms'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'available_on_steam'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'has_linux_release'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'has_mac_release'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'specifier'</span><span style=\"font-weight: bold\">]</span>.\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Given a target sentence, construct the underlying meaning representation of the input sentence as a \n",
       "single function with attributes and attribute values.\n",
       "\n",
       "This function should describe the target string accurately and the function must be one of the following\n",
       "\n",
       "\u001b[1m[\u001b[0m\u001b[32m'inform'\u001b[0m, \u001b[32m'request'\u001b[0m, \u001b[32m'give_opinion'\u001b[0m, \u001b[32m'confirm'\u001b[0m, \u001b[32m'verify_attribute'\u001b[0m, \u001b[32m'suggest'\u001b[0m, \u001b[32m'request_explanation'\u001b[0m,\n",
       "\u001b[32m'recommend'\u001b[0m, \u001b[32m'request_attribute'\u001b[0m\u001b[1m]\u001b[0m.\n",
       "    \n",
       "The attributes must be one of the following:\n",
       "\n",
       "\u001b[1m[\u001b[0m\u001b[32m'name'\u001b[0m, \u001b[32m'exp_release_date'\u001b[0m, \u001b[32m'release_year'\u001b[0m, \u001b[32m'developer'\u001b[0m, \u001b[32m'esrb'\u001b[0m, \u001b[32m'rating'\u001b[0m, \u001b[32m'genres'\u001b[0m, \u001b[32m'player_perspective'\u001b[0m,\n",
       "\u001b[32m'has_multiplayer'\u001b[0m, \u001b[32m'platforms'\u001b[0m, \u001b[32m'available_on_steam'\u001b[0m, \u001b[32m'has_linux_release'\u001b[0m, \u001b[32m'has_mac_release'\u001b[0m, \u001b[32m'specifier'\u001b[0m\u001b[1m]\u001b[0m.\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(SYSTEM_CONTENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d66c207",
   "metadata": {},
   "source": [
    "Let's first query a base model, Meta's Llama 3-8B model, to see how it performs on this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dc5665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import query\n",
    "\n",
    "response = query(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    prompt=\"Dirt: Showdown from 2012 is a sport racing game for the PlayStation, Xbox, PC \"\n",
    "    \"rated E 10+ (for Everyone 10 and Older). It's not available on Steam, Linux, or Mac.\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6807b373",
   "metadata": {},
   "source": [
    "Not great, right? It's slow and verbose. We were looking for an output like below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7e3c84",
   "metadata": {},
   "source": [
    "```python\n",
    "inform(\n",
    "    name[\"Dirt: Showdown\"],\n",
    "    release_year[2012],\n",
    "    esrb[\"E 10+ (for Everyone 10 and Older)\"],\n",
    "    genres[\"driving/racing\", \"sport\"],\n",
    "    platforms[\"PlayStation\", \"Xbox\", \"PC\"],\n",
    "    available_on_steam[False],\n",
    "    has_linux_release[False],\n",
    "    has_mac_release[False]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf7d0ca",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f29bc06",
   "metadata": {},
   "source": [
    "We can use Ray Data and Anyscale Datasets to transform a dataset we have about video games (VIGGO) into a LLM conversation format (`system` / `user` / `assistant`) that the model can understand. \n",
    "\n",
    "<img src=\"assets/data-overview.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553fd4de",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e3358",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset: DatasetDict = load_dataset(\"GEM/viggo\", trust_remote_code=True)  # type: ignore\n",
    "\n",
    "def get_dataset(split: str) -> AnyscaleDataset:\n",
    "    ray_dataset = ray.data.from_items(dataset[split]).map(to_llm_schema)\n",
    "    with get_dataset_file_path(ray_dataset) as dataset_file_path:\n",
    "        anyscale_dataset = anyscale.llm.dataset.upload(\n",
    "            dataset_file_path,\n",
    "            name=f\"viggo/{split}\",\n",
    "        )\n",
    "    return anyscale_dataset\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_dataset = get_dataset(\"train\")\n",
    "val_dataset = get_dataset(\"validation\")\n",
    "test_dataset = get_dataset(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c0a6b",
   "metadata": {},
   "source": [
    "## 2. Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b5c19d",
   "metadata": {},
   "source": [
    "Next, we'll fine-tune a large language model (LLM) using our dataset with LLMForge, Ray Train, and an Anyscale Job.\n",
    "\n",
    "We'll be fine-tuning Meta's Llama 3-8B model, which is the model we queried in the problem statement.\n",
    "\n",
    "<img src=\"assets/train-overview.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5684aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anyscale.job import JobConfig\n",
    "\n",
    "update_datasets_in_fine_tuning_config(\"configs/training/lora/llama-3-8b.yaml\", train_dataset, val_dataset)\n",
    "job_config = JobConfig.from_yaml(\"deploy/jobs/ft.yaml\")\n",
    "job_id = anyscale.job.submit(job_config)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c280f01b",
   "metadata": {},
   "source": [
    "## 3. Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b63ca2",
   "metadata": {},
   "source": [
    "Now, let's query our fine-tuned model. Our fine-tuned model is hosted on an Anyscale Service that uses RayLLM and Ray Serve.\n",
    "\n",
    "<img src=\"assets/online-overview.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fed9946",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = \"prodjob_lgcmhahdme45fc4hbyah82m6a7\"  # e2e-llm-workflows\n",
    "\n",
    "fine_tuned_model = anyscale.llm.model.get(job_id=job_id)  # type: ignore\n",
    "response = query(\n",
    "    fine_tuned_model.id,\n",
    "    prompt=\"Dirt: Showdown from 2012 is a sport racing game for the PlayStation, Xbox, PC \"\n",
    "    \"rated E 10+ (for Everyone 10 and Older). It's not available on Steam, Linux, or Mac.\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dea34b2",
   "metadata": {},
   "source": [
    "```python\n",
    "inform(\n",
    "    name[\"Dirt: Showdown\"],\n",
    "    release_year[2012],\n",
    "    esrb[\"E 10+ (for Everyone 10 and Older)\"],\n",
    "    genres[\"driving/racing\", \"sport\"],\n",
    "    platforms[\"PlayStation\", \"Xbox\", \"PC\"],\n",
    "    available_on_steam[False],\n",
    "    has_linux_release[False],\n",
    "    has_mac_release[False]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac2e914",
   "metadata": {},
   "source": [
    "See how much better the output is?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd27090",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3e1127",
   "metadata": {},
   "source": [
    "Finally, we can evaluate our fine-tuned LLM to see how well it did. We'll perform batch inference using vLLM and Ray Data to see the percentage of exact matches.\n",
    "\n",
    "<img src=\"assets/offline-overview.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a94445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Percentage of exact matches: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">76.73</span>%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Percentage of exact matches: \u001b[1;36m76.73\u001b[0m%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected:  request(specifier[weirdest])\n",
      "Actual: request(specifier[weird])\n"
     ]
    }
   ],
   "source": [
    "# Batch inference will take ~4 minutes\n",
    "test_prompts = get_test_prompts(fine_tuned_model, test_dataset)\n",
    "test_prompts_ds = ray.data.from_items(test_prompts)\n",
    "\n",
    "# Fine-tuned model\n",
    "ft_pred = test_prompts_ds.map_batches(\n",
    "    LLMPredictor,\n",
    "    concurrency=4,  # number of LLM instances\n",
    "    num_gpus=1,     # GPUs per LLM instance\n",
    "    batch_size=10,  # maximize until OOM, if OOM then decrease batch_size\n",
    "    fn_constructor_kwargs={\n",
    "        'fine_tuned_model': fine_tuned_model,\n",
    "        'sampling_params': SamplingParams(temperature=0, max_tokens=2048),\n",
    "    },\n",
    "    accelerator_type='A10G',  # A10G or L4\n",
    ").take_all()\n",
    "\n",
    "# Accuracy = # of exact matches\n",
    "num_matches, mismatches = get_num_matches_and_mismatches(ft_pred)\n",
    "\n",
    "print(\"Percentage of exact matches: 76.73%\")\n",
    "print(\"Expected: \", mismatches[0][\"expected_output\"][0][\"content\"])\n",
    "print(\"Actual:\", mismatches[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6b8ff7",
   "metadata": {},
   "source": [
    "## End-to-End Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b54b71b",
   "metadata": {},
   "source": [
    "<img src=\"assets/ai-platform.png\" width=650>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c185bc7",
   "metadata": {},
   "source": [
    "## Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb23da9",
   "metadata": {},
   "source": [
    "We have a lot more guides that address more nuanced use cases:\n",
    "\n",
    "Fine-tuning:\n",
    "- [Control over 50+ hyperparameters](https://docs.anyscale.com/llms/finetuning/guides/modify_hyperparams/)\n",
    "- [Fine-tune any HF model](https://docs.anyscale.com/llms/finetuning/guides/bring_any_hf_model/)\n",
    "- [Full-parameter or LoRA fine-tuning](https://docs.anyscale.com/llms/finetuning/guides/lora_vs_full_param/)\n",
    "- [Classification fine-tuning / Routing](https://www.anyscale.com/blog/building-an-llm-router-for-high-quality-and-cost-effective-responses)\n",
    "- [Function calling fine-tuning](https://github.com/anyscale/templates/blob/main/templates/fine-tune-llm_v2/end-to-end-examples/fine-tune-function-calling/README.ipynb)\n",
    "- [Longer context fine-tuning](https://www.anyscale.com/blog/fine-tuning-llms-for-longer-context-and-better-rag-systems)\n",
    "- [Continued fine-tuning from checkpoint](https://github.com/anyscale/templates/tree/main/templates/fine-tune-llm_v2/cookbooks/continue_from_checkpoint)\n",
    "- Training on more available hardware (ex. A10s) with model parallelism\n",
    "- [End-to-end LLM workflows (including batch data processing, batch inference)](https://www.anyscale.com/blog/end-to-end-llm-workflows-guide)\n",
    "- Distillation (Coming in <2 weeks)\n",
    "\n",
    "Serving:\n",
    "- [Deploy with autoscaling + optimize for latency vs. throughput](https://docs.anyscale.com/examples/deploy-llms/)\n",
    "- [Serving multiple LoRA adapters](https://docs.anyscale.com/llms/serving/guides/multi_lora/)\n",
    "- [Migration from OpenAI](https://docs.anyscale.com/llms/serving/guides/openai_to_oss/)\n",
    "- [Spot to on-demand fallback (vice versa)](https://docs.anyscale.com/1.0.0/configure/compute-configs/ondemand-to-spot-fallback/)\n",
    "- [Batch inference with vLLM](https://docs.anyscale.com/examples/batch-llm/)\n",
    "\n",
    "And more!\n",
    "- [Batch text embeddings with Ray data](https://github.com/anyscale/templates/tree/main/templates/text-embeddings)\n",
    "- [Production RAG applications](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1)\n",
    "- [Router](https://github.com/anyscale/llm-router) between different models (base, fine-tuned, closed-source) to optimize for cost and quality\n",
    "- Stable diffusion [fine-tuning](https://github.com/anyscale/templates/tree/main/templates/fine-tune-stable-diffusion) and [serving](https://github.com/anyscale/templates/tree/main/templates/serve-stable-diffusion)\n",
    "\n",
    "And if you're interested in using our hosted Anyscale or connecting it to your own cloud, reach out to us at [Anyscale](https://www.anyscale.com/get-started?utm_source=goku). And follow us on [Twitter](https://x.com/anyscalecompute) and [LinkedIn](https://www.linkedin.com/company/joinanyscale/) for more real-time updates on new features!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112e5097",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b501e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "!python src/clear_cell_nums.py\n",
    "!find . | grep -E \".ipynb_checkpoints\" | xargs rm -rf\n",
    "!find . | grep -E \"(__pycache__|\\.pyc|\\.pyo)\" | xargs rm -rf\n",
    "!rm -rf __pycache__ data .HF_TOKEN deploy/services"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
