{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy, configure, and serve LLMs \n",
    "\n",
    "**⏱️ Time to complete**: 10 min (20 on GCP)\n",
    "\n",
    "This guide walks you through how to deploy optimized LLMs in Anyscale through RayLLM. It includes a number of pre-tuned configs for Llama2, Mistral, Mixtral, embedding models, and more in the `models` directory.\n",
    "\n",
    "You can also find more advanced tutorials in the `cookbooks/` folder, including those for:\n",
    "- Embedding generation\n",
    "- Deploying custom models\n",
    "- Deploying LoRA and function-calling models\n",
    "- Deploying vision language models\n",
    "- How to configure autoscaling and other optimization parameters\n",
    "\n",
    "**Note**: This guide is hosted within an Anyscale workspace, which provides easy access to compute resources. Check out the `Introduction to Workspaces` template for more details.\n",
    "\n",
    "## Step 1 - Run the model locally in the Workspace\n",
    "\n",
    "We provide a starter command to run Llama and Mistral-family models via Ray Serve. You can specify the arguments, such as Lora, GPU type and tensor parallelism via the command. You can also follow the [guide](cookbooks/CustomModels.ipynb) to bring your own models.\n",
    "\n",
    "Please note that if you would like to serve a model whose architecture is different from the provided list of models, we highly recommend you manually going over the generated model config file to provide the correct values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the configuration file, run the following command directly in your terminal:\n",
    "```\n",
    "python generate_config.py\n",
    "```\n",
    "**Note:** This command requires interactive inputs and should be executed directly in the terminal, not within a Jupyter notebook cell.\n",
    "\n",
    "The command will generate 2 files - a model config file (saved in `model_config/`) and a serve config file (`serve_TIMESTAMP.yaml`) that you can reference and re-run in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you didn't start the serve application in the previous step, you can start it using the following command (replace the file name with the generated `serve_` file name):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-22 18:31:24,455\tINFO scripts.py:499 -- Running import path: 'serve_TIMESTAMP.yaml'.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/bin/serve\", line 8, in <module>\n",
      "    sys.exit(cli())\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1157, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1078, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1688, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1434, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/click/core.py\", line 783, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/serve/scripts.py\", line 501, in run\n",
      "    import_attr(import_path), args_dict\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/utils.py\", line 1191, in import_attr\n",
      "    module = importlib.import_module(module_name)\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 984, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'serve_TIMESTAMP'\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!serve run serve_TIMESTAMP.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Query the model\n",
    "\n",
    "Once deployed you can use the OpenAI SDK to interact with the models, ensuring an easy integration for your applications.\n",
    "\n",
    "Run the following command to query. You should get the following output:\n",
    "```\n",
    "The top rated restaurants in San Francisco include:\n",
    " • Chez Panisse\n",
    " • Momofuku Noodle Bar\n",
    " • Nopa\n",
    " • Saison\n",
    " • Mission Chinese Food\n",
    " • Sushi Nakazawa\n",
    " • The French Laundry\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RayLLM uses an OpenAI-compatible API, allowing us to use the OpenAI SDK to query the LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x706462744f40>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def query(base_url: str, api_key: str):\n",
    "    if not base_url.endswith(\"/\"):\n",
    "        base_url += \"/\"\n",
    "    \n",
    "    if \"/routes\" in base_url:\n",
    "        raise ValueError(\"base_url must end with '.com'\")\n",
    "\n",
    "    client = OpenAI(\n",
    "      base_url=base_url + \"v1\",\n",
    "      api_key=api_key,\n",
    "    )\n",
    "\n",
    "    # List all models.\n",
    "    models = client.models.list()\n",
    "    print(models)\n",
    "\n",
    "    # Note: not all arguments are currently supported and will be ignored by the backend.\n",
    "    chat_completions = client.chat.completions.create(\n",
    "        model=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What are some of the highest rated restaurants in San Francisco?'.\"},\n",
    "        ],\n",
    "        temperature=0.01,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    for chat in chat_completions:\n",
    "        if chat.choices[0].delta.content is not None:\n",
    "            print(chat.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Query the local serve application we just deployed.\n",
    "\n",
    "query(\"http://localhost:8000\", \"NOT A REAL KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Deploying a production service\n",
    "\n",
    "To deploy an application with one model as an Anyscale Service, update the file name to the generated one and run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the serve app to production with a given service name.\n",
    "# Reference the serve file created in step 1\n",
    "!anyscale service deploy -f serve_TIMESTAMP.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "After the command runs, click the deploy notification (or navigate to ``Home > Services``) to access the Service UI:\n",
    "\n",
    "<img src=\"assets/service-notify.png\" width=500px/>\n",
    "\n",
    "Navigate to the Service UI and wait for the service to reach \"Active\". It will begin in \"Starting\" state:\n",
    "\n",
    "<img src=\"assets/service-starting.png\" width=600px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4 - Query the service endpoint\n",
    "\n",
    "The above command should print something like `(anyscale +2.9s) curl -H 'Authorization: Bearer XXXXXXXXX_XXXXXX-XXXXXXXXXXXX' https://YYYYYYYYYYYY.anyscaleuserdata.com`, which contains information you need to query the service.\n",
    "\n",
    "You can also find this information by clicking the \"Query\" button in the Service UI.\n",
    "\n",
    "<img src=\"assets/service-query.png\" width=600px/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the remote serve application we just deployed.\n",
    "\n",
    "service_url = \"https://YYYYYYYYYYYYY.anyscaleuserdata.com\"  # FILL ME IN\n",
    "service_bearer_token = \"XXXXXXXXXX_XXXXXXX-XXXXXXXXXXXXXX\"  # FILL ME IN\n",
    "\n",
    "query(service_url, service_bearer_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Final workds on RayLLM\n",
    "\n",
    "RayLLM makes it easy for LLM Developers to interact with OpenAI compatible APIs for their applications by providing an easy to manage backend for serving OSS LLMs.\n",
    "\n",
    "It provides a number of features making LLM development easy, including:\n",
    "- An extensive suite of pre-configured open source LLMs and embedding models.\n",
    "- An OpenAI compatible REST API.\n",
    "\n",
    "As well as operational features for efficient scaling of LLM apps:\n",
    "- Optimizations such as continuous batching, quantization and streaming.\n",
    "- Production-grade autoscaling support, including scale-to-zero.\n",
    "- Native multi-GPU & multi-node model deployments.\n",
    "\n",
    "## Cookbooks\n",
    "\n",
    "After you are done with the above, you can find recipies that extend the functionality of this template under the cookbooks folder:\n",
    "\n",
    "* [Deploy models for embedding generation](cookbooks/embedding/EmbeddingModels.ipynb)\n",
    "* [Deploy multiple LoRA fine-tuned models](cookbooks/lora/DeployLora.ipynb)\n",
    "* [Deploy Function calling models](cookbooks/function_calling/DeployFunctionCalling.ipynb)\n",
    "* [Deploy Vision Language Model](cookbooks/vision_language_model/README.ipynb): Build a Gradio appliation on top of LLaVA-NeXT Mistral 7B.\n",
    "* [Learn how to bring your own models](cookbooks/CustomModels.ipynb)\n",
    "* [Learn how to leverage different configurations that can optimize the latency and throughput of your models](cookbooks/OptimizeModels.ipynb)\n",
    "* [Learn how to fully configure your deployment including auto-scaling, optimization parameters and tensor-parallelism](cookbooks/AdvancedModelConfigs.ipynb)\n",
    "\n",
    "\n",
    "## End-to-end Examples\n",
    "\n",
    "See examples of building applications with your deployed endpoint on Anyscale. Be sure to update the `api_base` and `token` for your private deployment. This information can be found under the \"Query\" button in the Anyscale Service UI.\n",
    "\n",
    "* [Build Tool Calling Feature for Any LLM via JSON Mode](./end-to-end-examples/function_calling/README.md): This example demonstrates how to build a tool calling feature for any LLM via JSON mode.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
