{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy vision language models\n",
    "\n",
    "**⏱️ Time to complete**: 10 min\n",
    "\n",
    "In this example, we will use RayLLM to serve [llava-hf/llava-v1.6-mistral-7b-hf](https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf). We will build an interesting application that brings art to life with ekphrasis, specially designed for kids. \n",
    "\n",
    "**Note**: This guide assumes you have reviewed the contents of the ../../README.ipynb. It extends that guide by focusing on how to query a vision language model with images. Additionally, it provides an example for developing a Gradio application on Workspace.\n",
    "\n",
    "## Step 1 - Run the model locally in the Workspace\n",
    "\n",
    "Similarly to the main guide, let's do in the terminal:\n",
    "```\n",
    "python generate_config.py\n",
    "```\n",
    "And start the serve application using (replace the file name with the generated `serve_` file name):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!serve run serve_TIMESTAMP.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Query the model\n",
    "\n",
    "Inspect the example kid drawings under `example_images/` folder.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/example_1.jpeg\" alt=\"Image 1\" width=\"250\" style=\"margin-right: 10px;\">\n",
    "  <img src=\"./assets/example_2.jpeg\" alt=\"Image 2\" width=\"250\" style=\"margin-right: 10px;\">\n",
    "  <img src=\"./assets/example_3.jpeg\" alt=\"Image 3\" width=\"250\">\n",
    "</p>\n",
    "\n",
    "For more images, download from the following s3 path.\n",
    "\n",
    "`s3://air-example-data-2/llava_example_kid_drawings/`\n",
    "\n",
    "For querying you can use the OpenAI SDK to interact with the models, ensuring an easy integration for your applications.\n",
    "Specifically for vision language models, images can be passed in using either image url or based64 encoded string.\n",
    "Notice that both scripts query in a streaming fashion.\n",
    "\n",
    "**Note:** LLaVA-NeXT supports only single image and single user message for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Query with image url\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "def query(base_url: str, api_key: str):\n",
    "\n",
    "   client = OpenAI(\n",
    "     base_url=base_url,\n",
    "     api_key=api_key,\n",
    "   )\n",
    "   chat_completions = client.chat.completions.create(\n",
    "       model=\"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "       messages=[\n",
    "           {\"role\": \"user\", \"content\": [\n",
    "               {\"type\": \"text\", \"text\": \"Write me a poetry for kid based on this image.\"},\n",
    "               {\"type\": \"image_url\", \"image_url\": {\n",
    "                   \"url\": \"https://air-example-data-2.s3.amazonaws.com/llava_example_kid_drawings/0.JPG\"}}]}\n",
    "       ],\n",
    "       temperature=0.01,\n",
    "       stream=True\n",
    "   )\n",
    "\n",
    "   for chat in chat_completions:\n",
    "       if chat.choices[0].delta.content is not None:\n",
    "           print(chat.choices[0].delta.content, end=\"\")\n",
    "\n",
    "query(\"http://localhost:8000/v1\", \"NOT A REAL KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Query with base64 encoded string\n",
    "\n",
    "```python\n",
    "from openai import OpenAI\n",
    "import base64\n",
    "\n",
    "\n",
    "def encode_image_to_base64(image_path): \n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def query(base_url: str, api_key: str):\n",
    "\n",
    "    client = OpenAI(\n",
    "      base_url=base_url,\n",
    "      api_key=api_key,\n",
    "    )\n",
    "\n",
    "    path = \"/mnt/local_storage/kid_drawings/0.JPG\"\n",
    "    chat_completions = client.chat.completions.create(\n",
    "        model=\"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What is the content of the image?\"}, \n",
    "                {\"type\": \"image_url\", \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{encode_image_to_base64(path)}\"}}]}\n",
    "        ],\n",
    "        temperature=0.01,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    for chat in chat_completions:\n",
    "        if chat.choices[0].delta.content is not None:\n",
    "            print(chat.choices[0].delta.content, end=\"\")\n",
    "\n",
    "query(\"http://localhost:8000/v1\", \"NOT A REAL KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Run the Gradio application!\n",
    "\n",
    "Now, instead of deploying to production, let's actually build a Gradio application on top of this.\n",
    "\n",
    "Run the following command to start a Gradio application on port 7860. Notice the script uses non streaming query fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python gradio_app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyscale workspace has port forwarding conveniently configured. Navigate to the “port” tab and click the corresponding url (the one with port 7680). This will open a local web browser that directly talks to, in our case, the Gradio application that runs on the workspace.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/where_ports.png\" alt=\"where ports\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/ports_list.png\" alt=\"ports list\">\n",
    "</p>\n",
    "\n",
    "Now let’s get ready for some verses and rhymes!\n",
    "\n",
    "Paste the following url to the input text box and click submit:\n",
    "\n",
    "https://air-example-data-2.s3.amazonaws.com/llava_example_kid_drawings/0.JPG\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/gradio_app.png\" alt=\"gradio\">\n",
    "</p>\n",
    "\n",
    "*In a land where books bloom,*  \n",
    "*A castle of stories, a dreamy room.*  \n",
    "*Where children explore,*  \n",
    "*And tales unfold, forevermore.*  \n",
    "\n",
    "*With steps that lead to the sky,*  \n",
    "*And a pool that glistens, oh so high.*  \n",
    "*Where laughter echoes,*  \n",
    "*And imagination grows,*  \n",
    "*In this magical place, where stories flow.*  \n",
    "\n",
    "\n",
    "# Summary\n",
    "\n",
    "Congrats! You have now served and queried [llava-hf/llava-v1.6-mistral-7b-hf](https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf). As a quick recap, here's what we demonstrated in this notebook:\n",
    "1. Run the model locally in a workspace.\n",
    "2. Query the model with images.\n",
    "3. Build a Gradio application on top.\n",
    "\n",
    "Hope that you enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
