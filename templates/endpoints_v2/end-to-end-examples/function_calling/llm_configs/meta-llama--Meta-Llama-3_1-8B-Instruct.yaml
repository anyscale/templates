runtime_env:
  env_vars:
    HUGGING_FACE_HUB_TOKEN: insert_your_hf_token_here

model_loading_config:
  model_id: meta-llama/Meta-Llama-3.1-8B-Instruct
  model_source: meta-llama/Meta-Llama-3.1-8B-Instruct

engine_kwargs:
  enable_chunked_prefill: true
  max_num_batched_tokens: 2048
  max_num_seqs: 64
  tokenizer_pool_extra_config:
    runtime_env:
      pip: null
  tokenizer_pool_size: 2
  trust_remote_code: true
  max_model_len: 8192
  tensor_parallel_size: 1

accelerator_type: A10G

deployment_config:
  autoscaling_config:
    target_ongoing_requests: 32
  max_ongoing_requests: 64
