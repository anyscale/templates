deployment_config:
  autoscaling_config:
    target_ongoing_requests: 32
  max_ongoing_requests: 64
  ray_actor_options:
    resources:
      accelerator_type:A10G: 0.001
    runtime_env:
      env_vars:
        ANYSCALE_VLLM_ENABLE_JSON_MODE: '1'
        ANYSCALE_VLLM_NUM_PROCESSOR_WORKERS: '8'
        ANYSCALE_VLLM_USE_V2: '1'
        HUGGING_FACE_HUB_TOKEN: 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
        RECREATE_FAILED_ACTORS: '1'
engine_config:
  engine_kwargs:
    enable_chunked_prefill: true
    max_num_batched_tokens: 2048
    max_num_seqs: 64
    tokenizer_pool_extra_config:
      runtime_env:
        pip: null
    tokenizer_pool_size: 2
    trust_remote_code: true
  generation:
    prompt_format:
      assistant: '<|start_header_id|>assistant<|end_header_id|>


        {instruction}<|eot_id|>'
      bos: <|begin_of_text|>
      default_system_message: ''
      system: '<|start_header_id|>system<|end_header_id|>


        {instruction}<|eot_id|>'
      system_in_user: false
      trailing_assistant: '<|start_header_id|>assistant<|end_header_id|>


        '
      user: '<|start_header_id|>user<|end_header_id|>


        {instruction}<|eot_id|>'
    stopping_sequences:
    - <|end_of_text|>
    - <|eot_id|>
  hf_model_id: meta-llama/Meta-Llama-3.1-8B-Instruct
  max_total_tokens: 8192
  model_id: meta-llama/Meta-Llama-3.1-8B-Instruct
  runtime_env:
    env_vars:
      ANYSCALE_VLLM_ENABLE_JSON_MODE: '1'
      ANYSCALE_VLLM_NUM_PROCESSOR_WORKERS: '8'
      ANYSCALE_VLLM_USE_V2: '1'
      RECREATE_FAILED_ACTORS: '1'
  type: VLLMEngine
scaling_config:
  num_cpus_per_worker: 8
  num_gpus_per_worker: 1
  num_workers: 1
  placement_strategy: STRICT_PACK
  resources_per_worker:
    accelerator_type:A10G: 0.001
standalone_function_calling_model: true
