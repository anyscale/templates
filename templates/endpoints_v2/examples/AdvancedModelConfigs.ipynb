{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Model Configuration\n",
    "\n",
    "Each model is defined by a YAML configuration file. We recommend starting with the `generate_config` script and adding further modifications as necessary.\n",
    "\n",
    "## Modify an existing model\n",
    "\n",
    "Each config file consists of three sections:\n",
    "\n",
    "- `deployment_config`,\n",
    "- `engine_config`,\n",
    "- `scaling_config`.\n",
    "\n",
    "It's best to check out examples of existing models to see how they are configured.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Deployment config\n",
    "\n",
    "The `deployment_config` section corresponds to\n",
    "[Ray Serve configuration](https://docs.ray.io/en/latest/serve/production-guide/config.html)\n",
    "and specifies how to [auto-scale the model](https://docs.ray.io/en/latest/serve/autoscaling-guide.html)\n",
    "(via `autoscaling_config`) and what specific options you may need for your model deployments (using `ray_actor_options`). We recommend using the values from our sample configuration files for `metrics_interval_s`, `look_back_period_s`, `smoothing_factor`, `downscale_delay_s` and `upscale_delay_s`. These are the configuration options you may want to modify:\n",
    "\n",
    "* `min_replicas`, `initial_replicas`, `max_replicas` - Minimum, initial and maximum number of replicas of the model to deploy on your Ray cluster.\n",
    "* `max_ongoing_requests` - Maximum number of concurrent requests that a Ray Serve replica can process at a time. Additional requests are queued at the proxy.\n",
    "* `target_ongoing_requests` - Guides the auto-scaling behavior. If the average number of ongoing requests across replicas is above this number, Ray Serve attempts to scale up the number of replicas, and vice-versa for downscaling. We typically set this to ~60% of the `max_ongoing_requests`.\n",
    "* `ray_actor_options` - Similar to the `resources_per_worker` configuration in the `scaling_config`. Refer to the `scaling_config` section for more guidance.\n",
    "* `smoothing_factor` - The multiplicative factor to amplify or moderate each upscaling or downscaling decision. A value less than 1.0 will slow down the scaling decision made in each step. See [advanced auto-scaling guide](https://docs.ray.io/en/latest/serve/advanced-guides/advanced-autoscaling.html#optional-define-how-the-system-reacts-to-changing-traffic) for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Engine config\n",
    "\n",
    "Engine is the abstraction for interacting with a model. It is responsible for scheduling and running the model.\n",
    "\n",
    "The `engine_config` section specifies the Hugging Face model ID (`model_id`), how to initialize it and what parameters to use when generating tokens with an LLM.\n",
    "\n",
    "RayLLM supports continuous batching, meaning incoming requests are processed as soon as they arrive, and can be added to batches that are already being processed. This means that the model is not slowed down by certain sentences taking longer to generate than others. RayLLM also supports quantization, meaning compressed models can be deployed with cheaper hardware requirements. \n",
    "\n",
    "* `model_id` is the ID that refers to the model in the RayLLM or OpenAI API.\n",
    "* `type` is the type of  inference engine. Only `VLLMEngine` is currently supported.\n",
    "* `engine_kwargs` and `max_total_tokens` are configuration options for the inference engine (e.g. gpu memory utilization, quantization, max number of concurrent sequences). These options may vary depending on the hardware accelerator type and model size. We have tuned the parameters in the configuration files included in RayLLM for you to use as reference.\n",
    "* `generation` contains configurations related to default generation parameters such as `prompt_format` and `stopping_sequences`.\n",
    "* `hf_model_id` is the Hugging Face model ID. If not specified, defaults to `model_id`.\n",
    "* `runtime_env` is a dictionary that contains Ray runtime environment configuration. It allows you to set per-model pip packages and environment variables. See [Ray documentation on Runtime Environments](https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#runtime-environments) for more information.\n",
    "* `s3_mirror_config` is a dictionary that contains configuration for loading the model from S3 instead of Hugging Face Hub. You can use this to speed up downloads.\n",
    "* `gcs_mirror_config` is a dictionary that contains configuration for loading the model from Google Cloud Storage instead of Hugging Face Hub. You can use this to speed up downloads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Scaling config\n",
    "\n",
    "Finally, the `scaling_config` section specifies what resources should be used to serve the model - this corresponds to Ray [ScalingConfig](https://docs.ray.io/en/latest/train/api/doc/ray.train.ScalingConfig.html). Note that the `scaling_config` applies to each model replica, and not the entire model deployment (in other words, each replica will have `num_workers` workers).\n",
    "\n",
    "* `num_workers` - Number of workers (created as Ray Actors) for each replica of the model. This controls the tensor parallelism for the model.\n",
    "* `num_gpus_per_worker` - Number of GPUs to be allocated per worker. This should always be 1.\n",
    "* `num_cpus_per_worker` - Number of CPUs to be allocated per worker. Usually set to 8.\n",
    "* `placement_strategy` - Ray supports different [placement strategies](https://docs.ray.io/en/latest/ray-core/scheduling/placement-group.html#placement-strategy) for guiding the physical distribution of workers. To ensure all workers are on the same node, use \"STRICT_PACK\".\n",
    "* `resources_per_worker` - we use `resources_per_worker` to set [Ray custom resources](https://docs.ray.io/en/latest/ray-core/scheduling/resources.html#id1) and place the models on specific node types. An example configuration of `resources_per_worker` involves setting `accelerator_type:L4` to 0.001 for a Llama-2-7b model to be deployed on an L4 GPU. This must always be set to 0.001. The `num_gpus_per_worker` configuration along with number of GPUs available on the node will determine the number of workers Ray schedules on the node. The supported accelerator types are: T4, L4, A10G, A100-40G, A100-80G and H100.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## My deployment isn't starting/working correctly, how can I debug?\n",
    "\n",
    "There can be several reasons for the deployment not starting or not working correctly. Here are some things to check:\n",
    "1. You might have specified an invalid model id.\n",
    "2. Your model is a gated Hugging Face model (eg. meta-llama). In that case, you need to set the `HUGGING_FACE_HUB_TOKEN` environment variable cluster-wide. You can do that either in the Ray cluster configuration or by setting it before running `serve run`.\n",
    "3. Your model may be running out of memory. You can usually spot this issue by looking for keywords related to \"CUDA\", \"memory\" and \"NCCL\" in the replica logs or `serve run` output. In that case, consider reducing the `max_batch_prefill_tokens` and `max_batch_total_tokens` (if applicable).\n",
    "\n",
    "In general, [Ray Dashboard](https://docs.ray.io/en/latest/serve/monitoring.html#ray-dashboard) is a useful debugging tool, letting you monitor your application and access Ray logs.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
