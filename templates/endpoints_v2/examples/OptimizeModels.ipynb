{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Models\n",
    "\n",
    "Run the `generate_config.py` script (`python generate_configy.py`) to generate model configurations for different accelerator types and tensor parallelism levels. The supported accelerator types are:\n",
    "T4, L4, A10G, A100-40G and A100-80G.\n",
    "\n",
    "Tensor parallelism is a type of model parallelism in which specific model weights, gradients, and optimizer states are split across devices. This typically involves distributed computation of specific operations, modules, or layers of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Configurations to optimize\n",
    "\n",
    "Update these configurations for different tensor parallelism levels or hardware types:\n",
    "1. The `num_workers` configuration is used to set the tensor parallelism for a model. A higher value for tensor parallelism will typically lead to lower latency at the cost of more GPUs per replica.\n",
    "2. `resources_per_worker` in `scaling_config` and `resources` under `ray_actor_options` in the `deployment_config`. These determine the accelerator type used. It must follow the format of `\"accelerator_type:T4\":0.01`.\n",
    "3. `engine_kwargs`: The full list of available engine arguments when using vLLM are [here](https://docs.vllm.ai/en/stable/models/engine_args.html). There are various configurations to consider while optimizing. For example, `max_num_batched_tokens` and `max_num_seqs`. These are the maximum number of batched tokens and sequences configured for each iteration in vLLM. With increase in available GPU memory, you can increase these values. \n",
    "4. `autoscaling_config`: `max_concurrent_queries` - the maximum number of queries that will be handled concurrently by each replica of the model (should be set equal to `max_num_seqs`) and `target_num_ongoing_requests_per_replica` - the number of ongoing requests per replica that will trigger auto-scaling. Similar to the arguments above, these can be increased as the GPU memory changes.\n",
    "\n",
    "You may consider optimizing for either latency or throughput. The per-request latency generally degrades as the number of concurrent requests increase. The provided configurations generally optimize for latency. We recommend starting with one of our configurations and running load tests if you would like to tune any of the above parameters.\n",
    "\n",
    "Note - You can only run one configuration for a model id at a time.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
