{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving Embedding Models\n",
    "\n",
    "We support serving embedding models available in HuggingFace as well as optimizing these with ONNX. Sample configs are available in the `sample_model_configs/embedding_models` folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Setting up Model\n",
    "\n",
    "See an example for serving embedding models in `embedding-serve.yaml`. Notably the `args` field in the yaml file needs to contain the `embedding_models` field. This field contains a list of YAML files (in the `models` directory) for the embedding models you want to deploy.\n",
    "\n",
    "In order to deploy an embedding model run:\n",
    "```bash\n",
    "$ serve run examples/embedding/embedding-serve.yaml\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Querying Embedding Models\n",
    "\n",
    "You can use the OpenAI SDK to query the embedding models. Batch queries are also supported. In order to query the example above, run:\n",
    "\n",
    "```bash\n",
    "$ python examples/embedding/embedding-query.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Deploying a production service\n",
    "\n",
    "To deploy an application with the embedding model as an Anyscale Service you can run:\n",
    "\n",
    "```bash\n",
    "$ anyscale service deploy --name=my_service_name -f examples/embedding/embedding-serve.yaml\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Optimizing Embedding Models\n",
    "\n",
    "We support optimizing embedding models with ONNX. In order to enable this, set the flag under `engine_config` in your model yaml file. See `sample_model_configs/embedding_models\\BAAI--bge-large-en-v1.5.yaml` for an example.\n",
    "\n",
    "```bash\n",
    "engine_config:\n",
    "  ...\n",
    "  optimize: onnx\n",
    "```\n",
    "\n",
    "By default, the embedding models are setup to run on CPU. You can modify the configuration to run it on GPU instead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
