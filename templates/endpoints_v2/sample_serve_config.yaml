applications:
  - args:
      models: []
      vllm_base_models:
        - "./sample_model_configs/llama/codellama--CodeLlama-34b-Instruct-hf_a100-40g_tp4.yaml"
      multiplex_lora_adapters: []
      function_calling_models: []
    runtime_env:
      env_vars:
        HUGGING_FACE_HUB_TOKEN: "UPDATE HF TOKEN HERE"  # <-- change this to a real token
    import_path: aviary_private_endpoints.backend.server.run:router_application
    route_prefix: /
    name: llm-endpoint
