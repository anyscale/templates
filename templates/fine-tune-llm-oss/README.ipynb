{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning LLMs with LLaMA-Factory on Anyscale\n",
    "\n",
    "**⏱️ Time to complete**: ~20 mins, which includes the time to train the model\n",
    "\n",
    "Looking to get the most out of your LLM workloads? Fine-tuning pretrained LLMs can often be the most cost effective way to improve model performance on the tasks that you care about. This template walks through how to use the popular open source library [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) to fine-tune LLMs on Anyscale. LLaMA-Factory comes with a [Ray](https://www.ray.io/) integration for scaling training to multiple GPUs and nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Getting started with LLaMA-Factory \n",
    "First, you need to install the LLaMA-Factory code. You can view the latest changes from the [LLaMA-Factory GitHub](https://github.com/hiyouga/LLaMA-Factory.git). \n",
    "\n",
    "```bash\n",
    "git clone --branch v0.9.3 --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
    "cd LLaMA-Factory\n",
    "# Install extras separately so Anyscale can track the dependencies on worker nodes.\n",
    "pip install torch==2.5.0 torchvision==0.20.0 torchaudio==2.5.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "pip install deepspeed==0.16.4 transformers==4.51.3 jieba nltk rouge-chinese \n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "Next, you need to set your `HF_TOKEN` and `WANDB_API_KEY` environment variables. It's recommended to set them in the environment variables section of the [**Dependencies** tab](https://docs.anyscale.com/configuration/dependency-management/dependency-development/#environment-variables) in Workspaces. Simply running `export HF_TOKEN=<HF_TOKEN_HERE>`, or `huggingface-cli login` works for single node compute configurations, but doesn't propagate the environment variables to worker nodes.\n",
    "\n",
    "Additionally it's recommended to use `hf_transfer` for fast model downloading by running `pip install hf_transfer`, and setting `HF_HUB_ENABLE_HF_TRANSFER=1` in the **Dependencies** tab. Setting `HF_HUB_ENABLE_HF_TRANSFER=1` without first installing `hf_transfer` will cause errors. An example of setting relevant environment variables in the **Dependencies** tab is shown below.\n",
    "\n",
    "<img src=\"assets/env_vars.png\" width=1500px />\n",
    "\n",
    "You can find some example configs that use Ray with LLaMA-Factory for pretraining and SFT (instruction tuning) in the `llamafactory_config` directory. Find a full set of examples with various models, tasks, and datasets on the [LLaMA-Factory GitHub](https://github.com/hiyouga/LLaMA-Factory/tree/main/examples). \n",
    "\n",
    "LLaMA-Factory provides a CLI `llamafactory-cli` that allows you to launch training with a config YAML file. For example, you can launch a supervised fine-tuning job with Ray by running the following command:\n",
    "```bash\n",
    "cd .. # Return to top level directory.\n",
    "USE_RAY=1 llamafactory-cli train llamafactory_configs/llama3_lora_sft_ray.yaml\n",
    "```\n",
    "This command runs an instruction tuning training job with `Meta-Llama-3-8B-Instruct` on an example subset of the alpaca dataset. By default, Ray logs training statistics with all installed logging libraries, like W&B, MLflow, comet, tensorboard, because LLaMA-Factory relies on Hugging Face's [Trainer integration](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments.report_to) for logging. To specify a single specific library to log with, set `report_to: <LIBRARY_NAME>` in the config YAML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing your dataset\n",
    "\n",
    "The above config (`llamafactory_configs/llama3_lora_sft_ray.yaml`) runs out of the box, because it assumes that the dataset and dataset information is located on the Hugging Face hub, and specifies it as below:\n",
    "```yaml\n",
    "...\n",
    "### dataset\n",
    "dataset: identity,alpaca_en_demo\n",
    "dataset_dir: REMOTE:llamafactory/demo_data  # or use local absolute path\n",
    "...\n",
    "```\n",
    "\n",
    "However, by default LLaMA-Factory reads dataset information from a local file `dataset_info.json`, which you can find in `LLaMA-Factory/data/`)\n",
    "\n",
    "To specify new datasets that are accessible across Ray worker nodes, you must first add `dataset_info.json` and any data files to shared storage like `/mnt/cluster_storage`. \n",
    "\n",
    "For example, if you wanted to run pretraining on the `c4_demo` dataset, first go through the following setup steps:\n",
    "```bash\n",
    "# Create a copy of the data in /mnt/cluster_storage\n",
    "cp LLaMA-Factory/data/c4_demo.json /mnt/cluster_storage/\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "# Create `/mnt/cluster_storage.dataset_info.json` containing:\n",
    "# {\n",
    "#     \"c4_demo\": {\n",
    "#         \"file_name\": \"/mnt/cluster_storage/c4_demo.json\",\n",
    "#         \"columns\": {\n",
    "#         \"prompt\": \"text\"\n",
    "#         }\n",
    "#     },\n",
    "# }\n",
    "echo '{\"c4_demo\":{\"file_name\":\"/mnt/cluster_storage/c4_demo.json\",\"columns\":{\"prompt\":\"text\"}}}' > /mnt/cluster_storage/dataset_info.json\n",
    "```\n",
    "\n",
    "In `llamafactory_configs/llama3_lora_pretrain_ray.yaml` the relevant `dataset_dir` argument is already specified.\n",
    "```yaml\n",
    "...\n",
    "### dataset\n",
    "dataset: c4_demo\n",
    "dataset_dir: /mnt/cluster_storage  # or use local absolute path\n",
    "...\n",
    "```\n",
    "\n",
    "To launch fine-tuning with this local dataset, you can run\n",
    "```bash\n",
    "USE_RAY=1 llamafactory-cli train llamafactory_configs/llama3_lora_pretrain_ray.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LLaMA-Factory in an Anyscale job\n",
    "To run LLaMA-Factory as an Anyscale job, create a [custom container image](https://docs.anyscale.com/configuration/dependency-management/dependency-container-images/#customizing-a-container-image) that comes with LLaMA-Factory installed. You can specify the relevant packages in your dockerfile. For example, you could create an image using the `anyscale/ray-ml:2.42.0-py310-gpu` base image as follows.\n",
    "\n",
    "```docker\n",
    "# Start with an Anyscale base image.\n",
    "FROM anyscale/ray-ml:2.42.0-py310-gpu\n",
    "WORKDIR /app\n",
    "RUN git clone --branch v0.9.3 --depth 1 https://github.com/hiyouga/LLaMA-Factory.git && \\\n",
    "    cd LLaMA-Factory && \\\n",
    "    pip install torch==2.5.0 torchvision==0.20.0 torchaudio==2.5.0 --index-url https://download.pytorch.org/whl/cu121 && \\\n",
    "    pip install --no-cache-dir deepspeed==0.16.4 transformers==4.51.3 jieba nltk rouge-chinese && \\\n",
    "    pip install -e . \n",
    "```\n",
    "\n",
    "You can then use this new image to run your job with the [Anyscale jobs CLI](https://docs.anyscale.com/platform/jobs/manage-jobs) locally or from a workspace. We provide an example job config in `sft_job_config.yaml` for running a job from this workspace, which contains the following:\n",
    "\n",
    "```yaml\n",
    "name: llama3-lora-sft-ray\n",
    "image_uri: <your_image_uri>:<version>\n",
    "requirements:\n",
    "  - hf_transfer\n",
    "env_vars:\n",
    "  WANDB_API_KEY: <your_wandb_api_key>\n",
    "  HF_HUB_ENABLE_HF_TRANSFER: '1'\n",
    "  HF_TOKEN: <your_hf_token>\n",
    "  USE_RAY: '1'\n",
    "cloud: <your-cloud-name>\n",
    "ray_version: 2.42.0\n",
    "entrypoint: llamafactory-cli train llamafactory_configs/llama3_lora_sft_ray.yaml\n",
    "max_retries: 1\n",
    "```\n",
    "\n",
    "Once you fill in the image uri of the image you created above, your WandB API key and HF token, and your cloud name, you can simply run the below command to start your training job! Note that the compute config and working directory for the job are inherited from the current workspace.\n",
    "```bash\n",
    "anyscale job submit --wait --config-file sft_job_config.yaml\n",
    "```\n",
    "\n",
    "The training job should take less than 15 minutes to start up and complete, after which you should see that the job status has been updated to \"Succeeded\"!\n",
    "\n",
    "<img src=\"assets/completed.png\" width=1500px />\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
