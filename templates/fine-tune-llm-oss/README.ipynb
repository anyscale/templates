{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning LLMs with LLaMA-Factory on Anyscale\n",
    "\n",
    "**⏱️ Time to complete**: ~20 mins (includes the time for training the model)\n",
    "\n",
    "Looking to get the most out of your LLM workloads? Fine-tuning pretrained LLMs can often be the most cost effective way to improve your model's performance on the tasks that you care about. This template will walk you through how you can use the popular open source library [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) to fine-tune LLMs on Anyscale. LLaMA-Factory comes with a [Ray](https://www.ray.io/) integration for scaling training to multiple GPUs and nodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Getting started with LLaMA-Factory \n",
    "First, you need to install the LLaMA-Factory code. You can view the latest changes from the [LLaMA-Factory github](https://github.com/hiyouga/LLaMA-Factory.git). \n",
    "\n",
    "```bash\n",
    "git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
    "cd LLaMA-Factory\n",
    "# install extras separately so Anyscale can track the dependencies on worker nodes\n",
    "pip install torch jieba nltk rouge-chinese \n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "Next, you need to set your `HF_TOKEN` and `WANDB_API_KEY` environment variables. It is recommended to do this via the environment variables section of the [dependencies tab](https://docs.anyscale.com/configuration/dependency-management/dependency-development/#environment-variables) in Workspaces. Simply running `export HF_TOKEN=<HF_TOKEN_HERE>`, or `huggingface-cli login` will work for single node compute configurations, but will not propagate the environment variables to worker nodes.\n",
    "\n",
    "Additionally it is recommended to use `hf_transfer` for fast model downloading. You can do this by running `pip install hf_transfer`, and setting `HF_HUB_ENABLE_HF_TRANSFER=1` in the dependencies tab. Setting `HF_HUB_ENABLE_HF_TRANSFER=1` without first installing `hf_transfer` will cause errors. An example of setting relevant environment variables in the dependencies tab is shown below. \n",
    "\n",
    "<img src=\"assets/env_vars.png\" width=500px />\n",
    "\n",
    "You can find some example configs that use Ray with LLaMA-Factory for pretraining and SFT (instruction tuning) in the `llamafactory_config` directory. A full set of examples with various models, tasks, and datasets can be found on the [LLaMA-Factory github](https://github.com/hiyouga/LLaMA-Factory/tree/main/examples). \n",
    "\n",
    "LLaMA-Factory provides a cli `llamafactory-cli` that allows you to launch training with a config yaml file. For example, you can launch a supervised finetuning job with ray by running the following command:\n",
    "```bash\n",
    "cd .. # return to top level directory\n",
    "USE_RAY=1 llamafactory-cli train llamafactory_configs/llama3_lora_sft_ray.yaml\n",
    "```\n",
    "This will run an instruction tuning training job with `Meta-Llama-3-8B-Instruct` on an example subset of the alpaca dataset. Training statistics will by default be logged with all installed logging libraries (i.e. wandb, mlflow, comet, tensorboard), since LLaMA-Factory relies on HuggingFace's [Trainer integration](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments.report_to) for logging. To specify a single specific library to log with, set `report_to: <LIBRARY_NAME>` in the config YAML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing your dataset\n",
    "\n",
    "The above config (`llamafactory_configs/llama3_lora_sft_ray.yaml`) runs out of the box, since it assumes that the dataset and dataset information is located on the huggingface hub, and specifies it as below\n",
    "```yaml\n",
    "...\n",
    "### dataset\n",
    "dataset: identity,alpaca_en_demo\n",
    "dataset_dir: REMOTE:llamafactory/demo_data  # or use local absolute path\n",
    "...\n",
    "```\n",
    "\n",
    "However, by default LLaMA-Factory reads dataset information from a local file `dataset_info.json` (which can be found in `LLaMA-Factory/data/`)\n",
    "\n",
    "To specify new datasets and have them be accessible across Ray worker nodes, you must first add `dataset_info.json` and any data files to shared storage like `/mnt/cluster_storage`. \n",
    "\n",
    "For example, if you wanted to run pretraining on the `c4_demo` dataset, you would first go through the following setup steps:\n",
    "```bash\n",
    "# create a copy of the data in /mnt/cluster_storage\n",
    "cp LLaMA-Factory/data/c4_demo.json /mnt/cluster_storage/\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "# create `/mnt/cluster_storage.dataset_info.json` containing\n",
    "# {\n",
    "#     \"c4_demo\": {\n",
    "#         \"file_name\": \"/mnt/cluster_storage/c4_demo.json\",\n",
    "#         \"columns\": {\n",
    "#         \"prompt\": \"text\"\n",
    "#         }\n",
    "#     },\n",
    "# }\n",
    "echo '{\"c4_demo\":{\"file_name\":\"/mnt/cluster_storage/c4_demo.json\",\"columns\":{\"prompt\":\"text\"}}}' > /mnt/cluster_storage/dataset_info.json\n",
    "```\n",
    "\n",
    "In `llamafactory_configs/llama3_lora_pretrain_ray.yaml` the relevant `dataset_dir` argument is already specified.\n",
    "```yaml\n",
    "...\n",
    "### dataset\n",
    "dataset: c4_demo\n",
    "dataset_dir: /mnt/cluster_storage  # or use local absolute path\n",
    "...\n",
    "```\n",
    "\n",
    "To launch finetuning with this local dataset, you can run\n",
    "```bash\n",
    "USE_RAY=1 llamafactory-cli train llamafactory_configs/llama3_lora_pretrain_ray.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LLaMA-Factory in an Anyscale job\n",
    "In order to run LLaMA-Factory as an Anyscale job, you have to create a [custom container image](https://docs.anyscale.com/configuration/dependency-management/dependency-container-images/#customizing-a-container-image()) that comes with LLaMA-Factory installed. You can specify the relevant packages in your dockerfile. For example, you could create an image using the `anyscale/ray-ml:2.42.0-py310-gpu` base image as follows.\n",
    "\n",
    "```dockerfile\n",
    "# Start with an Anyscale base image.\n",
    "FROM anyscale/ray-ml:2.42.0-py310-gpu\n",
    "WORKDIR /app\n",
    "RUN git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git && \\\n",
    "    cd LLaMA-Factory && \\\n",
    "    pip install --no-cache-dir torch jieba nltk rouge-chinese && \\\n",
    "    pip install -e .\n",
    "```\n",
    "\n",
    "You can then submit use this image to run your job via the [Anyscale jobs cli](https://docs.anyscale.com/platform/jobs/manage-jobs) locally or from a workspace. For example, from a workspace configured with the image you built above, you could run the following command to launch finetuning as a job.\n",
    "```bash\n",
    "anyscale job submit --wait --env USE_RAY=1 -- llamafactory-cli train llamafactory_configs/llama3_lora_sft_ray.yaml\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
