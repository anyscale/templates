{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning LLMs with LLaMA-Factory and Axolotl on Anyscale\n",
    "\n",
    "<!-- **⏱️ Time to complete**: ~3 hours (includes the time for training the model) -->\n",
    "\n",
    "Looking to get the most out of your LLM workloads? Fine-tuning pretrained LLMs can often be the most cost effective way to improve your model's performance on the tasks that you care about. This template will walk you through how you can use the popular open source libraries [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) and [Axolotl](https://github.com/axolotl-ai-cloud/axolotl) to train LLMs on Anyscale. Both libraries also come with [Ray](https://www.ray.io/) integrations for scaling training to multiple GPUs and nodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Getting started with LLaMA-Factory \n",
    "First, we need to install the LLaMA-Factory code. You can view the latest changes at the [LLaMA-Factory github](https://github.com/hiyouga/LLaMA-Factory.git). \n",
    "\n",
    "```bash\n",
    "git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
    "cd LLaMA-Factory\n",
    "# install extras separately so Anyscale can track the dependencies on worker nodes\n",
    "pip install torch jieba nltk rouge-chinese \n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "You can find some example configs in the `llamafactory_config` directory. LLaMA-Factory provides a cli `llamafactory-cli` that allows you to launch training with a config yaml file.\n",
    "\n",
    "For example, you can launch a supervised finetuning job with ray by running the following command:\n",
    "```bash\n",
    "cd .. # return to top level directory\n",
    "WANDB_API_KEY=<WANDB_KEY_HERE> USE_RAY=1 llamafactory-cli train llamafactory_configs/llama3_lora_sft_ray.yaml\n",
    "```\n",
    "This will run an Instruction Tuning training job with `Meta-Llama-3-8B-Instruct` on an example subset of the alpaca dataset. Training statistics will be logged with Weights and Biases through the LLaMA-Factory integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing your dataset\n",
    "\n",
    "The above config (`llamafactory_configs/llama3_lora_sft_ray.yaml`) runs out of the box, since it assumes that the dataset and dataset information is located on the huggingface hub, and specifies it as below\n",
    "```yaml\n",
    "...\n",
    "### dataset\n",
    "dataset: identity,alpaca_en_demo\n",
    "dataset_dir: REMOTE:llamafactory/demo_data  # or use local absolute path\n",
    "...\n",
    "```\n",
    "\n",
    "However, by default LLaMA-Factory reads dataset information from a local file `dataset_info.json` (which can be found in `LLaMA-Factory/data/`)\n",
    "\n",
    "To specify new datasets and have them be accessible across Ray worker nodes, you must first add `dataset_info.json` and any data files to shared storage like `/mnt/cluster_storage`. \n",
    "\n",
    "For example, if you wanted to run pretraining on the `c4_demo` dataset, you would first go through the following setup steps:\n",
    "```bash\n",
    "# create a copy of the data in /mnt/cluster_storage\n",
    "cp LLaMA-Factory/data/c4_demo.json /mnt/cluster_storage/\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "# create `/mnt/cluster_storage.dataset_info.json` containing\n",
    "# {\n",
    "#     \"c4_demo\": {\n",
    "#         \"file_name\": \"/mnt/cluster_storage/c4_demo.json\",\n",
    "#         \"columns\": {\n",
    "#         \"prompt\": \"text\"\n",
    "#         }\n",
    "#     },\n",
    "# }\n",
    "echo '{\"c4_demo\":{\"file_name\":\"/mnt/cluster_storage/c4_demo.json\",\"columns\":{\"prompt\":\"text\"}}}' > /mnt/cluster_storage/dataset_info.json\n",
    "```\n",
    "\n",
    "In `llamafactory_configs/llama3_lora_pretrain_ray.yaml` the relevant `dataset_dir` argument is already specified.\n",
    "```yaml\n",
    "...\n",
    "### dataset\n",
    "dataset: c4_demo\n",
    "dataset_dir: /mnt/cluster_storage  # or use local absolute path\n",
    "...\n",
    "```\n",
    "\n",
    "To launch finetuning with this local dataset, you can run\n",
    "```bash\n",
    "cd .. # return to top level directory\n",
    "WANDB_API_KEY=<WANDB_KEY_HERE> USE_RAY=1 llamafactory-cli train llamafactory_configs/llama3_lora_pretrain_ray.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LLaMA-Factory in an Anyscale job\n",
    "In order to run LLaMA-Factory as an Anyscale job, you have to create a [custom container image](https://docs.anyscale.com/configuration/dependency-management/dependency-container-images/#customizing-a-container-image()) that comes with LLaMA-Factory installed. You can specify the relevant packages in your dockerfile. For example, you could create an image using the `anyscale/ray-ml:2.42.0-py310-gpu` base image as follows.\n",
    "\n",
    "```dockerfile\n",
    "# Start with an Anyscale base image.\n",
    "FROM anyscale/ray-ml:2.42.0-py310-gpu\n",
    "WORKDIR /app\n",
    "RUN git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git && \\\n",
    "    cd LLaMA-Factory && \\\n",
    "    pip install --no-cache-dir torch jieba nltk rouge-chinese && \\\n",
    "    pip install -e .\n",
    "```\n",
    "\n",
    "You can then submit use this image to run your job via the [Anyscale jobs cli](https://docs.anyscale.com/platform/jobs/manage-jobs) locally or from a workspace. For example, from a workspace configured with the image you built above, you could run the following command to launch finetuning as a job.\n",
    "```bash\n",
    "anyscale job submit --wait --env USE_RAY=1 -- llamafactory-cli train llamafactory_configs/llama3_lora_sft_ray.yaml\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
