{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Fine-tuning Open-weight LLMs with Anyscale\n",
    "\n",
    "**‚è±Ô∏è Time to complete**: ~3 hours (includes the time for training the model)\n",
    "\n",
    "\n",
    "This template comes with a installed library for training LLMs on Anyscale called LLMForge. It provides the fastest way to try out training LLMs with Ray on Anyscale. You can read more about this library and its features in the [docs](https://docs.anyscale.com/latest/llms/finetuning/intro). For learning on how to serve the model online or offline for doing batch inference you can refer to the [serving template](https://console.anyscale.com/v2/template-preview/endpoints_v2) or the [offline batch inference template](https://console.anyscale.com/v2/template-preview/batch-llm), respecitvely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "You can find some tested config files examples in the `training_configs` directoy. LLMForge comes with a CLI that lets you pass in a config YAML file to start your training.\n",
    "\n",
    "\n",
    "```bash\n",
    "WANDB_API_KEY=<PUT_YOUR_WANDB_KEY_HERE> llmforge anyscale finetune training_configs/custom/meta-llama--Meta-Llama-3-8B-Instruct/lora/16xA10-512.yaml\n",
    "```\n",
    "\n",
    "This code will run LoRA fine-tuning on the Meta-Llama-3-8B-Instruct model with 16xA10-512 configuration on a GSM-8k math dataset.\n",
    "\n",
    "When the training is done, you will see a message like this:\n",
    "\n",
    "```bash\n",
    "Note: LoRA weights will also be stored in path <path>\n",
    "````\n",
    "\n",
    "This is the path where the adapted weights are stored, you can use them fore inference. You can also see the list of your fine-tuned models in the `serving` tab in the Anyscale console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Next?\n",
    "\n",
    "* Make sure to checkout the [LLMForge documentation](https://docs.anyscale.com/latest/llms/finetuning/intro) and [user guides](https://docs.anyscale.com/latest/llms/finetuning/user-guides) for more information on how to use the library and the features it supports.\n",
    "* You can follow the [serving template](https://console.anyscale.com/v2/template-preview/endpoints_v2) to learn how to serve the model online.\n",
    "* You can follow the [offline batch inference template](https://console.anyscale.com/v2/template-preview/batch-llm) to learn how to do batch inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--------- \n",
    "**Task:** \n",
    "\n",
    "Fine-tune llama-3-8b-instruct in default mode (LoRA rank 8). Just giving the dataset.\n",
    "\n",
    "**Command:**\n",
    "```bash\n",
    "llmforge anyscale finetune training_configs/default/meta-llama/Meta-Llama-3-8B-Instruct-simple.yaml --default\n",
    "```\n",
    "\n",
    "**Config:**\n",
    "\n",
    "```yaml\n",
    "model_id: meta-llama/Meta-Llama-3-8B-Instruct\n",
    "train_path: s3://...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--------- \n",
    "\n",
    "**Task:** \n",
    "\n",
    "Fine-tune llama-3-8b-instruct in default mode but also control parameters like `learning_rate` and `num_epochs`. \n",
    "\n",
    "**Command:**\n",
    "```bash\n",
    "llmforge anyscale finetune training_configs/default/meta-llama/Meta-Llama-3-8B-Instruct-custom.yaml --default\n",
    "```\n",
    "\n",
    "**Config:**\n",
    "\n",
    "```yaml\n",
    "model_id: meta-llama/Meta-Llama-3-8B-Instruct\n",
    "train_path: s3://...\n",
    "valid_path: s3://...\n",
    "num_epochs: 3\n",
    "learning_rate: 1e-4         \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "**Task:** \n",
    "\n",
    "Fine-tune llama-3-8b-instruct (a \"core\" model) in custom mode on 16xA10s (auto mode uses 8xA100-80G) with context length of 512.\n",
    "\n",
    "\n",
    "**Command:** \n",
    "\n",
    "```bash\n",
    "llmforge anyscale finetune training_configs/custom/meta-llama--Meta-Llama-3-8B-Instruct/lora/16xA10-512.yaml \n",
    "```\n",
    "\n",
    "**Config:**\n",
    "\n",
    "```yaml\n",
    "model_id: meta-llama/Meta-Llama-3-8B-Instruct\n",
    "train_path: s3://...\n",
    "valid_path: s3://...\n",
    "context_length: 512\n",
    "deepspeed:\n",
    "  config_path: deepspeed_configs/zero_3_offload_optim+param.json\n",
    "worker_resources:\n",
    "  accelerator_type:A10G: 0.001\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---------\n",
    "**Task:** \n",
    "\n",
    "Fine-tune gemma-2-27b in custom mode on 8xA100-80G.\n",
    "\n",
    "\n",
    "**Command:** \n",
    "\n",
    "```bash\n",
    "llmforge anyscale finetune training_configs/custom/google--gemma-2-27b-it/lora/8xA100-80G-512.yaml \n",
    "```\n",
    "\n",
    "**Config:**\n",
    "\n",
    "```yaml\n",
    "model_id: google/gemma-2-27b-it\n",
    "train_path: s3://...\n",
    "valid_path: s3://...\n",
    "num_devices: 8\n",
    "worker_resources:\n",
    "  accelerator_type:A100-80G: 0.001\n",
    "generation_config:\n",
    "  prompt_format:\n",
    "    system: \"{instruction} + \"\n",
    "    assistant: \"<start_of_turn>model\\n{instruction}<end_of_turn>\\n\"\n",
    "    trailing_assistant: \"<start_of_turn>model\\n\"\n",
    "    user: \"<start_of_turn>user\\n{system}{instruction}<end_of_turn>\\n\"\n",
    "    system_in_user: True\n",
    "    bos: \"<bos>\"\n",
    "    default_system_message: \"\"\n",
    "  stopping_sequences: [\"<end_of_turn>\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More examples can be found in `./training_configs`. For specific features read [cookbooks](#cookbooks) and [end-to-end examples](#end-to-end-examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cookbooks\n",
    "\n",
    "After you are with the above, you can find recipies that extend the functionality of this template under the cookbooks folder:\n",
    "\n",
    "* [Bring your own data](cookbooks/bring_your_own_data/README.md): Everything you need to know about using custom datasets for fine-tuning.\n",
    "* [Bring any huggingface model and prompt format](cookbooks/bring_any_hf_model/README.md): Learn how you can finetune any ü§óHugging Face model with a custom prompt format (chat template). \n",
    "* [LoRA vs. full-parameter training](cookbooks/continue_from_checkpoint/README.md): Learn the differences between LoRA and full-parameter training and how to configure both.\n",
    "* [Continue fine-tuning from a previous checkpoint](cookbooks/continue_from_checkpoint/README.md): A detailed guide on how you can use a previous checkpoint for another round of fine-tuning.\n",
    "* [Modifying hyperparameters](cookbooks/modifying_hyperparameters/README.md): A brief guide on customization of your fine-tuning job.\n",
    "* [Optimizing Cost and Performance for Finetuning](cookbooks/optimize_cost/README.md): A detailed guide on default performance-related parameters and how you can optimize throughput for training on your own data.\n",
    "* [Run finetuning as Anyscale Job](cookbooks/launch_as_anyscale_job/README.md): A detailed guide on how to submit a finetuning workflow as a job (outside the context of workspaces.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-end Examples\n",
    "\n",
    "Here is a list of end-to-end examples that involve more steps such as data preprocessing, evaluation, etc but with a main focus on improving model quality via fine-tuning.\n",
    "\n",
    "* [Fine-tuning for Function calling on custom data](end-to-end-examples/fine-tune-function-calling/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMForge Versions\n",
    "\n",
    "Here is a list of LLMForge image versions:\n",
    "\n",
    "| version | image_uri |\n",
    "|---------|-----------|\n",
    "| `0.5.2`  | `localhost:5555/anyscale/llm-forge:0.5.2` |\n",
    "| `0.5.1`  | `localhost:5555/anyscale/llm-forge:0.5.1` |\n",
    "| `0.5.0.1`  | `localhost:5555/anyscale/llm-forge:0.5.0.1-ngmM6BdcEdhWo0nvedP7janPLKS9Cdz2` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
