{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Llama-3, Mistral and Mixtral with Anyscale\n",
    "\n",
    "**⏱️ Time to complete**: 2.5 hours for 7/8B models (9 hours for 13B, 25 hours for 70B)\n",
    "\n",
    "The guide below walks you through the steps required for fine-tuning of LLMs. This template provides an easy to configure solution for ML Platform teams, Infrastructure engineers, and Developers to fine-tune LLMs.\n",
    "\n",
    "### Popular base models to fine-tune\n",
    "\n",
    "- meta-llama/Meta-Llama-3-8B-Instruct (Full-param and LoRA)\n",
    "- meta-llama/Meta-Llama-3-70B-Instruct (Full-param and LoRA)\n",
    "- mistralai/Mistral-7B-Instruct-v0.1 (Full-param and LoRA)\n",
    "- mistralai/Mixtral-8x7b (LoRA only)\n",
    "\n",
    "A full list of supported models is in the [FAQ](#faqs) section. In the end we provide more guides in form of [cookbooks](#cookbooks) and [end-to-end examples](#end-to-end-examples) that provide more detailed information about using this template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Launch a fine-tuning job\n",
    "\n",
    "We provide example configurations under the `./training_configs` directory for different base models and accelerator types. You can use these as a starting point for your own fine-tuning jobs. The full-list of public configurations that are customizable see [Anyscale docs](https://docs.anyscale.com/reference/finetuning-config-api).\n",
    "\n",
    "**Optional**: You can get a WandB API key from [WandB](https://wandb.ai/authorize) to track the fine-tuning process. If not provided, you can only track the experiments through the standard output logs.\n",
    "\n",
    "Next, you can launch a fine-tuning job with your WandB API key passed as an environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Optional] You can set the WandB API key to track model performance\n",
    "# import os\n",
    "# os.environ[\"WANDB_API_KEY\"]=\"YOUR_WANDB_API_KEY\"\n",
    "\n",
    "# Launch a LoRA fine-tuning job for Llama 3 8B with 16 A10s\n",
    "!python main.py training_configs/lora/llama-3-8b.yaml\n",
    "\n",
    "# Launch a full-param fine-tuning job for Llama 3 8B with 16 A10s\n",
    "# !python main.py training_configs/full_param/llama-3-8b.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the command runs, you can monitor a number of built-in metrics in the `Metrics` tab under `Ray Dashboard`, such as the number of GPU nodes and GPU utilization.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/templates/main/templates/fine-tune-llm_v2/assets/gpu-usage.png\" width=500px/>\n",
    "\n",
    "Depending on whether you are running LoRA or full-param fine-tuning, you can continue with step 2(a) or step 2(b). To learn more about LoRA vs. full-parameter, see the cookbooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Step 2(a) - Serving the LoRA fine-tuned model\n",
    "\n",
    "Upon the job completion, you can see the LoRA weight storage location and model ID in the log, such as the one below:\n",
    "\n",
    "```shell\n",
    "Note: LoRA weights will also be stored in path {ANYSCALE_ARTIFACT_STORAGE}/lora_fine_tuning under meta-llama/Llama-2-8b-chat-hf:sql:12345 bucket.\n",
    "```\n",
    "\n",
    "You can specify this URI as the dynamic_lora_loading_path [docs](https://docs.anyscale.com/examples/deploy-llms#more-guides) in the llm serving template, and then query the endpoint.\n",
    "\n",
    "> Note: Such LoRA model IDs follow the format `{base_model_id}:{suffix}:{id}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2(b) - Serving the full-parameter fine-tuned model\n",
    "\n",
    "Once the fine-tuning job is complete, you can view the stored full-parameter fine-tuned checkpoint at the very end of the job logs. Here is an example fine-tuning job output:\n",
    "\n",
    "```shell\n",
    "Best checkpoint is stored in:\n",
    "{ANYSCALE_ARTIFACT_STORAGE}/username/llmforge-finetuning/meta-llama/Llama-2-70b-hf/TorchTrainer_2024-01-25_18-07-48/TorchTrainer_b3de9_00000_0_2024-01-25_18-07-48/checkpoint_000000\n",
    "```\n",
    "\n",
    "Follow the [Learn how to bring your own models](https://docs.anyscale.com/examples/deploy-llms#more-guides) section under the llm serving template to serve this fine-tuned model with the specified storage uri."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cookbooks\n",
    "\n",
    "After you are with the above, you can find recipies that extend the functionality of this template under the cookbooks folder:\n",
    "\n",
    "* [Bring your own data](cookbooks/bring_your_own_data/README.md): Everything you need to know about using custom datasets for fine-tuning.\n",
    "* [Continue fine-tuning from a previous checkpoint](cookbooks/continue_from_checkpoint/README.md): A detailed guide on how you can use a previous checkpoint for another round of fine-tuning.\n",
    "* [LoRA vs. full-parameter training](cookbooks/continue_from_checkpoint/README.md): Learn the differences between LoRA and full-parameter training and how to configure both.\n",
    "* [Modifying hyperparameters](cookbooks/modifying_hyperparameters/README.md): A brief guide on tailoring your fine-tuning job.\n",
    "* [Optimizing Cost and Performance for Finetuning](cookbooks/optimize_cost/README.md): A detailed guide on default performance-related parameters and how you can optimize throughput for training on your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-end Examples\n",
    "\n",
    "Here is a list of end-to-end examples that involve more steps such as data preprocessing, evaluation, etc but with a main focus on improving model quality via fine-tuning.\n",
    "\n",
    "* [Fine-tuning for Function calling on custom data](end-to-end-examples/fine-tune-function-calling/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## FAQs\n",
    "\n",
    "### Where can I view the bucket where my LoRA weights are stored?\n",
    "\n",
    "All the LoRA weights are stored under the URI `${ANYSCALE_ARTIFACT_STORAGE}/lora_fine_tuning` where `ANYSCALE_ARTIFACT_STORAGE` is an environmental variable in your workspace.\n",
    "\n",
    "### What's the full list of supported models?\n",
    "\n",
    "This is a growing list but it includes the following models:\n",
    "\n",
    "- mistralai/Mistral-7B-Instruct-v0.1\n",
    "- mistralai/Mixtral-8x7b\n",
    "- meta-llama/Llama-2-7b-chat-hf\n",
    "- meta-llama/Llama-2-13b-hf\n",
    "- meta-llama/Llama-2-13b-chat-hf\n",
    "- meta-llama/Llama-2-70b-hf\n",
    "- meta-llama/Llama-2-70b-chat-hf\n",
    "- meta-llama/Meta-Llama-3-8B\n",
    "- meta-llama/Meta-Llama-3-8B-Instruct\n",
    "- meta-llama/Meta-Llama-3-70B\n",
    "- meta-llama/Meta-Llama-3-70B-Instruct\n",
    "\n",
    "In general, any model that is compatible with the architecture of these models can be fine-tuned using the same configs as the base models.\n",
    "\n",
    "NOTE: currently mixture of expert models (such as `mistralai/Mixtral-8x7B)` only support LoRA fine-tuning\n",
    "\n",
    "### Should I use LoRA or full-parameter fine-tuning?\n",
    "\n",
    "There is no general answer to this but here are some things to consider:\n",
    "\n",
    "- The quality of the fine-tuned models will, in most cases, be comparable if not the same\n",
    "- LoRA shines if...\n",
    "    - ... you want to serve many fine-tuned models at once yourself\n",
    "    - ... you want to rapidly experiment (because fine-tuning, downloading and serving the model take less time)\n",
    "- Full-parameter shines if...\n",
    "    - ... you want to make sure that your fine-tuned model has the maximum quality\n",
    "    - ... you want to serve only one fine-tuned version of the model\n",
    "\n",
    "You can learn more about this in one of our [blogposts](https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2).\n",
    "There, you'll also find some guidance on the LoRA parameters and why, in most cases, you don't need to change them.\n",
    "\n",
    "### I have the right model, context length and everything. Can I optimize compute cost?\n",
    "\n",
    "Optimizing your fine-tuning runs for compute cost is a non-trivial problem.\n",
    "The default configs in this template require the following compute:\n",
    "Llama-3-8B and Mistral require 16 A10Gs. Llama-3-70B and Mixtral require 32 A10Gs.\n",
    "\n",
    "Before optimizing for compute, make sure that you have selected a context length that is long enough for your dataset. If you have very few datapoints in your dataset that requires a much larger context than the others, consider removing them. The model of your choice and fine-tuning technique should also suit your data.\n",
    "\n",
    "If you want different compute, we *suggest* the following workflow to find a suitable configuration:\n",
    "\n",
    "* Start with a batch size of 1\n",
    "* Choose a GPU instance type that you think will give you good flops/$. If you are not sure, here is a rough guideline:\n",
    "    * g5 nodes for high availability\n",
    "    * p4d/p4de nodes for lower availability but better flops/$\n",
    "    * Anything higher-end if you have the means of acquiring them\n",
    "* Do some iterations of trial and error on instance types and deepspeed settings to fit the workload while keeping other settings fixed\n",
    "    * Use deepspeed stage 3 (all default configs in this template use stage 3)\n",
    "    * Try to use deepspeed offloading only if it reduces the minimum number of instances you have to use\n",
    "        * Deepspeed offloading slows down training but allows for larger batch sizes because of a more relaxed GRAM foot-print\n",
    "    * Use as few instances as possible. Fine-tune on the same machine if possible.\n",
    "        *  The GPU to GPU communication across machines is very expensive compared to the memory savings it could provide. You can use a cheap CPU-instance as a head-node for development and a GPU-instance that can scale down as a worker node for the heavy lifting.\n",
    "        * Training single-node on A100s may end up cheaper than multi-node on A10s if availablity is not an issue\n",
    "* Be aware that evaluation and checkpointing introduce their own memory-requirements\n",
    "   * If things look good, run fine-tuning for a full epoch.\n",
    "* After you have followed the steps above, increase batch size as much as possible without OOMing.\n",
    "\n",
    "We do not guarantee that this will give you optimal settings, but have found this workflow to be helpful ourselves in the past.\n",
    "\n",
    "### I've reviewed the customizable hyperparameters available. How can I get even more control?\n",
    "\n",
    "This template fine-tunes with Anyscale's library `llmforge`, which uses [DeepSpeed](https://github.com/microsoft/DeepSpeed) and [Ray Train](https://docs.ray.io/en/latest/train/train.html) for distributed training. The full set of config parameters are documented in the [API reference](https://docs.anyscale.com/reference/finetuning-config-api), and we provide a [cookbook](cookbooks/modifying_hyperparameters/README.md) detailing the important ones.  For anything that goes beyond using `llmforge`, you can build your own fine-tuning stack on Anyscale.\n",
    "\n",
    "### What's with the `main` file that is created during fine-tuning?\n",
    "\n",
    "It's an artifact of our fine-tuning libraries. Please ignore it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
