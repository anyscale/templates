{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring your own data \n",
    "**⏱️ Time to complete**: 10 minutes\n",
    "\n",
    "This guide focuses on how you can bring your own data to fine-tune your model on the Anyscale Platform. Make sure you have gone over the [basic fine-tuning guide](../../README.md) before going over this cookbook.\n",
    "\n",
    "\n",
    "# Table of Contents\n",
    "1. [Data stored in remote storage](#data-stored-in-remote-storage)\n",
    "    - [Public datasets](#public-datasets)\n",
    "    - [Private datasets](#private-datasets)\n",
    "2. [Data stored locally](#data-stored-locally)\n",
    "\n",
    "## Example YAML\n",
    "\n",
    "We specify training and validation file paths in the `train_path` and `valid_path` entries in the config file as shown in the example YAML below. \n",
    "\n",
    "```yaml\n",
    "model_id: meta-llama/Meta-Llama-3-8B \n",
    "train_path: s3://air-example-data/gsm8k/train.jsonl # <-- change this to the path to your training data\n",
    "valid_path: s3://air-example-data/gsm8k/test.jsonl # <-- change this to the path to your validation data. This is optional\n",
    "context_length: 512\n",
    "num_devices: 16 \n",
    "num_epochs: 10\n",
    "train_batch_size_per_device: 8\n",
    "eval_batch_size_per_device: 16\n",
    "learning_rate: 5e-6\n",
    "padding: \"longest\" \n",
    "num_checkpoints_to_keep: 1\n",
    "dataset_size_scaling_factor: 10000\n",
    "output_dir: /mnt/local_storage\n",
    "deepspeed:\n",
    "  config_path: deepspeed_configs/zero_3_offload_optim+param.json\n",
    "dataset_size_scaling_factor: 10000 \n",
    "flash_attention_2: true\n",
    "trainer_resources:\n",
    "  memory: 53687091200\n",
    "worker_resources:\n",
    "  accelerator_type:A10G: 0.001\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data stored in remote storage \n",
    "\n",
    "## Public datasets\n",
    "For datasets configured for public access, you simply need to add the relevant training and validation file URI in your training YAML. We support loading from data stored on S3 and GCS.\n",
    "\n",
    "\n",
    "## Private datasets\n",
    "With private data, you have two options: \n",
    "\n",
    "### Option 1: Configure permissions directly in your cloud account\n",
    "The most convenient option is to provide read permissions for your Anyscale workspace for the specific bucket. You can follow our guide to do so [here](https://docs.anyscale.com/configuration/cloud-storage-buckets#access-private-cloud-storage).\n",
    "\n",
    "\n",
    "### Option 2: Sync data into default cloud storage provided by Anyscale\n",
    "The other option you have is to sync your data into Anyscale-provided storage and then continue with fine-tuning. Let's consider private data on AWS S3. First, we'll need to configure your workspace to be able to access the data. We recommend that you simply export relevant environment variables directly (`AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_SESSION_TOKEN`, etc) into your current terminal session.  With that aside, what we want to do is to move this data into \n",
    "the [default object storage bucket](https://docs.anyscale.com/platform/workspaces/workspaces-storage#object-storage-s3-or-gcs-buckets) provided by Anyscale (`$ANYSCALE_ARTIFACT_STORAGE`). That way, across runs/ workspace restarts, you don't have to repeat this process (compared to just downloading the files into your workspace).\n",
    "1. First, download the data into your workspace:  \n",
    "    ```bash\n",
    "    aws s3 sync s3://<bucket_name>/<path_to_data_dir>/ myfiles/\n",
    "    ```\n",
    "2. The default object storage bucket configured for you in your workspace uses Anyscale-managed credentials internally. It is recommended to reset the credentials you provided so as to not interfere with the Anyscale-managed access setup. For example, if your Anyscale hosted cloud is on AWS, then adding your AWS credentials means that `aws` can't access the default object storage bucket anymore. Thus, reset your credentials by simply setting the relevant environment variables to the empty string.\n",
    "3. Next, you can upload your data to `$ANYSCALE_ARTIFACT_STORAGE` via the relevant cli (AWS S3/ GCS depending on your Anyscale Cloud). For example:\n",
    "\n",
    "    GCP: \n",
    "    ```bash\n",
    "    gcloud storage cp -r myfiles/ $ANYSCALE_ARTIFACT_STORAGE/myfiles/\n",
    "    ```\n",
    "\n",
    "    AWS:\n",
    "    ```bash\n",
    "    aws s3 sync myfiles/ $ANYSCALE_ARTIFACT_STORAGE/myfiles/\n",
    "    ``` \n",
    "\n",
    "4. Finally, you can update the training and validation paths in your training config YAML.\n",
    "\n",
    "# Data stored locally\n",
    "\n",
    "For local files you have two options: \n",
    "1. Upload to remote storage and follow the instructions above (the more reliable option for large datasets). \n",
    "2. Upload directly to your Anyscale workspace: This is the simplest option for small files. You can use the UI in your VSCode window (simply right click -> upload files/folder) and upload your training files. This data needs to be placed in the shared cluster storage `/mnt/cluster_storage` so that it's accessible by all the worker nodes. (For more on workspace storage, see our guide [here](https://docs.anyscale.com/platform/workspaces/workspaces-storage/)). For example, let's say I uploaded a folder `my_files` with the following structure:\n",
    "\n",
    "    ```\n",
    "    myfiles/  \n",
    "    ├── train.jsonl\n",
    "    └── val.jsonl\n",
    "    ```\n",
    "\n",
    "    I would now do:\n",
    "\n",
    "    ```bash\n",
    "    mv myfiles /mnt/cluster_storage\n",
    "    ```\n",
    "\n",
    "    Next, update your training config YAML to point to the right training and validation files. \n",
    "\n",
    "    ```yaml\n",
    "    train_path: /mnt/cluster_storage/myfiles/train.jsonl\n",
    "    valid_path: /mnt/cluster_storage/myfiles/test.jsonl\n",
    "    ```\n",
    "\n",
    "**Note:** If you anticipate to use the same dataset files for multiple runs/ across workspace sessions, you should upload the files to `$ANYSCALE_ARTIFACT_STORAGE`.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
