{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning on datasets with long context\n",
    "**⏱️ Time to complete**: 90 minutes (assuming GCP A2 nodes can be acquired immidiately)\n",
    "\n",
    "This guide demonstrates how to prepare a dataset with long context lengths.\n",
    "Make sure you have gone over the [basic fine-tuning guide](../../README.md) before going over this cookbook.\n",
    "\n",
    "In the following example, we filter a hugginface dataset and assemble it into the right format.\n",
    "By estimating how many tokens an example from the dataset will result in, we can disgard examples that are too long.\n",
    "You can use this as a template for creating your own datasets.\n",
    "\n",
    "> **_NOTE:_** To fine-tune with a context length of 8k tokens and the llama-3-8b.yaml config in this cookbook, we require GCP A2 nodes. For that, you need to instantiate your workspace in a region such as GCP's `us-central1`  due to the relatively high availability. See [GCP's availability info](https://cloud.google.com/compute/docs/gpus/gpu-regions-zones) for more information. If you only want to generate the dataset, you can do that anywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Fill in your personal hugginface token with access to the tokenizer (You can use a similar tokenizer as a work-around)\n",
    "HHUGGINFACE_TOKEN = \"\"\n",
    "# The name of the model you want to fine-tune with. We use this only for tokenization so models with the same tokenizer are interoperable here.\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# Our target fine-tuning context length\n",
    "MAX_CONTEXT_LENGTH = 8192\n",
    "# Depending on your model, tokenized messages will have special tokens such as a \"beginning of sequence\" or \"system message\" token added.\n",
    "# The size of this \"safety buffer\" should be larger than what you expect these additional tokens to be in sum per example.\n",
    "# 500 is a conservative size for a single-turn user-assistant conversation. Have a look at dataset statistics when starting your fine-tuning job to check the minimum and maximum example size.\n",
    "SAFETY_BUFFER = 500\n",
    "# Design this to fit your dataset. This will help your model learn and converge to a better solution.\n",
    "SYSTEM_MESSAGE = \"You are an expert for patent law who generates abstracts from patents. Base your answer solely on the provided patent.\"\n",
    "# Construct these dataframe depending on where you get your dataset from\n",
    "TRAIN_DF = pd.read_csv(\"hf://datasets/Trelis/big_patent_60k_characters/train.csv\")\n",
    "TEST_DF = pd.read_csv(\"hf://datasets/Trelis/big_patent_60k_characters/test.csv\")\n",
    "\n",
    "# Fit this to how you want to construct your messages from your dataset. Pay attention to the names of columns from the dataset here.\n",
    "def to_messages_dict(d: dict):\n",
    "    \"\"\"Assembles a single example of the dataset for fine-tuning.\"\"\"\n",
    "    return {\"messages\": [{\"role\": \"system\", \"content\": SYSTEM_MESSAGE}, {\"role\": \"user\", \"content\": d[\"description\"]},  {\"role\": \"assistant\", \"content\": d[\"abstract\"]}]}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HHUGGINFACE_TOKEN)\n",
    "\n",
    "def is_too_long(messages: dict):\n",
    "    \"\"\"Filters out rows that exceed MAX_CONTEXT_LENGTH in their total length\"\"\"\n",
    "    return sum([len(tokenizer(m[\"content\"])[\"input_ids\"]) for m in messages[\"messages\"]]) + SAFETY_BUFFER > MAX_CONTEXT_LENGTH\n",
    "\n",
    "for frame, output_file in [(TRAIN_DF, \"/mnt/cluster_storage/train.jsonl\"), (TEST_DF, \"/mnt/cluster_storage/test.jsonl\")]:\n",
    "    with open(output_file, 'w') as f:\n",
    "        for _, row in frame.iterrows():\n",
    "            messages = to_messages_dict(row.to_dict())\n",
    "            if not is_too_long(messages):\n",
    "                json_str = json.dumps(messages)\n",
    "                f.write(json_str + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this dataset to fine-tune an LLM that helps us with creating abstracts from patents.\n",
    "The fine-tuned model will have a context-length of 8192 tokens during fine-tuning.\n",
    "Check out the logs when fine-tuning to double-check every time you create a dataset this way!\n",
    "\n",
    "## FAQ:\n",
    "\n",
    "### How do I find out how many tokens the examples in my dataset have?\n",
    "\n",
    "If you fine-tune with this template, you will find exact statistics at the beginning of your fine-tuning job.\n",
    "Exact numbers are hard to compute in advance. To get a rough idea, you can instantiate a tokenizer for your model and feed it a few samples from your dataset. Remember that one example consists of system message, a prompt, and an answer. There may also be [online services](https://belladoreai.github.io/llama3-tokenizer-js/example-demo/build/) that can help.\n",
    "\n",
    "### What if my dataset results in examples that are longer that the native context length of the model?\n",
    "\n",
    "Some datasets have examples that, when tokenize, exceed the context length of your LLM.\n",
    "Anyscale Endpoints and the Anyscale platform support extending the native context length of LLMs. For example, for Llama 3 8B, we support fine-tuning with up to 32768 tokens (see [Anyscale Endpoints](https://docs.anyscale.com/canary/endpoints/fine-tuning/supported-models/) docs for a list).\n",
    "You ca read more aobut model quality considerations in [our blog](https://www.anyscale.com/blog/fine-tuning-llms-for-longer-context-and-better-rag-systems)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
