{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying hyperparameters\n",
    "\n",
    "**‚è±Ô∏è Time to complete**: 15 minutes\n",
    "\n",
    "This guide will focus on how you can customize your fine-tuning run by modifying the various hyperparameters configurable. Make sure you've read the [basic fine-tuning guide](../../README.md) for better context. \n",
    "\n",
    "We provide a number of options to configure via the training YAML. To view the full set of customizable parameters, head over to the [fine-tuning config API reference](https://docs.anyscale.com/reference/finetuning-config-api). Below, we'll take a closer look at some of the important hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## GPU resources\n",
    "\n",
    "Configuring GPU resources to be used is one of the most important pre-requisities for training. There are two fields in our YAML that are relevant:\n",
    "\n",
    "```yaml\n",
    "num_devices: 16 # number of GPUs \n",
    "worker_resources:\n",
    "  accelerator_type:A10G: 0.001 # specifies GPU type available, and a minimum allocation per worker\n",
    "```\n",
    "\n",
    "Internally, our fine-tuning code will launch Ray workers with each being allocated one GPU. The cluster will be auto-scaled if needed to meet the requirements. The different GPU types you can specify can depend on the specific Anyscale Cloud. The value you specify for the accelerator type (0.001 here) does not matter much, as long as it is non-zero (so that each worker is allocated a GPU) and less than or equal to 1 (so that the requested number of GPUs is the same as `num_devices`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Learning rate\n",
    "\n",
    "There are two entities of interest here: the actual learning rate value itself and the particular learning rate scheduler you use. The parameters you can control in the YAML are below: \n",
    "\n",
    "```yaml\n",
    "learning_rate: 1e-4\n",
    "lr_scheduler_type: cosine\n",
    "num_warmup_steps: 10\n",
    "```\n",
    "\n",
    "In the above config, the training run would use a cosine learning rate schedule (the default) with an initial warmup of 10 steps (the default). The peak learning rate would be 1e-4 (the value specified). \n",
    "\n",
    "We support both `'linear'` and `'cosine'` schedules. The learning rate schedules have been plotted below with `num_warmup_steps=10` and for 1000 training steps.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"../../assets/schedulers.png\" alt=\"cosine\" height=\"300px\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size\n",
    "\n",
    "The batch size for training and validation depends on the below parameters:\n",
    "\n",
    "```yaml\n",
    "num_devices: 8\n",
    "train_batch_size_per_device: 16\n",
    "eval_batch_size_per_device: 16\n",
    "```\n",
    "The effective batch size for training would be `train_batch_size_per_device * num_devices`. For the hardware you specify, the amount you can push `train_batch_size_per_device` or `eval_batch_size_per_device` depends on dataset statistics (average sequence length) and the context length used. For a context length of 512 and the default NVIDIA A10 GPUs, the per-device batch size of 16 is a good default for 7B/8B models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Number of epochs \n",
    "The number of epochs is controlled by two parameters:\n",
    "\n",
    "```yaml\n",
    "num_epochs: 10\n",
    "min_update_steps: 100 # default\n",
    "```\n",
    "\n",
    "`min_num_update_steps` is used only when `num_epochs` is not provided in the YAML. In such cases, the model is trained for atleast `min_num_update_steps` to ensure model convergence. For example, consider the case where `num_epochs` is not provided and the number of steps in an epoch is 24. If `min_num_update_steps` is set to 100 (the default), then the number of epochs is set to be `ceil(100/24) = 5`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation and checkpointing \n",
    "\n",
    "Validation loop and checkpointing are always run consecutively (so that we can clearly reason about checkpoint performance). The relevant options are:\n",
    "\n",
    "```yaml\n",
    "checkpoint_every_n_epochs: 5 # validation and checkpointing frequency\n",
    "max_num_checkpoints: 10 # maximum number of validation + checkpointing events to be triggered during training\n",
    "num_checkpoints_to_keep: 1   # maximum number of checkpoints to keep\n",
    "```\n",
    "\n",
    "By default, `checkpoint_every_n_epochs` is `None`, and in this case  `max_num_checkpoints` checkpointing events are triggered. While checkpointing, we keep only `num_checkpoints_to_keep` number of checkpoints, prioritizing those with lower values of validation loss/perplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA configs\n",
    "We support all the LoRA parameters you can configure in [ü§óPEFT](https://huggingface.co/docs/peft/en/package_reference/lora#peft.LoraConfig):\n",
    "\n",
    "```yaml\n",
    "lora_config:\n",
    "  r: 8\n",
    "  lora_alpha: 16\n",
    "  lora_dropout: 0.05\n",
    "  target_modules:\n",
    "    - q_proj\n",
    "    - v_proj\n",
    "    - k_proj\n",
    "    - o_proj\n",
    "    - gate_proj\n",
    "    - up_proj\n",
    "    - down_proj\n",
    "    - embed_tokens\n",
    "    - lm_head\n",
    "  task_type: \"CAUSAL_LM\"\n",
    "  modules_to_save: []\n",
    "  bias: \"none\"\n",
    "  fan_in_fan_out: false\n",
    "  init_lora_weights: true\n",
    "```\n",
    "\n",
    "For best performance, we recommend that you make sure to include all model layers in `target_modules`. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
