{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning for Function calling on custom data.\n",
    "\n",
    "**⏱️ Time to complete**: 6 hours\n",
    "\n",
    "Function calling is an important capability of large language models. Connecting your model to external tools is at the heart of many LLM applications. In Anyscale Endpoints, you can use the [function calling API](https://docs.anyscale.com/preview/endpoints/text-generation/function-calling) to enable get a quick access on this feature on a select number of models. This is made possible [through JSON mode](https://www.anyscale.com/blog/anyscale-endpoints-json-mode-and-function-calling-features). However, it is beneficial to have *native* function calling capabilities in your model through fine-tuning on a relevant function calling dataset. JSON-mode-based function calling can only guarantee that the output is in the right schema, and can also be more expensive than a regular chat completion. However, fine-tuning on a function calling dataset can improve the model's capabilities with intent recognition (understanding when to call and when not to call a tool) and function call accuracy (employing the right function with accurate parameters) in addition to structured data formatting (formatting the function call json in the correct schema).  Fine-tuning would also be the only systematic way to improve performance on use-case-specific data. \n",
    "\n",
    "In this example, we demonstrate fine-tuning on [Glaive's function calling dataset](https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2?row=0) using Anyscale platform. The goal for this example is to serve as a blue-print for performing data processing, training, and evaluation on open source LLMs for specific tasks like function calling, in the most effective way.  During this guide we mainly use the [fine-tuning API](https://docs.anyscale.com/endpoints/fine-tuning/fine-tuning-api), but the same blue-print can also be used with [fine-tuning template](https://docs.anyscale.com/examples/finetune-llms). \n",
    "\n",
    "The mentioned dataset consists of about 113,000 examples of synthetically generated function calling data. The dataset composition is given below:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/distr_glaive_pie.png\" alt=\"Distribution\" width=800>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Data Preprocessing](#step-1-data-preprocessing): In this section we will cover how we can use Ray Data to clean and format our raw dataset properly and create our train, valid, and test datasets.\n",
    "2. [Finetuning](#step-2-fine-tuning): This section will cover a few different ways you can fine-tune LLMs via Anyscale.\n",
    "3. [Serving](#step-3-serving): This section will cover how we can serve the fine-tuned model via Anyscale.\n",
    "4. [Evaluation](#step-4-evaluation): The section will lay down a blue-print for evaluation and compare performance to that of closed source models like OpenAI's GPT-4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make the necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import openai\n",
    "\n",
    "import ray.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fc_utils.data_format import TOOL_CALL_TAGS, TOOL_RESULT_TAGS, TOOL_LIST_TAGS, DatasetFormat\n",
    "from fc_utils.preprocessing import glaive_to_openai, openai_to_anyscale, save_to_jsonl\n",
    "from fc_utils.response_parsers import OpenAIResponseParser, AnyscaleResponseParser\n",
    "from fc_utils.eval_core import evaluate_model, Model\n",
    "from fc_utils.eval_data_utils import get_evaluation_dataset\n",
    "from fc_utils.plot_utils import plot_results\n",
    "from fc_utils.print_utils import pprint_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Preprocessing\n",
    "Our data processing will occur in 2-stages, as shown in the below figure:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/data_processing.png\" alt=\"Data preprocessing\" width=500>\n",
    "</p>\n",
    "\n",
    "\n",
    "Glaive's function calling dataset is formatted with specific indicators for roles and special tokens. We'll first map this dataset into the more general OpenAI chat format and then prepare it for fine-tuning with basic-chat template API supported by Anyscale. \n",
    "\n",
    "We'll use Ray Data for scalable data processing. First, let's load the dataset from the HuggingFace Hub and inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d0c8219dd440a78c056b59b3e240a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26126e625294f37ab84afe7b54119bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/271M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22db9a12fa6141beb66cedc8a3bcc57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a304c700da42ea8edb2696e832075a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 11:59:42,611\tINFO worker.py:1564 -- Connecting to existing Ray cluster at address: 10.0.13.171:6379...\n",
      "2024-06-04 11:59:42,617\tINFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-4m8ydikxzi8wditsrfy29llhky.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2024-06-04 11:59:42,623\tINFO packaging.py:358 -- Pushing file package 'gcs://_ray_pkg_d6f9d37acc49b477cb0cf4a3f5fd6469e16583c5.zip' (2.20MiB) to Ray cluster...\n",
      "2024-06-04 11:59:42,628\tINFO packaging.py:371 -- Successfully pushed file package 'gcs://_ray_pkg_d6f9d37acc49b477cb0cf4a3f5fd6469e16583c5.zip'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +33s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "\u001b[36m(autoscaler +33s)\u001b[0m [autoscaler] [8CPU-32GB] Upscaling 1 node(s).\n",
      "\u001b[36m(autoscaler +34s)\u001b[0m [autoscaler] [8CPU-32GB|m5.2xlarge] [us-west-2a] [on-demand] Launched 1 instances.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 12:01:02,368\tINFO dataset.py:2453 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n",
      "2024-06-04 12:01:02,371\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-06-04_09-26-56_061780_2460/logs/ray-data\n",
      "2024-06-04 12:01:02,372\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> LimitOperator[limit=1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91003abcfab417491617d9c17d01807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- limit=1 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7958af89c945c8a77e96641839feb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +5m50s)\u001b[0m [autoscaler] Downscaling node i-0ff7cd93f2844398b (node IP: 10.0.47.13) due to node idle termination.\n"
     ]
    }
   ],
   "source": [
    "hf_ds = datasets.load_dataset(\"glaiveai/glaive-function-calling-v2\", split=\"train\").shuffle(seed=21) \n",
    "# Sample only 10% of the dataset\n",
    "hf_ds_subset =  hf_ds.select(range(int(len(hf_ds)*0.10))) \n",
    "ray_ds = ray.data.from_huggingface(hf_ds_subset)\n",
    "first_ex = ray_ds.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre><span style=\"color: cyan;\">Chat: </span>USER: I need to set a reminder for my doctor&#x27;s appointment.\n",
       "\n",
       "\n",
       "ASSISTANT: Sure, I can help with that. Could you please provide me with the date and time of your appointment? &lt;|endoftext|&gt;\n",
       "\n",
       "\n",
       "USER: The appointment is on 2022-09-15 at 10:00 AM.\n",
       "\n",
       "\n",
       "ASSISTANT: &lt;functioncall&gt; {&quot;name&quot;: &quot;create_reminder&quot;, &quot;arguments&quot;: &#x27;{&quot;reminder_text&quot;: &quot;Doctor&#x27;s appointment&quot;, &quot;reminder_date&quot;: &quot;2022-09-15&quot;, &quot;reminder_time&quot;: &quot;10:00&quot;}&#x27;} &lt;|endoftext|&gt;\n",
       "\n",
       "\n",
       "FUNCTION RESPONSE: {&quot;status&quot;: &quot;success&quot;, &quot;message&quot;: &quot;Reminder for &#x27;Doctor&#x27;s appointment&#x27; on 2022-09-15 at 10:00 AM has been created successfully.&quot;}\n",
       "\n",
       "\n",
       "ASSISTANT: Your reminder for the doctor&#x27;s appointment on 2022-09-15 at 10:00 AM has been created successfully. You will be notified at the specified time. &lt;|endoftext|&gt;\n",
       "\n",
       "\n",
       "\n",
       "<span style=\"color: red;\">System: </span>SYSTEM: You are a helpful assistant with access to the following functions. Use them if required -\n",
       "{\n",
       "    &quot;name&quot;: &quot;create_reminder&quot;,\n",
       "    &quot;description&quot;: &quot;Create a reminder for a specific date and time&quot;,\n",
       "    &quot;parameters&quot;: {\n",
       "        &quot;type&quot;: &quot;object&quot;,\n",
       "        &quot;properties&quot;: {\n",
       "            &quot;reminder_text&quot;: {\n",
       "                &quot;type&quot;: &quot;string&quot;,\n",
       "                &quot;description&quot;: &quot;The content of the reminder&quot;\n",
       "            },\n",
       "            &quot;reminder_date&quot;: {\n",
       "                &quot;type&quot;: &quot;string&quot;,\n",
       "                &quot;format&quot;: &quot;date&quot;,\n",
       "                &quot;description&quot;: &quot;The date of the reminder&quot;\n",
       "            },\n",
       "            &quot;reminder_time&quot;: {\n",
       "                &quot;type&quot;: &quot;string&quot;,\n",
       "                &quot;format&quot;: &quot;time&quot;,\n",
       "                &quot;description&quot;: &quot;The time of the reminder&quot;\n",
       "            }\n",
       "        },\n",
       "        &quot;required&quot;: [\n",
       "            &quot;reminder_text&quot;,\n",
       "            &quot;reminder_date&quot;,\n",
       "            &quot;reminder_time&quot;\n",
       "        ]\n",
       "    }\n",
       "}\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint_example(first_ex, dataset_format=DatasetFormat.GLAIVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample in the dataset has two entries: system and chat. As mentioned, this dataset is formatted in a specific way (e.g. using USER, \\<|endoftext|\\> and other tokens). To enable fine-tuning on various open source models we need to convert each row to a more general format like the OpenAI chat format, which is the preferred format for fine-tuning instruction-tuned models on Anyscale ([dataset format guide](https://docs.endpoints.anyscale.com/endpoints/fine-tuning/dataset-prep)). The below code accomplishes the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 12:05:15,999\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-06-04_09-26-56_061780_2460/logs/ray-data\n",
      "2024-06-04 12:05:15,999\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[Map(_glaive_to_openai)->Filter(<lambda>)->Filter(filter_func)] -> LimitOperator[limit=1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662e76dd96e149dbae54985c9921467e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Map(_glaive_to_openai)->Filter(<lambda>)->Filter(filter_func) 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a4a0ff190214e4586a42d64187a37b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- limit=1 2:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a02f19eab948fc8699c41561f7e586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initial preprocessing to get to the OpenAI format\n",
    "openai_fmt_ds = glaive_to_openai(ray_ds)\n",
    "first_ex = openai_fmt_ds.take(1)[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre><span style=\"color: cyan;\">Messages: </span>\n",
       "\t<span style=\"color: red;\">system: </span>You are a helpful assistant.\n",
       "\t<span style=\"color: green;\">user: </span>I need to set a reminder for my doctor&#x27;s appointment.\n",
       "\t<span style=\"color: blue;\">assistant: \n",
       "\t\tcontent: </span>Sure, I can help with that. Could you please provide me with the date and time of your appointment? \n",
       "\t\t<span style=\"color: blue;\">tool_calls: </span>[]\n",
       "\t<span style=\"color: green;\">user: </span>The appointment is on 2022-09-15 at 10:00 AM.\n",
       "\t<span style=\"color: blue;\">assistant: \n",
       "\t\tcontent: </span>\n",
       "\t\t<span style=\"color: blue;\">tool_calls: </span>[{&#x27;function&#x27;: {&#x27;arguments&#x27;: &#x27;{&quot;reminder_text&quot;: &quot;Doctors appointment&quot;, &quot;reminder_date&quot;: &quot;2022-09-15&quot;, &quot;reminder_time&quot;: &quot;10:00&quot;}&#x27;, &#x27;name&#x27;: &#x27;create_reminder&#x27;}, &#x27;type&#x27;: &#x27;function&#x27;}]\n",
       "\t<span style=\"color: yellow;\">tool: </span>{&quot;name&quot;: &quot;create_reminder&quot;, &quot;content&quot;: &quot;{\\&quot;status\\&quot;: \\&quot;success\\&quot;, \\&quot;message\\&quot;: \\&quot;Reminder for &#x27;Doctor&#x27;s appointment&#x27; on 2022-09-15 at 10:00 AM has been created successfully.\\&quot;}&quot;, &quot;tool_call_id&quot;: &quot;call_1&quot;}\n",
       "\t<span style=\"color: blue;\">assistant: \n",
       "\t\tcontent: </span>Your reminder for the doctor&#x27;s appointment on 2022-09-15 at 10:00 AM has been created successfully. You will be notified at the specified time. \n",
       "\t\t<span style=\"color: blue;\">tool_calls: </span>[]\n",
       "<span style=\"color: magenta;\">Tools: </span>[{&quot;type&quot;: &quot;function&quot;, &quot;function&quot;: {&quot;name&quot;: &quot;create_reminder&quot;, &quot;description&quot;: &quot;Create a reminder for a specific date and time&quot;, &quot;parameters&quot;: {&quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: {&quot;reminder_text&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;The content of the reminder&quot;}, &quot;reminder_date&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;format&quot;: &quot;date&quot;, &quot;description&quot;: &quot;The date of the reminder&quot;}, &quot;reminder_time&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;format&quot;: &quot;time&quot;, &quot;description&quot;: &quot;The time of the reminder&quot;}}, &quot;required&quot;: [&quot;reminder_text&quot;, &quot;reminder_date&quot;, &quot;reminder_time&quot;]}}}]\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect one example\n",
    "pprint_example(first_ex, dataset_format=DatasetFormat.OPENAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you notice, the tool calls are almost exactly in the OpenAI format, i.e. `assistant` messages can include `tool_calls` and `tool` is also a role just like `user` or `assistant`. The only slight difference is that it is short of the `id` entry provided by the OpenAI API. For training, we choose to leave the model out of ID generation. Internally, each tool call is kept track by its index in the list of tool calls made. This is used later in the tool response (In the above example, there is only one tool call made and the tool response has `tool_call_id` \"call_1\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess to the Anyscale format\n",
    "We'll now further process this conversation format and make it compatible with Anyscale Endpoints. We'll make use of special indicators \"\\[TOOL_CALLS\\]\" and \"\\[/TOOL_CALLS\\]\" to format assistant tool calls into the message \"content\" field. The role \"tool\" will be converted to the role \"user\" with a special indicator to highlight that this is a tool response. Further, the tool list will be included in the system prompt with special indicators. The following code block handles the necessary preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map to Anyscale format\n",
    "processed_ds = openai_to_anyscale(openai_fmt_ds)\n",
    "first_ex = processed_ds.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect one example\n",
    "pprint_example(first_ex, dataset_format=DatasetFormat.ANYSCALE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a train, validation and test split and save the datasets in the `jsonl` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80/10/10 split\n",
    "train_ds, val_ds, test_ds = processed_ds.split_proportionately([0.8, 0.1])\n",
    "# Restrict to 200 examples for testing\n",
    "test_ds, _  = test_ds.split_at_indices([200]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect final counts\n",
    "train_ds.count(), val_ds.count(), test_ds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up file save paths. Feel free to change these\n",
    "train_file_path = \"glaiveai-function-calling-v2-train.jsonl\"\n",
    "validation_file_path = \"glaiveai-function-calling-v2-val.jsonl\"\n",
    "test_file_path = \"glaiveai-function-calling-v2-test.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the datasets to jsonl format\n",
    "save_to_jsonl(train_ds, train_file_path)\n",
    "save_to_jsonl(val_ds,  validation_file_path)\n",
    "save_to_jsonl(test_ds, test_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Fine-tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fine-tuning, you have two options with Anyscale:\n",
    "1. Fine-tuning on the Anyscale Platform through our fine-tuning template \n",
    "    - This would be the preferred route for those wishing to get more flexibility in choice of models and hyperparameters, better monitoring, etc.\n",
    "2. Fine-tuning through Anyscale's serverless endpoints\n",
    "    - A quick and easy way to fine-tune a model via an OpenAI compatiable SDK.\n",
    "\n",
    "For this guide, we will use `Llama-3-8B-Instruct` as the base model for fine-tuning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2(a): Fine-tuning on the Anyscale Platform\n",
    "\n",
    "Head over to the Anyscale Platform: https://console.anyscale.com/v2 and spin up the \"Fine-tune LLMs\" template (under \"AI application templates\")\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/templates.png\" alt=\"Templates\">\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "Follow the instructions to run your fine-tuning job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2(b): Fine-tuning through serverless endpoints\n",
    "First, obtain your credentials from the [Anyscale platform](https://console.anyscale.com/credentials) and upload the training and validation files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your API key from https://console.anyscale.com/credentials\n",
    "ANYSCALE_API_KEY = \"esecret_yourKeyHere\"  \n",
    "ANYSCALE_API_BASE = \"https://api.endpoints.anyscale.com/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anyscale Endpoints are OpenAI compatible\n",
    "client = openai.OpenAI(\n",
    "    base_url = ANYSCALE_API_BASE,\n",
    "    api_key = ANYSCALE_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the files to Anyscale\n",
    "training_file_id = client.files.create(\n",
    "    file=open(train_file_path,'rb'),\n",
    "    purpose=\"fine-tune\",\n",
    ").id\n",
    "\n",
    "valid_file_id = client.files.create(\n",
    "    file=open(validation_file_path,'rb'),\n",
    "    purpose=\"fine-tune\",\n",
    ").id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now launch a fine-tuning job for 4 epochs. The expected time for this job is < 3 hours. For instructions on viewing job status, other hyperparameters used, etc, you can refer to our [fine-tuning guide](https://docs.anyscale.com/preview/examples/e2e-finetune-and-serve-example#4-start-the-fine-tuning). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create finetuning job. Other parameters like context length will be chosen appropriately based on dataset size\n",
    "fine_tuning_job_id = client.fine_tuning.jobs.create(\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    hyperparameters={\"n_epochs\": 4},\n",
    "    training_file=training_file_id,\n",
    "    validation_file=valid_file_id,\n",
    ").id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Serving\n",
    "\n",
    "## Step 3(a): Finetuned on the Anyscale Platform\n",
    "\n",
    "Make a note of the final checkpoint after fine-tuning (this should be the last line in the logs). You can now spin up the \"Deploy LLMs\" template which has all the instructions and required dependencies to serve your finetuned model efficiently. You will find the tutorials on [serving LoRA models](https://github.com/anyscale/templates/blob/main/templates/endpoints_v2/examples/lora/DeployLora.ipynb) (if applicable) and on deploying a [custom model](https://github.com/anyscale/templates/blob/main/templates/endpoints_v2/examples/CustomModels.ipynb) helpful. Once you have set up your fine-tuned model as an Anyscale Service, head over to the \"Services\" tab in the console and select your deployed service. \n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/services_list.png\" alt=\"Services list\">\n",
    "</p>\n",
    "\n",
    "\n",
    "Click on the \"Query\" drop down box to get instructions on how to query your deployed model. Note down the base URL and API key and place them here.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/service_token.png\" alt=\"Services token\" width=\"600\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To be run only if you finetuned on the Anyscale platform\n",
    "FINETUNED_MODEL_API_KEY=\"your-service-api-key-here\"\n",
    "# Example api base url: https://endpoints-v2-zzzz.s.anyscaleuserdata.com\n",
    "FINETUNED_MODEL_API_BASE=\"your-service-url-here\" \n",
    "FINETUNED_MODEL_API_BASE = f\"{FINETUNED_MODEL_API_BASE}/v1\"\n",
    "# Enter the model id here. This would be different depending on whether you performed LoRA or full parameter fine-tuning.\n",
    "# Example: meta-llama/Meta-Llama-3-8B-Instruct:mysuffix:myid \n",
    "MODEL_ID = \"your-model-id-here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3(b): Finetuned through serverless endpoints\n",
    "\n",
    "To serve the fine-tuned model, you just need to navigate to the \"Serving\" section on the Anyscale Platform. Your fine-tuned model should already be visible in the list of available models! Make sure to note down the model ID here.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/serving_endpoints.png\" alt=\"Serve Endpoints\">\n",
    "</p>\n",
    "\n",
    "\n",
    "As in the above image, click on the three dots and then click on \"Query\". This will provide you the starter code to interact with the model via curl, python, etc. Note that the API key here is valid only for one hour. Since our evaluation can take up longer, we will generate a long-lived credential. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/serve_api_key.png\" alt=\"Serve API Key\">\n",
    "</p>\n",
    "\n",
    "In the \"API Keys\" page, click on \"Create\" and note down the API key.\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/long_lived_api_key.png\" alt=\"Long Lived API Key\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is only if you finetuned through serverless endpoints\n",
    "FINETUNED_MODEL_API_BASE = \"https://api.endpoints.anyscale.com/v1\"\n",
    "FINETUNED_MODEL_API_KEY = \"esecret_yourKeyHere\"\n",
    "MODEL_ID = \"yourModelIdHere\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Try out the model via Playground\n",
    "\n",
    "(For Endpoints users) You can try out your new model in the Playground: https://console.anyscale.com/v2/playground . In the model dropdown, you should be able to see your finetuned model as shown below\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/playground.png\" alt=\"Playground\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Evaluation\n",
    "\n",
    "Let's evaluate our trained model. Here we'll use two baselines: (1) the base model before finetuning and (2) GPT-4. Note that in a real world setting, you would evaluate your base model *first* before going forward with fine-tuning. \n",
    "\n",
    "\n",
    "## Evaluation strategy\n",
    "\n",
    "Evaluation of function calling capability is non-trivial, given that we're looking to extract structured data from an inherently unpredictable and unstructured stream of text. We will use the following simple evaluation strategy: The models are evaluated on the accuracy metric and their responses are graded as accurate if their response for each assistant entry in the conversation is correct. An assistant response is graded as correct under the below conditions:\n",
    "1. In case the ground truth response contains no function call, then the model's response should not have a function call. \n",
    "2. In case the ground truth response contains a function call, then the model's response should also have a function call. The assistant function call should further have the correct function name and the correct function arguments. \n",
    "\n",
    "The following psuedocode shows the high-level branching conditions considered during evaluation:\n",
    "\n",
    "```\n",
    "correct = True\n",
    "if(ground_truth has no function call):\n",
    "    correct = (response has no function call)\n",
    "else\n",
    "    if response has no function call: \n",
    "        correct = False\n",
    "    else\n",
    "          if response.function_name != ground_truth.function_name:\n",
    "                correct = False\n",
    "          else\n",
    "               for every (param, value) in ground_truth.argument_dict:\n",
    "                    if (param, value) not in response.argument_dict:\n",
    "                        correct = False\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset formatting\n",
    "  \n",
    "We process our test dataset individually for each model as follows:\n",
    "- For GPT-4, we undo some of the preprocessing previously done to get back the conversations in the OpenAI format. All expected assistant responses in the dataset are processed to have the `\"content\"` and the `\"tool_calls\"` field. \n",
    "- We follow the same preprocessing as during training for the finetuned model. However, for the expected assistant response, we process it in the same way as GPT-4 (i.e parse all tool calls and store them in a separate `\"tool_calls\"` field).\n",
    "- For the base model, we include a special system prompt that instructs it to output the tool calls, if any, in our pre-defined format (enclosing it in special indicators, etc) and further format tool responses in the same way as we did for the fine-tuned model. This lays out an even ground for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the test dataset for evaluation\n",
    "eval_ds_base =  get_evaluation_dataset(test_ds, TOOL_CALL_TAGS, TOOL_RESULT_TAGS, TOOL_LIST_TAGS, Model.BASE)\n",
    "eval_ds_finetuned = get_evaluation_dataset(test_ds, TOOL_CALL_TAGS, TOOL_RESULT_TAGS, TOOL_LIST_TAGS, Model.FINETUNED)\n",
    "eval_ds_gpt = get_evaluation_dataset(test_ds, TOOL_CALL_TAGS, TOOL_RESULT_TAGS, TOOL_LIST_TAGS, Model.GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect one example from the eval dataset for the finetuned model\n",
    "pprint_example(eval_ds_finetuned[1], dataset_format=DatasetFormat.OPENAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "For evaluation, we initialise parsers - one for each model - to handle obtaining chat completions from the respective API and parsing the result. Then, our evaluation logic takes care of matching the assistant response with the expected response and, if the response is incorrect, making note of the type of error (wrong intent, wrong function name, etc). A high-level overview of our evaluation code for the fine-tuned model is given below:\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/eval_logic.png\" alt=\"Evaluation\" width=800>\n",
    "</p>\n",
    "\n",
    "Internally, evaluation of each example (for the given parser) is handled by the function `parse_and_eval`. We'll use a dataset-level function `evaluate_model` that provides the full results along with model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate the API keys below (make sure you have already populated the API keys for your finetuned model) and run the below code blocks to get evaluation results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your OpenAI key below.\n",
    "OPENAI_API_KEY = \"your-openai-key-here\" \n",
    "OPENAI_API_BASE = \"https://api.openai.com/v1\"\n",
    "\n",
    "# Base model config \n",
    "BASE_MODEL_API_BASE = \"https://api.endpoints.anyscale.com/v1\"\n",
    "BASE_MODEL_ID=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# Enter your Endpoints API key below from https://console.anyscale.com/credentials\n",
    "BASE_MODEL_API_KEY = \"your-endpoints-key-here\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parsers\n",
    "base_model_parser = AnyscaleResponseParser(api_key=BASE_MODEL_API_KEY, api_base=BASE_MODEL_API_BASE, model=BASE_MODEL_ID, tool_call_tags=TOOL_CALL_TAGS)\n",
    "\n",
    "finetuned_model_parser = AnyscaleResponseParser(api_key=FINETUNED_MODEL_API_KEY, api_base=FINETUNED_MODEL_API_BASE, model=MODEL_ID, tool_call_tags=TOOL_CALL_TAGS) \n",
    "\n",
    "openai_parser = OpenAIResponseParser(api_key=OPENAI_API_KEY, api_base=OPENAI_API_BASE, model=\"gpt-4\", tool_call_tags=TOOL_CALL_TAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate base model \n",
    "results_base, accuracy_base = evaluate_model(eval_ds_base, base_model_parser, Model.BASE)\n",
    "print(\"Base Model Accuracy: \", accuracy_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate our finetuned model\n",
    "results_finetuned, accuracy_finetuned = evaluate_model(eval_ds_finetuned, finetuned_model_parser, Model.FINETUNED)\n",
    "print(\"Fine-tuned Model Accuracy: \", accuracy_finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate gpt-4\n",
    "results_gpt, accuracy_gpt = evaluate_model(eval_ds_gpt, openai_parser, Model.GPT)\n",
    "print(\"GPT-4 Accuracy: \", accuracy_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plot_results(results_base, results_finetuned, results_gpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how your plot might look like for `Llama-3-8B-Instruct`:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/error_analysis.png\" alt=\"Error Analysis\">\n",
    "</p>\n",
    "\n",
    "The base model is a lot more trigger happy when tools are available and further makes a number of mistakes in formatting (generating tool calls with the right schema) and providing the right argument values (making accurate tool calls). A number of these issues are eliminated with fine-tuning and the final fine-tuned model rivals GPT-4 level performance on this dataset.  Note that the difference would be larger in a real-world setting, because our test dataset construction was straightforward and it is very similar to the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Congrats! You have now fine-tuned an open source model that can rival GPT-4 on function calling. As a quick recap, here's what we demonstrated in this notebook:\n",
    "1. Preprocessing a function calling dataset into a conversational format\n",
    "2. Fine-tuning a language model through either the Anyscale Platform or through Anyscale Endpoints\n",
    "3. Serving the fine-tuned model on Anyscale\n",
    "4. Evaluating the model against GPT-4 and analysing the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
