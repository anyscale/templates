{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preference Tuning for Summarization using Synthetic Data\n",
    "\n",
    "**⏱️ Time to complete**: \\<TODO\\>\n",
    "\n",
    "Preference tuning is a powerful tool that can optimize LLMs towards complex preferences that can not easily captured through supervised fine-tuning. However, manually annotating preferences between model outputs using human raters can be extremely time-consuming and expensive. Instead, synthetic preference data can be generated by scoring responses with large foundation models, allowing for much cheaper and scalable data collection!\n",
    "\n",
    "Here we'll go through an end-to-end example for preference tuning of an open-source language model with synthetic data, covering data preprocessing, fine-tuning and evaluation. We will focus on the task of summarization for the [CNN/DailyMail](https://huggingface.co/datasets/abisee/cnn_dailymail) dataset. \n",
    "\n",
    "This notebook is based on the following blog post: `TODO`.\n",
    "\n",
    "In our blog post, we use `Meta-Llama-3.1-70B-Instruct` to judge the summaries and perform full parameter fine-tuning for DPO. However, this requires A100 GPUs, so in this notebook we set the default configs to leverage `Meta-Llama-3.1-8B-Instruct` and LoRA fine-tuning using A10G GPUs. We also have additional configs for A100s if you wish to replicate the results from the blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Data Preprocessing](#step-1-data-preprocessing): In this section we cover how we can prepare preference data for the summarization task using an LLM-as-a-judge. \n",
    "2. [DPO Finetuning](#step-2-fine-tuning): This section will cover how you can fine-tune an open source model on the preference data on the Anyscale platform.\n",
    "3. [Evaluation](#step-3-evaluation): The section will lay down a blue-print for evaluation and compare performance to that of closed source models like OpenAI's GPT-4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make the necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import datasets\n",
    "import openai\n",
    "\n",
    "import ray.data\n",
    "\n",
    "import pprint\n",
    "import textwrap\n",
    "\n",
    "os.environ[\"PYTHONPATH\"] = f\"{os.environ.get('PYTHONPATH', '')}:src\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Synthetic Data Generation\n",
    "\n",
    "First, let's inspect the training dataset and look at an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_ds = datasets.load_dataset(\"abisee/cnn_dailymail\", '3.0.0', split=\"train\").shuffle(seed=21)\n",
    "# extract a subset of 20000 articles\n",
    "hf_ds_subset =  hf_ds.select(range(20000))\n",
    "\n",
    "ray_ds = ray.data.from_huggingface(hf_ds_subset)\n",
    "raw_example = ray_ds.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'article': 'Scam: Lisa Harrison, 34, promised customers low currency rates on '\n",
      "            'US dollars and special deals . A wedding planner who stole '\n",
      "            \"£80,000 from couples in a bid to satisfy an 'out-of-control' \"\n",
      "            'online gambling addiction has been jailed. Lisa Harrison, 34, '\n",
      "            'began taking money from her clients in summer 2013 by enticing '\n",
      "            'them with low currency rates on US dollars and flight upgrades. '\n",
      "            'She took money from 19 couples who had entrusted their savings to '\n",
      "            'her after being promised the wedding of their dreams. It is '\n",
      "            'understood that the company she worked for, iPlan New York, '\n",
      "            'specialised in weddings in New York City. Her website '\n",
      "            \"iplannewyork.com, which has been taken down, said: 'iPlan New \"\n",
      "            'York was set up to create and style the perfect tailor made '\n",
      "            \"wedding for couples travelling to New York to get married! 'We \"\n",
      "            'are passionate about what we do and passionate about New York! We '\n",
      "            'have experience in planning NYC weddings for couples from all '\n",
      "            \"over the world.' But she was arrested in December last year after \"\n",
      "            'eventually coming clean to her victims in an email and saying she '\n",
      "            'had been forced to close her business. Police soon found she had '\n",
      "            'taken £80,107 from the couples and spent a staggering £77,933 on '\n",
      "            \"gambling sites Paddy Power and William Hill. The business' \"\n",
      "            'Facebook page has also been deleted, but outraged victims have '\n",
      "            'shared their victims on a wedding forum. One victim called '\n",
      "            \"Jennifer wrote in November 2013: 'I had previously given Lisa a \"\n",
      "            \"positive review because our vow renewal went wonderful. 'Little \"\n",
      "            \"did I know until last week that she didn't even pay the vendors \"\n",
      "            \"that helped with our ceremony. 'I am so disgusted and can't \"\n",
      "            'fathom such an act. We paid her in full and to think that our '\n",
      "            \"photographer didn't even get paid is just astonishing to me. 'I \"\n",
      "            'feel so horrible for the other couples that had their perfect day '\n",
      "            \"planned and this woman decided to perform such an act. 'I pray \"\n",
      "            'for each of you in hopes that you will be able to move on from '\n",
      "            'this and live a healthy and happy life with your significant '\n",
      "            \"other. 'I can't believe this woman took our money and did such an \"\n",
      "            'unthinkable act. God bless all of you and I hope this mess gets '\n",
      "            \"corrected quickly.' While another anonymous victim posted a copy \"\n",
      "            'of the email they claim they had been sent by Harrison when she '\n",
      "            \"admitted the scam. It read: 'I have to announce the closure of \"\n",
      "            \"iPlan New York. 'For some time now I have been battling against a \"\n",
      "            'gambling addiction that has seen me lose all of the company money '\n",
      "            \"including money paid to me by you for services and dollars. 'I \"\n",
      "            'cannot go on another day with this situation as this illness has '\n",
      "            'taken me over completely and I have to both face up to the '\n",
      "            'consequences of my actions and seek help for the debilitating '\n",
      "            \"addiction. 'I am extremely ill with it and need to seek help as \"\n",
      "            \"soon as possible. 'I am completely devastated that not only have \"\n",
      "            'I lost money of yours but betrayed your trust as a wedding '\n",
      "            \"planner. 'Right now I am uncertain as to what the future holds \"\n",
      "            'with regards to future weddings already planned, I will be in '\n",
      "            'touch with the suppliers in NYC to inform them also. Sentence: '\n",
      "            'Harrison, of Earith, Cambridgeshire, was jailed for two years at '\n",
      "            \"Peterborough Crown Court, above . 'I will today be having my \"\n",
      "            'computer and all electronic devices ceased (sic) under an '\n",
      "            'intervention and handing myself into the police to give a '\n",
      "            \"statement and to tell them everything. 'No doubt you will be \"\n",
      "            'informing the police too and for those purposes it will be the '\n",
      "            'Cambridgeshire Constabulary and my full name is Lisa Harrison and '\n",
      "            \"I will be handing myself in after sending these emails. 'I won’t \"\n",
      "            'be able to reply to any emails or calls for the time being as I '\n",
      "            \"will not have access. 'I am truly from the bottom of my heart so \"\n",
      "            'sorry for everything, as with addictions I thought I had '\n",
      "            'everything under control and was in denial that I could put '\n",
      "            'everything right, which I have been trying so desperately to do. '\n",
      "            \"'As soon as and if I am able to communicate further about any \"\n",
      "            \"outstanding issues I will do so. Lisa.' Posting on an online \"\n",
      "            'review site for the wedding service, one former customer said: '\n",
      "            \"'We are due to go in less than 48 hours and we have nothing!! She \"\n",
      "            \"has now closed down her website too! She has left us devastated!' \"\n",
      "            \"Another, using the name Shaun, wrote: 'Alarm bells rang for me \"\n",
      "            'when she asked for all our spending money cos she had a deal on a '\n",
      "            \"currency card. While one woman, using the name Andrea, said: 'I \"\n",
      "            'am absolutely devastated for anyone who has used iPlan New York '\n",
      "            \"and subsequently been let down'. She added that she had a 'gut \"\n",
      "            \"feeling' not to pay upfront. Harrison, of Earith, Cambridgeshire, \"\n",
      "            'admitted fraudulent trading and was jailed for two years at '\n",
      "            'Peterborough Crown Court on Tuesday. Det Sgt Iain Moor, from '\n",
      "            \"Cambridgeshire Constabulary, said: 'This was an extremely \"\n",
      "            'distressing case for the 19 couples who lost life savings and had '\n",
      "            \"their dream day ruined by Harrison. 'I hope the victims received \"\n",
      "            'some comfort in the prison sentence imposed on Harrison, meaning '\n",
      "            \"they can now start to re-build their lives.'\",\n",
      " 'highlights': 'Lisa Harrison enticed clients with low currency rates and '\n",
      "               'flight upgrades .\\n'\n",
      "               'She took money from 19 couples who were promised dream '\n",
      "               'weddings .\\n'\n",
      "               'Harrison spent nearly £78,000 on gambling sites including '\n",
      "               'Paddy Power .\\n'\n",
      "               'She admitted the scam to victims in an email before handing '\n",
      "               'herself in .\\n'\n",
      "               \"Outraged victims say they are 'disgusted' and have been left \"\n",
      "               \"'devastated'\\n\"\n",
      "               \"In email Harrison says she will 'seek help for the \"\n",
      "               \"debilitating addiction'\\n\"\n",
      "               'She admitted fraudulent trading and was jailed for two years '\n",
      "               'on Tuesday .',\n",
      " 'id': '4feb82c680166f0b8f90bf3a6f9779b04f229325'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(raw_example, width=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to get preference data for pairs of summaries generated from the same article. Traditionally, this would involve generating summaries using the base model you wish to fine-tune and asking human annotators to provide a rating for each sample. In this example, we will employ a _synthetic_ summary scoring method using an LLM as a judge. We score the correctness of a summary using the following metrics:\n",
    "\n",
    "**Summary Scoring Metrics**\n",
    "1. Multiple choice Q&A accuracy:\n",
    "    - Given the original text, we use an LLM judge to generate 5 multiple choice questions about the text.\n",
    "    - We then ask the LLM judge to answer the questions using only the summary, and record the number of questions correctly answered.\n",
    "2. Word count: We simply count the number of words in the summary.\n",
    "\n",
    "This allows us to construct a simple preference function between two summaries:\n",
    "\n",
    "**Preference Function**\n",
    "1. If both summary responses attain ≥3 multiple choice questions correct, we will prefer the shorter response. We do not care about Q&A accuracy beyond 3 correct answers, since the summary should not contain all information from the text.\n",
    "2. Otherwise, we select the response that leads to more correctly answered multiple choice questions.\n",
    "\n",
    "To generate the training data, we will generate 10 summaries from each article using the model we wish to fine-tune. Then, we will randomly sample pairs of summaries and use our preference function to annotate the preference between them.\n",
    "\n",
    "For this example, we will use `Mistral-7B-Instruct-v0.1` as the base model to fine-tune and `Llama-3.1-70B-Instruct` as a judge.\n",
    "\n",
    "Combining all this together, our data pre-processing pipeline is going to look as follows: \n",
    "\n",
    "![preprocessing](./assets/preprocessing.png?1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "TODO\n",
    "\\<We have the relevant preprocessing code in `utils/generate_questions.py` and `utils/generate_summaries_and_scores.py`. You can run data generation as an Anyscale job with configs/generate_questions_job.yaml and configs/generate_summaries_job.yaml.\\>\n",
    "\n",
    "\\<After preprocessing, here's an example for the Q&A generated by Llama 70B and here's an example for the summaries generated by Mistral 7B Instruct \\>\n",
    "\n",
    "\n",
    "\\<We sample chosen and rejected messages from the summaries based on the Q&A Accuracy score. We use a threshold of 3/5 for classifying examples as 'chosen' and 'rejected'. Here's an example training dataset sample for the DPO model\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Multiple Choice Questions from Articles\n",
    "\n",
    "First, we will generate the multiple choice questions and answers for each article using `Llama-3.1-8B-Instruct` (or `70B` if you have A100/H100s). Leveraging vLLM and Ray, we can very easily scale this generation process across multiple GPUs.\n",
    "\n",
    "The following command will run the [src/scripts/generate_questions.py](./src/scripts/generate_questions.py) script, which generates the questions and answers and saves them in `.parquet` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mOutput\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +1.4s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mSubmitting job with config JobConfig(name='preference-tuning-summarization-question-generation', image_uri='localhost:5555/anyscale/endpoints_aica:0.5.0-6402', compute_config=None, env_vars=None, py_modules=None, cloud=None, project=None, ray_version=None, job_queue_config=None).\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +3.6s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mUsing workspace runtime dependencies env vars: {'WANDB_API_KEY': 'cbc4aed2de2d9c9acb21324a3297b85b7299479b'}.\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +3.6s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mUploading local dir '.' to cloud storage.\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +5.0s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mIncluding workspace-managed pip dependencies.\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +5.6s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mJob 'preference-tuning-summarization-question-generation' submitted, ID: 'prodjob_sdaruzx8uu3c2bu3x5dn6gpf77'.\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +5.6s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mView the job in the UI: https://console.anyscale.com/jobs/prodjob_sdaruzx8uu3c2bu3x5dn6gpf77\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +5.6s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mUse `--wait` to wait for the job to run and stream logs.\u001b[0m\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!anyscale job submit -f configs/generate_questions_job.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the job, you should see the remote path to the folder with Q&A. Make sure to make note to use it for the next steps! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with the link to the output folder from the previous job\n",
    "qa_folder = f\"s3://air-example-data/preference-tuning-summarization-example/qa_generation/qa_annotations_full_train/\"\n",
    "qa_ds = ray.data.read_parquet(qa_folder)\n",
    "# The dataset is small, we can materalize it\n",
    "example_rows = qa_ds.materialize().take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT:\n",
      "(RollingStone.com) -- Jennifer Lawrence, the 20-year-old Oscar nominee for Best\n",
      "Actress, is sitting in a fancy Manhattan hotel sipping tea and feeling a little\n",
      "out of place. See, she grew up in Louisville, Kentucky, where her dad owned a\n",
      "construction company and her mom ran a summer camp. They had land and horses.\n",
      "She loved to fish. She was a total tomboy: field hockey, softball, basketball on\n",
      "an all-boys team. (\"I was so dykey.\") One of her nicknames was Nitro. She lives\n",
      "in Los Angeles now, but \"little redneck things still come out.\" Like what? \"I'm\n",
      "attracted to my brother. Stuff like that.\" 10 Best Movies of 2010 . At 14, she\n",
      "decided she wanted to be an actress and dragged her mom to New York for\n",
      "auditions. The people at Reese's Peanut Butter Cups told her she was the best\n",
      "they'd ever seen. Her mom told her they were lying. (Her mom didn't like showbiz\n",
      "much.) She auditioned for the role of Bella in \"Twilight,\" which would have been\n",
      "perfect if Bella were a badass, but since she's a frightened waif, Lawrence\n",
      "ended up not getting the part. Which was for the best because the role she did\n",
      "get was for \"Winter's Bone,\" in which she's fantastic: harrowing and tender as\n",
      "the 17-year-old daughter of an Ozarks meth-cooker who's fighting to take care of\n",
      "her little brother and sister. This article appears in the February 17, 2011\n",
      "issue of Rolling Stone. The issue is available now on newsstands and will appear\n",
      "in the online archive February 4. To prep for the part, Lawrence learned how to\n",
      "shoot a gun and field-dress squirrels. She already knew how to chop wood: \"I\n",
      "went through a wood-chopping phase when I was nine or 10.\" She says she hasn't\n",
      "even bothered preparing an Oscar speech: \"I have been practicing my losing face,\n",
      "though. Do you want to see it?\" (For the record, it's a very good losing face.)\n",
      "Peter Travers Reviews 'Winter's Bone' Later this year comes \"X-Men: First\n",
      "Class,\" where she'll play the mutant Mystique, blue-skinned and topless. (\"Did I\n",
      "feel naked being naked?\" she asks, so you don't have to. \"Yeah. Totally.\") But\n",
      "before that there's Jodie Foster's \"The Beaver,\" premiering in May, in which she\n",
      "appears alongside a certifiable Mel Gibson. Which means she has some crazy Mel\n",
      "Gibson stories, right? She leans in close. \"If I say, 'Off the record' -- that\n",
      "means you can't print it, right?' \" Right. \"OK. So, off the record ...\" She's\n",
      "learning. Photos: 2011 Screen Actors Guild Award Winners . Copyright © 2011\n",
      "Rolling Stone.\n",
      "QUESTIONS:\n",
      "Q1) Where did Jennifer Lawrence grow up? A. The Ozarks B. Manhattan C. Los\n",
      "Angeles, California D. New York City, New York E. Louisville, Kentucky  Q2) What\n",
      "is the name of the movie in which Jennifer Lawrence will play the mutant\n",
      "Mystique? A. Winter's Bone B. X-Men: First Class C. The Beaver D. Twilight E.\n",
      "The Hunger Games  Q3) What is one skill Jennifer Lawrence learned to prepare for\n",
      "her role in \"Winter's Bone\"? A. How to play field hockey B. How to chop wood C.\n",
      "How to ride a horse D. How to act like a frightened waif E. How to shoot a gun\n",
      "Q4) Why did Jennifer Lawrence's mom take her to New York at the age of 14? A. To\n",
      "visit relatives B. To attend a sports tournament C. To go shopping D. For\n",
      "auditions E. For a family vacation  Q5) What was Jennifer Lawrence's role in the\n",
      "movie \"Winter's Bone\"? A. A character in a Jodie Foster movie B. A mutant with\n",
      "blue skin C. A character in a Reese's Peanut Butter Cups commercial D. A\n",
      "frightened waif E. A 17-year-old daughter of an Ozarks meth-cooker\n",
      "ANSWERS:\n",
      "['E' 'B' 'E' 'D' 'E']\n",
      "'===================================================================================================='\n"
     ]
    }
   ],
   "source": [
    "from src.utils.models import DataSchema\n",
    "\n",
    "for row in example_rows:\n",
    "    print(\"TEXT:\")\n",
    "    print(textwrap.fill(row[DataSchema.ARTICLE], width=80))\n",
    "    print(\"QUESTIONS:\")\n",
    "    print(textwrap.fill(row[DataSchema.MCQ_QUESTIONS], width=80))\n",
    "    print(\"ANSWERS:\")\n",
    "    print(textwrap.fill(str(row[DataSchema.GROUND_TRUTH_MCQ_ANSWERS]), width=80))\n",
    "    pprint.pprint(\"=\" * 100, width=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Summaries + Scores\n",
    "\n",
    "Next, we will generate 10 summaries for each article in the training set and score them with our Q&A judging setup. \n",
    "\n",
    "The following command will run the `TODO` script, which takes in the folder of questions and generates the results to a new folder of `.parquet` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mOutput\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +1.1s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mSubmitting job with config JobConfig(name='preference-tuning-summarization-question-generation', image_uri='localhost:5555/anyscale/endpoints_aica:0.5.0-6402', compute_config=None, env_vars=None, py_modules=None, cloud=None, project=None, ray_version=None, job_queue_config=None).\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +3.5s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mUsing workspace runtime dependencies env vars: {'WANDB_API_KEY': 'cbc4aed2de2d9c9acb21324a3297b85b7299479b'}.\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +3.5s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mUploading local dir '.' to cloud storage.\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +4.5s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mIncluding workspace-managed pip dependencies.\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +5.1s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mJob 'preference-tuning-summarization-question-generation' submitted, ID: 'prodjob_8m2iu1lcd44s2e7q95rcrxvzzx'.\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +5.1s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mView the job in the UI: https://console.anyscale.com/jobs/prodjob_8m2iu1lcd44s2e7q95rcrxvzzx\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[36m(anyscale +5.1s)\u001b[0m \u001b[0m\u001b[0m\u001b[0m\u001b[0mUse `--wait` to wait for the job to run and stream logs.\u001b[0m\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!anyscale job submit -f configs/generate_summaries_train_job.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with the link to the generated summaries\n",
    "summary_folder = f\"s3://air-example-data/preference-tuning-summarization-example/summary_generation_base/train/\" \n",
    "summary_ds = ray.data.read_parquet(summary_folder)\n",
    "example_rows = summary_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT:\n",
      "A 43-year-old mother has died in a house fire in Mount Helen, near Ballarat.\n",
      "Police originally feared the woman and her son and daughter, both 21, had died\n",
      "in the blaze but they have been safely located, the Ballarat Courier reported. A\n",
      "neighbour said she heard a 'whooshing' noise like a firecracker and a loud\n",
      "explosion before the fire started at about 1.30am on Friday. Two women and a man\n",
      "feared dead in house fire at Mount Helen, Victoria . Next-door neighbour\n",
      "Margaret Bell witnessed the explosion and called 000. 'I got up to grab a drink\n",
      "of water and I went back to bed,' Ms Bell told Daily Mail Australia. 'Then I\n",
      "heard a noise, you know how when a firecracker goes off it makes that whooshing\n",
      "sort of a noise, then that stopped and I heard a big explosion.' Ms Bell got out\n",
      "of bed and looked out her window to see the house in flames so she called the\n",
      "fire service. She added that the family had moved to Mount Helen from Ballarat\n",
      "just over one week ago. 'I'd spoken to the lady and her son, she was telling me\n",
      "a bit about herself,' she said. A body has reportedly been found but not\n",
      "identified and a mother, 43, and her son and daughter, both 21, are still\n",
      "unaccounted for . 'They'd moved out here because it's nice and quiet out here.\n",
      "'It's really sad to think they might have been in the house.' Detective senior\n",
      "sergeant Dave Hermit said two dogs were found dead at the property. The fire\n",
      "took 20 firefighters 90 minutes to bring under control, and the house was\n",
      "completely destroyed. Country Fire Authority operations officer Kade Dowie said\n",
      "investigators believed the family were in the process of moving into the home.\n",
      "'Investigations are continuing but the story from the neighbours is that they'd\n",
      "only recently moved in, as in last weekend,' Mr Dowie told Daily Mail Australia.\n",
      "Mr Dowie said the scene was handed over to the police forensics and arson squad\n",
      "on Friday morning but crews struggled to access inside the property. 'The\n",
      "intensity of the fire caused the ceiling and roof to collapse, which is causing\n",
      "difficulty with extinguishing and accessing,' he said. The cause of the fire is\n",
      "not yet known.\n",
      "QUESTIONS:\n",
      "Q1) What was the condition of the house after the fire? A. Partially damaged B.\n",
      "Slightly damaged C. Unknown D. Completely destroyed E. Not affected  Q2) Where\n",
      "did the house fire occur? A. Mount Helen B. Ballarat C. Sydney D. Melbourne E.\n",
      "Perth  Q3) How long did it take firefighters to bring the fire under control? A.\n",
      "30 minutes B. 2 hours C. 3 hours D. 60 minutes E. 90 minutes  Q4) What is the\n",
      "current status of the cause of the fire? A. It is still unknown B. It was a\n",
      "cooking accident C. It was a gas leak D. It was an electrical fault E. It was an\n",
      "arson attack  Q5) What was the age of the mother who died in the house fire? A.\n",
      "43 B. 21 C. 30 D. 50 E. 60\n",
      "MODEL GENERATED SUMMARY:\n",
      "A 43-year-old mother and her two children, aged 21, have died in a house fire in\n",
      "Mount Helen, near Ballarat. The family had recently moved to the area. The cause\n",
      "of the fire is not yet known.\n",
      "ANSWERS:\n",
      "['D' 'A' 'E' 'A' 'A']\n",
      "JUDGE ANSWERS FROM SUMMARY:\n",
      "['Unsure', 'A', 'Unsure', 'A', 'A']\n",
      "'===================================================================================================='\n"
     ]
    }
   ],
   "source": [
    "from src.utils.models import DataSchema\n",
    "\n",
    "for row in example_rows:\n",
    "    print(\"TEXT:\")\n",
    "    print(textwrap.fill(row[DataSchema.ARTICLE], width=80))\n",
    "    print(\"QUESTIONS:\")\n",
    "    print(textwrap.fill(row[DataSchema.MCQ_QUESTIONS], width=80))\n",
    "    print(\"MODEL GENERATED SUMMARY:\")\n",
    "    print(textwrap.fill(row[DataSchema.SUMMARY_GENERATION_RAW_OUTPUT], width=80))\n",
    "    print(\"ANSWERS:\")\n",
    "    print(textwrap.fill(str(row[DataSchema.GROUND_TRUTH_MCQ_ANSWERS]), width=80))\n",
    "    print(\"JUDGE ANSWERS FROM SUMMARY:\")\n",
    "    print(textwrap.fill(str(row[DataSchema.JUDGE_MCQ_ANSWERS]), width=80))\n",
    "    pprint.pprint(\"=\" * 100, width=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Preference Tuning Data\n",
    "\n",
    "Next, we will generate 10 summaries for each article in the training set and score them with our Q&A judging setup. \n",
    "\n",
    "The following command will run the `TODO` script, which takes in the folder of summaries and outputs `.jsonl` files for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/scripts/generate_dpo_data.py configs/training_data_generation/mistral_8b.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with the link to your validation file\n",
    "validation_file = f\"s3://air-example-data/preference-tuning-summarization-example/dpo_training_data/valid.jsonl\"\n",
    "\n",
    "valid_ds = ray.data.read_json(validation_file)\n",
    "example_rows = valid_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      "Given the following text, create a very short summary that is at most 2\n",
      "sentences.  Text: By . Tamara Cohen, Political Reporter . PUBLISHED: . 18:32\n",
      "EST, 27 January 2013 . | . UPDATED: . 08:48 EST, 28 January 2013 . Deputy Prime\n",
      "Minister Nick Clegg and his wife Miriam are determined to keep the education of\n",
      "their 11-year-old son 'out of politics' Nick Clegg yesterday defended the\n",
      "possibility he may send his children to private schools as it emerged he and his\n",
      "wife Miriam have not even visited their local state school. He said the\n",
      "education of his 11-year-old son Antonio, who starts secondary school this year,\n",
      "should not be used as 'a political football' and that the couple would do\n",
      "'what's best' for their children although he was braced for criticism. Last week\n",
      "the Liberal Democrat leader told listeners to his radio show he would send his\n",
      "son to a private school if he failed to find a place in a good comprehensive,\n",
      "saying he would use the state system 'if it works out', but that there is 'huge\n",
      "competition' for places in London. But Mr Clegg, who attended Westminster public\n",
      "school, has apparently not looked around nearby Ark Putney academy in south-west\n",
      "London, it was revealed yesterday by its headmaster Mark Phillips. Mr Phillips\n",
      "who has turned the school around since he was hired three years ago, said the\n",
      "school which was once in special measures but is now lauded by the Government\n",
      "for its improvements, could provide an 'exceptional' education for any child and\n",
      "that there was no need to pay fees for schooling. Unless the Cleggs had visited\n",
      "'under cover' he had not seen them, he said.'I am always very clear that all\n",
      "parents living locally are welcome to choose our school and it is important that\n",
      "every parent comes with their child and takes an objective look to see whether\n",
      "what we offer will meet the needs of their child', he said. 'It wouldn't claim\n",
      "to be the answer to every child and every parent. But I hope that if a parent\n",
      "does come, and sees an environment their child will thrive in, they will pick\n",
      "us...I am confident they will do exceptionally well. I don't believe you have to\n",
      "pay for it.' Mr Clegg told the BBC's Andrew Marr Show yesterday that he and his\n",
      "wife will do whatever is in the interests of their son . If he chooses to\n",
      "educate his children . privately, Mr Clegg is likely to be accused of hypocrisy\n",
      "after using a . speech last year to attack 'the great rift in our education\n",
      "system' caused by many of the best schools being fee-paying and said it had a .\n",
      "'corrosive' effect on society and the economy. In . an interview on BBC1's\n",
      "Andrew Marr Show, he said: 'I accept that it's a . dilemma for anyone in public\n",
      "life, particularly in politics, how do you . balance that with the fact Miriam\n",
      "and I have small children, and the . approach Miriam and I took right from the\n",
      "outset was to keep our . children completely out of politics. 'We . never put\n",
      "them in front of the camera or to make them or their . education a political\n",
      "football. 'I totally accept that when we make a . decision that'll be subject to\n",
      "public commentary, criticism and so on, . but in the meantime we want to protect\n",
      "the privacy of an 11-year-old boy . and make the decision that we as parents\n",
      "think is best for our son.' The deadline for applying for entry to Ark Putney\n",
      "for 2013-14 was last October. The school is part of the Ark academy chain, set\n",
      "up in 2004, whose chairman is Paul Marshall, one of the Liberal Democrats'\n",
      "biggest donors. Last year 62 per cent of pupils gained at least five good GCSEs,\n",
      "prompting schools minister Nick Gibb to write to Mr Phillips to congratulate him\n",
      "on the 'excellent results' saying the school was in the top 100 best-performing,\n",
      "based on sustained improvements every year since 2008. However Michael Gove last\n",
      "year approved the sale of five acres of playing fields at the school including\n",
      "six tennis courts, a football pitch and a playground, to developers to fund\n",
      "refurbishments, after a £40million revamp under the Building Schools for the\n",
      "Future programme was cancelled. Alumni of Ark Putney, which used to be Elliott\n",
      "School, include actor Pierce Brosnan, and 1960s England bowler Geoff Arnold.\n",
      "Former Welsh secretary Peter Hain sent his children to the school, which was the\n",
      "scene of the Christmas play in the film Love, Actually. David Cameron has said\n",
      "his children will attend state school, but George Osborne has been criticised\n",
      "for sending his to the fee-paying preparatory school in Kensington that he\n",
      "attended.\n",
      "\n",
      "CHOSEN RESPONSE: \n",
      "Nick Clegg and his wife Miriam are considering sending their 11-year-old son\n",
      "Antonio to a private school, despite their previous claims that they would use\n",
      "the state system. Clegg has not visited Ark Putney Academy, a nearby state\n",
      "school, but its headmaster, Mark Phillips, claims that the school could provide\n",
      "an 'exceptional' education for any child and that there is no need to pay fees\n",
      "for schooling. The deadline for applying to Ark Putney for 2013-14 was last\n",
      "October and the school is part of the Ark academy chain, set up in 2004, whose\n",
      "chairman is Paul Marshall, one of the Liberal Democrats' biggest donors.\n",
      "\n",
      "REJECTED RESPONSE\n",
      "Deputy Prime Minister Nick Clegg has defended the possibility of sending his\n",
      "children to private schools, stating that their education should not be used as\n",
      "a political football. He and his wife Miriam have not visited their local state\n",
      "school, but the headmaster of Ark Putney academy in London, where they were\n",
      "invited to attend, said the school could provide an \"exceptional\" education for\n",
      "any child and that parents should come to see it firsthand before making a\n",
      "decision.\n"
     ]
    }
   ],
   "source": [
    "for row in example_rows:\n",
    "    print(\"PROMPT:\")\n",
    "    print(textwrap.fill(row['chosen'][0]['content'], width=80))\n",
    "    print(\"\\nCHOSEN RESPONSE: \")\n",
    "    print(textwrap.fill(row['chosen'][1]['content'], width=80))\n",
    "    print(\"\\nREJECTED RESPONSE: \")\n",
    "    print(textwrap.fill(row['rejected'][1]['content'], width=80))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Fine-tuning\n",
    "\n",
    "Now that we have the pre-processed dataset, we are ready to fine-tune `Mistral-7B-Instruct-v0.1` using DPO. On Anyscale, we've created an easy-to-use interface to do preference-tuning using `DPO`. We leverage Ray to overlap reference model log-probability calculation with model training to improve GPU utilization. Most implementations compute log probabilities synchronously with model training,\n",
    "\n",
    "![hf model](assets/hf_dpo.png)\n",
    "\n",
    "While our implementation using Ray is asynchronous:  \n",
    "\n",
    "\n",
    "![assistant model](assets/anyscale_dpo.png)\n",
    "\n",
    "Further, our use of Ray Data also implies that the compute configuration for the reference model can be completely decoupled with the policy model. For example, reference model calculation can run on a different node (with configurable number of GPUs, etc) with zero code changes needed. \n",
    "\n",
    "\n",
    "To get started with DPO training, we provide the config for DPO in [configs/mistral_dpo_summarization.yaml](configs/mistral_dpo_summarization.yaml) . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id: mistralai/Mistral-7B-Instruct-v0.1\n",
      "# Example summarization dataset with 10k examples for training with an average of 2.2k tokens per sample\n",
      "train_path: s3://air-example-data/preference-tuning-summarization/train.jsonl\n",
      "valid_path: s3://air-example-data/preference-tuning-summarization/valid.jsonl\n",
      "task: \"preference_tuning\"\n",
      "context_length: 4096\n",
      "# For DPO, it is recommended to set a high `num_data_blocks_per_device` to not bottleneck the logp processor.\n",
      "# We recommend not going beyond 20 so as to not spawn too many Ray actors. \n",
      "num_data_blocks_per_device: 16\n",
      "num_devices: 6 # <--- runs training on 6 GPUs\n",
      "train_batch_size_per_device: 2\n",
      "eval_batch_size_per_device: 2\n",
      "learning_rate: 5e-6\n",
      "num_epochs: 3\n",
      "no_gradient_checkpoint: False\n",
      "output_dir: /mnt/local_storage/\n",
      "deepspeed:\n",
      "  config_path: deepspeed_configs/zero_3.json\n",
      "worker_resources:\n",
      "  accelerator_type:A10G: 1\n",
      "flash_attention_2: True\n",
      "padding: \"longest\"\n",
      "preference_tuning_config:\n",
      "  beta: 0.01\n",
      "  logprob_processor_scaling_config:\n",
      "    custom_resources:\n",
      "      accelerator_type:A10G: 1\n",
      "    concurrency: 2 # <--- runs reference model logp calculation on 2 GPUs\n",
      "    batch_size: 2\n",
      "lora_config:\n",
      "  r: 8\n",
      "  lora_alpha: 16\n",
      "  lora_dropout: 0.05\n",
      "  target_modules:\n",
      "    - q_proj\n",
      "    - k_proj\n",
      "    - v_proj\n",
      "    - o_proj\n",
      "    - gate_proj\n",
      "    - up_proj\n",
      "    - down_proj\n",
      "  modules_to_save: []\n",
      "  bias: \"none\"\n",
      "  fan_in_fan_out: false\n",
      "  init_lora_weights: true"
     ]
    }
   ],
   "source": [
    "!cat configs/mistral_dpo_summarization.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!llmforge anyscale finetune end-to-end-examples/fine-tune-preference/configs/mistral_dpo_summarization.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Evaluation\n",
    "\n",
    "Let's evaluate our trained model. Here we'll use two baselines: (1) the base model before finetuning (reference model in DPO) and (2) GPT-4o.\n",
    "\n",
    "## Evaluation strategy\n",
    "\n",
    "Our evaluation strategy involves the same Q&A scoring system as used while generating the preference data. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/eval.png?\" alt=\"Evaluation\" width=800>\n",
    "</p>\n",
    "\n",
    "We evaluate the baseline model and the trained DPO model on the test set. \n",
    "\n",
    "## Obtain summaries on the test set\n",
    "First, we'll need to obtain the summaries (and scores) for both the models on the given test set. \n",
    "\n",
    ">  **_NOTE:_**  The configs will by default use H100s for model scoring and summary generation. for faster processing. Feel free to change the accelerator type in [configs/summary_generation](configs/summary_generation/) but note that the speed would be much slower. \n",
    "\n",
    "For the baseline model, you can simply run the below command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!anyscale job submit -f configs/generate_summaries_eval_baseline_job.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the fine-tuned DPO model, we provide a dummy config in [configs/summary_generation/mistral_finetuned_eval.yaml](configs/summary_generation/mistral_finetuned_eval.yaml). If you used the default training config provided, the model would be trained using LoRA and you should have a path to the LoRA weights. Make sure to download the weights locally (using `aws` or `gcloud` CLI depending on the remote path). Enter the local path to the weights in the config. (Make sure that the weights are in the same directory as the current notebook to be included in the job).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: eval\n",
      "input_folder: s3://air-example-data/preference-tuning-summarization-example/qa_generation/qa_annotations_full_test\n",
      "model_inference_config:\n",
      "  model_id_or_path: mistralai/Mistral-7B-Instruct-v0.1\n",
      "  adapter_id_or_path: <path to lora model> # <--- Add the local path to your lora weights here\n",
      "  temperature: 0\n",
      "  top_p: 0.95\n",
      "  scaling_config:\n",
      "    batch_size: 128\n",
      "    concurrency: 2\n",
      "    num_gpus: 1\n",
      "    custom_resources:\n",
      "      accelerator_type:H100: 1\n",
      "num_generations: 1\n",
      "judge_inference_config:\n",
      "  model_id_or_path: meta-llama/Meta-Llama-3.1-70B-Instruct\n",
      "  temperature: 0\n",
      "  scaling_config:\n",
      "    batch_size: 128\n",
      "    concurrency: 3\n",
      "    num_gpus: 2\n",
      "    custom_resources:\n",
      "      accelerator_type:H100: 1\n",
      "num_mcq_questions: 5\n"
     ]
    }
   ],
   "source": [
    "!cat configs/summary_generation/mistral_finetuned_eval.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!anyscale job submit -f configs/summary_generation/mistral_finetuned_eval.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the logs for the above jobs, you should see the final path to the output summaries for both the models. \n",
    "\n",
    "Optionally, you can also obtain the summaries and scores for the `gpt-4o` model from OpenAI. Simply run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!anyscale job submit -f configs/summary_generation/gpt4o_eval.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Evaluation Statistics\n",
    "\n",
    "We've provided a convenient script `src/scripts/get_eval_stats.py` to get evaluation statistics and obtain the \"win rate\" of the DPO model (the percentage of times the DPO model performs better than the baseline). We've provided an example configuration below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to substitute -outputs-path with your path\n",
    "!python src/scripts/get_eval_stats.py --outputs-path s3://air-example-data/preference-tuning-summarization-example/summary_generation_dpo_model/test/ --baseline-outputs-path s3://air-example-data/preference-tuning-summarization-example/summary_generation_base/test/  \n",
    "\n",
    "# (Optional): if you obtained results for GPT-4o, you should uncomment and run the following command instead\n",
    "# !python src/scripts/get_eval_stats.py --outputs-path s3://air-example-data/preference-tuning-summarization-example/summary_generation_dpo_model/test/ --baseline-outputs-path s3://air-example-data/preference-tuning-summarization-example/summary_generation_base/test/  --gpt4o-outputs-path <add-path-to-gpt4o-results>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the following results:\n",
    "\n",
    "```text \n",
    "╒═════════════════════════╤═══════════╤════════════╤═══════════╕\n",
    "│         Metric          │   Model   │  Baseline  │  GPT-4o   │\n",
    "╞═════════════════════════╪═══════════╪════════════╪═══════════╡\n",
    "│      Accuracy >=3       │ 65.4286 % │ 43.0476 %  │ 37.2381 % │\n",
    "├─────────────────────────┼───────────┼────────────┼───────────┤\n",
    "│      Accuracy >=4       │ 25.7143 % │ 13.5238 %  │ 10.0000 % │\n",
    "├─────────────────────────┼───────────┼────────────┼───────────┤\n",
    "│   Median Compression    │ 11.5794 % │ 12.7316 %  │ 8.0496 %  │\n",
    "├─────────────────────────┼───────────┼────────────┼───────────┤\n",
    "│    Mean Compression     │ 13.0029 % │ 14.3444 %  │ 9.3554 %  │\n",
    "├─────────────────────────┼───────────┼────────────┼───────────┤\n",
    "│   Failed Compressions   │ 0.0000 %  │  0.0000 %  │ 0.0000 %  │\n",
    "├─────────────────────────┼───────────┼────────────┼───────────┤\n",
    "│ Contains OOD Characters │ 0.0000 %  │  0.0952 %  │ 0.0000 %  │\n",
    "╘═════════════════════════╧═══════════╧════════════╧═══════════╛\n",
    "\n",
    "\n",
    "Model Win Rate against Baseline: 74.0000 %\n",
    "GPT-4o Win Rate against Baseline: 64.8095 %\n",
    "```\n",
    "\n",
    "Our fine-tuned model is able to generate much better summaries, that are more concise (compression ratio is lower) with lesser out-of-distribution characters (gibberish tokens) than the baseline. You can see more details on the same in our blog!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congrats! You have now fine-tuned an open source model on preference data. As a quick recap, here's what we demonstrated in this notebook:\n",
    "1. Synthetically generating preference data for DPO \n",
    "2. DPO fine-tuning of a language model on the Anyscale Platform\n",
    "4. Evaluating the model against the baseline and GPT-4o, and analysing the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
