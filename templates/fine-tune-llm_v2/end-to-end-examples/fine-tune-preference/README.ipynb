{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preference Tuning for Summarization using Synthetic Data\n",
    "\n",
    "**‚è±Ô∏è Time to complete**: 10 hours+\n",
    "\n",
    "Alignment of LLMs has traditionally been broken down into two post-training stages: Supervised fine-tuning (SFT) followed by preference tuning (aka RLHF). SFT requires high quality data collection where each data sample illustrates behavior which we would like the LLM to imitate exactly. While for some tasks like SQL generation and math reasoning, it is feasible to collect the ground truth data, this approach does not always scale easily to align for subjective use cases (ex. chat, summarization, etc.). \n",
    "\n",
    "On the other hand, preference tuning only requires information about whether a given response is preferred to another response. Each data sample consists of a chosen and rejected completion for a given prompt, such that the chosen completion is preferred over the rejected completion. Preference tuning is thus a powerful tool that can optimize LLMs towards complex preferences that cannot be easily captured through supervised fine-tuning. However, manually annotating preferences between model outputs using human raters can be extremely time-consuming and expensive. Instead, synthetic preference data can be generated by scoring responses with large foundation models, allowing for much cheaper and scalable data collection!\n",
    "\n",
    "Here we'll go through an end-to-end example for preference tuning of an open-source language model with synthetic data, covering scalable methodologies for data preprocessing, fine-tuning and evaluation, using Ray. We will focus on the task of summarization for the [CNN/DailyMail](https://huggingface.co/datasets/abisee/cnn_dailymail) dataset. \n",
    "\n",
    "\n",
    "Notebook guide:\n",
    "- üîÑ REPLACE indicates to replace with your unique values\n",
    "- üí° INSIGHT indicates infrastructure insight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Data Preprocessing](#step-1-data-preprocessing): In this section we cover how we can prepare preference data for the summarization task using an LLM-as-a-judge. \n",
    "    1. [Generate Multiple Choice Questions From Articles](#part-a-generate-multiple-choice-questions-from-articles)\n",
    "    2. [Generate Summaries and Scores](#part-b-generate-summaries--scores)\n",
    "    3. [Generate Preference Tuning Data](#part-c-generate-preference-tuning-data)\n",
    "2. [DPO Finetuning](#step-2-fine-tuning): This section will cover how you can fine-tune an open source model on the preference data on the Anyscale platform.\n",
    "3. [Evaluation](#step-3-evaluation): The section will lay down a blue-print for evaluation and compare performance to that of closed source models like OpenAI's GPT-4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Running the jobs in this notebook requires a HuggingFace token that can access [Llama-3.1-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct) and [Mistral-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1). For GPT-4o evaluation, you'd also need a valid `OPENAI_API_KEY`.  Make sure to provide `HF_TOKEN` and `OPENAI_API_KEY` by defining it under dependencies in your cluster setup.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/env_var.png?\" alt=\"Environment variable\" width=800>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make the necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "\n",
    "import ray.data\n",
    "import datasets\n",
    "\n",
    "from src.utils.models import DataSchema\n",
    "from src.utils.common import print_wrapped\n",
    "\n",
    "os.environ[\"PYTHONPATH\"] = f\"{os.environ.get('PYTHONPATH', '')}:src\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Synthetic Data Generation\n",
    "\n",
    "First, let's inspect the training dataset and look at an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_ds = datasets.load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\", split=\"train\")\n",
    "\n",
    "raw_example = hf_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'article': 'LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe '\n",
      "            'gains access to a reported ¬£20 million ($41.1 million) fortune as '\n",
      "            \"he turns 18 on Monday, but he insists the money won't cast a \"\n",
      "            'spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter '\n",
      "            'and the Order of the Phoenix\" To the disappointment of gossip '\n",
      "            'columnists around the world, the young actor says he has no plans '\n",
      "            'to fritter his cash away on fast cars, drink and celebrity '\n",
      "            'parties. \"I don\\'t plan to be one of those people who, as soon as '\n",
      "            'they turn 18, suddenly buy themselves a massive sports car '\n",
      "            'collection or something similar,\" he told an Australian '\n",
      "            'interviewer earlier this month. \"I don\\'t think I\\'ll be '\n",
      "            'particularly extravagant. \"The things I like buying are things '\n",
      "            'that cost about 10 pounds -- books and CDs and DVDs.\" At 18, '\n",
      "            'Radcliffe will be able to gamble in a casino, buy a drink in a '\n",
      "            'pub or see the horror film \"Hostel: Part II,\" currently six '\n",
      "            'places below his number one movie on the UK box office chart. '\n",
      "            \"Details of how he'll mark his landmark birthday are under wraps. \"\n",
      "            'His agent and publicist had no comment on his plans. \"I\\'ll '\n",
      "            'definitely have some sort of party,\" he said in an interview. '\n",
      "            '\"Hopefully none of you will be reading about it.\" Radcliffe\\'s '\n",
      "            'earnings from the first five Potter films have been held in a '\n",
      "            'trust fund which he has not been able to touch. Despite his '\n",
      "            'growing fame and riches, the actor says he is keeping his feet '\n",
      "            'firmly on the ground. \"People are always looking to say \\'kid '\n",
      "            'star goes off the rails,\\'\" he told reporters last month. \"But I '\n",
      "            'try very hard not to go that way because it would be too easy for '\n",
      "            'them.\" His latest outing as the boy wizard in \"Harry Potter and '\n",
      "            'the Order of the Phoenix\" is breaking records on both sides of '\n",
      "            'the Atlantic and he will reprise the role in the last two films.  '\n",
      "            \"Watch I-Reporter give her review of Potter's latest ¬ª . There is \"\n",
      "            'life beyond Potter, however. The Londoner has filmed a TV movie '\n",
      "            'called \"My Boy Jack,\" about author Rudyard Kipling and his son, '\n",
      "            'due for release later this year. He will also appear in \"December '\n",
      "            'Boys,\" an Australian film about four boys who escape an '\n",
      "            'orphanage. Earlier this year, he made his stage debut playing a '\n",
      "            'tortured teenager in Peter Shaffer\\'s \"Equus.\" Meanwhile, he is '\n",
      "            \"braced for even closer media scrutiny now that he's legally an \"\n",
      "            'adult: \"I just think I\\'m going to be more sort of fair game,\" he '\n",
      "            'told Reuters. E-mail to a friend . Copyright 2007 Reuters. All '\n",
      "            'rights reserved.This material may not be published, broadcast, '\n",
      "            'rewritten, or redistributed.',\n",
      " 'highlights': 'Harry Potter star Daniel Radcliffe gets ¬£20M fortune as he '\n",
      "               'turns 18 Monday .\\n'\n",
      "               'Young actor says he has no plans to fritter his cash away .\\n'\n",
      "               \"Radcliffe's earnings from first five Potter films have been \"\n",
      "               'held in trust fund .',\n",
      " 'id': '42c027e4ff9730fbb3de84c1af0d2c506e41c3e4'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(raw_example, width=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the example article above. Our goal is to train a model to summarize it accurately, with a preference for short summaries, making sure we also preserve important details. In this guide, we will employ a _synthetic_ summary scoring method using another LLM as a judge. We score the correctness of a summary using the following metrics:\n",
    "\n",
    "**Summary Scoring Metrics**\n",
    "1. Multiple choice Q&A accuracy:\n",
    "    - Given the original text, we use an LLM judge to generate 5 multiple choice questions about the text.\n",
    "    - We then ask the LLM judge to answer the questions using only the summary, and record the number of questions correctly answered.\n",
    "2. Word count: We simply count the number of words in the summary.\n",
    "\n",
    "This allows us to construct a simple preference function between two summaries:\n",
    "\n",
    "**Preference Function**\n",
    "1. If both summary responses attain more than 3/5 multiple choice questions correct, we will prefer the shorter response. We do not care about Q&A accuracy beyond 3 correct answers, since the summary should not contain all information from the text.\n",
    "2. Otherwise, we select the response that leads to more correctly answered multiple choice questions.\n",
    "\n",
    "We consider a subset of 21,000 articles in this example. To generate the preference pairs, we will generate 10 summaries from each article using the model we wish to fine-tune. Then, we will randomly sample pairs of summaries and use our preference function to annotate the preference between them.\n",
    "\n",
    "For this example, we will use `Mistral-7B-Instruct-v0.1` as the base model to fine-tune and `Llama-3.1-70B-Instruct` as a judge. Note that mistral-instruct is already instruction tuned, so that given a prompt to do summarization it might do a good job, but it may not be aligned with how we want the summarization to look like. We can use preference data to further align the instruct variant towards our specific needs.\n",
    "\n",
    "We've provided a helpful visualization here:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/preference_function.png?\" alt=\"Preference function\" width=800>\n",
    "</p>\n",
    "\n",
    "Combining all this together, our data pre-processing pipeline is going to look as follows: \n",
    "\n",
    "![preprocessing](./assets/preprocessing.png?1)\n",
    "\n",
    "üí° INSIGHT: \n",
    "Our synthetic preference data collection looks pretty involved at first glance. The key ideas in plain English are as follows:\n",
    "- We use a combination of Q&A scoring + word length to indicate like/dislike (our preference function) given a pair of summaries.\n",
    "- Our ultimate goal is to generate (chosen, rejected) pairs to train our reference model and evaluate it based on this criteria.\n",
    "- We use another LLM (judge model) to generate said questions for each article. This model is also used in our scoring system. (To see how many questions can be answered from a summary)\n",
    "- To generate training data, we first sample candidate summaries from the reference model for each article. We then obtain scores for each summary from the judge. Using the scores, we select pairs of summaries and mark our like/dislike to form (chosen, rejected) pairs for the actual training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a): Generate Multiple Choice Questions from Articles\n",
    "\n",
    "First, we will generate the multiple choice questions and answers for each article using `Llama-3.1-8B-Instruct` (or `70B` if A100/H100s are available). Leveraging vLLM and Ray, we can very easily scale this generation process across multiple GPUs.\n",
    "\n",
    ">  **_NOTE:_**  We provide two sets of configs: One with an 8B parameter model as the judge, and another with the 70B model. Using the 8B model is recommended, since we make use of highly available A10Gs. For good performance, and to replicate the results in our blog, you should use the 70B judge model which uses A100s (but these are harder to obtain on-demand)\n",
    "\n",
    "The following command will run the [src/scripts/generate_questions.py](./src/scripts/generate_questions.py) script, which generates the questions and answers and saves them in `.parquet` files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "üí° INSIGHT:  \n",
    "We are running this script as an anyscale job. The resources required by each step are requested at runtime and provisioned by Anyscale's autoscaler based on availability and quotas. You are free to change the [qa_generation](./configs/qa_generation) config in any way. The important parameters regarding resources are `accelerator_type`, `num_gpus_per_instance`, and `concurrency`. This script will generate 5 multiple choice question and answer pairs per article for 21k examples. According to the [llama_8b](./configs/qa_generation/llama_8b.yaml) config we are requesting 3 replicas of 4xA10G machines processing a batch-size of 128 examples each which saturates the GPUs all the way through.\n",
    "\n",
    "This step will take ~75 min for 8B running on A10s and  for 70B running on A100s.\n",
    "\n",
    "```bash\n",
    "anyscale job submit -f configs/jobs/8b_judge/generate_questions_job.yaml\n",
    "# Optional: use the 70b model for better performance (runs on A100s)\n",
    "# anyscale job submit -f configs/jobs/70b_judge/generate_questions_job.yaml\n",
    "```\n",
    "\n",
    "> **NOTE**: We recommend that you execute all the commands in this notebook in a terminal. Make sure you `cd` into the directory of this notebook (and the `src` files) before executing the commands. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the job, you should see the remote path to the folder with Q&A in the logs.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/question_generation_done.png?\" alt=\"Evaluation\" width=800>\n",
    "</p>\n",
    "\n",
    " Make sure to make note to use it for the next steps! \n",
    "\n",
    " üîÑ REPLACE the resulting S3 URI here. If you want to skip the prior step, you can continue with the prepared example data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with the link to the output folder from the previous job\n",
    "qa_folder = \"s3://air-example-data/preference-tuning-summarization-example/qa_generation/qa_annotations_full_train/\"\n",
    "qa_ds = ray.data.read_parquet(qa_folder)\n",
    "# The dataset is small, we can materalize it\n",
    "example_rows = qa_ds.materialize().take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT:\n",
      "By . Sean Poulter . PUBLISHED: . 20:04 EST, 10 March 2014 . | . UPDATED: . 04:33\n",
      "EST, 11 March 2014 . Advances: The new system will make it easier to move money\n",
      "around . Technology to allow direct payments between mobile phones was unveiled\n",
      "by the big banks yesterday. The system cuts out the need to remember sort codes\n",
      "and bank account details. Instead, you type on your phone the mobile number of\n",
      "the person or business you want to pay. The ‚ÄòPaym‚Äô transfers, which will be\n",
      "password-protected, need your bank account to be linked to your own mobile\n",
      "number. Users will simply tap in the number of the recipient on their phone to\n",
      "authorise an electronic transfer from one account to another. The industry hopes\n",
      "the system will replace cheques, which are expensive to transport and process.\n",
      "At the same time it could provide a substitute for cash to make relatively small\n",
      "payments to tradesmen, window cleaners or gardeners. The idea is that millions\n",
      "of people will have their mobile phone number tied to their bank account ‚Äì as a\n",
      "substitute for the normal account number and sort code. There is a security\n",
      "safeguard in that the name of the recipient will appear on the smartphone\n",
      "screen, once the number has been tapped in, so confirming the cash is going to\n",
      "the right person. Access to the Paym system will be covered by password or code\n",
      "protection in an attempt to avoid the risk of the phone being stolen and used to\n",
      "raid customers‚Äô accounts. Paym has been devised by the finance industry‚Äôs\n",
      "Payments Council, which is the trade body responsible for running the electronic\n",
      "payments system on behalf of banks and building societies. The council‚Äôs chief\n",
      "executive, Adrian Kamellard, said: ‚ÄòWe‚Äôre all used to the idea of a ‚Äòmobile\n",
      "update‚Äô to improve our apps - Paym is a mobile update for payments that means\n",
      "you can pay securely using just a mobile number. ‚ÄòPaym will make it easier to\n",
      "repay a friend for cinema tickets, split a restaurant bill or settle up for a\n",
      "colleague‚Äôs birthday collection.‚Äô Changes: The system 'has the potential to link\n",
      "up every bank account in the country with a mobile number' He added: ‚ÄòPaym is a\n",
      "great example of industry-wide collaboration that delivers tangible benefits for\n",
      "customers. ‚ÄòThe service has the potential to link up every bank account in the\n",
      "country with a mobile number. ‚ÄòMillions of people will be able to use it this\n",
      "year and we look forward to expanding Paym even further, so everyone can benefit\n",
      "from this easy, secure new way to pay.‚Äô Paym will be integrated into customers‚Äô\n",
      "existing mobile banking or payment apps as an additional way to pay, making it\n",
      "possible to send and receive payments using a mobile number. When it is launched\n",
      "in the spring, customers of nine bank and building society brands ‚Äì Lloyds,\n",
      "HSBC, Barclays, Halifax, Santader, TSB, Bank of Scotland, Cumberland Building\n",
      "Society, and Danske Bank - will be able to use the new service. The nine launch\n",
      "brands will offer their customers the opportunity to register their mobile\n",
      "number and select the current account they want payments made into before the\n",
      "service goes live. This will immediately make the new system the most wide-\n",
      "ranging payment service capable of moving funds directly from account to\n",
      "account, without the need for sort codes or account numbers. Paym will expand\n",
      "later this year to include First Direct, NatWest, RBS, Clydesdale Bank,\n",
      "Yorkshire Bank and Isle of Man Bank. The Nationwide building society is\n",
      "committed to joined in early 2015, while the Metro Bank and Ulster Bank are also\n",
      "finalising their participation.\n",
      "\n",
      "QUESTIONS:\n",
      "Q1) How will users initiate a payment using the 'Paym' system? A. By typing in\n",
      "the recipient's sort code and account number B. By visiting a bank branch C. By\n",
      "scanning a QR code D. By using a physical payment card E. By typing in the\n",
      "recipient's mobile number  Q2) What is the main purpose of the new 'Paym'\n",
      "system? A. To replace credit cards B. To increase the use of cheques C. To\n",
      "reduce the use of cash D. To increase bank fees E. To make it easier to move\n",
      "money around using mobile phones  Q3) How many bank and building society brands\n",
      "will offer the 'Paym' service at launch? A. 9 B. 15 C. 12 D. 20 E. 5  Q4) Which\n",
      "of the following is a potential benefit of the 'Paym' system? A. It will make it\n",
      "easier to repay a friend for cinema tickets B. It will reduce the security of\n",
      "mobile payments C. It will increase bank fees D. It will make it harder to split\n",
      "a restaurant bill E. It will increase the use of cheques  Q5) What is the name\n",
      "of the trade body responsible for devising the 'Paym' system? A. The Electronic\n",
      "Payments Council B. The Banking Association C. The Mobile Payments Association\n",
      "D. The Finance Industry Council E. The Payments Council\n",
      "\n",
      "ANSWERS:\n",
      "['E' 'E' 'A' 'A' 'E']\n",
      "\n",
      "'================================================================================'\n",
      "TEXT:\n",
      "Jerusalem (CNN) -- Five Israeli right-wing extremists have been indicted by an\n",
      "Israeli court which accused them of attempting to prevent the demolition of\n",
      "illegal settlements in the West Bank, organizing a break-in at a military base\n",
      "and planning riots. The extremists are also suspected of masterminding an attack\n",
      "on an Israel Defense Forces base in the West Bank on December 13. In that\n",
      "incident, about 50 extremists infiltrated and attacked the Ephraim Regional\n",
      "Division Headquarters. The activists entered the base, damaged property, set\n",
      "tires on fire, threw stones and damaged vehicles, according to an IDF statement\n",
      "at the time. A commander's car was attacked, and he sustained minor injuries.\n",
      "Israeli Prime Minister Benjamin Netanyahu said the incident \"crossed all lines.\"\n",
      "And Matan Vilnai, deputy defense minister, called the perpetrators \"Jewish\n",
      "terrorists\" in an interview on army radio. Israeli media reported in December\n",
      "that the attack came in response to rumors that Israeli security forces were\n",
      "about to demolish two illegal outposts in the West Bank. The indictment alleges\n",
      "the extremists operated from an apartment in Jerusalem, where they gathered\n",
      "intelligence information through surveillance, lookouts and patrols of Israeli\n",
      "troops. The intelligence was aimed at preventing \"the evacuation of outposts by\n",
      "illegal means\" and to prevent IDF operations, said the indictment, which was\n",
      "presented in court Sunday. \"The indictment exposes the true and ugly face of the\n",
      "prosecution that proves once again the blatant discrimination against the\n",
      "settlers,\" said Adi Keidar, an attorney representing three of the five. The\n",
      "suspects are also being questioned about \"price tag\" attacks against\n",
      "Palestinians in the West Bank and Jerusalem, according to Israeli police. \"Price\n",
      "tag\" is a term used to describe attacks by Israeli extremists against\n",
      "Palestinians and Israeli security forces in retaliation for any action taken\n",
      "against settlers.\n",
      "\n",
      "QUESTIONS:\n",
      "Q1) What were the five Israeli right-wing extremists accused of attempting to\n",
      "prevent? A. The establishment of a new Israeli government B. The demolition of\n",
      "illegal settlements in the West Bank C. The expansion of a military base in the\n",
      "West Bank D. The construction of new settlements in the West Bank E. The\n",
      "relocation of Israeli troops  Q2) What was the result of the attack on the\n",
      "Ephraim Regional Division Headquarters? A. The base was completely destroyed B.\n",
      "A commander sustained minor injuries and property was damaged C. No one was\n",
      "injured and no property was damaged D. A commander was killed and several\n",
      "vehicles were damaged E. Several extremists were injured and no property was\n",
      "damaged  Q3) What term is used to describe attacks by Israeli extremists against\n",
      "Palestinians and Israeli security forces in retaliation for any action taken\n",
      "against settlers? A. West Bank war B. Israeli resistance C. Price tag D.\n",
      "Retaliation attack E. Settler strike  Q4) What was the reaction of Israeli Prime\n",
      "Minister Benjamin Netanyahu to the attack on the Ephraim Regional Division\n",
      "Headquarters? A. He remained silent on the issue B. He said the incident\n",
      "\"crossed all lines\" C. He blamed the Palestinians for the attack D. He called\n",
      "the perpetrators \"Jewish terrorists\" E. He praised the extremists for their\n",
      "actions  Q5) Why did the extremists allegedly attack the Ephraim Regional\n",
      "Division Headquarters? A. In response to the relocation of Israeli troops B. In\n",
      "response to rumors of an impending Israeli military operation C. In response to\n",
      "the construction of a new Palestinian settlement D. In response to rumors that\n",
      "Israeli security forces were about to demolish two illegal outposts E. In\n",
      "response to the establishment of a new Israeli government\n",
      "\n",
      "ANSWERS:\n",
      "['B' 'B' 'C' 'B' 'D']\n",
      "\n",
      "'================================================================================'\n",
      "TEXT:\n",
      "Photos of the police officer performing a sex act in uniform were leaked and an\n",
      "investigation was launched by senior police bosses . Police in Puerto Rico have\n",
      "suspended a female police officer after photos of her performing a sex act in\n",
      "uniform ended up being leaked to fellow officers. According to a police insider,\n",
      "the images were taken by a male colleague at the police station and were posted\n",
      "onto social media. But not long after, they ended up on the phones of most\n",
      "officers in the country and quickly went viral. When senior police officers\n",
      "heard about it they launched an investigation to find out if the images were\n",
      "genuine, and quickly identified the police officer as being a serving member of\n",
      "the force. They said that the young woman, Cynthia Marrero Pomales, 29, was\n",
      "serving in Carolina, a town in north-eastern Puerto Rico near the capital San\n",
      "Juan. They added that she had been suspended while an investigation was carried\n",
      "out over allegations that she had offended the honour of the police force and\n",
      "damaged its reputation in public. She has reportedly been interviewed and given\n",
      "a statement but no details were given to local media. In one of the pictures the\n",
      "young woman also has the flag of the United States draped over part of her body.\n",
      "She reportedly uploaded the pictures herself on social media although it is\n",
      "understood that she had not intended for them to be shared outside of a small\n",
      "private circle of friends. The superintendent of police in Puerto Rico, Jose\n",
      "Luis Caldero Lopez, confirmed the authenticity of the images and announced that\n",
      "the young policewoman has since been suspended. This is not the first time a sex\n",
      "scandal has disgraced police in Puerto Rico. In 2014, photographs of two cops\n",
      "performing a sex act inside the presidential palace were shared and published,\n",
      "resulting in the officers being expelled from the force. Cynthia Marrero\n",
      "Pomales, 29, was serving in  a town  near the capital San Juan when the images\n",
      "emerged . She has now been suspended as police chiefs conduct an investigation\n",
      "into the pictures . The police station in Carolina, north-eastern Puerto Rico,\n",
      "where the incident is alleged to have occurred .\n",
      "\n",
      "QUESTIONS:\n",
      "Q1) What was the reason for the investigation launched by senior police bosses\n",
      "in Puerto Rico? A. A police officer was accused of theft B. Photos of a police\n",
      "officer performing a sex act in uniform were leaked C. A police officer was\n",
      "accused of corruption D. A police officer was involved in a fight E. A police\n",
      "officer was involved in a traffic accident  Q2) What was the consequence for the\n",
      "police officer, Cynthia Marrero Pomales, after the investigation? A. She was\n",
      "fired immediately B. She was given a warning C. She was suspended while an\n",
      "investigation was carried out D. She was transferred to a different department\n",
      "E. She was promoted to a higher rank  Q3) Where were the photos of the police\n",
      "officer performing a sex act in uniform initially taken? A. At a public park B.\n",
      "At the police station C. At a private residence D. At a hotel E. At a restaurant\n",
      "Q4) Why did the police officer, Cynthia Marrero Pomales, upload the pictures on\n",
      "social media? A. To share with the public B. To share with the media C. To share\n",
      "with her colleagues D. To share with a small private circle of friends E. To\n",
      "share with her family  Q5) What was the reaction of the superintendent of police\n",
      "in Puerto Rico, Jose Luis Caldero Lopez, to the incident? A. He denied the\n",
      "authenticity of the images B. He resigned from his position C. He launched a\n",
      "separate investigation D. He confirmed the authenticity of the images and\n",
      "announced the suspension of the police officer E. He ignored the incident\n",
      "\n",
      "ANSWERS:\n",
      "['B' 'C' 'B' 'D' 'D']\n",
      "\n",
      "'================================================================================'\n"
     ]
    }
   ],
   "source": [
    "for row in example_rows:\n",
    "    print_wrapped(\"TEXT\", row[DataSchema.ARTICLE])\n",
    "    print_wrapped(\"QUESTIONS\", row[DataSchema.MCQ_QUESTIONS])\n",
    "    print_wrapped(\"ANSWERS\", str(row[DataSchema.GROUND_TRUTH_MCQ_ANSWERS]))\n",
    "    pprint.pprint(\"=\" * 80, width=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b): Generate Summaries + Scores\n",
    "\n",
    "Next, we will generate 10 summaries for each article in the training set and score them with our Q&A judging setup. \n",
    "\n",
    "The following command will run the [generate_summaries_and_scores.py](src/scripts/generate_summaries_and_scores.py) script, which takes in the folder with generated questions + articles and stores the results to a new folder of `.parquet` files. This script will use the model under training to produce 10 summaries per each example on all of the input data examples. Followed by each summarization, it will also perform summary accuracy measurement, asking the down-stream LLM to answer the questions generated earlier solely based on the summaries generated by the desired model. \n",
    "\n",
    "üîÑ REPLACE the S3 URI in [`configs/summary_generation/8b_judge/mistral_finetuned_eval.yaml`](configs/summary_generation/8b_judge/mistral_finetuned_eval.yaml) with the path to the folder with generated questions from the previous job\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This job will take ~420 min for 8B on A10Gs and ~300 min for 70B on A100s given the default configurations.\n",
    "\n",
    "\n",
    "```bash\n",
    "anyscale job submit -f configs/jobs/8b_judge/generate_summaries_train_job.yaml \n",
    "# Optional: use the 70b model for better performance (runs on A100s)\n",
    "# anyscale job submit -f configs/jobs/70b_judge/generate_summaries_train_job.yaml\n",
    "```\n",
    "\n",
    "üí° INSIGHT: If your job is unable to acquire the specified resources, it might indicate a lack of availability of GPUs. Try decreasing the `concurrency` argument for reference model or the judge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîÑ REPLACE the below S3 URI with the link to the generated summaries from the job. You can optionally skip the previous with the example dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with the link to the generated summaries\n",
    "summary_folder = \"s3://air-example-data/preference-tuning-summarization-example/summary_generation_base/train/\"\n",
    "summary_ds = ray.data.read_parquet(summary_folder)\n",
    "example_rows = summary_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT:\n",
      "A Newcastle man got the shock of his life while he was filming a lightning storm\n",
      "in New South Wales. Rob McGee was watching the storm roll over the city - two\n",
      "hours north of Sydney - when he captured the moment a lightning strike hit a\n",
      "nearby gym. Mr McGee posted the video on social media just after 7.30pm on\n",
      "Sunday. Scroll down for video . Rob McGee was watching the storm roll over\n",
      "Newcastle - two hours north of Sydney - when he captured the moment a lightning\n",
      "strike hit a nearby gym . 'Scared the crap out of me!' he wrote along with the\n",
      "video. In the five-second footage, a deafening crack is heard when lightning\n",
      "hits a building. Mr McGee told shocked friends who commented on the post that\n",
      "the landing place was a gym behind Civic train station. 'Had a better look.....\n",
      "it is gym behind the station. Bet a few people dropped a weight or two,' he\n",
      "wrote. The landing place was a nearby gym behind Civic train station in\n",
      "Newcastle . The storm that rolled in over NSW on Sunday followed hot weather\n",
      "during the day . He added 'a few' others had hit his building but this was the\n",
      "only one he had captured on camera. At the time of publication, Mr McGees' video\n",
      "had attracted almost 70,000 views on social media. Sunday's hot weather during\n",
      "the day was followed by thunderstorms late in the afternoon. In NSW, a number of\n",
      "fires were believed to have been started by lightning strikes. At least 90\n",
      "blazes were burning across the state, with more than half deemed out-of-control.\n",
      "Most were caused by lightning strikes late Sunday that followed scorching\n",
      "temperatures in Sydney and around the state, NSW Rural Fire Service spokesman\n",
      "Ben Shepherd said. At the time of publication, Mr McGees' video had attracted\n",
      "almost 70,000 views on social media . 'The good thing at this stage is there is\n",
      "nothing impacting on any lives or property,' he told AAP on Monday. 'However,\n",
      "given the remoteness of some of these fires, it is going to take a considerable\n",
      "effort over coming days in order to actually start some containment on a number\n",
      "of these fires.' Mr Shepherd said the RFS had sent out planes on Monday morning\n",
      "to try to detect new outbreaks. 'We are expecting more [fires],' he said. About\n",
      "430 firefighters were on the ground and more are on standby. 'We've probably got\n",
      "a day or two at least of more favourable weather, which will assist in getting\n",
      "some of these fires under control,' Mr Shepherd said. Sydney was set to reach a\n",
      "top of 30 degrees Celsius on Monday while Wallsend, in the Hunter region, was\n",
      "expected to peak at 38, and Penrith in Sydney's west had a maximum of 36. Total\n",
      "fire bans remain in place for the lower central west plains, including Dubbo.\n",
      "\n",
      "QUESTIONS:\n",
      "Q1) What was the cause of at least 90 blazes burning across NSW? A. Wild animals\n",
      "B. Drought C. Lightning strikes D. Human error E. Strong winds  Q2) How many\n",
      "firefighters were on the ground to combat the fires? A. 200 B. 500 C. 430 D. 300\n",
      "E. 100  Q3) Where did Rob McGee capture the moment a lightning strike hit a\n",
      "nearby gym? A. Penrith B. Dubbo C. Newcastle D. Sydney E. Wallsend  Q4) What was\n",
      "the approximate number of views on Rob McGee's video at the time of publication?\n",
      "A. 70,000 B. 1,000 C. 7,000 D. 7,000,000 E. 700,000  Q5) What was the\n",
      "temperature expected to reach in Sydney on Monday? A. 30 degrees Celsius B. 40\n",
      "degrees Celsius C. 20 degrees Celsius D. 25 degrees Celsius E. 35 degrees\n",
      "Celsius\n",
      "\n",
      "MODEL GENERATED SUMMARY:\n",
      "A Newcastle man captured a lightning strike hitting a nearby gym on camera while\n",
      "filming a storm in New South Wales. The storm followed hot weather during the\n",
      "day and caused multiple fires across the state. At least 90 blazes were burning,\n",
      "with more than half out of control. The Rural Fire Service is working to contain\n",
      "the fires and expects more to occur.\n",
      "\n",
      "ANSWERS:\n",
      "['C' 'C' 'C' 'A' 'A']\n",
      "\n",
      "JUDGE ANSWERS FROM SUMMARY:\n",
      "['C', 'Unsure', 'C', 'Unsure', 'Unsure']\n",
      "\n",
      "'===================================================================================================='\n"
     ]
    }
   ],
   "source": [
    "for row in example_rows:\n",
    "    print_wrapped(\"TEXT\", row[DataSchema.ARTICLE])\n",
    "    print_wrapped(\"QUESTIONS\", row[DataSchema.MCQ_QUESTIONS])\n",
    "    print_wrapped(\"MODEL GENERATED SUMMARY\", row[DataSchema.SUMMARY_GENERATION_RAW_OUTPUT])\n",
    "    print_wrapped(\"ANSWERS\", str(row[DataSchema.GROUND_TRUTH_MCQ_ANSWERS]))\n",
    "    print_wrapped(\"JUDGE ANSWERS FROM SUMMARY\", str(row[DataSchema.JUDGE_MCQ_ANSWERS]))\n",
    "    pprint.pprint(\"=\" * 100, width=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c): Generate Preference Tuning Data\n",
    "\n",
    "The final step for getting our data ready! We'll now generate (chosen, rejected) summary pairs for each article based on the scores.\n",
    "\n",
    "The following command will run the [generate_dpo_data.py](src/scripts/generate_dpo_data.py) script, which takes in the folder of summaries and outputs `.jsonl` files for training and validation.\n",
    "\n",
    "üîÑ REPLACE the S3 URI in [`configs/training_data_generation/mistral_8b.yaml`](configs/training_data_generation/mistral_8b.yaml) with the path to the folder with generated summaries from the previous job\n",
    "\n",
    "Run the following command in the terminal to generate DPO data:\n",
    "```bash\n",
    "python src/scripts/generate_dpo_data.py configs/training_data_generation/mistral_8b.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the results\n",
    "# Replace with the link to your validation file\n",
    "validation_file = \"s3://air-example-data/preference-tuning-summarization-example/dpo_training_data/valid.jsonl\"\n",
    "\n",
    "valid_ds = ray.data.read_json(validation_file)\n",
    "example_rows = valid_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      "Given the following text, create a very short summary that is at most 2\n",
      "sentences.  Text: By . Tamara Cohen, Political Reporter . PUBLISHED: . 18:32\n",
      "EST, 27 January 2013 . | . UPDATED: . 08:48 EST, 28 January 2013 . Deputy Prime\n",
      "Minister Nick Clegg and his wife Miriam are determined to keep the education of\n",
      "their 11-year-old son 'out of politics' Nick Clegg yesterday defended the\n",
      "possibility he may send his children to private schools as it emerged he and his\n",
      "wife Miriam have not even visited their local state school. He said the\n",
      "education of his 11-year-old son Antonio, who starts secondary school this year,\n",
      "should not be used as 'a political football' and that the couple would do\n",
      "'what's best' for their children although he was braced for criticism. Last week\n",
      "the Liberal Democrat leader told listeners to his radio show he would send his\n",
      "son to a private school if he failed to find a place in a good comprehensive,\n",
      "saying he would use the state system 'if it works out', but that there is 'huge\n",
      "competition' for places in London. But Mr Clegg, who attended Westminster public\n",
      "school, has apparently not looked around nearby Ark Putney academy in south-west\n",
      "London, it was revealed yesterday by its headmaster Mark Phillips. Mr Phillips\n",
      "who has turned the school around since he was hired three years ago, said the\n",
      "school which was once in special measures but is now lauded by the Government\n",
      "for its improvements, could provide an 'exceptional' education for any child and\n",
      "that there was no need to pay fees for schooling. Unless the Cleggs had visited\n",
      "'under cover' he had not seen them, he said.'I am always very clear that all\n",
      "parents living locally are welcome to choose our school and it is important that\n",
      "every parent comes with their child and takes an objective look to see whether\n",
      "what we offer will meet the needs of their child', he said. 'It wouldn't claim\n",
      "to be the answer to every child and every parent. But I hope that if a parent\n",
      "does come, and sees an environment their child will thrive in, they will pick\n",
      "us...I am confident they will do exceptionally well. I don't believe you have to\n",
      "pay for it.' Mr Clegg told the BBC's Andrew Marr Show yesterday that he and his\n",
      "wife will do whatever is in the interests of their son . If he chooses to\n",
      "educate his children . privately, Mr Clegg is likely to be accused of hypocrisy\n",
      "after using a . speech last year to attack 'the great rift in our education\n",
      "system' caused by many of the best schools being fee-paying and said it had a .\n",
      "'corrosive' effect on society and the economy. In . an interview on BBC1's\n",
      "Andrew Marr Show, he said: 'I accept that it's a . dilemma for anyone in public\n",
      "life, particularly in politics, how do you . balance that with the fact Miriam\n",
      "and I have small children, and the . approach Miriam and I took right from the\n",
      "outset was to keep our . children completely out of politics. 'We . never put\n",
      "them in front of the camera or to make them or their . education a political\n",
      "football. 'I totally accept that when we make a . decision that'll be subject to\n",
      "public commentary, criticism and so on, . but in the meantime we want to protect\n",
      "the privacy of an 11-year-old boy . and make the decision that we as parents\n",
      "think is best for our son.' The deadline for applying for entry to Ark Putney\n",
      "for 2013-14 was last October. The school is part of the Ark academy chain, set\n",
      "up in 2004, whose chairman is Paul Marshall, one of the Liberal Democrats'\n",
      "biggest donors. Last year 62 per cent of pupils gained at least five good GCSEs,\n",
      "prompting schools minister Nick Gibb to write to Mr Phillips to congratulate him\n",
      "on the 'excellent results' saying the school was in the top 100 best-performing,\n",
      "based on sustained improvements every year since 2008. However Michael Gove last\n",
      "year approved the sale of five acres of playing fields at the school including\n",
      "six tennis courts, a football pitch and a playground, to developers to fund\n",
      "refurbishments, after a ¬£40million revamp under the Building Schools for the\n",
      "Future programme was cancelled. Alumni of Ark Putney, which used to be Elliott\n",
      "School, include actor Pierce Brosnan, and 1960s England bowler Geoff Arnold.\n",
      "Former Welsh secretary Peter Hain sent his children to the school, which was the\n",
      "scene of the Christmas play in the film Love, Actually. David Cameron has said\n",
      "his children will attend state school, but George Osborne has been criticised\n",
      "for sending his to the fee-paying preparatory school in Kensington that he\n",
      "attended.\n",
      "\n",
      "CHOSEN RESPONSE:\n",
      "Nick Clegg and his wife Miriam are considering sending their 11-year-old son\n",
      "Antonio to a private school, despite their previous claims that they would use\n",
      "the state system. Clegg has not visited Ark Putney Academy, a nearby state\n",
      "school, but its headmaster, Mark Phillips, claims that the school could provide\n",
      "an 'exceptional' education for any child and that there is no need to pay fees\n",
      "for schooling. The deadline for applying to Ark Putney for 2013-14 was last\n",
      "October and the school is part of the Ark academy chain, set up in 2004, whose\n",
      "chairman is Paul Marshall, one of the Liberal Democrats' biggest donors.\n",
      "\n",
      "REJECTED RESPONSE:\n",
      "Deputy Prime Minister Nick Clegg has defended the possibility of sending his\n",
      "children to private schools, stating that their education should not be used as\n",
      "a political football. He and his wife Miriam have not visited their local state\n",
      "school, but the headmaster of Ark Putney academy in London, where they were\n",
      "invited to attend, said the school could provide an \"exceptional\" education for\n",
      "any child and that parents should come to see it firsthand before making a\n",
      "decision.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in example_rows:\n",
    "    print_wrapped(\"PROMPT\", row[\"chosen\"][0][\"content\"])\n",
    "    print_wrapped(\"CHOSEN RESPONSE\", row[\"chosen\"][1][\"content\"])\n",
    "    print_wrapped(\"REJECTED RESPONSE\", row[\"rejected\"][1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Fine-tuning\n",
    "\n",
    "Now that we have the pre-processed dataset, we are ready to fine-tune `Mistral-7B-Instruct-v0.1` using DPO. On Anyscale, we've created an easy-to-use interface to do preference-tuning using DPO. We leverage Ray to overlap reference model log-probability calculation with model training to improve GPU utilization. Most implementations compute log probabilities synchronously with model training,\n",
    "\n",
    "![hf model](./assets/hf_dpo.png)\n",
    "\n",
    "While our implementation using Ray is asynchronous:  \n",
    "\n",
    "\n",
    "![assistant model](./assets/anyscale_dpo.png)\n",
    "\n",
    "Further, our use of Ray Data also implies that the compute configuration for the reference model can be completely decoupled with the policy model. For example, reference model calculation can run on a different node (with configurable number of GPUs, etc) with zero code changes needed. \n",
    "\n",
    "> **NOTE** Make sure you've gove over the [user guides](https://docs.anyscale.com/category/fine-tuning-beta) for fine-tuning to understand the different configurations available\n",
    "\n",
    "To get started with DPO training, we provide the config for DPO in [configs/mistral_dpo_summarization.yaml](configs/mistral_dpo_summarization.yaml) . You can add your `WANDB_API_KEY` as an environment variable in the dependencies tab if you wish to track progress of your run on WandB.\n",
    "\n",
    "\n",
    " üîÑ REPLACE the training and validation file paths in the config with the output file paths in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id: mistralai/Mistral-7B-Instruct-v0.1\n",
      "# Example summarization dataset with 10k examples for training with an average of 2.2k tokens per sample.\n",
      "# Make sure to replace `train_path` and `valid_path` with the path to the files you generated\n",
      "train_path: s3://air-example-data/preference-tuning-summarization/train.jsonl\n",
      "valid_path: s3://air-example-data/preference-tuning-summarization/valid.jsonl\n",
      "\n",
      "task: \"preference_tuning\"\n",
      "context_length: 4096\n",
      "# For DPO, it is recommended to set a high `num_data_blocks_per_device` to not bottleneck the logp processor.\n",
      "num_data_blocks_per_device: 16\n",
      "# Runs training on 12 GPUs\n",
      "num_devices: 16\n",
      "train_batch_size_per_device: 2\n",
      "eval_batch_size_per_device: 2\n",
      "learning_rate: 5e-6\n",
      "num_epochs: 3\n",
      "no_gradient_checkpoint: False\n",
      "output_dir: /mnt/local_storage/\n",
      "# Deepspeed configuration, you can provide your own deepspeed setup\n",
      "deepspeed:\n",
      "  config_path: configs/zero_3.json\n",
      "worker_resources:\n",
      "  accelerator_type:A10G: 1\n",
      "flash_attention_2: True\n",
      "padding: \"longest\"\n",
      "preference_tuning_config:\n",
      "  beta: 0.01\n",
      "  logprob_processor_scaling_config:\n",
      "    custom_resources:\n",
      "      accelerator_type:A10G: 1 # custom resource per worker.\n",
      "    # Runs reference model logp calculation on 4 GPUs\n",
      "    concurrency: 4\n",
      "    batch_size: 2\n",
      "lora_config:\n",
      "  r: 8\n",
      "  lora_alpha: 16\n",
      "  lora_dropout: 0.05\n",
      "  target_modules:\n",
      "    - q_proj\n",
      "    - k_proj\n",
      "    - v_proj\n",
      "    - o_proj\n",
      "    - gate_proj\n",
      "    - up_proj\n",
      "    - down_proj\n",
      "  modules_to_save: []\n",
      "  bias: \"none\"\n",
      "  fan_in_fan_out: false\n",
      "  init_lora_weights: true\n"
     ]
    }
   ],
   "source": [
    "!cat configs/mistral_dpo_summarization.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can fine-tune the model now by submitting it as an Anyscale job: \n",
    "\n",
    "```bash\n",
    "anyscale job submit configs/jobs/mistral_dpo_job.yaml\n",
    "```\n",
    "\n",
    "This should take about 8 hours for the 8B model on A10Gs and 6 hours for the 70B model on A100s. \n",
    "\n",
    "üí° INSIGHT: This fine-tuning job inherits the compute configuration of the current workspace - meaning the job runs on a CPU-only head node with auto-scaling enabled. Sometimes, the nodes you get with auto-scaling can be in-efficient for fine-tuning due to inter-node communication costs (Say you get 2 4xA10 nodes instead of 8xA10s due to availability). You can explicitly set the compute configuration for the job with a set number of worker nodes to avoid this.  \n",
    " - More on compute configurations here: https://docs.anyscale.com/configuration/compute-configuration \n",
    " - The complete Anyscale Job API reference: https://docs.anyscale.com/reference/job-api#jobconfig "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Evaluation\n",
    "\n",
    "Let's evaluate our trained model. Here we'll use two baselines: (1) the base model before finetuning (reference model in DPO) and (2) GPT-4o.\n",
    "\n",
    "## Evaluation strategy\n",
    "\n",
    "Our evaluation strategy involves the same Q&A scoring system as used while generating the preference data. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/eval.png?\" alt=\"Evaluation\" width=800>\n",
    "</p>\n",
    "\n",
    "We evaluate the baseline model and the trained DPO model on the test set. \n",
    "\n",
    "## Obtain summaries on the test set\n",
    "First, we'll need to obtain the summaries (and scores) for both the models on the given test set. \n",
    "\n",
    "For the baseline model, you can simply run the below command:\n",
    "```bash\n",
    "anyscale job submit -f configs/jobs/8b_judge/generate_summaries_eval_baseline_job.yaml\n",
    "# Optional: use the 70b model for better performance (runs on A100s)\n",
    "# anyscale job submit -f configs/jobs/70b_judge/generate_summaries_eval_baseline_job.yaml \n",
    "```\n",
    "\n",
    "This should take ~10 min for the 8B model and the 70B model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the fine-tuned DPO model, we provide a dummy config in [configs/summary_generation/8b_judge/mistral_finetuned_eval.yaml](configs/summary_generation/8b_judge/mistral_finetuned_eval.yaml). If you used the default training config provided, the model would be trained using LoRA and you should have a path to the LoRA weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: eval\n",
      "input_folder: s3://air-example-data/preference-tuning-summarization-example/qa_generation/qa_annotations_full_test\n",
      "inference_type: offline\n",
      "model_inference_config:\n",
      "  # Modify with s3 link to full param weights if you did full-param training\n",
      "  model_id_or_path: mistralai/Mistral-7B-Instruct-v0.1\n",
      "\n",
      "  # Add path to lora weights here. If you did full param training, you can instead remove this field.\n",
      "  adapter_id_or_path: s3://large-dl-models-mirror/finetuning_template/mistral_dpo_summarization_lora\n",
      "\n",
      "  temperature: 0\n",
      "  top_p: 0.95\n",
      "  scaling_config:\n",
      "    batch_size: 64\n",
      "    concurrency: 2\n",
      "    num_gpus_per_instance: 2\n",
      "    accelerator_type: A10G\n",
      "num_generations: 1\n",
      "judge_inference_config:\n",
      "  model_id_or_path: meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "  temperature: 0\n",
      "  scaling_config:\n",
      "    batch_size: 64\n",
      "    concurrency: 3\n",
      "    num_gpus_per_instance: 2\n",
      "    accelerator_type: A10G\n",
      "num_mcq_questions: 5\n"
     ]
    }
   ],
   "source": [
    "!cat configs/summary_generation/8b_judge/mistral_finetuned_eval.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " üîÑ REPLACE the `adapter_id_or_path` entry in the config with the path to your LoRA weights before proceeding (if you used the fine-tuning defaults). Alternatively, make sure to replace `model_id_or_path` entry (and remove the `adapter_id_or_path` entry) if you did full-param fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to evaluate our fine-tuned model: \n",
    "\n",
    "```bash\n",
    "anyscale job submit -f configs/jobs/8b_judge/generate_summaries_eval_finetuned_job.yaml\n",
    "# Optional: use the 70b model for better performance (runs on A100s)\n",
    "# anyscale job submit -f configs/jobs/70b_judge/generate_summaries_eval_finetuned_job.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the logs for the above jobs, you should see the final path to the output summaries for both the models. \n",
    "\n",
    "Optionally, you can also obtain the summaries and scores for the `gpt-4o` model from OpenAI. Simply run: \n",
    "\n",
    "```bash\n",
    "anyscale job submit -f configs/jobs/8b_judge/generate_summaries_eval_gpt_job.yaml\n",
    "# Optional: use the 70b model for better performance (runs on A100s)\n",
    "# anyscale job submit -f configs/jobs/70b_judge/generate_summaries_eval_gpt_job.yaml\n",
    "```\n",
    "\n",
    "This should take about ~10 min for the 8B model and the 70B model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Evaluation Statistics\n",
    "\n",
    "We've provided a convenient script [get_eval_stats.py](src/scripts/get_eval_stats.py) to get evaluation statistics and obtain the \"win rate\" of the DPO model (the percentage of times the DPO model performs better than the baseline). We've provided an example configuration below. \n",
    "\n",
    "```bash \n",
    "# make sure to substitute -outputs-path with your path\n",
    "python src/scripts/get_eval_stats.py --outputs-path s3://air-example-data/preference-tuning-summarization-example/summary_generation_dpo_model/test/ --baseline-outputs-path s3://air-example-data/preference-tuning-summarization-example/summary_generation_base/test/  \n",
    "\n",
    "# (Optional): if you obtained results for GPT-4o, you should uncomment and run the following command instead\n",
    "# python src/scripts/get_eval_stats.py --outputs-path s3://air-example-data/preference-tuning-summarization-example/summary_generation_dpo_model/test/ --baseline-outputs-path s3://air-example-data/preference-tuning-summarization-example/summary_generation_base/test/  --gpt4o-outputs-path <add-path-to-gpt4o-results>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the following results for the 70B model:\n",
    "\n",
    "```text \n",
    "‚ïí‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïï\n",
    "‚îÇ           Metric            ‚îÇ   Model   ‚îÇ  Baseline  ‚îÇ  GPT-4o   ‚îÇ\n",
    "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
    "‚îÇ        Accuracy >=3         ‚îÇ 65.4286 % ‚îÇ 43.0476 %  ‚îÇ 37.2381 % ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ        Accuracy >=4         ‚îÇ 25.7143 % ‚îÇ 13.5238 %  ‚îÇ 10.0000 % ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ     Median Compression      ‚îÇ 11.5794 % ‚îÇ 12.7316 %  ‚îÇ 8.0496 %  ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ      Mean Compression       ‚îÇ 13.0029 % ‚îÇ 14.3444 %  ‚îÇ 9.3554 %  ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ      Summary Too Long       ‚îÇ 0.0000 %  ‚îÇ  0.0000 %  ‚îÇ 0.0000 %  ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ Contains Invalid Characters ‚îÇ 0.0000 %  ‚îÇ  0.0952 %  ‚îÇ 0.0000 %  ‚îÇ\n",
    "‚ïò‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïõ\n",
    "\n",
    "\n",
    "Model Win Rate against Baseline: 74.0000 %\n",
    "GPT-4o Win Rate against Baseline: 64.8095 %\n",
    "```\n",
    "\n",
    "Our fine-tuned model is able to generate much better summaries, that are more concise (compression ratio is lower) with lesser out-of-distribution characters (gibberish tokens) than the baseline. You can see more details on the same in our blog!\n",
    "\n",
    "| **NOTE:** The evaluation results will differ if you used the 8B model which is less capable as a LLM-judge (and thus the numbers can be less accurate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congrats! You have now fine-tuned an open source model on preference data. As a quick recap, here's what we demonstrated in this notebook:\n",
    "1. Synthetically generating preference data for DPO \n",
    "2. DPO fine-tuning of a language model on the Anyscale Platform\n",
    "4. Evaluating the model against the baseline and GPT-4o, and analysing the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
