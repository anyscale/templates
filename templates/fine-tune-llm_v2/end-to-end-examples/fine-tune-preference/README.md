# Preference Tuning for Summarization using Synthetic Data

**‚è±Ô∏è Time to complete**: 10 hours

Preference tuning is a powerful tool that can optimize LLMs towards complex preferences that cannot easily captured through supervised fine-tuning. However, manually annotating preferences between model outputs using human raters can be extremely time-consuming and expensive. Instead, synthetic preference data can be generated by scoring responses with large foundation models, allowing for much cheaper and scalable data collection!

Here we'll go through an end-to-end example for preference tuning of an open-source language model with synthetic data, covering scalable methodologies for data preprocessing, fine-tuning and evaluation, using Ray. We will focus on the task of summarization for the [CNN/DailyMail](https://huggingface.co/datasets/abisee/cnn_dailymail) dataset. 

This notebook is based on the following blog post: `TODO`.

# Table of Contents
1. [Data Preprocessing](#step-1-data-preprocessing): In this section we cover how we can prepare preference data for the summarization task using an LLM-as-a-judge. 
    1. [Generate Multiple Choice Questions From Articles](#part-a-generate-multiple-choice-questions-from-articles)
    2. [Generate Summaries and Scores](#part-b-generate-summaries--scores)
    3. [Generate Preference Tuning Data](#part-c-generate-preference-tuning-data)
2. [DPO Finetuning](#step-2-fine-tuning): This section will cover how you can fine-tune an open source model on the preference data on the Anyscale platform.
3. [Evaluation](#step-3-evaluation): The section will lay down a blue-print for evaluation and compare performance to that of closed source models like OpenAI's GPT-4.

First, let's make the necessary imports


```python
import os
import pprint
import textwrap

import ray.data
import datasets


from src.utils.models import DataSchema
from src.utils.common import print_wrapped

os.environ["PYTHONPATH"] = f"{os.environ.get('PYTHONPATH', '')}:src"
```

# Step 1: Synthetic Data Generation

First, let's inspect the training dataset and look at an example. 


```python
hf_ds = datasets.load_dataset("abisee/cnn_dailymail", "3.0.0", split="train").shuffle(
    seed=21
)
# extract a subset of 20000 articles
hf_ds_subset = hf_ds.select(range(20000))

ray_ds = ray.data.from_huggingface(hf_ds_subset)
raw_example = ray_ds.take(1)[0]
```


    Downloading readme:   0%|          | 0.00/15.6k [00:00<?, ?B/s]



    Downloading data:   0%|          | 0.00/257M [00:00<?, ?B/s]



    Downloading data:   0%|          | 0.00/257M [00:00<?, ?B/s]



    Downloading data:   0%|          | 0.00/259M [00:00<?, ?B/s]



    Downloading data:   0%|          | 0.00/34.7M [00:00<?, ?B/s]



    Downloading data:   0%|          | 0.00/30.0M [00:00<?, ?B/s]



    Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]



    Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]



    Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]



    Downloading readme:   0%|          | 0.00/15.6k [00:00<?, ?B/s]



    Downloading data:   0%|          | 0.00/257M [00:00<?, ?B/s]



    Downloading data:   0%|          | 0.00/257M [00:00<?, ?B/s]



    Downloading data:   0%|          | 0.00/259M [00:00<?, ?B/s]



    Downloading data:   0%|          | 0.00/34.7M [00:00<?, ?B/s]



    Downloading data:   0%|          | 0.00/30.0M [00:00<?, ?B/s]



    Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]



    Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]



    Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]


    2024-08-16 12:04:56,395	INFO worker.py:1603 -- Connecting to existing Ray cluster at address: 10.0.21.126:6379...
    2024-08-16 12:04:56,400	INFO worker.py:1779 -- Connected to Ray cluster. View the dashboard at [1m[32mhttps://session-ujamras9pcvamresjwww6p5jr4.i.anyscaleuserdata.com [39m[22m
    2024-08-16 12:04:56,404	INFO packaging.py:358 -- Pushing file package 'gcs://_ray_pkg_d3e147f0bc0554b452c97a1fb7d4cbbb46505ca5.zip' (1.13MiB) to Ray cluster...
    2024-08-16 12:04:56,407	INFO packaging.py:371 -- Successfully pushed file package 'gcs://_ray_pkg_d3e147f0bc0554b452c97a1fb7d4cbbb46505ca5.zip'.


    [36m(autoscaler +30s)[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.
    [36m(autoscaler +30s)[0m [autoscaler] [48CPU-192GB] Upscaling 1 node(s).
    [36m(autoscaler +32s)[0m [autoscaler] [48CPU-192GB|m5.12xlarge] [us-east-1a] [on-demand] Launched 1 instances.


    2024-08-16 12:05:55,375	INFO dataset.py:2373 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.
    2024-08-16 12:05:55,379	INFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-08-16_11-08-34_539321_2359/logs/ray-data
    2024-08-16 12:05:55,379	INFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> LimitOperator[limit=1]



    - limit=1 1:   0%|          | 0/1 [00:00<?, ?it/s]



    Running 0:   0%|          | 0/1 [00:00<?, ?it/s]



```python
pprint.pprint(raw_example, width=80)
```

    {'article': 'Scam: Lisa Harrison, 34, promised customers low currency rates on '
                'US dollars and special deals . A wedding planner who stole '
                "¬£80,000 from couples in a bid to satisfy an 'out-of-control' "
                'online gambling addiction has been jailed. Lisa Harrison, 34, '
                'began taking money from her clients in summer 2013 by enticing '
                'them with low currency rates on US dollars and flight upgrades. '
                'She took money from 19 couples who had entrusted their savings to '
                'her after being promised the wedding of their dreams. It is '
                'understood that the company she worked for, iPlan New York, '
                'specialised in weddings in New York City. Her website '
                "iplannewyork.com, which has been taken down, said: 'iPlan New "
                'York was set up to create and style the perfect tailor made '
                "wedding for couples travelling to New York to get married! 'We "
                'are passionate about what we do and passionate about New York! We '
                'have experience in planning NYC weddings for couples from all '
                "over the world.' But she was arrested in December last year after "
                'eventually coming clean to her victims in an email and saying she '
                'had been forced to close her business. Police soon found she had '
                'taken ¬£80,107 from the couples and spent a staggering ¬£77,933 on '
                "gambling sites Paddy Power and William Hill. The business' "
                'Facebook page has also been deleted, but outraged victims have '
                'shared their victims on a wedding forum. One victim called '
                "Jennifer wrote in November 2013: 'I had previously given Lisa a "
                "positive review because our vow renewal went wonderful. 'Little "
                "did I know until last week that she didn't even pay the vendors "
                "that helped with our ceremony. 'I am so disgusted and can't "
                'fathom such an act. We paid her in full and to think that our '
                "photographer didn't even get paid is just astonishing to me. 'I "
                'feel so horrible for the other couples that had their perfect day '
                "planned and this woman decided to perform such an act. 'I pray "
                'for each of you in hopes that you will be able to move on from '
                'this and live a healthy and happy life with your significant '
                "other. 'I can't believe this woman took our money and did such an "
                'unthinkable act. God bless all of you and I hope this mess gets '
                "corrected quickly.' While another anonymous victim posted a copy "
                'of the email they claim they had been sent by Harrison when she '
                "admitted the scam. It read: 'I have to announce the closure of "
                "iPlan New York. 'For some time now I have been battling against a "
                'gambling addiction that has seen me lose all of the company money '
                "including money paid to me by you for services and dollars. 'I "
                'cannot go on another day with this situation as this illness has '
                'taken me over completely and I have to both face up to the '
                'consequences of my actions and seek help for the debilitating '
                "addiction. 'I am extremely ill with it and need to seek help as "
                "soon as possible. 'I am completely devastated that not only have "
                'I lost money of yours but betrayed your trust as a wedding '
                "planner. 'Right now I am uncertain as to what the future holds "
                'with regards to future weddings already planned, I will be in '
                'touch with the suppliers in NYC to inform them also. Sentence: '
                'Harrison, of Earith, Cambridgeshire, was jailed for two years at '
                "Peterborough Crown Court, above . 'I will today be having my "
                'computer and all electronic devices ceased (sic) under an '
                'intervention and handing myself into the police to give a '
                "statement and to tell them everything. 'No doubt you will be "
                'informing the police too and for those purposes it will be the '
                'Cambridgeshire Constabulary and my full name is Lisa Harrison and '
                "I will be handing myself in after sending these emails. 'I won‚Äôt "
                'be able to reply to any emails or calls for the time being as I '
                "will not have access. 'I am truly from the bottom of my heart so "
                'sorry for everything, as with addictions I thought I had '
                'everything under control and was in denial that I could put '
                'everything right, which I have been trying so desperately to do. '
                "'As soon as and if I am able to communicate further about any "
                "outstanding issues I will do so. Lisa.' Posting on an online "
                'review site for the wedding service, one former customer said: '
                "'We are due to go in less than 48 hours and we have nothing!! She "
                "has now closed down her website too! She has left us devastated!' "
                "Another, using the name Shaun, wrote: 'Alarm bells rang for me "
                'when she asked for all our spending money cos she had a deal on a '
                "currency card. While one woman, using the name Andrea, said: 'I "
                'am absolutely devastated for anyone who has used iPlan New York '
                "and subsequently been let down'. She added that she had a 'gut "
                "feeling' not to pay upfront. Harrison, of Earith, Cambridgeshire, "
                'admitted fraudulent trading and was jailed for two years at '
                'Peterborough Crown Court on Tuesday. Det Sgt Iain Moor, from '
                "Cambridgeshire Constabulary, said: 'This was an extremely "
                'distressing case for the 19 couples who lost life savings and had '
                "their dream day ruined by Harrison. 'I hope the victims received "
                'some comfort in the prison sentence imposed on Harrison, meaning '
                "they can now start to re-build their lives.'",
     'highlights': 'Lisa Harrison enticed clients with low currency rates and '
                   'flight upgrades .\n'
                   'She took money from 19 couples who were promised dream '
                   'weddings .\n'
                   'Harrison spent nearly ¬£78,000 on gambling sites including '
                   'Paddy Power .\n'
                   'She admitted the scam to victims in an email before handing '
                   'herself in .\n'
                   "Outraged victims say they are 'disgusted' and have been left "
                   "'devastated'\n"
                   "In email Harrison says she will 'seek help for the "
                   "debilitating addiction'\n"
                   'She admitted fraudulent trading and was jailed for two years '
                   'on Tuesday .',
     'id': '4feb82c680166f0b8f90bf3a6f9779b04f229325'}


Now, we need to get preference data for pairs of summaries generated from the same article. Traditionally, this would involve generating summaries using the base model you wish to fine-tune and asking human annotators to provide a rating for each sample. In this example, we will employ a _synthetic_ summary scoring method using an LLM as a judge. We score the correctness of a summary using the following metrics:

**Summary Scoring Metrics**
1. Multiple choice Q&A accuracy:
    - Given the original text, we use an LLM judge to generate 5 multiple choice questions about the text.
    - We then ask the LLM judge to answer the questions using only the summary, and record the number of questions correctly answered.
2. Word count: We simply count the number of words in the summary.

This allows us to construct a simple preference function between two summaries:

**Preference Function**
1. If both summary responses attain more than 3/5 multiple choice questions correct, we will prefer the shorter response. We do not care about Q&A accuracy beyond 3 correct answers, since the summary should not contain all information from the text.
2. Otherwise, we select the response that leads to more correctly answered multiple choice questions.

To generate the preference pairs, we will generate 10 summaries from each article using the model we wish to fine-tune. Then, we will randomly sample pairs of summaries and use our preference function to annotate the preference between them.

For this example, we will use `Mistral-7B-Instruct-v0.1` as the base model to fine-tune and `Llama-3.1-70B-Instruct` as a judge. Note that mistral-instruct is already instruction tuned, so that given a prompt to do summarization it might do a good job, but it may not be aligned with how we want the summarization to look like. We can use preference data to further align the instruct variant towards our specific needs.

Combining all this together, our data pre-processing pipeline is going to look as follows: 

![preprocessing](./assets/preprocessing.png?1)

### Part (a): Generate Multiple Choice Questions from Articles

First, we will generate the multiple choice questions and answers for each article using `Llama-3.1-8B-Instruct` (or `70B` if you have A100/H100s). Leveraging vLLM and Ray, we can very easily scale this generation process across multiple GPUs.


>  **_NOTE:_**  We provide two sets of configs: One with a 8B parameter model as the judge, and another with the 70B model. Using the 8B model is recommended for quicker runtimes, since we make use of highly available A10Gs. For good performance, and to replicate the results in our blog, you should use the 70B judge model which uses A100s. 

The following command will run the [src/scripts/generate_questions.py](./src/scripts/generate_questions.py) script, which generates the questions and answers and saves them in `.parquet` files.


```python
!anyscale job submit -f configs/jobs/8b_judge/generate_questions_job.yaml
# Optional: use the 70b model for better performance (runs on A100s)
# !anyscale job submit -f configs/jobs/70b_judge/generate_questions_job.yaml
```

    [1m[36mOutput[0m[0m
    [0m[1m[36m(anyscale +1.4s)[0m [0m[0m[0m[0mSubmitting job with config JobConfig(name='preference-tuning-summarization-question-generation', image_uri='localhost:5555/anyscale/endpoints_aica:0.5.0-6402', compute_config=None, env_vars=None, py_modules=None, cloud=None, project=None, ray_version=None, job_queue_config=None).[0m
    [0m[1m[36m(anyscale +3.6s)[0m [0m[0m[0m[0mUsing workspace runtime dependencies env vars: {'WANDB_API_KEY': 'cbc4aed2de2d9c9acb21324a3297b85b7299479b'}.[0m
    [0m[1m[36m(anyscale +3.6s)[0m [0m[0m[0m[0mUploading local dir '.' to cloud storage.[0m
    [0m[1m[36m(anyscale +5.0s)[0m [0m[0m[0m[0mIncluding workspace-managed pip dependencies.[0m
    [0m[1m[36m(anyscale +5.6s)[0m [0m[0m[0m[0mJob 'preference-tuning-summarization-question-generation' submitted, ID: 'prodjob_sdaruzx8uu3c2bu3x5dn6gpf77'.[0m
    [0m[1m[36m(anyscale +5.6s)[0m [0m[0m[0m[0mView the job in the UI: https://console.anyscale.com/jobs/prodjob_sdaruzx8uu3c2bu3x5dn6gpf77[0m
    [0m[1m[36m(anyscale +5.6s)[0m [0m[0m[0m[0mUse `--wait` to wait for the job to run and stream logs.[0m
    [0m[0m

At the end of the job, you should see the remote path to the folder with Q&A in the logs.

<p align="center">
  <img src="./assets/question_generation_done.png?" alt="Evaluation" width=800>
</p>

 Make sure to make note to use it for the next steps! 


```python
# Replace this with the link to the output folder from the previous job
qa_folder = "s3://air-example-data/preference-tuning-summarization-example/qa_generation/qa_annotations_full_train/"
qa_ds = ray.data.read_parquet(qa_folder)
# The dataset is small, we can materalize it
example_rows = qa_ds.materialize().take(3)
```


```python
for row in example_rows:
    print_wrapped("TEXT", row[DataSchema.ARTICLE])
    print_wrapped("QUESTIONS", row[DataSchema.MCQ_QUESTIONS])
    print_wrapped("ANSWERS", str(row[DataSchema.GROUND_TRUTH_MCQ_ANSWERS]))
    pprint.pprint("=" * 80, width=80)
```

    TEXT:
    Built behind three sets of double gates, it boasts 12 bedrooms, 23 bathrooms and
    a 27-car garage. This 25-acre estate in Beverly Hills, Los Angeles, has  been
    put on the market for $195million - making it the most expensive property for
    sale in America. Dubbed 'Palazzo di Amore' (or 'Palace of Love'), the sprawling
    villa¬†is accessible via a quarter-mile-long drive through a tree-lined path and
    a vineyard. It features more than 35,000 square feet of living space, including
    a two-story entrance hall with two sweeping staircases and a detached
    guesthouse. Scroll down for video . Sprawling: This villa in Beverly Hills, Los
    Angeles, boasts 12 bedrooms, 23 bathrooms and a 27-car garage. Above, the
    property features more than 35,000 square feet of living space, including a two-
    story entrance hall with a chandelier and two sweeping staircases . Production:
    The mansion has its own vineyard, which creates 400 to 500 cases of wine a year
    under the label, Beverly Hills Vineyards. Above, the property's 13,000-bottle
    wine cellar, featuring bottles of syrah, cabernet and sauvignon blanc, alongside
    other wines . Spacious: It also includes a 15,000-square-foot entertainment
    complex, complete with a disco/ballroom, a revolving dance floor, a DJ booth and
    a laser-light system. Inside the complex, as many as 250 guests can make use of
    a 50-seat theater and a bowling alley . Stunning: The mansion can host up to
    1,000 people. Guests are greeted by an Italian-made fountain carved of Carrara
    marble (pictured) Wealthy businessmen: The property is being sold by real estate
    entrepreneur and billionaire Jeff Greene (left). Right,¬†developer Mohamed Hadid
    (pictured) worked on the mansion with the aid of architect Bob Ray Offenhauser
    and designer Alberto Pint . It also includes a 15,000-square-foot entertainment
    complex, complete with a disco/ballroom, a revolving dance floor, a DJ booth and
    a laser-light system. The mansion is being sold by real estate entrepreneur Jeff
    Greene, who owns properties in Florida, New York and California, the¬†LA
    Times¬†reported. Mr Greene, 59, who refers to the¬†Mediterranean-style California
    property as 'a palace for the modern era', bought it in 2007 for $35million. He
    then spent eight years and $25million expanding the residence to its current
    size. Now, its master suite alone sits at a whopping 5,000 square feet. The
    villa was developed by Mohamed Hadid, who specializes in large estates, with the
    aid of architect Bob Ray Offenhauser and designer Alberto Pint. Mr Hadid was
    married to The Real Housewives of Beverly Hills star Yolanda Foster, 50, ¬†from
    1994 until 2000. The couple have three children together. Well-furnished: Mr
    Greene, 59, who refers to the Mediterranean-style California property as 'a
    palace for the modern era', bought it in 2007 for $35million. He then spent
    eight years and $25million expanding the residence to its current size. Above, a
    sitting room . Beautiful: Outside the mansion, residents can swim in a 128-foot
    reflecting pool, relax in a Turkish-style spa and walk through gardens .
    Breathtaking scenery: The huge villa boasts spectacular views (pictured) of the
    West Side of Los Angeles, Century City and the sea . Inside the entertainment
    complex, up to 250 guests can make use of a 50-seat theater, a bowling alley and
    a game room under hand-painted ceilings. They can exit the complex via a
    floating-style, glass-floor pathway, which sits over several swimming pools
    lined by 70-year-old olive trees. Once outside, residents can swim in a 128-foot
    reflecting pool, relax in a Turkish-style spa, walk through formal gardens and
    play on a tennis court. They can also visit the beautiful vineyard, which
    produces 400 to 500 cases of wine a year under its own private label, Beverly
    Hills Vineyards. Fit for royalty: The property has been put on the market for
    $195million - making it the most expensive property for sale in America . 25
    acres of land: The property (pictured) is listed under agents Joyce Rey and
    Stacy Gottula, of Coldwell Banker Previews International . The mansion, which
    boasts spectacular views of the West Side of Los Angeles, Century City and the
    sea, can host up to 1,000 people, with on-site parking for as many as 150
    vehicles,¬†according to¬†KTLA 5. Guests are greeted at the front doors by an
    Italian-made fountain carved of Carrara marble. The property is listed as the
    priciest home for sale in the U.S. under¬†agents Joyce Rey and Stacy Gottula, of
    Coldwell Banker Previews International. Mr Greene, who has been renting the
    villa out for $475,000 a month and married his wife, Mei Sze Greene, there in
    2007 with Mike Tyson as the best man, said he had always intended to sell it,
    but construction was not completed 'until literally a month ago', according to
    the Wall Street Journal. Another perspective: Mr Greene said he had always
    intended to sell it, but construction was not completed 'until literally a month
    ago' Couple: Mr Greene, who has been renting the villa out for $475,000 a month
    and married his wife, Mei Sze Greene (both picture) there in 2007 with Mike
    Tyson as the best man, said he had always intended to sell it, but construction
    was only completed a month ago . 'Honestly, I didn‚Äôt think it was going to be
    this much work,' he said, adding that the mansion has been mostly rented out to
    members of Saudi¬†royalty. In March, a similarly-sized mansion in nearby Holmby
    Hills sold for $102million, according to official records. However, it had just
    five acres of land. 'This property is five times the size of that,' said
    Gottula. Other mansions have recently sold in Los Angeles - but none for more
    than $100million. While Jeff Greene's sprawling 25-acre estate is on the market
    for $195million, this property will not even set you back $200. At $188, this
    house in Flint, Michigan, is believed to be the cheapest house publicly listed
    for sale in America. The property, listed on Realtor.com, is described as being
    a 'Fixer Upper Home, Needs lots of work, has major fire damage, seller selling
    AS IS'. Built in 1928, just before the Great Depression, it covers just 1,225
    square feet, with three bedrooms, a single bath and a one-car garage. Contrast:
    At $188, this three-bedroom house in Flint, Michigan, is believed to be the
    cheapest property on the market in America . Flint, which has a population of
    less than 100,000, was one of the hardest hit cities before and during the
    recession, USA Today reported. Now, the city's median household income  is just
    above $27,000 - significantly lower than the national average of just more than
    $51,000. The house is located at 2518 Dakota Avenue, just off Michigan's Route
    59 and not far from I-475.
    
    QUESTIONS:
    Q1) Who is the real estate entrepreneur selling the Palazzo di Amore estate? A.
    Bob Ray Offenhauser B. Alberto Pint C. Joyce Rey D. Jeff Greene E. Mohamed Hadid
    Q2) How many bedrooms and bathrooms does the Palazzo di Amore estate have? A. 20
    bedrooms, 30 bathrooms B. 10 bedrooms, 15 bathrooms C. 12 bedrooms, 23 bathrooms
    D. 12 bedrooms, 20 bathrooms E. 15 bedrooms, 25 bathrooms  Q3) What is the
    approximate size of the master suite in the Palazzo di Amore estate? A. 3,000
    square feet B. 10,000 square feet C. 2,000 square feet D. 5,000 square feet E.
    1,000 square feet  Q4) What is the name of the 25-acre estate in Beverly Hills,
    Los Angeles, that has been put on the market for $195 million? A. The Palace of
    Love B. None of the above C. Palazzo di Amore D. A and C E. Beverly Hills
    Vineyards  Q5) How much did Jeff Greene originally buy the Palazzo di Amore
    estate for in 2007? A. $50 million B. $35 million C. $20 million D. $10 million
    E. $100 million
    
    ANSWERS:
    ['D' 'C' 'D' 'D' 'B']
    
    '================================================================================'
    TEXT:
    Abortions outnumber live births among African American women living in New York
    City, according to a new health report. Newly released Department of Health and
    Mental Hygiene figures show black women had 31,328 'crude induced terminations'
    in 2012, compared with 24,758 'live births'. The terminations comprised more
    than 42 percent of the Big Apple's total number. Birth rate: The report found
    that the 2012 citywide crude birth rate was 14.8 births per 1,000 population,
    the lowest rate since 1979 when . the rate was also 14.8 . Changes: New York's
    abortion rate has fallen below 40 percent for the first time in at least a
    decade, according to the report . The second highest number of abortions were
    for Hispanic at 22,917, with this group's live births reaching 36,642. Both
    groups outnumbered abortions for whites and Asian and Pacific Islanders, which
    had 9,704 and 4,493 terminations respectively. Women aged 20-29 had the most
    abortions among all ethnicities, with more women in Brooklyn choosing to
    terminate rather than have a child. The . abortion ratio citywide fell to just
    over 37 per cent in 2012 - a record low . since 1970 when abortion was legalized
    in New York. Anti-abortion and religious groups have hit out at the figures,
    describing abortion as a 'problem' which the city is failing to adequately
    address. Statistics: Abortions outnumbered live births among New York City's
    black community in 2012, according to a new report by the Department of Health
    and Mental Hygiene (stock picture) In the interests of the fetus: Religious
    groups are unsurprisingly outraged at the report's abortion figures, believing
    that women don't have the right to make decisions about their own bodies .
    
    QUESTIONS:
    Q1) What was the citywide crude birth rate in New York City in 2012? A. 30
    births per 1,000 population B. 10 births per 1,000 population C. 25 births per
    1,000 population D. 20 births per 1,000 population E. 14.8 births per 1,000
    population  Q2) What was the reaction of anti-abortion and religious groups to
    the abortion figures in the report? A. They were pleased with the decline in
    abortion rates B. They were indifferent to the report's findings C. They were
    outraged and described abortion as a 'problem' D. They were neutral and did not
    comment E. They were surprised by the low number of abortions  Q3) Which ethnic
    group had the second highest number of abortions in New York City in 2012? A.
    African American B. White C. Other D. Asian and Pacific Islanders E. Hispanic
    Q4) What was the abortion ratio citywide in New York City in 2012? A. 37 percent
    B. 25 percent C. 50 percent D. 30 percent E. 40 percent  Q5) What was the number
    of 'crude induced terminations' among African American women in New York City in
    2012? A. 24,758 B. 36,642 C. 31,328 D. 40,000 E. 20,000
    
    ANSWERS:
    ['E' 'C' 'E' 'A' 'C']
    
    '================================================================================'
    TEXT:
    (CNN) -- An Orange County, California, man has been indicted on one federal
    count of knowingly attempting to provide material support and resources to al
    Qaeda, federal authorities said Friday. The indictment also charges 24-year-old
    Sinh Vinh Ngo Nguyen, also known as Hasan Abu Omar Ghannoum, with one count of
    making a false statement to obtain a passport to facilitate an act of
    international terrorism. Nguyen of Garden Grove was arrested Friday morning in
    Santa Ana attempting to boarding a bus to Mexico, FBI spokeswoman Laura Eimiller
    said. "Investigators do not believe Nguyen was traveling with others, nor are
    they aware of a continuing threat to the public at this time, based on his
    alleged activities," she said. Eimiller did not detail the alleged activities,
    and the indictment did not spell out any specifics of the alleged actions.
    Nguyen was scheduled to make an initial appearance in U.S. District Court in
    Santa Ana later in the day, authorities said. CNN's Chelsea J. Carter
    contributed to this report.
    
    QUESTIONS:
    Q1) What was Sinh Vinh Ngo Nguyen attempting to do when he was arrested? A.
    Crossing the border into Canada B. Applying for a passport C. Boarding a bus to
    Mexico D. Boarding a plane to the Middle East E. Making a false statement to
    authorities  Q2) What was Sinh Vinh Ngo Nguyen charged with in addition to
    attempting to provide material support to al Qaeda? A. Attempting to board a
    plane to the Middle East B. Making a false statement to obtain a passport C.
    Providing material support to ISIS D. Attempting to cross the border into Mexico
    E. Making a false statement to authorities  Q3) What organization was Sinh Vinh
    Ngo Nguyen allegedly trying to support? A. Hamas B. al Qaeda C. ISIS D. The
    Muslim Brotherhood E. The Taliban  Q4) Where was Sinh Vinh Ngo Nguyen arrested?
    A. Mexico B. Santa Ana C. Orange County D. U.S. District Court E. Garden Grove
    Q5) What was the status of the threat to the public according to investigators?
    A. High and ongoing B. Low and contained C. Moderate and increasing D. There was
    no threat E. There was no information available
    
    ANSWERS:
    ['C' 'B' 'B' 'B' 'B']
    
    '================================================================================'


### Part (b): Generate Summaries + Scores

Next, we will generate 10 summaries for each article in the training set and score them with our Q&A judging setup. 

The following command will run the [generate_summaries_and_scores.py](src/scripts/generate_summaries_and_scores.py) script, which takes in the folder with generated questions + articles and stores the results to a new folder of `.parquet` files.


```python
!anyscale job submit -f configs/jobs/8b_judge/generate_summaries_train_job.yaml 
# Optional: use the 70b model for better performance (runs on A100s)
# !anyscale job submit -f configs/jobs/70b_judge/generate_summaries_train_job.yaml
```

    [1m[36mOutput[0m[0m
    [0m[1m[36m(anyscale +1.1s)[0m [0m[0m[0m[0mSubmitting job with config JobConfig(name='preference-tuning-summarization-question-generation', image_uri='localhost:5555/anyscale/endpoints_aica:0.5.0-6402', compute_config=None, env_vars=None, py_modules=None, cloud=None, project=None, ray_version=None, job_queue_config=None).[0m
    [0m[1m[36m(anyscale +3.5s)[0m [0m[0m[0m[0mUsing workspace runtime dependencies env vars: {'WANDB_API_KEY': 'cbc4aed2de2d9c9acb21324a3297b85b7299479b'}.[0m
    [0m[1m[36m(anyscale +3.5s)[0m [0m[0m[0m[0mUploading local dir '.' to cloud storage.[0m
    [0m[1m[36m(anyscale +4.5s)[0m [0m[0m[0m[0mIncluding workspace-managed pip dependencies.[0m
    [0m[1m[36m(anyscale +5.1s)[0m [0m[0m[0m[0mJob 'preference-tuning-summarization-question-generation' submitted, ID: 'prodjob_8m2iu1lcd44s2e7q95rcrxvzzx'.[0m
    [0m[1m[36m(anyscale +5.1s)[0m [0m[0m[0m[0mView the job in the UI: https://console.anyscale.com/jobs/prodjob_8m2iu1lcd44s2e7q95rcrxvzzx[0m
    [0m[1m[36m(anyscale +5.1s)[0m [0m[0m[0m[0mUse `--wait` to wait for the job to run and stream logs.[0m
    [0m[0m


```python
# replace with the link to the generated summaries
summary_folder = "s3://air-example-data/preference-tuning-summarization-example/summary_generation_base/train/"
summary_ds = ray.data.read_parquet(summary_folder)
example_rows = summary_ds.take(1)
```


    Parquet Files Sample 0:   0%|          | 0/2 [00:00<?, ?it/s]


    2024-08-16 12:07:01,737	INFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-08-16_11-08-34_539321_2359/logs/ray-data
    2024-08-16 12:07:01,738	INFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> LimitOperator[limit=1]



    - ReadParquet 1:   0%|          | 0/96 [00:00<?, ?it/s]



    - limit=1 2:   0%|          | 0/96 [00:00<?, ?it/s]



    Running 0:   0%|          | 0/96 [00:00<?, ?it/s]



```python
from src.utils.models import DataSchema

for row in example_rows:
    print_wrapped("TEXT", row[DataSchema.ARTICLE])
    print_wrapped("QUESTIONS", row[DataSchema.MCQ_QUESTIONS])
    print_wrapped("MODEL GENERATED SUMMARY", row[DataSchema.SUMMARY_GENERATION_RAW_OUTPUT])
    print_wrapped("ANSWERS", str(row[DataSchema.GROUND_TRUTH_MCQ_ANSWERS]))
    print_wrapped("JUDGE ANSWERS FROM SUMMARY", str(row[DataSchema.JUDGE_MCQ_ANSWERS]))
    pprint.pprint("=" * 100, width=80)
```

    TEXT:
    (RollingStone.com) -- Jennifer Lawrence, the 20-year-old Oscar nominee for Best
    Actress, is sitting in a fancy Manhattan hotel sipping tea and feeling a little
    out of place. See, she grew up in Louisville, Kentucky, where her dad owned a
    construction company and her mom ran a summer camp. They had land and horses.
    She loved to fish. She was a total tomboy: field hockey, softball, basketball on
    an all-boys team. ("I was so dykey.") One of her nicknames was Nitro. She lives
    in Los Angeles now, but "little redneck things still come out." Like what? "I'm
    attracted to my brother. Stuff like that." 10 Best Movies of 2010 . At 14, she
    decided she wanted to be an actress and dragged her mom to New York for
    auditions. The people at Reese's Peanut Butter Cups told her she was the best
    they'd ever seen. Her mom told her they were lying. (Her mom didn't like showbiz
    much.) She auditioned for the role of Bella in "Twilight," which would have been
    perfect if Bella were a badass, but since she's a frightened waif, Lawrence
    ended up not getting the part. Which was for the best because the role she did
    get was for "Winter's Bone," in which she's fantastic: harrowing and tender as
    the 17-year-old daughter of an Ozarks meth-cooker who's fighting to take care of
    her little brother and sister. This article appears in the February 17, 2011
    issue of Rolling Stone. The issue is available now on newsstands and will appear
    in the online archive February 4. To prep for the part, Lawrence learned how to
    shoot a gun and field-dress squirrels. She already knew how to chop wood: "I
    went through a wood-chopping phase when I was nine or 10." She says she hasn't
    even bothered preparing an Oscar speech: "I have been practicing my losing face,
    though. Do you want to see it?" (For the record, it's a very good losing face.)
    Peter Travers Reviews 'Winter's Bone' Later this year comes "X-Men: First
    Class," where she'll play the mutant Mystique, blue-skinned and topless. ("Did I
    feel naked being naked?" she asks, so you don't have to. "Yeah. Totally.") But
    before that there's Jodie Foster's "The Beaver," premiering in May, in which she
    appears alongside a certifiable Mel Gibson. Which means she has some crazy Mel
    Gibson stories, right? She leans in close. "If I say, 'Off the record' -- that
    means you can't print it, right?' " Right. "OK. So, off the record ..." She's
    learning. Photos: 2011 Screen Actors Guild Award Winners . Copyright ¬© 2011
    Rolling Stone.
    
    QUESTIONS:
    Q1) Where did Jennifer Lawrence grow up? A. The Ozarks B. Manhattan C. Los
    Angeles, California D. New York City, New York E. Louisville, Kentucky  Q2) What
    is the name of the movie in which Jennifer Lawrence will play the mutant
    Mystique? A. Winter's Bone B. X-Men: First Class C. The Beaver D. Twilight E.
    The Hunger Games  Q3) What is one skill Jennifer Lawrence learned to prepare for
    her role in "Winter's Bone"? A. How to play field hockey B. How to chop wood C.
    How to ride a horse D. How to act like a frightened waif E. How to shoot a gun
    Q4) Why did Jennifer Lawrence's mom take her to New York at the age of 14? A. To
    visit relatives B. To attend a sports tournament C. To go shopping D. For
    auditions E. For a family vacation  Q5) What was Jennifer Lawrence's role in the
    movie "Winter's Bone"? A. A character in a Jodie Foster movie B. A mutant with
    blue skin C. A character in a Reese's Peanut Butter Cups commercial D. A
    frightened waif E. A 17-year-old daughter of an Ozarks meth-cooker
    
    MODEL GENERATED_SUMMARY:
    Jennifer Lawrence, an Oscar-nominated actress from Kentucky, reflects on her
    journey from a tomboy to a successful actress in Hollywood. She talks about her
    love for field hockey, softball, and basketball, and her nickname "Nitro."
    Lawrence auditioned for the role of Bella in "Twilight" but didn't get it, but
    went on to star in "Winter's Bone," where she received critical acclaim. She
    discusses her preparation for the role, including learning how to shoot a gun
    and field-dress squirrels. Lawrence also talks about her upcoming roles in
    "X-Men: First Class" and "The Beaver," and shares some crazy Mel Gibson stories.
    
    ANSWERS:
    ['E' 'B' 'E' 'D' 'E']
    
    JUDGE ANSWERS FROM SUMMARY:
    ['E', 'B', 'E', 'Unsure', 'Unsure']
    
    '===================================================================================================='


### Part (c): Generate Preference Tuning Data

Next, we will generate 10 summaries for each article in the training set and score them with our Q&A judging setup. 

The following command will run the [generate_dpo_data.py](src/scripts/generate_dpo_data.py) script, which takes in the folder of summaries and outputs `.jsonl` files for training and validation.


```python
!python src/scripts/generate_dpo_data.py configs/training_data_generation/mistral_8b.yaml
```


```python
# Inspect the results
# Replace with the link to your validation file
validation_file = "s3://air-example-data/preference-tuning-summarization-example/dpo_training_data/valid.jsonl"

valid_ds = ray.data.read_json(validation_file)
example_rows = valid_ds.take(1)
```

    2024-08-16 12:23:43,661	INFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-08-16_11-08-34_539321_2359/logs/ray-data
    2024-08-16 12:23:43,661	INFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ExpandPaths] -> TaskPoolMapOperator[ReadFiles] -> LimitOperator[limit=1]



    - ExpandPaths 1:   0%|          | 0/1 [00:00<?, ?it/s]



    - ReadFiles 2:   0%|          | 0/1 [00:00<?, ?it/s]



    - limit=1 3:   0%|          | 0/1 [00:00<?, ?it/s]



    Running 0:   0%|          | 0/1 [00:00<?, ?it/s]



```python

```




    {'chosen': [{'content': "Given the following text, create a very short summary that is at most 2 sentences.\n\nText:\nBy . Tamara Cohen, Political Reporter . PUBLISHED: . 18:32 EST, 27 January 2013 . | . UPDATED: . 08:48 EST, 28 January 2013 . Deputy Prime Minister Nick Clegg and his wife Miriam are determined to keep the education of their 11-year-old son 'out of politics' Nick Clegg yesterday defended the possibility he may send his children to private schools as it emerged he and his wife Miriam have not even visited their local state school. He said the education of his 11-year-old son Antonio, who starts secondary school this year, should not be used as 'a political football' and that the couple would do 'what's best' for their children although he was braced for criticism. Last week the Liberal Democrat leader told listeners to his radio show he would send his son to a private school if he failed to find a place in a good comprehensive, saying he would use the state system 'if it works out', but that there is 'huge competition' for places in London. But Mr Clegg, who attended Westminster public school, has apparently not looked around nearby Ark Putney academy in south-west London, it was revealed yesterday by its headmaster Mark Phillips. Mr Phillips who has turned the school around since he was hired three years ago, said the school which was once in special measures but is now lauded by the Government for its improvements, could provide an 'exceptional' education for any child and that there was no need to pay fees for schooling. Unless the Cleggs had visited 'under cover' he had not seen them, he said.'I am always very clear that all parents living locally are welcome to choose our school and it is important that every parent comes with their child and takes an objective look to see whether what we offer will meet the needs of their child', he said. 'It wouldn't claim to be the answer to every child and every parent. But I hope that if a parent does come, and sees an environment their child will thrive in, they will pick us...I am confident they will do exceptionally well. I don't believe you have to pay for it.' Mr Clegg told the BBC's Andrew Marr Show yesterday that he and his wife will do whatever is in the interests of their son . If he chooses to educate his children . privately, Mr Clegg is likely to be accused of hypocrisy after using a . speech last year to attack 'the great rift in our education system' caused by many of the best schools being fee-paying and said it had a . 'corrosive' effect on society and the economy. In . an interview on BBC1's Andrew Marr Show, he said: 'I accept that it's a . dilemma for anyone in public life, particularly in politics, how do you . balance that with the fact Miriam and I have small children, and the . approach Miriam and I took right from the outset was to keep our . children completely out of politics. 'We . never put them in front of the camera or to make them or their . education a political football. 'I totally accept that when we make a . decision that'll be subject to public commentary, criticism and so on, . but in the meantime we want to protect the privacy of an 11-year-old boy . and make the decision that we as parents think is best for our son.' The deadline for applying for entry to Ark Putney for 2013-14 was last October. The school is part of the Ark academy chain, set up in 2004, whose chairman is Paul Marshall, one of the Liberal Democrats' biggest donors. Last year 62 per cent of pupils gained at least five good GCSEs, prompting schools minister Nick Gibb to write to Mr Phillips to congratulate him on the 'excellent results' saying the school was in the top 100 best-performing, based on sustained improvements every year since 2008. However Michael Gove last year approved the sale of five acres of playing fields at the school including six tennis courts, a football pitch and a playground, to developers to fund refurbishments, after a ¬£40million revamp under the Building Schools for the Future programme was cancelled. Alumni of Ark Putney, which used to be Elliott School, include actor Pierce Brosnan, and 1960s England bowler Geoff Arnold. Former Welsh secretary Peter Hain sent his children to the school, which was the scene of the Christmas play in the film Love, Actually. David Cameron has said his children will attend state school, but George Osborne has been criticised for sending his to the fee-paying preparatory school in Kensington that he attended.",
       'role': 'user'},
      {'content': "Nick Clegg and his wife Miriam are considering sending their 11-year-old son Antonio to a private school, despite their previous claims that they would use the state system. Clegg has not visited Ark Putney Academy, a nearby state school, but its headmaster, Mark Phillips, claims that the school could provide an 'exceptional' education for any child and that there is no need to pay fees for schooling. The deadline for applying to Ark Putney for 2013-14 was last October and the school is part of the Ark academy chain, set up in 2004, whose chairman is Paul Marshall, one of the Liberal Democrats' biggest donors.",
       'role': 'assistant'}],
     'rejected': [{'content': "Given the following text, create a very short summary that is at most 2 sentences.\n\nText:\nBy . Tamara Cohen, Political Reporter . PUBLISHED: . 18:32 EST, 27 January 2013 . | . UPDATED: . 08:48 EST, 28 January 2013 . Deputy Prime Minister Nick Clegg and his wife Miriam are determined to keep the education of their 11-year-old son 'out of politics' Nick Clegg yesterday defended the possibility he may send his children to private schools as it emerged he and his wife Miriam have not even visited their local state school. He said the education of his 11-year-old son Antonio, who starts secondary school this year, should not be used as 'a political football' and that the couple would do 'what's best' for their children although he was braced for criticism. Last week the Liberal Democrat leader told listeners to his radio show he would send his son to a private school if he failed to find a place in a good comprehensive, saying he would use the state system 'if it works out', but that there is 'huge competition' for places in London. But Mr Clegg, who attended Westminster public school, has apparently not looked around nearby Ark Putney academy in south-west London, it was revealed yesterday by its headmaster Mark Phillips. Mr Phillips who has turned the school around since he was hired three years ago, said the school which was once in special measures but is now lauded by the Government for its improvements, could provide an 'exceptional' education for any child and that there was no need to pay fees for schooling. Unless the Cleggs had visited 'under cover' he had not seen them, he said.'I am always very clear that all parents living locally are welcome to choose our school and it is important that every parent comes with their child and takes an objective look to see whether what we offer will meet the needs of their child', he said. 'It wouldn't claim to be the answer to every child and every parent. But I hope that if a parent does come, and sees an environment their child will thrive in, they will pick us...I am confident they will do exceptionally well. I don't believe you have to pay for it.' Mr Clegg told the BBC's Andrew Marr Show yesterday that he and his wife will do whatever is in the interests of their son . If he chooses to educate his children . privately, Mr Clegg is likely to be accused of hypocrisy after using a . speech last year to attack 'the great rift in our education system' caused by many of the best schools being fee-paying and said it had a . 'corrosive' effect on society and the economy. In . an interview on BBC1's Andrew Marr Show, he said: 'I accept that it's a . dilemma for anyone in public life, particularly in politics, how do you . balance that with the fact Miriam and I have small children, and the . approach Miriam and I took right from the outset was to keep our . children completely out of politics. 'We . never put them in front of the camera or to make them or their . education a political football. 'I totally accept that when we make a . decision that'll be subject to public commentary, criticism and so on, . but in the meantime we want to protect the privacy of an 11-year-old boy . and make the decision that we as parents think is best for our son.' The deadline for applying for entry to Ark Putney for 2013-14 was last October. The school is part of the Ark academy chain, set up in 2004, whose chairman is Paul Marshall, one of the Liberal Democrats' biggest donors. Last year 62 per cent of pupils gained at least five good GCSEs, prompting schools minister Nick Gibb to write to Mr Phillips to congratulate him on the 'excellent results' saying the school was in the top 100 best-performing, based on sustained improvements every year since 2008. However Michael Gove last year approved the sale of five acres of playing fields at the school including six tennis courts, a football pitch and a playground, to developers to fund refurbishments, after a ¬£40million revamp under the Building Schools for the Future programme was cancelled. Alumni of Ark Putney, which used to be Elliott School, include actor Pierce Brosnan, and 1960s England bowler Geoff Arnold. Former Welsh secretary Peter Hain sent his children to the school, which was the scene of the Christmas play in the film Love, Actually. David Cameron has said his children will attend state school, but George Osborne has been criticised for sending his to the fee-paying preparatory school in Kensington that he attended.",
       'role': 'user'},
      {'content': 'Deputy Prime Minister Nick Clegg has defended the possibility of sending his children to private schools, stating that their education should not be used as a political football. He and his wife Miriam have not visited their local state school, but the headmaster of Ark Putney academy in London, where they were invited to attend, said the school could provide an "exceptional" education for any child and that parents should come to see it firsthand before making a decision.',
       'role': 'assistant'}],
     'num_words_chosen': 105,
     'num_words_rejected': 79,
     'accuracy_chosen': 3,
     'accuracy_rejected': 2}




```python
for row in example_rows:
    print_wrapped("PROMPT", row["chosen"][0]["content"])
    print_wrapped("CHOSEN RESPONSE", row["chosen"][1]["content"])
    print_wrapped("REJECTED RESPONSE", row["rejected"][1]["content"])
```

    PROMPT:
    Given the following text, create a very short summary that is at most 2
    sentences.  Text: By . Tamara Cohen, Political Reporter . PUBLISHED: . 18:32
    EST, 27 January 2013 . | . UPDATED: . 08:48 EST, 28 January 2013 . Deputy Prime
    Minister Nick Clegg and his wife Miriam are determined to keep the education of
    their 11-year-old son 'out of politics' Nick Clegg yesterday defended the
    possibility he may send his children to private schools as it emerged he and his
    wife Miriam have not even visited their local state school. He said the
    education of his 11-year-old son Antonio, who starts secondary school this year,
    should not be used as 'a political football' and that the couple would do
    'what's best' for their children although he was braced for criticism. Last week
    the Liberal Democrat leader told listeners to his radio show he would send his
    son to a private school if he failed to find a place in a good comprehensive,
    saying he would use the state system 'if it works out', but that there is 'huge
    competition' for places in London. But Mr Clegg, who attended Westminster public
    school, has apparently not looked around nearby Ark Putney academy in south-west
    London, it was revealed yesterday by its headmaster Mark Phillips. Mr Phillips
    who has turned the school around since he was hired three years ago, said the
    school which was once in special measures but is now lauded by the Government
    for its improvements, could provide an 'exceptional' education for any child and
    that there was no need to pay fees for schooling. Unless the Cleggs had visited
    'under cover' he had not seen them, he said.'I am always very clear that all
    parents living locally are welcome to choose our school and it is important that
    every parent comes with their child and takes an objective look to see whether
    what we offer will meet the needs of their child', he said. 'It wouldn't claim
    to be the answer to every child and every parent. But I hope that if a parent
    does come, and sees an environment their child will thrive in, they will pick
    us...I am confident they will do exceptionally well. I don't believe you have to
    pay for it.' Mr Clegg told the BBC's Andrew Marr Show yesterday that he and his
    wife will do whatever is in the interests of their son . If he chooses to
    educate his children . privately, Mr Clegg is likely to be accused of hypocrisy
    after using a . speech last year to attack 'the great rift in our education
    system' caused by many of the best schools being fee-paying and said it had a .
    'corrosive' effect on society and the economy. In . an interview on BBC1's
    Andrew Marr Show, he said: 'I accept that it's a . dilemma for anyone in public
    life, particularly in politics, how do you . balance that with the fact Miriam
    and I have small children, and the . approach Miriam and I took right from the
    outset was to keep our . children completely out of politics. 'We . never put
    them in front of the camera or to make them or their . education a political
    football. 'I totally accept that when we make a . decision that'll be subject to
    public commentary, criticism and so on, . but in the meantime we want to protect
    the privacy of an 11-year-old boy . and make the decision that we as parents
    think is best for our son.' The deadline for applying for entry to Ark Putney
    for 2013-14 was last October. The school is part of the Ark academy chain, set
    up in 2004, whose chairman is Paul Marshall, one of the Liberal Democrats'
    biggest donors. Last year 62 per cent of pupils gained at least five good GCSEs,
    prompting schools minister Nick Gibb to write to Mr Phillips to congratulate him
    on the 'excellent results' saying the school was in the top 100 best-performing,
    based on sustained improvements every year since 2008. However Michael Gove last
    year approved the sale of five acres of playing fields at the school including
    six tennis courts, a football pitch and a playground, to developers to fund
    refurbishments, after a ¬£40million revamp under the Building Schools for the
    Future programme was cancelled. Alumni of Ark Putney, which used to be Elliott
    School, include actor Pierce Brosnan, and 1960s England bowler Geoff Arnold.
    Former Welsh secretary Peter Hain sent his children to the school, which was the
    scene of the Christmas play in the film Love, Actually. David Cameron has said
    his children will attend state school, but George Osborne has been criticised
    for sending his to the fee-paying preparatory school in Kensington that he
    attended.
    
    CHOSEN RESPONSE:
    Nick Clegg and his wife Miriam are considering sending their 11-year-old son
    Antonio to a private school, despite their previous claims that they would use
    the state system. Clegg has not visited Ark Putney Academy, a nearby state
    school, but its headmaster, Mark Phillips, claims that the school could provide
    an 'exceptional' education for any child and that there is no need to pay fees
    for schooling. The deadline for applying to Ark Putney for 2013-14 was last
    October and the school is part of the Ark academy chain, set up in 2004, whose
    chairman is Paul Marshall, one of the Liberal Democrats' biggest donors.
    
    REJECTED RESPONSE:
    Deputy Prime Minister Nick Clegg has defended the possibility of sending his
    children to private schools, stating that their education should not be used as
    a political football. He and his wife Miriam have not visited their local state
    school, but the headmaster of Ark Putney academy in London, where they were
    invited to attend, said the school could provide an "exceptional" education for
    any child and that parents should come to see it firsthand before making a
    decision.
    


# Step 2: Fine-tuning

Now that we have the pre-processed dataset, we are ready to fine-tune `Mistral-7B-Instruct-v0.1` using DPO. On Anyscale, we've created an easy-to-use interface to do preference-tuning using `DPO`. We leverage Ray to overlap reference model log-probability calculation with model training to improve GPU utilization. Most implementations compute log probabilities synchronously with model training,

<img src="https://raw.githubusercontent.com/anyscale/templates/main/templates/fine-tune-llm_v2/end-to-end-examples/fine-tune-preference/assets/hf_dpo.png"/>

While our implementation using Ray is asynchronous:  


<img src="https://raw.githubusercontent.com/anyscale/templates/main/templates/fine-tune-llm_v2/end-to-end-examples/fine-tune-preference/assets/anyscale_dpo.png"/>

Further, our use of Ray Data also implies that the compute configuration for the reference model can be completely decoupled with the policy model. For example, reference model calculation can run on a different node (with configurable number of GPUs, etc) with zero code changes needed. 


To get started with DPO training, we provide the config for DPO in [configs/mistral_dpo_summarization.yaml](configs/mistral_dpo_summarization.yaml) . 


```python
!cat configs/mistral_dpo_summarization.yaml
```

You can run the below command from the root directory for the template (`~/default`): 

```
llmforge anyscale finetune end-to-end-examples/fine-tune-preference/configs/mistral_dpo_summarization.yaml
```

# Step 3: Evaluation

Let's evaluate our trained model. Here we'll use two baselines: (1) the base model before finetuning (reference model in DPO) and (2) GPT-4o.

## Evaluation strategy

Our evaluation strategy involves the same Q&A scoring system as used while generating the preference data. 

<p align="center">
  <img src="./assets/eval.png?" alt="Evaluation" width=800>
</p>

We evaluate the baseline model and the trained DPO model on the test set. 

## Obtain summaries on the test set
First, we'll need to obtain the summaries (and scores) for both the models on the given test set. 

For the baseline model, you can simply run the below command:


```python
!anyscale job submit -f configs/jobs/8b_judge/generate_summaries_eval_baseline_job.yaml
# Optional: use the 70b model for better performance (runs on A100s)
# !anyscale job submit -f configs/jobs/70b_judge/generate_summaries_eval_baseline_job.yaml
```

For the fine-tuned DPO model, we provide a dummy config in [configs/summary_generation/8b_judge/mistral_finetuned_eval.yaml](configs/summary_generation/8b_judge/mistral_finetuned_eval.yaml). If you used the default training config provided, the model would be trained using LoRA and you should have a path to the LoRA weights.


```python
!cat configs/summary_generation/8b_judge/mistral_finetuned_eval.yaml
```

    mode: eval
    input_folder: s3://air-example-data/preference-tuning-summarization-example/qa_generation/qa_annotations_full_test
    inference_type: offline
    model_inference_config:
      model_id_or_path: mistralai/Mistral-7B-Instruct-v0.1 # <---- Modify with s3 link to full param weights if you did full-param training
      adapter_id_or_path: <lora_path_here> # <---  Add path to lora weights here. If you did full param training, you can instead remove this field.
      temperature: 0
      top_p: 0.95
      scaling_config:
        batch_size: 64
        concurrency: 2
        num_gpus_per_instance: 1
        accelerator_type: A10G
    num_generations: 1
    judge_inference_config:
      model_id_or_path: meta-llama/Meta-Llama-3.1-8B-Instruct
      temperature: 0
      scaling_config:
        batch_size: 64
        concurrency: 3
        num_gpus_per_instance: 2
        accelerator_type: A10G
    num_mcq_questions: 5



```python
!anyscale job submit -f configs/jobs/8b_judge/generate_summaries_eval_finetuned_job.yaml
# Optional: use the 70b model for better performance (runs on A100s)
# !anyscale job submit -f configs/jobs/70b_judge/generate_summaries_eval_finetuned_job.yaml
```

In the logs for the above jobs, you should see the final path to the output summaries for both the models. 

Optionally, you can also obtain the summaries and scores for the `gpt-4o` model from OpenAI. Simply run:


```python
!anyscale job submit -f configs/jobs/8b_judge/generate_summaries_eval_gpt_job.yaml
# Optional: use the 70b model for better performance (runs on A100s)
# !anyscale job submit -f configs/jobs/70b_judge/generate_summaries_eval_gpt_job.yaml
```

## Get Evaluation Statistics

We've provided a convenient script [get_eval_stats.py](src/scripts/get_eval_stats.py) to get evaluation statistics and obtain the "win rate" of the DPO model (the percentage of times the DPO model performs better than the baseline). We've provided an example configuration below. 


```python
# make sure to substitute -outputs-path with your path
!python src/scripts/get_eval_stats.py --outputs-path s3://air-example-data/preference-tuning-summarization-example/summary_generation_dpo_model/test/ --baseline-outputs-path s3://air-example-data/preference-tuning-summarization-example/summary_generation_base/test/  

# (Optional): if you obtained results for GPT-4o, you should uncomment and run the following command instead
# !python src/scripts/get_eval_stats.py --outputs-path s3://air-example-data/preference-tuning-summarization-example/summary_generation_dpo_model/test/ --baseline-outputs-path s3://air-example-data/preference-tuning-summarization-example/summary_generation_base/test/  --gpt4o-outputs-path <add-path-to-gpt4o-results>
```

You should see the following results for the 70B model:

```text 
‚ïí‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïï
‚îÇ           Metric            ‚îÇ   Model   ‚îÇ  Baseline  ‚îÇ  GPT-4o   ‚îÇ
‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°
‚îÇ        Accuracy >=3         ‚îÇ 65.4286 % ‚îÇ 43.0476 %  ‚îÇ 37.2381 % ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ        Accuracy >=4         ‚îÇ 25.7143 % ‚îÇ 13.5238 %  ‚îÇ 10.0000 % ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ     Median Compression      ‚îÇ 11.5794 % ‚îÇ 12.7316 %  ‚îÇ 8.0496 %  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ      Mean Compression       ‚îÇ 13.0029 % ‚îÇ 14.3444 %  ‚îÇ 9.3554 %  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ      Summary Too Long       ‚îÇ 0.0000 %  ‚îÇ  0.0000 %  ‚îÇ 0.0000 %  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Contains Invalid Characters ‚îÇ 0.0000 %  ‚îÇ  0.0952 %  ‚îÇ 0.0000 %  ‚îÇ
‚ïò‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïõ


Model Win Rate against Baseline: 74.0000 %
GPT-4o Win Rate against Baseline: 64.8095 %
```

Our fine-tuned model is able to generate much better summaries, that are more concise (compression ratio is lower) with lesser out-of-distribution characters (gibberish tokens) than the baseline. You can see more details on the same in our blog!

| **NOTE:** The evaluation results will differ if you used the 8B model which is less capable as a LLM-judge. 

## Summary

Congrats! You have now fine-tuned an open source model on preference data. As a quick recap, here's what we demonstrated in this notebook:
1. Synthetically generating preference data for DPO 
2. DPO fine-tuning of a language model on the Anyscale Platform
4. Evaluating the model against the baseline and GPT-4o, and analysing the results.
