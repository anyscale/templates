output_folder: preference_tuning_summarization_example/qa_annotations_full # Output folder in artifact storage.
model_inference_config:
  model_id_or_path: meta-llama/Meta-Llama-3.1-70B-Instruct
  temperature: 0
  max_tokens: 4096
  scaling_config:
    batch_size: 128
    concurrency: 3
    num_gpus: 4
    custom_resources:
      accelerator_type:H100: 1 # use either H100 or A100 instances here
num_data_blocks_per_device: int
num_samples_total: 21000
train_test_split: 0.01
