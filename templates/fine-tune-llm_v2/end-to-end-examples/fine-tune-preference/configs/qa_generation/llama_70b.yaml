output_folder: preference_tuning_summarization_example/qa_annotations_full # Output folder in artifact storage.
model_inference_config:
  model_id_or_path: meta-llama/Meta-Llama-3.1-70B-Instruct
  temperature: 0
  max_tokens: 4096
  scaling_config:
    batch_size: 128
    concurrency: 2
    num_gpus_per_instance: 4
    accelerator_type: H100 # use either H100/A100 here
num_data_blocks_per_device: 2
num_samples_total: 21000
train_test_split: 0.01
