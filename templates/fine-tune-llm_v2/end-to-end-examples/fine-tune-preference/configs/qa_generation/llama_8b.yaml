output_folder: preference_tuning_summarization_example/qa_annotations_full # Output folder in artifact storage.
model_inference_config: # Inference config for the model
  model_id_or_path: meta-llama/Meta-Llama-3.1-8B-Instruct # Model ID for question generation
  temperature: 0 # Temperature for sampling generations
  max_tokens: 4096 # Max tokens for generation
  scaling_config: # Ray Data `map_batches` config: https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html
    batch_size: 128 # Batch size per model instance
    concurrency: 3 # Number of Ray Actors/ model instances to use
    num_gpus_per_instance: 4 # Number of GPUs per instance. We use vLLM with tensor parallelism over efficient inference
    accelerator_type: A10G # GPU type
num_data_blocks_per_device: 2 # Number of Ray data blocks per GPU device.
num_samples_total: 21000 # Number of articles to sample in total
train_test_split: 0.01 # Percentage of articles to use for the test set
