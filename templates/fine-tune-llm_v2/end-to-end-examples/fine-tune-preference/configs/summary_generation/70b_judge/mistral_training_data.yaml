# Data generation mode. Can be train or eval. This is mainly for bookkeeping
mode: train
input_folder: s3://air-example-data/preference-tuning-summarization-example/qa_generation/qa_annotations_full_train
# Inference type. Can be online (through an OpenAI-compatible server) or Offline (Batched inference with Ray + vLLM)
inference_type: offline

model_inference_config:
  model_id_or_path: mistralai/Mistral-7B-Instruct-v0.1
  temperature: 0.8
  top_p: 0.95
  scaling_config:
    batch_size: 128
    concurrency: 2
    num_gpus_per_instance: 1
    accelerator_type: A100
num_generations: 10
judge_inference_config:
  model_id_or_path: meta-llama/Meta-Llama-3.1-70B-Instruct
  temperature: 0
  scaling_config:
    batch_size: 128
    concurrency: 3
    num_gpus_per_instance: 2
    accelerator_type: A100
num_mcq_questions: 5
