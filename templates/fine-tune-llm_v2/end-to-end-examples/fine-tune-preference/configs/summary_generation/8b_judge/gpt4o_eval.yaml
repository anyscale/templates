mode: eval # Summary generation mode.
inference_type: online # Inference type. Can be online (through an OpenAI-compatible server) or Offline (Batched inference with Ray + vLLM)
input_folder:  s3://air-example-data/preference-tuning-summarization-example/qa_generation/qa_annotations_full_test
model_inference_config:
  model_id: gpt-4o # model ID for the inference endpoint
  base_url: https://api.openai.com/v1 # base url for the service
  api_key_env_var: OPENAI_API_KEY # API key env variable to use. make sure to set this in "Dependencies"
  temperature: 0 # temperature for sampling generations
  max_tokens: 4096 # Max tokens for generation
  concurrency: 10 # Number of concurrent requests to send to the server.
num_generations: 1 # Number of generations to sample from the model being evaluated by the judge.
judge_inference_config:
  model_id_or_path: meta-llama/Meta-Llama-3.1-8B-Instruct  # Model ID or remote (s3, gcs) path for the judge model.
  temperature: 0 # temperature for sampling generations
  scaling_config: # Ray Data `map_batches` config: https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html
    batch_size: 128
    concurrency: 2
    num_gpus_per_instance: 2
    accelerator_type: A10G
num_mcq_questions: 5 # Number of multiple choice questions used in the scoring method
