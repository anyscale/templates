mode: eval
input_folder: s3://air-example-data/preference-tuning-summarization-example/qa_generation/qa_annotations_full_test
inference_type: offline
model_inference_config:
  # Modify with s3 link to full param weights if you did full-param training
  model_id_or_path: mistralai/Mistral-7B-Instruct-v0.1

  # Add path to lora weights here. If you did full param training, you can instead remove this field.
  adapter_id_or_path: s3://large-dl-models-mirror/finetuning_template/mistral_dpo_summarization_lora

  temperature: 0
  top_p: 0.95
  scaling_config:
    batch_size: 64
    concurrency: 4
    num_gpus_per_instance: 1
    accelerator_type: A10G
num_generations: 1
judge_inference_config:
  model_id_or_path: meta-llama/Meta-Llama-3.1-8B-Instruct
  temperature: 0
  scaling_config:
    batch_size: 64
    concurrency: 3
    num_gpus_per_instance: 2
    accelerator_type: A10G
num_mcq_questions: 5
