mode: eval
input_folder: s3://air-example-data/preference-tuning-summarization-example/qa_generation/qa_annotations_full_test
model_inference_config:
  model_id_or_path: mistralai/Mistral-7B-Instruct-v0.1 # <---  Add the path to your merged model here
  temperature: 0
  top_p: 0.95
  scaling_config:
    batch_size: 128
    concurrency: 2
    num_gpus_per_instance: 1
    accelerator_type: H100
num_generations: 1
judge_inference_config:
  model_id_or_path: meta-llama/Meta-Llama-3.1-70B-Instruct
  temperature: 0
  scaling_config:
    batch_size: 128
    concurrency: 3
    num_gpus_per_instance: 2
    accelerator_type: H100
num_mcq_questions: 5
