{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Stable Diffusion XL with Ray Train\n",
    "\n",
    "⏱️ Time to complete: 15 min\n",
    "\n",
    "This template shows you how to do [Dreambooth](https://arxiv.org/abs/2208.12242) fine-tuning, which is a method of personalizing a stable diffusion model on a few examples (3~5) of a subject.\n",
    "\n",
    "![Sample results](assets/finetune-sample-results.png)\n",
    "\n",
    "In this tutorial, you will learn about:\n",
    "1. How to easily scale out an existing HuggingFace `diffusers` example to run on a Ray cluster with minimal modifications.\n",
    "2. Basic features of [Ray Train](https://docs.ray.io/en/latest/train/train.html) such as specifying the number of training workers and the desired accelerator type.\n",
    "3. Anyscale's smart instance selection and autoscaling that makes it simple to scale up your training workload to any size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install python dependencies\n",
    "\n",
    "The application requires a few extra Python dependencies. Install them using `pip` and they'll be automatically installed on remote workers when they're launched!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U accelerate==0.28.0 diffusers==0.27.2 peft==0.10.0 transformers==4.39.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set up a dataset of your subject\n",
    "\n",
    "First, provide some images of the subject you want to fine-tune on.\n",
    "\n",
    "We'll use a sample dog dataset to demonstrate, but you can use pictures of your own subject.\n",
    "Fine-tuning works best if your images are all cropped to a square with your subject in the center!\n",
    "\n",
    "A few notes on these constants that you can modify when training on your own custom subject:\n",
    "* `SUBJECT_TOKEN` is the a unique token that you will teach the model to correspond to your subject. This can be is any token that does not appear much in normal text.\n",
    "    * Think of it as the name of your subject that the diffusion model will learn to recognize. Feel free to leave it as `sks`.\n",
    "    * When generating images, make sure to include `sks` in your prompt -- otherwise the model will just generate any random dog, not the dog that we fine-tuned it on!\n",
    "* `SUBJECT_CLASS` is the category that your subject falls into.\n",
    "    * For example, if you have a human subject, the class could be `\"man\"` or `\"woman\"`.\n",
    "    * This class combined with the `SUBJECT_TOKEN` can be used in a prompt to convey the meaning: \"a dog named sks\".\n",
    "* Put training images of your subject in `SUBJECT_IMAGES_PATH`. We'll later upload it to cloud storage so that all worker nodes can access the dataset.\n",
    "    * The easiest way to use your own images is to drag files into a folder in the VSCode file explorer, then moving the folder to `SUBJECT_IMAGES_PATH` in the command line. (Ex: `mv ./images /mnt/local_storage/subject_images`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBJECT_TOKEN = \"sks\"\n",
    "SUBJECT_CLASS = \"dog\"\n",
    "SUBJECT_IMAGES_PATH = \"/mnt/local_storage/subject_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the sample dog dataset -- feel free to comment this out.\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\n",
    "    \"diffusers/dog-example\",\n",
    "    local_dir=SUBJECT_IMAGES_PATH, repo_type=\"dataset\",\n",
    "    ignore_patterns=\".gitattributes\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from pathlib import Path\n",
    "\n",
    "display(*[Image(filename=image_path, width=250) for image_path in Path(SUBJECT_IMAGES_PATH).iterdir()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, upload the dataset to cloud storage so that we can download it on each worker node at the start of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import upload_to_cloud\n",
    "\n",
    "DATA_CLOUD_PATH = os.environ[\"ANYSCALE_ARTIFACT_STORAGE\"] + \"/subject_images\"\n",
    "upload_to_cloud(\n",
    "    local_path=SUBJECT_IMAGES_PATH, cloud_uri=DATA_CLOUD_PATH\n",
    ")\n",
    "print(\"Uploaded data to: \", DATA_CLOUD_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's come up with some prompts to test our model on after fine-tuning. Notice the `{SUBJECT_TOKEN} {SUBJECT_CLASS}` included in each of them.\n",
    "\n",
    "You can change these to be more fitting for your subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPTS = [\n",
    "    f\"{SUBJECT_TOKEN} {SUBJECT_CLASS} at the beach\",\n",
    "    f\"{SUBJECT_TOKEN} {SUBJECT_CLASS} in a bucket\",\n",
    "    f\"{SUBJECT_TOKEN} {SUBJECT_CLASS} sleeping soundly\",\n",
    "    f\"{SUBJECT_TOKEN} {SUBJECT_CLASS} as a superhero\",\n",
    "]\n",
    "PROMPTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run fine-tuning with Ray Train + HuggingFace Accelerate\n",
    "\n",
    "Next, let's launch the distributed fine-tuning job.\n",
    "\n",
    "We will use the training script provided by the [HuggingFace diffusers Dreambooth fine-tuning example](https://github.com/huggingface/diffusers/blob/d7634cca87641897baf90f5a006f2d6d16eac6ec/examples/dreambooth/README_sdxl.md) with very slight modifications.\n",
    "\n",
    "See `train_dreambooth_lora_sdxl.py` for the training script. The example does fine-tuning with [Low Rank Adaptation](https://arxiv.org/abs/2106.09685) (LoRA), which is a method that freezes most layers but injects a small set of trainable layers that get added to existing layers. This method greatly reduces the amount of training state in GPU memory and reduces the checkpoint size, while maintaining the fine-tuned model quality.\n",
    "\n",
    "This script is built on HuggingFace Accelerate, and we will show how easy it is to run an existing training script on a Ray cluster with Ray Train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse training arguments\n",
    "\n",
    "The `diffusers` script is originally launched via the command line. Here, we'll launch it with Ray Train instead and pass in the parsed command line arguments, in order to make as few modifications to the training script as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from train_dreambooth_lora_sdxl import parse_args\n",
    "\n",
    "# [Optional] Setup wandb to visualize generated samples during fine-tuning.\n",
    "# os.environ[\"WANDB_API_KEY\"] = \"YOUR_WANDB_API_KEY\"\n",
    "\n",
    "# See `parse_args` in train_dreambooth_lora_sdxl.py to see all the possible configurations.\n",
    "cmd_line_args = [\n",
    "    f\"--pretrained_model_name_or_path=stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    f\"--pretrained_vae_model_name_or_path=madebyollin/sdxl-vae-fp16-fix\",\n",
    "    f\"--instance_data_dir={SUBJECT_IMAGES_PATH}\",\n",
    "    \"--output_dir=/mnt/local_storage/lora-trained-xl\",\n",
    "    \"--mixed_precision=fp16\",\n",
    "    # A neutral prompt that serves as the caption for the subject image during training.\n",
    "    f\"--instance_prompt=a photo of {SUBJECT_TOKEN} {SUBJECT_CLASS}\",\n",
    "    \"--resolution=1024\",\n",
    "    \"--train_batch_size=1\",\n",
    "    \"--gradient_accumulation_steps=1\",\n",
    "    \"--learning_rate=1e-4\",\n",
    "    \"--lr_scheduler=constant\",\n",
    "    \"--lr_warmup_steps=0\",\n",
    "    \"--max_train_steps=100\",\n",
    "    \"--checkpointing_steps=100\",\n",
    "    # Use the first prompt as a sample to generate during training.\n",
    "    f\"--validation_prompt={PROMPTS[0]}\",\n",
    "    \"--validation_epochs=25\",\n",
    "    \"--seed=0\",\n",
    "] + ([\"--report_to=wandb\"] if os.environ.get(\"WANDB_API_KEY\") else [])\n",
    "\n",
    "TRAINING_ARGS = parse_args(input_args=cmd_line_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch distributed training with Ray Train\n",
    "\n",
    "To run distributed training, we'll use a `ray.train.torch.TorchTrainer` to request GPU workers and connect them together in a distributed worker group. Then, when the workers run the training script, HuggingFace Accelerate detects this distributed process group and sets up the model to do data parallel training.\n",
    "\n",
    "A few notes:\n",
    "* `ray.init(runtime_env={\"env_vars\": ...})` sets the environment variables on all workers in the cluster -- setting the environment variable in this notebook on the head node is not enough in a distributed setting.\n",
    "* `train_fn_per_worker` is the function that will run on all distributed training workers. In this case, it's just a light wrapper on top of the `diffusers` example script that copies the latest checkpoint to shared cluster storage.\n",
    "* `ScalingConfig` is the configuration that determines how many workers and what kind of accelerator to use for training. Once the training is launched, **Anyscale will automatically scale up nodes to meet this resource request!**\n",
    "\n",
    "The result of this fine-tuning will be a fine-tuned LoRA model checkpoint at `MODEL_CHECKPOINT_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CHECKPOINT_PATH = os.environ[\"ANYSCALE_ARTIFACT_STORAGE\"] + \"/checkpoint-final\"\n",
    "\n",
    "print(\"Final checkpoint will be uploaded to: \", MODEL_CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import ray\n",
    "import ray.train\n",
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "from train_dreambooth_lora_sdxl import main\n",
    "from utils import download_from_cloud, upload_to_cloud\n",
    "\n",
    "\n",
    "# Set environment variables across the entire cluster.\n",
    "ray.init(\n",
    "    runtime_env={\n",
    "        \"env_vars\": {\"WANDB_API_KEY\": os.environ.get(\"WANDB_API_KEY\")}\n",
    "    },\n",
    "    ignore_reinit_error=True,\n",
    ")\n",
    "\n",
    "\n",
    "def train_fn_per_worker(config: dict):\n",
    "    download_from_cloud(cloud_uri=DATA_CLOUD_PATH, local_path=SUBJECT_IMAGES_PATH)\n",
    "\n",
    "    # See train_dreambooth_lora_sdxl.py for all of the training details.\n",
    "    final_checkpoint_path, final_metrics = main(config[\"args\"])\n",
    "\n",
    "    # Upload final checkpoint to cloud. (Only the rank 0 worker will return a path here.)\n",
    "    if final_checkpoint_path is not None:\n",
    "        upload_to_cloud(local_path=final_checkpoint_path, cloud_uri=MODEL_CHECKPOINT_PATH)\n",
    "\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_fn_per_worker,\n",
    "    # Pass command line arguments from the driver to the `config` dict of the `train_fn_per_worker`\n",
    "    train_loop_config={\"args\": TRAINING_ARGS},\n",
    "    scaling_config=ray.train.ScalingConfig(\n",
    "        # Do data parallel training with A10G GPU workers\n",
    "        num_workers=4, use_gpu=True, accelerator_type=\"A10G\"\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the training.\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate some images with your fine-tuned model!\n",
    "\n",
    "Finally, let's generate some images!\n",
    "\n",
    "We'll launch 2 remote GPU tasks to generate images from the `PROMPTS` we defined earlier, one using just the base model and one that loads our fine-tuned LoRA weights. Let's compare them to see the results of fine-tuning!\n",
    "\n",
    "Note: If your cluster has already scaled down from the training job due to the workers being idle, then this step might take a little longer to relaunch new GPU workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from utils import generate\n",
    "\n",
    "[base_model_images, finetuned_images] = ray.get([\n",
    "    generate.remote(prompts=PROMPTS, args=TRAINING_ARGS),\n",
    "    generate.remote(\n",
    "        prompts=PROMPTS, args=TRAINING_ARGS, model_checkpoint_path=MODEL_CHECKPOINT_PATH\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images generated with the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "display(*base_model_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images generated with the finetuned model\n",
    "\n",
    "These images should resemble your subject. If the generated image quality is not satisfactory, refer to the tips in [this blog post](https://huggingface.co/blog/dreambooth#tldr-recommended-settings) to tweak your hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(*finetuned_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congrats, you've fine-tuned Stable Diffusion XL!\n",
    "\n",
    "As a recap, this notebook:\n",
    "1. Installed cluster-wide dependencies.\n",
    "2. Scaled out fine-tuning to many GPU workers.\n",
    "3. Compared the generated output results before and after fine-tuning.\n",
    "\n",
    "As a next step, you can take the fine-tuned model checkpoint and use it to serve the model. See the tutorial on serving stable diffusion on the home page to get started!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
