{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced RAG Deep Dive\n",
    "\n",
    "In this tutorial, we walk through end-to-end Retrieval-Augmented Generation (RAG) pipelines using [Ray](https://docs.ray.io/), from data ingestion and LLM deployment to prompt engineering, evaluation and scaling out all workloads in the application.\n",
    "\n",
    "## Notebooks\n",
    "\n",
    "1. **01 (Optional) Regular_Document_Processing_Pipeline.ipynb**  \n",
    "   Demonstrates a baseline document processing workflow for extracting, cleaning, and indexing text prior to RAG.\n",
    "\n",
    "2. **02 Scalable_RAG_Data_Ingestion_with_Ray_Data.ipynb**  \n",
    "   Shows how to build a high-throughput data ingestion pipeline for RAG using Ray Data.\n",
    "\n",
    "3. **03 Deploy_LLM_with_Ray_Serve.ipynb**  \n",
    "   Guides you through containerizing and serving a large language model at scale with Ray Serve.\n",
    "\n",
    "4. **04 Build_Basic_RAG_Chatbot.ipynb**  \n",
    "   Combines your indexed documents and served LLM to create a simple, interactive RAG chatbot.\n",
    "\n",
    "5. **05 Improve_RAG_with_Prompt_Engineering.ipynb**  \n",
    "   Explores prompt-engineering techniques to boost relevance and accuracy in RAG responses.\n",
    "\n",
    "6. **06 (Optional) Evaluate_RAG_with_Online_Inference.ipynb**  \n",
    "   Provides methods to assess RAG quality in real time via live queries and metrics tracking.\n",
    "\n",
    "7. **07 Evaluate_RAG_with_Ray_Data_LLM_Batch_inference.ipynb**  \n",
    "   Implements large-scale batch evaluation of RAG outputs using Ray Data + LLM batch inference.\n",
    "\n",
    "---\n",
    "\n",
    "> **Note:** Notebooks marked “(Optional)” cover complementary topics and can be skipped if you prefer to focus on the core RAG flow."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
