{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa39a4cf",
   "metadata": {},
   "source": [
    "# Batch Inference Performance Optimization with Ray Data\n",
    "\n",
    "**⏱ Time to complete**: 30 min | **Difficulty**: Advanced | **Prerequisites**: ML inference experience, understanding of distributed systems\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "Learn to optimize ML inference pipelines by seeing common mistakes and their fixes. You'll transform a slow, inefficient inference pipeline into a high-performance system.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Performance Baseline](#step-1-the-slow-way-common-mistakes) (8 min)\n",
    "2. [Model Optimization](#step-2-optimizing-model-loading) (8 min)\n",
    "3. [Resource Tuning](#step-3-resource-and-batch-optimization) (8 min)\n",
    "4. [Final Optimization](#step-4-putting-it-all-together) (6 min)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this tutorial, you'll understand:\n",
    "\n",
    "- **Why performance optimization matters**: The difference between fast and slow inference at scale\n",
    "- **Common Ray Data mistakes**: Anti-patterns that kill performance and how to avoid them\n",
    "- **Optimization techniques**: Proven methods to maximize throughput and minimize costs\n",
    "- **Performance debugging**: How to identify and fix bottlenecks in inference pipelines\n",
    "\n",
    "## Overview\n",
    "\n",
    "**The Challenge**: ML inference can be deceptively slow. Small configuration mistakes can significantly impact pipeline performance, wasting time and money.\n",
    "\n",
    "**The Learning Approach**: We'll start with a deliberately inefficient pipeline, then systematically fix each issue to show you exactly what makes the difference.\n",
    "\n",
    "**Real-world Impact**:\n",
    "- **Cost Savings**: Optimized inference reduces cloud costs through better resource utilization\n",
    "- **Speed Gains**: Process data efficiently with proper configuration\n",
    "- **Scalability**: Handle larger workloads through distributed processing\n",
    "- **Resource Efficiency**: Maximize GPU utilization and minimize waste\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites Checklist\n",
    "\n",
    "Before starting, ensure you have:\n",
    "- [ ] Experience with ML model inference\n",
    "- [ ] Understanding of GPU/CPU resource management\n",
    "- [ ] Familiarity with batch processing concepts\n",
    "- [ ] Knowledge of performance optimization principles\n",
    "\n",
    "## Quick Start (3 minutes)\n",
    "\n",
    "Want to see the performance difference immediately?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff981b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import time\n",
    "\n",
    "# Slow way (don't do this!)\n",
    "start = time.time()\n",
    "ds = ray.data.from_items([{\"x\": i} for i in range(1000)])\n",
    "result_slow = ds.map(lambda x: x[\"x\"] * 2)  # Inefficient\n",
    "print(f\"⏰ Slow approach: {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# Fast way (Ray Data optimized!)\n",
    "start = time.time()\n",
    "result_fast = ds.map_batches(lambda batch: {\"x\": [x * 2 for x in batch[\"x\"]]})\n",
    "print(f\" Fast approach: {time.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2d078d",
   "metadata": {},
   "source": [
    "## The Performance Problem\n",
    "\n",
    "**Why Inference Optimization Matters**:\n",
    "- **Scale**: Production ML systems process millions of inputs daily\n",
    "- **Cost**: GPU time is expensive - optimization saves thousands monthly\n",
    "- **Latency**: Faster inference enables real-time applications\n",
    "- **Throughput**: Optimized systems handle 10-100x more requests\n",
    "\n",
    "## Overview\n",
    "\n",
    "### **Ray Data Batch Inference: From Slow to Fast**\n",
    "\n",
    "Batch inference is one of the most common Ray Data use cases, yet it's surprisingly easy to get wrong. Small configuration mistakes can lead to significantly slower performance, wasted GPU resources, and frustrated data scientists.\n",
    "\n",
    "**Why This Matters for ML Engineers:**\n",
    "- **Performance Impact**: Poor configuration can make inference much slower than necessary\n",
    "- **Resource Waste**: Incorrect settings waste expensive GPU time and cluster resources\n",
    "- **Scalability Issues**: Bad patterns don't scale and become bottlenecks in production\n",
    "- **Development Friction**: Slow inference slows down experimentation and iteration cycles\n",
    "\n",
    "This template takes a unique approach: we'll first build a batch inference pipeline using common anti-patterns and mistakes, measure its poor performance, then systematically fix each issue to achieve optimal speed.\n",
    "\n",
    "### **Common Ray Data Batch Inference Mistakes**\n",
    "\n",
    "Most performance issues in Ray Data batch inference stem from these fundamental misunderstandings:\n",
    "\n",
    "1. **Model Loading Anti-Pattern**: Loading models inside the inference function instead of initialization\n",
    "2. **Resource Misconfiguration**: Wrong `num_cpus`, `num_gpus`, or `concurrency` settings  \n",
    "3. **Block Size Problems**: Inefficient `override_num_blocks` causing too many small tasks or too few large tasks\n",
    "4. **Batch Size Issues**: Using batch sizes that are too small (underutilizing GPU) or too large (causing OOM)\n",
    "5. **Data Format Inefficiency**: Using inefficient formats like JSON/CSV instead of Parquet for large outputs\n",
    "6. **Memory Management**: Not understanding Ray Data's memory model and causing OOM errors\n",
    "7. **Incorrect Compute Strategy**: Using `compute=\"tasks\"` when `compute=\"actors\"` is needed for model persistence\n",
    "8. **Wrong Return Format**: Returning list of objects instead of proper batch dictionary format\n",
    "9. **Data Transfer Overhead**: Inefficient data serialization and transfer between workers\n",
    "10. **Preprocessing Inefficiency**: Doing expensive preprocessing inside inference instead of separate stage\n",
    "11. **API Version Issues**: Using deprecated Ray Data API patterns that cause errors\n",
    "\n",
    "### **The Learning Journey: Broken → Fixed**\n",
    "\n",
    "This template follows a clear progression:\n",
    "\n",
    "**Phase 1: The \"Wrong Way\"**\n",
    "- Build inference pipeline with common mistakes\n",
    "- Identify bottlenecks and anti-patterns\n",
    "- Understand why each mistake creates problems\n",
    "\n",
    "**Phase 2: Understanding Ray Data Architecture**\n",
    "- Learn how Ray Data blocks and tasks work\n",
    "- Understand resource allocation and parallelism\n",
    "- Master configuration parameters and their effects\n",
    "\n",
    "**Phase 3: The \"Right Way\"**  \n",
    "- Fix each issue systematically\n",
    "- Apply Ray Data best practices and optimizations\n",
    "- Build an optimized inference pipeline\n",
    "\n",
    "**Phase 4: Comparison and Analysis**\n",
    "- Compare before/after implementations\n",
    "- Understand the impact of each optimization\n",
    "- Learn to identify and debug performance issues\n",
    "\n",
    "### **Memory Management Best Practices** (rule #254)\n",
    "\n",
    "Ray Data provides streaming operations for large datasets to avoid memory issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd156bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-efficient processing configuration\n",
    "MEMORY_CONFIG = {\n",
    "    \"preserve_order\": False,  # Allow reordering for better memory usage\n",
    "    \"local_shuffle_buffer_size\": 1000,  # Limit shuffle buffer size\n",
    "    \"target_max_block_size\": 512 * 1024 * 1024,  # 512MB max block size\n",
    "}\n",
    "\n",
    "# Example: Memory-efficient inference pipeline\n",
    "def memory_efficient_inference(dataset):\n",
    "    \"\"\"Demonstrate memory-efficient inference patterns.\"\"\"\n",
    "    return dataset.map_batches(\n",
    "        inference_function,\n",
    "        batch_size=16,  # Smaller batches for memory efficiency\n",
    "        concurrency=2,  # Limit concurrent workers\n",
    "        **MEMORY_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68abda41",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this template, you'll understand:\n",
    "- Common Ray Data batch inference anti-patterns and their performance impact\n",
    "- Ray Data's block-based architecture and how it affects performance\n",
    "- Proper resource configuration for GPU-accelerated batch inference\n",
    "- Optimal batch sizing and memory management strategies\n",
    "- How to measure, profile, and optimize Ray Data pipelines\n",
    "- Production-ready patterns for scalable batch inference\n",
    "\n",
    "## Use Case: ImageNet Classification at Scale\n",
    "\n",
    "### **Real-World ML Inference Challenge**\n",
    "\n",
    "We'll use a realistic computer vision scenario: classifying thousands of ImageNet images using a pre-trained ResNet model. This represents a common production workload where:\n",
    "\n",
    "**Dataset Characteristics**\n",
    "- **Volume**: 50,000+ high-resolution images from ImageNet validation set\n",
    "- **Size**: ~6GB of image data requiring efficient loading and preprocessing\n",
    "- **Format**: JPEG images with varying dimensions requiring standardization\n",
    "- **Distribution**: Images stored in distributed storage (S3-compatible format)\n",
    "\n",
    "**Model Requirements**\n",
    "- **Architecture**: ResNet-50 pre-trained on ImageNet (25MB model)\n",
    "- **Input**: 224x224 RGB images with ImageNet normalization\n",
    "- **Output**: 1,000-class probability distributions\n",
    "- **Hardware**: GPU acceleration required for reasonable inference speed\n",
    "\n",
    "**Performance Expectations**\n",
    "- **Throughput**: Efficient processing of large image batches\n",
    "- **Latency**: Reasonable per-batch inference times\n",
    "- **Resource Utilization**: Good GPU utilization during inference\n",
    "- **Scalability**: Proper scaling with additional GPU workers\n",
    "\n",
    "This scenario mirrors real production ML workloads where performance optimization directly impacts business metrics like cost, user experience, and system capacity.\n",
    "\n",
    "## 5-Minute Quick Start\n",
    "\n",
    "Let's start by running the \"wrong way\" implementation to see common mistakes in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a32ece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.data\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import time\n",
    "\n",
    "# Ray cluster is already running on Anyscale\n",
    "print('Connected to Anyscale Ray cluster!')\n",
    "print(f'Available resources: {ray.cluster_resources()}')\n",
    "\n",
    "# Load ImageNet data (publicly available)\n",
    "# Using a larger sample to demonstrate real-world scenarios\n",
    "image_dataset = ray.data.read_images(\n",
    "    \"s3://anonymous@air-example-data-2/imagenette2/train/\",\n",
    "    mode=\"RGB\"\n",
    ").limit(5000)  # Larger sample for realistic demonstration\n",
    "\n",
    "print(f\"Loaded {image_dataset.count()} images for inference\")\n",
    "\n",
    "# WRONG WAY: Common mistakes that kill performance\n",
    "class SlowInference:\n",
    "    \"\"\"Example of what NOT to do - loads model in __call__ method.\"\"\"\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        # MISTAKE 1: Loading model inside inference function\n",
    "        model = models.resnet50(pretrained=True)\n",
    "        model.eval()\n",
    "        \n",
    "        # MISTAKE 2: Not using GPU even when available\n",
    "        # model = model.cuda()  # Commented out - missing GPU usage\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        # MISTAKE 3: Processing images one by one instead of batching\n",
    "        for image_path in batch[\"path\"]:\n",
    "            # Inefficient single-image processing\n",
    "            predictions.append({\"prediction\": \"demo_class\", \"confidence\": 0.95})\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# MISTAKE 4: Wrong resource configuration\n",
    "slow_results = image_dataset.map_batches(\n",
    "    SlowInference,\n",
    "    # MISTAKE 5: Bad batch size and concurrency\n",
    "    batch_size=1,      # Too small - no GPU utilization\n",
    "    concurrency=1,     # Too low - no parallelism\n",
    "    # MISTAKE 6: Missing GPU allocation\n",
    "    # num_gpus=1       # Commented out - not using available GPU\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "predictions = slow_results.take_all()\n",
    "slow_time = time.time() - start_time\n",
    "\n",
    "print(f\"Slow implementation: {len(predictions)} predictions in {slow_time:.2f}s\")\n",
    "print(f\"Throughput: {len(predictions)/slow_time:.1f} images/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d36468",
   "metadata": {},
   "source": [
    "**Expected Output (Slow Implementation):**\n",
    "```\n",
    "Slow implementation: 5000 predictions in [time varies]\n",
    "Processing completed with multiple performance issues\n",
    "```\n",
    "\n",
    "Notice the various inefficiencies in this implementation! The next sections will show you why this happens and how to fix it.\n",
    "\n",
    "## Complete Tutorial\n",
    "\n",
    "### **Phase 1: The \"Wrong Way\" - Common Mistakes**\n",
    "\n",
    "Let's build a complete batch inference pipeline using all the common anti-patterns. This will help you recognize these mistakes in real code and understand their performance impact.\n",
    "\n",
    "**Step 1: Data Loading with Poor Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eac0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.data\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Load ImageNet validation data (publicly available subset)\n",
    "def load_imagenet_data():\n",
    "    \"\"\"Load ImageNet data with suboptimal configuration.\"\"\"\n",
    "    \n",
    "    # MISTAKE 1: No block size optimization\n",
    "    # Using default blocks which may be too small or too large\n",
    "    dataset = ray.data.read_images(\n",
    "        \"s3://anonymous@air-example-data-2/imagenette2/\",\n",
    "        mode=\"RGB\",\n",
    "        # MISTAKE: Not specifying override_num_blocks\n",
    "        # This can lead to inefficient task distribution\n",
    "    ).limit(10000)  # 10K images for realistic demonstration\n",
    "    \n",
    "    print(f\"Dataset blocks: {dataset.num_blocks()}\")  # Likely suboptimal\n",
    "    print(f\"Estimated dataset size: {dataset.size_bytes() / 1024 / 1024:.1f} MB\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "imagenet_data = load_imagenet_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59332a61",
   "metadata": {},
   "source": [
    "**Step 2: Inefficient Inference Class (All the Wrong Patterns)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3d3ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BadInferenceClass:\n",
    "    \"\"\"\n",
    "    Example of what NOT to do in Ray Data batch inference.\n",
    "    This class demonstrates every common anti-pattern.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # MISTAKE 2: Minimal initialization\n",
    "        # Not loading the model here where it should be loaded once\n",
    "        self.transform = None\n",
    "        print(\"BadInferenceClass initialized (but model not loaded)\")\n",
    "    \n",
    "    def __call__(self, batch: Dict[str, np.ndarray]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process batch with all the wrong patterns.\"\"\"\n",
    "        \n",
    "        # MISTAKE 3: Loading model INSIDE the inference function\n",
    "        # This means we reload the 25MB model for every single batch!\n",
    "        print(\"Loading ResNet model (this should only happen once!)\")\n",
    "        model = models.resnet50(pretrained=True)\n",
    "        model.eval()\n",
    "        \n",
    "        # MISTAKE 4: Not using GPU even when available\n",
    "        device = torch.device('cpu')  # Forcing CPU instead of GPU\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # MISTAKE 5: Creating transform every time instead of reusing\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # MISTAKE 6: Processing images one by one instead of true batching\n",
    "        for i, image_array in enumerate(batch[\"image\"]):\n",
    "            try:\n",
    "                # MISTAKE 7: Inefficient single-image processing\n",
    "                # Converting numpy -> PIL -> tensor for each image individually\n",
    "                image_tensor = transform(image_array).unsqueeze(0)\n",
    "                image_tensor = image_tensor.to(device)\n",
    "                \n",
    "                # MISTAKE 8: Individual forward passes instead of batch inference\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(image_tensor)\n",
    "                    probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
    "                    confidence, predicted_class = torch.max(probabilities, 0)\n",
    "                \n",
    "                results.append({\n",
    "                    \"image_id\": i,\n",
    "                    \"predicted_class\": int(predicted_class),\n",
    "                    \"confidence\": float(confidence),\n",
    "                    \"processing_time\": time.time()  # Unnecessary overhead\n",
    "                })\n",
    "                \n",
    "                # MISTAKE 9: Unnecessary logging/printing in hot path\n",
    "                if i % 10 == 0:\n",
    "                    print(f\"Processed image {i}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                # MISTAKE 10: Poor error handling that slows down processing\n",
    "                print(f\"Error processing image {i}: {e}\")\n",
    "                results.append({\n",
    "                    \"image_id\": i,\n",
    "                    \"predicted_class\": -1,\n",
    "                    \"confidence\": 0.0,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "        \n",
    "        # MISTAKE 11: Wrong return format for Ray Data\n",
    "        # Returning a list of objects instead of proper batch format\n",
    "        # This violates Ray Data's batch output requirements\n",
    "        return results  # ERROR: Ray 2.5+ requires wrapped format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a18a649",
   "metadata": {},
   "source": [
    "**Step 3: Running with Wrong Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b2930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bad_inference():\n",
    "    \"\"\"Run inference with all the wrong configuration settings.\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"RUNNING BAD INFERENCE PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # MISTAKE 12: Terrible resource configuration\n",
    "    bad_results = imagenet_data.map_batches(\n",
    "        BadInferenceClass,\n",
    "        \n",
    "        # MISTAKE 13: Tiny batch size that doesn't utilize GPU\n",
    "        batch_size=2,  # Way too small for GPU efficiency\n",
    "        \n",
    "        # MISTAKE 14: Wrong concurrency setting\n",
    "        concurrency=8,  # Too high for GPU tasks, causes resource contention\n",
    "        \n",
    "        # MISTAKE 15: Not specifying GPU resources\n",
    "        # num_gpus=0.1,  # Commented out - not using available GPU\n",
    "        \n",
    "        # MISTAKE 16: Wrong compute strategy\n",
    "        compute=\"tasks\",  # Should be \"actors\" for model loading\n",
    "        \n",
    "        # MISTAKE 17: No resource limits\n",
    "        # max_concurrency not set - can overwhelm cluster\n",
    "    )\n",
    "    \n",
    "    # MISTAKE 18: This will fail with Ray 2.5+ due to wrong return format\n",
    "    # The error message will be:\n",
    "    # \"Returning a list of objects from `map_batches` is not allowed\"\n",
    "    \n",
    "    # MISTAKE 19: Inefficient data collection (if it worked)\n",
    "    # all_predictions = bad_results.take_all()  # Would load everything into memory\n",
    "    \n",
    "    # MISTAKE 20: Inefficient output format (if data collection worked)\n",
    "    # Would save to JSON instead of efficient format like Parquet\n",
    "    # import json\n",
    "    # with open(\"/tmp/bad_results.json\", \"w\") as f:\n",
    "    #     json.dump(all_predictions, f)  # Slow serialization\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nBAD IMPLEMENTATION RESULTS:\")\n",
    "    print(f\"Total images processed: {len(all_predictions)}\")\n",
    "    print(f\"Total time: {total_time:.2f} seconds\")\n",
    "    print(f\"Throughput: {len(all_predictions)/total_time:.1f} images/second\")\n",
    "    print(f\"GPU utilization: Very low (not using GPU)\")\n",
    "    print(f\"Model loading overhead: Significant (model reloaded many times)\")\n",
    "    \n",
    "    return {\n",
    "        'total_time': total_time,\n",
    "        'throughput': len(all_predictions)/total_time,\n",
    "        'predictions': all_predictions[:5]  # Sample results\n",
    "    }\n",
    "\n",
    "# Run the bad implementation\n",
    "bad_performance = run_bad_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06f8821",
   "metadata": {},
   "source": [
    "**Understanding Why This is Inefficient**\n",
    "\n",
    "The bad implementation above demonstrates many common inefficiencies. In fact, this code will actually fail with a Ray Data error:\n",
    "\n",
    "```\n",
    "ValueError: Returning a list of objects from `map_batches` is not allowed in Ray 2.5. \n",
    "To return Python objects, wrap them in a named dict field, e.g., return `{'results': objects}` \n",
    "instead of just `objects`.\n",
    "```\n",
    "\n",
    "This error is extremely common when migrating from older Ray Data versions or when following outdated examples. Here's why each mistake creates problems:\n",
    "\n",
    "1. **Model Reloading**: Loading the 25MB ResNet model for every batch adds significant overhead\n",
    "2. **No GPU Usage**: CPU inference is much slower than GPU for neural networks\n",
    "3. **Tiny Batches**: Batch size of 2 means poor GPU utilization and maximum overhead\n",
    "4. **High Concurrency**: 8 concurrent tasks compete for the same resources\n",
    "5. **Individual Processing**: Processing images one-by-one prevents vectorized operations\n",
    "6. **Memory Inefficiency**: Creating transforms and tensors repeatedly wastes memory\n",
    "7. **Wrong Return Format**: Returning list of objects violates Ray Data's batch output requirements\n",
    "8. **Data Format Issues**: JSON serialization is slower than binary formats\n",
    "9. **Resource Competition**: Tasks competing for limited GPU memory\n",
    "10. **API Misuse**: Using deprecated or incorrect Ray Data API patterns\n",
    "11. **Return Format Error**: The list return format causes Ray Data validation errors\n",
    "\n",
    "### **Phase 2: Understanding Ray Data Architecture**\n",
    "\n",
    "Before we fix the problems, let's understand how Ray Data works internally. This knowledge is crucial for writing efficient batch inference pipelines.\n",
    "\n",
    "**Ray Data Block Architecture**\n",
    "\n",
    "Ray Data organizes datasets into blocks, which are the fundamental units of parallel processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07904652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def understand_ray_data_blocks():\n",
    "    \"\"\"Explore Ray Data's block-based architecture.\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"RAY DATA ARCHITECTURE DEEP DIVE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load dataset and examine its structure\n",
    "    dataset = ray.data.read_images(\n",
    "        \"s3://anonymous@air-example-data-2/imagenette2/train/\", \n",
    "        mode=\"RGB\"\n",
    "    ).limit(500)\n",
    "    \n",
    "    print(f\"Dataset Statistics:\")\n",
    "    print(f\"  Total records: {dataset.count()}\")\n",
    "    print(f\"  Number of blocks: {dataset.num_blocks()}\")\n",
    "    print(f\"  Dataset size: {dataset.size_bytes() / 1024 / 1024:.1f} MB\")\n",
    "    \n",
    "    # Understanding block distribution\n",
    "    block_sizes = []\n",
    "    for i in range(min(5, dataset.num_blocks())):\n",
    "        block = dataset.get_internal_block_refs()[i]\n",
    "        # This would show block size distribution in real implementation\n",
    "        print(f\"  Block {i}: ~{dataset.size_bytes() // dataset.num_blocks() / 1024:.0f} KB\")\n",
    "    \n",
    "    print(f\"\\nKey Insights:\")\n",
    "    print(f\"  • Each block becomes a separate task\")\n",
    "    print(f\"  • Tasks run in parallel across Ray workers\")\n",
    "    print(f\"  • Block size affects memory usage and parallelism\")\n",
    "    print(f\"  • Too many small blocks = high overhead\")\n",
    "    print(f\"  • Too few large blocks = poor parallelism\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Explore the architecture\n",
    "sample_dataset = understand_ray_data_blocks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae280c10",
   "metadata": {},
   "source": [
    "**Resource Allocation and Task Scheduling**\n",
    "\n",
    "Understanding how Ray Data schedules tasks is critical for performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c50b813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_resource_allocation():\n",
    "    \"\"\"Explain Ray Data resource allocation patterns.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESOURCE ALLOCATION PATTERNS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    cluster_resources = ray.cluster_resources()\n",
    "    print(f\"Available Cluster Resources:\")\n",
    "    for resource, amount in cluster_resources.items():\n",
    "        print(f\"  {resource}: {amount}\")\n",
    "    \n",
    "    print(f\"\\nTask Scheduling Rules:\")\n",
    "    print(f\"  • map_batches() creates one task per block\")\n",
    "    print(f\"  • Each task requests specified resources (CPU/GPU)\")\n",
    "    print(f\"  • Tasks wait in queue if resources unavailable\")\n",
    "    print(f\"  • Higher concurrency ≠ better performance\")\n",
    "    \n",
    "    print(f\"\\nGPU Allocation Best Practices:\")\n",
    "    print(f\"  • Use num_gpus=1.0 for dedicated GPU per task\")\n",
    "    print(f\"  • Use num_gpus=0.5 to share GPU between 2 tasks\")\n",
    "    print(f\"  • Match concurrency to available GPUs\")\n",
    "    print(f\"  • Avoid fractional GPU allocation unless memory-constrained\")\n",
    "    \n",
    "    print(f\"\\nMemory Management:\")\n",
    "    print(f\"  • Ray Data streams blocks through memory\")\n",
    "    print(f\"  • Large batches use more memory but improve GPU utilization\")\n",
    "    print(f\"  • Object store provides automatic memory management\")\n",
    "    \n",
    "explain_resource_allocation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4cb82c",
   "metadata": {},
   "source": [
    "**Optimal Configuration Guidelines**\n",
    "\n",
    "Here are the key principles for configuring Ray Data batch inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c85e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configuration_guidelines():\n",
    "    \"\"\"Provide guidelines for optimal Ray Data configuration.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CONFIGURATION OPTIMIZATION GUIDELINES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"1. BATCH SIZE OPTIMIZATION\")\n",
    "    print(\"   • GPU Memory Available: Use largest batch that fits\")\n",
    "    print(\"   • CPU-only: 32-128 samples per batch\")\n",
    "    print(\"   • GPU: 64-512 samples per batch (depends on model size)\")\n",
    "    print(\"   • Rule of thumb: Start with 128, increase until OOM\")\n",
    "    \n",
    "    print(\"\\n2. CONCURRENCY SETTINGS\")\n",
    "    print(\"   • GPU tasks: concurrency = number of GPUs\")\n",
    "    print(\"   • CPU tasks: concurrency = number of CPU cores / 2\")\n",
    "    print(\"   • Never exceed available hardware resources\")\n",
    "    \n",
    "    print(\"\\n3. BLOCK SIZE TUNING\")\n",
    "    print(\"   • Target: 100-500 MB per block\")\n",
    "    print(\"   • Use override_num_blocks to control block count\")\n",
    "    print(\"   • Formula: num_blocks = dataset_size_mb / target_block_size_mb\")\n",
    "    \n",
    "    print(\"\\n4. RESOURCE ALLOCATION\")\n",
    "    print(\"   • Model loading: Use compute='actors' for persistence\")\n",
    "    print(\"   • Stateless operations: Use compute='tasks'\")\n",
    "    print(\"   • GPU allocation: Match to actual hardware\")\n",
    "    \n",
    "    print(\"\\n5. OUTPUT FORMAT OPTIMIZATION\")\n",
    "    print(\"   • Parquet: Best for structured data, analytics\")\n",
    "    print(\"   • JSON: Good for small results, debugging\")\n",
    "    print(\"   • Avoid: CSV for large datasets, Python pickle\")\n",
    "\n",
    "configuration_guidelines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9d50aa",
   "metadata": {},
   "source": [
    "### **Phase 3: The \"Right Way\" - Optimized Implementation**\n",
    "\n",
    "Now let's fix every mistake and build a properly optimized batch inference pipeline:\n",
    "\n",
    "**Step 1: Optimized Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e2ab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imagenet_data_optimized():\n",
    "    \"\"\"Load ImageNet data with optimal configuration.\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"OPTIMIZED DATA LOADING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # OPTIMIZATION 1: Proper block size configuration\n",
    "    # Calculate optimal number of blocks based on cluster resources\n",
    "    cluster_cpus = int(ray.cluster_resources().get('CPU', 4))\n",
    "    target_blocks = cluster_cpus * 2  # 2x CPU cores for good parallelism\n",
    "    \n",
    "    dataset = ray.data.read_images(\n",
    "        \"s3://anonymous@air-example-data-2/imagenette2/\",\n",
    "        mode=\"RGB\",\n",
    "        # OPTIMIZATION: Specify optimal block count\n",
    "        override_num_blocks=target_blocks\n",
    "    ).limit(10000)  # Same 10K images for fair comparison\n",
    "    \n",
    "    print(f\"Optimized Dataset Configuration:\")\n",
    "    print(f\"  Total records: {dataset.count()}\")\n",
    "    print(f\"  Number of blocks: {dataset.num_blocks()}\")\n",
    "    print(f\"  Records per block: ~{dataset.count() // dataset.num_blocks()}\")\n",
    "    print(f\"  Dataset size: {dataset.size_bytes() / 1024 / 1024:.1f} MB\")\n",
    "    print(f\"  Average block size: ~{dataset.size_bytes() / dataset.num_blocks() / 1024 / 1024:.1f} MB\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "optimized_data = load_imagenet_data_optimized()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda94ee0",
   "metadata": {},
   "source": [
    "**Step 2: Proper Inference Class (All the Right Patterns)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9ba590",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedInferenceClass:\n",
    "    \"\"\"\n",
    "    Example of how to do Ray Data batch inference correctly.\n",
    "    This class demonstrates all the best practices.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"OPTIMIZATION 2: Load model once during initialization.\"\"\"\n",
    "        print(\"Loading ResNet model once during actor initialization...\")\n",
    "        \n",
    "        # Load model during initialization (happens once per actor)\n",
    "        self.model = models.resnet50(pretrained=True)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # OPTIMIZATION 3: Use GPU when available\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = self.model.to(self.device)\n",
    "        print(f\"Model loaded on device: {self.device}\")\n",
    "        \n",
    "        # OPTIMIZATION 4: Create transform once and reuse\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        print(\"OptimizedInferenceClass fully initialized\")\n",
    "    \n",
    "    def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, List[Any]]:\n",
    "        \"\"\"Process batch efficiently with proper batching.\"\"\"\n",
    "        \n",
    "        # OPTIMIZATION 5: True batch processing\n",
    "        batch_size = len(batch[\"image\"])\n",
    "        \n",
    "        # OPTIMIZATION 6: Vectorized preprocessing\n",
    "        # Process all images in the batch together\n",
    "        processed_images = []\n",
    "        for image_array in batch[\"image\"]:\n",
    "            image_tensor = self.transform(image_array)\n",
    "            processed_images.append(image_tensor)\n",
    "        \n",
    "        # OPTIMIZATION 7: Stack into proper batch tensor\n",
    "        batch_tensor = torch.stack(processed_images).to(self.device)\n",
    "        \n",
    "        # OPTIMIZATION 8: Single forward pass for entire batch\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(batch_tensor)\n",
    "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            confidences, predicted_classes = torch.max(probabilities, 1)\n",
    "        \n",
    "        # OPTIMIZATION 9: Correct Ray Data batch format\n",
    "        # Return dictionary with arrays, not list of objects\n",
    "        return {\n",
    "            \"predicted_class\": predicted_classes.cpu().numpy().tolist(),\n",
    "            \"confidence\": confidences.cpu().numpy().tolist(),\n",
    "            \"batch_size\": [batch_size] * batch_size  # Metadata for analysis\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc95e84",
   "metadata": {},
   "source": [
    "**Step 3: Optimal Configuration and Execution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa921bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimized_inference():\n",
    "    \"\"\"Run inference with optimal configuration settings.\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"RUNNING OPTIMIZED INFERENCE PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # OPTIMIZATION 10: Proper resource configuration\n",
    "    optimized_results = optimized_data.map_batches(\n",
    "        OptimizedInferenceClass,\n",
    "        \n",
    "        # OPTIMIZATION 11: Optimal batch size for GPU utilization\n",
    "        batch_size=64,  # Large enough to utilize GPU effectively\n",
    "        \n",
    "        # OPTIMIZATION 12: Appropriate concurrency\n",
    "        concurrency=2,  # Match available GPU resources\n",
    "        \n",
    "        # OPTIMIZATION 13: Proper GPU allocation\n",
    "        num_gpus=1,  # Use available GPU\n",
    "        \n",
    "        # OPTIMIZATION 14: Use actors for model persistence\n",
    "        compute=ray.data.ActorPoolStrategy(size=2),  # Keep model loaded in memory\n",
    "        \n",
    "        # OPTIMIZATION 15: Additional optimizations\n",
    "        max_concurrency=2,  # Prevent resource oversubscription\n",
    "    )\n",
    "    \n",
    "    # OPTIMIZATION 16: Efficient output format\n",
    "    # Save to Parquet for efficient storage and future processing\n",
    "    output_path = \"/mnt/cluster_storage/inference_results\"\n",
    "    optimized_results.write_parquet(output_path)\n",
    "    \n",
    "    # Get sample results for display\n",
    "    sample_results = optimized_results.take(100)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nOPTIMIZED IMPLEMENTATION RESULTS:\")\n",
    "    print(f\"Total images processed: {len(sample_results)}\")\n",
    "    print(f\"Total time: {total_time:.2f} seconds\")\n",
    "    print(f\"Throughput: {len(sample_results)/total_time:.1f} images/second\")\n",
    "    print(f\"GPU utilization: Much higher (efficient GPU usage)\")\n",
    "    print(f\"Model loading overhead: Minimal (loaded once per actor)\")\n",
    "    \n",
    "    return {\n",
    "        'total_time': total_time,\n",
    "        'throughput': len(sample_results)/total_time,\n",
    "        'predictions': sample_results[:5]  # Sample results\n",
    "    }\n",
    "\n",
    "# Run the optimized implementation\n",
    "optimized_performance = run_optimized_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887e8601",
   "metadata": {},
   "source": [
    "### **Phase 4: Performance Comparison and Analysis**\n",
    "\n",
    "Let's compare the performance improvements and understand the impact of each optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ee1c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def implementation_comparison():\n",
    "    \"\"\"Compare the two implementations and their key differences.\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"IMPLEMENTATION COMPARISON: BEFORE vs AFTER\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"KEY DIFFERENCES:\")\n",
    "    print(f\"  Bad Implementation:\")\n",
    "    print(f\"    • Model reloaded for every batch\")\n",
    "    print(f\"    • CPU-only inference\")\n",
    "    print(f\"    • Tiny batch sizes (batch_size=2)\")\n",
    "    print(f\"    • High concurrency causing resource contention\")\n",
    "    print(f\"    • Individual image processing\")\n",
    "    print(f\"    • JSON output format\")\n",
    "    print(f\"    • No resource optimization\")\n",
    "    \n",
    "    print(f\"\\n  Optimized Implementation:\")\n",
    "    print(f\"    • Model loaded once per actor\")\n",
    "    print(f\"    • GPU acceleration when available\")\n",
    "    print(f\"    • Proper batch sizes (batch_size=64)\")\n",
    "    print(f\"    • Appropriate concurrency settings\")\n",
    "    print(f\"    • Vectorized batch processing\")\n",
    "    print(f\"    • Efficient Parquet output\")\n",
    "    print(f\"    • Optimized resource allocation\")\n",
    "    \n",
    "    print(f\"\\nKEY OPTIMIZATION IMPACTS:\")\n",
    "    print(f\"  1. Model Loading Once:    Eliminated reload overhead\")\n",
    "    print(f\"  2. GPU Acceleration:      Efficient GPU-based inference\")\n",
    "    print(f\"  3. Proper Batching:       Better resource utilization\")\n",
    "    print(f\"  4. Optimal Configuration: Improved parallelism\")\n",
    "    print(f\"  5. Efficient I/O:         Faster data serialization\")\n",
    "\n",
    "implementation_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ffd097",
   "metadata": {},
   "source": [
    "## Ray Data Architecture Deep Dive\n",
    "\n",
    "Understanding Ray Data's internal architecture is crucial for optimization. Let's explore the key concepts:\n",
    "\n",
    "### **Block-Based Parallelism**\n",
    "\n",
    "Ray Data organizes datasets into blocks, which are the fundamental units of parallel processing:\n",
    "\n",
    "**How Blocks Work:**\n",
    "- Each dataset is divided into multiple blocks (similar to Spark partitions)\n",
    "- Each block contains a subset of the data (typically 100-500MB)\n",
    "- Operations like `map_batches()` create one task per block\n",
    "- Tasks execute in parallel across Ray workers\n",
    "\n",
    "**Block Size Impact on Performance:**\n",
    "- **Too Many Small Blocks**: High task overhead, poor GPU utilization\n",
    "- **Too Few Large Blocks**: Poor parallelism, memory pressure\n",
    "- **Optimal Block Size**: Balance parallelism with resource efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de0e538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Understanding block distribution\n",
    "dataset = ray.data.read_images(\"path/to/images\")\n",
    "print(f\"Blocks: {dataset.num_blocks()}\")\n",
    "print(f\"Records per block: ~{dataset.count() // dataset.num_blocks()}\")\n",
    "\n",
    "# Control block size with override_num_blocks\n",
    "optimized_dataset = ray.data.read_images(\n",
    "    \"path/to/images\",\n",
    "    override_num_blocks=ray.cluster_resources()['CPU'] * 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bcab99",
   "metadata": {},
   "source": [
    "### **Resource Allocation Model**\n",
    "\n",
    "Ray Data's resource allocation determines how tasks are scheduled and executed:\n",
    "\n",
    "**CPU vs GPU Tasks:**\n",
    "- **CPU Tasks**: Lightweight, many can run concurrently\n",
    "- **GPU Tasks**: Resource-intensive, limited by available GPUs\n",
    "- **Mixed Workloads**: Balance CPU preprocessing with GPU inference\n",
    "\n",
    "**Concurrency Patterns:**\n",
    "- `concurrency=1`: Sequential processing, lowest resource usage\n",
    "- `concurrency=num_gpus`: One task per GPU, optimal for GPU workloads  \n",
    "- `concurrency=num_cpus`: Maximum parallelism for CPU workloads\n",
    "\n",
    "**Memory Management:**\n",
    "- Ray Data streams blocks through memory automatically\n",
    "- Large batches improve GPU utilization but use more memory\n",
    "- Object store provides distributed memory management\n",
    "\n",
    "### **Task vs Actor Compute**\n",
    "\n",
    "Choosing between tasks and actors affects performance significantly:\n",
    "\n",
    "**Tasks (`compute=\"tasks\"`):**\n",
    "- **Best for**: Stateless operations, simple transformations\n",
    "- **Characteristics**: Fast startup, no state persistence\n",
    "- **Use case**: Data preprocessing, simple computations\n",
    "\n",
    "**Actors (`compute=\"actors\"`):**\n",
    "- **Best for**: Stateful operations, model inference\n",
    "- **Characteristics**: Slower startup, persistent state\n",
    "- **Use case**: ML model inference, database connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1509bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-based: Model loaded for every batch (slow)\n",
    "results = dataset.map_batches(\n",
    "    InferenceClass,\n",
    "    compute=\"tasks\"  # Model reloaded each time\n",
    ")\n",
    "\n",
    "# Actor-based: Model loaded once per worker (fast)\n",
    "results = dataset.map_batches(\n",
    "    InferenceClass,\n",
    "    compute=\"actors\"  # Model persists in memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1382a17c",
   "metadata": {},
   "source": [
    "## Configuration Optimization Guide\n",
    "\n",
    "### **Batch Size Optimization**\n",
    "\n",
    "Batch size is the most critical parameter for inference performance:\n",
    "\n",
    "**GPU Memory Constraints:**\n",
    "- Start with batch_size=64 for most models\n",
    "- Increase until you hit GPU OOM errors\n",
    "- Monitor GPU memory usage with `nvidia-smi`\n",
    "- Reduce batch size if you see memory errors\n",
    "\n",
    "**Performance vs Memory Trade-off:**\n",
    "- **Larger batches**: Better GPU utilization, higher throughput\n",
    "- **Smaller batches**: Lower memory usage, more stable\n",
    "- **Sweet spot**: Usually 64-256 for vision models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81bf326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal batch size through experimentation\n",
    "for batch_size in [16, 32, 64, 128, 256]:\n",
    "    try:\n",
    "        results = dataset.map_batches(\n",
    "            OptimizedInference,\n",
    "            batch_size=batch_size,\n",
    "            num_gpus=1\n",
    "        )\n",
    "        throughput = measure_throughput(results)\n",
    "        print(f\"Batch size {batch_size}: {throughput:.1f} imgs/sec\")\n",
    "    except Exception as e:\n",
    "        print(f\"Batch size {batch_size}: OOM error\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7483d751",
   "metadata": {},
   "source": [
    "### **Concurrency Tuning**\n",
    "\n",
    "Concurrency controls how many parallel tasks execute simultaneously:\n",
    "\n",
    "**GPU Workloads:**\n",
    "- Set `concurrency` equal to number of available GPUs\n",
    "- Never exceed available GPU resources\n",
    "- Use fractional GPU allocation for memory-constrained models\n",
    "\n",
    "**CPU Workloads:**\n",
    "- Start with `concurrency = num_cpus // 2`\n",
    "- Increase if CPU utilization is low\n",
    "- Consider memory constraints for large models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b784a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU-optimized concurrency\n",
    "num_gpus = int(ray.cluster_resources().get('GPU', 1))\n",
    "results = dataset.map_batches(\n",
    "    OptimizedInference,\n",
    "    concurrency=num_gpus,  # One task per GPU\n",
    "    num_gpus=1,\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c40fb6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Block Size Tuning**\n",
    "\n",
    "Control dataset partitioning for optimal parallelism:\n",
    "\n",
    "**Target Block Size:**\n",
    "- Aim for 100-500MB per block\n",
    "- Balance parallelism with task overhead\n",
    "- Consider downstream processing requirements\n",
    "\n",
    "**Calculation Formula:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bffa363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate optimal blocks\n",
    "dataset_size_mb = dataset.size_bytes() / 1024 / 1024\n",
    "target_block_size_mb = 200  # Target 200MB blocks\n",
    "optimal_blocks = max(1, int(dataset_size_mb / target_block_size_mb))\n",
    "\n",
    "# Apply optimization\n",
    "optimized_dataset = dataset.repartition(optimal_blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ae7a7b",
   "metadata": {},
   "source": [
    "## Troubleshooting Performance Issues\n",
    "\n",
    "### **Common Performance Problems**\n",
    "\n",
    "**1. Low GPU Utilization**\n",
    "- **Symptoms**: `nvidia-smi` shows <50% GPU usage\n",
    "- **Causes**: Batch size too small, CPU bottleneck, wrong concurrency\n",
    "- **Solutions**: Increase batch_size, optimize preprocessing, check resource allocation\n",
    "\n",
    "**2. Out of Memory Errors**\n",
    "- **Symptoms**: CUDA OOM, Ray object store full\n",
    "- **Causes**: Batch size too large, too many concurrent tasks\n",
    "- **Solutions**: Reduce batch_size, lower concurrency, add memory\n",
    "\n",
    "**3. Slow Throughput**\n",
    "- **Symptoms**: Much slower than expected performance\n",
    "- **Causes**: Model reloading, wrong compute mode, inefficient batching\n",
    "- **Solutions**: Use actors, optimize batch processing, profile bottlenecks\n",
    "\n",
    "### **Performance Profiling**\n",
    "\n",
    "Use Ray's built-in profiling tools to identify bottlenecks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b918aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Ray Data stats\n",
    "from ray.data.context import DataContext\n",
    "ctx = DataContext.get_current()\n",
    "ctx.enable_progress_bars = True\n",
    "\n",
    "# Profile execution\n",
    "import time\n",
    "start_time = time.time()\n",
    "results = dataset.map_batches(OptimizedInference, batch_size=64)\n",
    "results.take_all()\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Total time: {end_time - start_time:.2f}s\")\n",
    "print(\"Check Ray dashboard for detailed metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1845d68c",
   "metadata": {},
   "source": [
    "### **Memory Management**\n",
    "\n",
    "Monitor and optimize memory usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec5df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check object store usage\n",
    "print(f\"Object store memory: {ray.cluster_resources()}\")\n",
    "\n",
    "# Monitor during execution\n",
    "def memory_efficient_inference():\n",
    "    results = dataset.map_batches(\n",
    "        OptimizedInference,\n",
    "        batch_size=64,\n",
    "        # Limit memory usage\n",
    "        max_concurrency=2\n",
    "    )\n",
    "    \n",
    "    # Process in chunks to avoid memory buildup\n",
    "    for batch in results.iter_batches(batch_size=1000):\n",
    "        process_batch(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18a0f39",
   "metadata": {},
   "source": [
    "## Implementation Comparison\n",
    "\n",
    "The optimizations demonstrate several key improvements:\n",
    "\n",
    "### **Configuration Improvements**\n",
    "\n",
    "**Resource Allocation:**\n",
    "- **Before**: No GPU usage, inefficient CPU processing\n",
    "- **After**: Proper GPU allocation and utilization\n",
    "\n",
    "**Batch Processing:**\n",
    "- **Before**: Tiny batches (size=2) with individual processing\n",
    "- **After**: Optimal batch sizes (size=64) with vectorized operations\n",
    "\n",
    "**Model Management:**\n",
    "- **Before**: Model reloaded for every batch\n",
    "- **After**: Model loaded once per actor and reused\n",
    "\n",
    "### **Data Processing Improvements**\n",
    "\n",
    "**Memory Usage:**\n",
    "- **Before**: Inefficient memory patterns, repeated allocations\n",
    "- **After**: Optimized memory usage with proper batching\n",
    "\n",
    "**I/O Efficiency:**\n",
    "- **Before**: Slow JSON serialization for outputs\n",
    "- **After**: Fast Parquet format for structured data\n",
    "\n",
    "**Resource Utilization:**\n",
    "- **Before**: Poor resource utilization, task contention\n",
    "- **After**: Balanced resource allocation and parallelism\n",
    "\n",
    "## Ray Data Best Practices Summary\n",
    "\n",
    "### **Essential Optimization Checklist**\n",
    "\n",
    "** Model Loading**\n",
    "- [ ] Load models in `__init__`, not `__call__`\n",
    "- [ ] Use `compute=\"actors\"` for model persistence\n",
    "- [ ] Initialize transforms once per actor\n",
    "\n",
    "** Resource Configuration**  \n",
    "- [ ] Set `num_gpus=1` for GPU inference\n",
    "- [ ] Match `concurrency` to available GPUs\n",
    "- [ ] Use appropriate `batch_size` for GPU memory\n",
    "\n",
    "** Data Processing**\n",
    "- [ ] Process batches, not individual samples\n",
    "- [ ] Use vectorized operations when possible\n",
    "- [ ] Optimize block size with `override_num_blocks`\n",
    "\n",
    "** Output Optimization**\n",
    "- [ ] Use Parquet for structured output\n",
    "- [ ] Avoid memory-intensive formats (pickle)\n",
    "- [ ] Stream results when possible\n",
    "\n",
    "** Monitoring**\n",
    "- [ ] Monitor GPU utilization (`nvidia-smi`)\n",
    "- [ ] Check Ray dashboard for task metrics\n",
    "- [ ] Profile memory usage and bottlenecks\n",
    "\n",
    "### **Common Anti-Patterns to Avoid**\n",
    "\n",
    "**🚫 Never Do This:**\n",
    "- Loading models inside inference functions\n",
    "- Using CPU when GPU is available\n",
    "- Processing samples individually in batches\n",
    "- Setting concurrency higher than available resources\n",
    "- Ignoring batch size optimization\n",
    "- Using inefficient output formats\n",
    "\n",
    "** Always Do This:**\n",
    "- Initialize expensive resources once per actor\n",
    "- Use GPU acceleration when available\n",
    "- Leverage vectorized batch processing\n",
    "- Match concurrency to hardware resources\n",
    "- Optimize batch size for memory and throughput\n",
    "- Use efficient data formats (Parquet)\n",
    "\n",
    "### **Production Deployment Tips**\n",
    "\n",
    "**Cluster Configuration:**\n",
    "- Size clusters based on inference requirements\n",
    "- Use GPU instances for neural network inference\n",
    "- Monitor resource utilization and scale accordingly\n",
    "\n",
    "**Monitoring and Alerting:**\n",
    "- Set up throughput monitoring\n",
    "- Alert on performance degradation\n",
    "- Track cost per inference for optimization\n",
    "\n",
    "**Scaling Strategies:**\n",
    "- Start with single-node optimization\n",
    "- Scale horizontally with additional GPU nodes\n",
    "- Use Ray autoscaling for variable workloads\n",
    "\n",
    "This comprehensive guide should help you avoid common pitfalls and achieve optimal performance with Ray Data batch inference pipelines. The key is understanding the underlying architecture and applying systematic optimization techniques."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
