# Ray Data Batch Inference Optimization - Cluster Configurations
# This file shows different cluster configurations for various use cases

# Local Development Cluster (CPU-only)
local_cpu:
  description: "Local development cluster with CPU-only resources"
  ray_start_command: "ray start --head --num-cpus=4 --num-gpus=0"
  resources:
    cpu: 4
    gpu: 0
    memory: "8GB"
  use_case: "Development, testing, small datasets"
  notes: "Good for learning and development. Limited parallelism."

# Local GPU Cluster
local_gpu:
  description: "Local development cluster with GPU resources"
  ray_start_command: "ray start --head --num-cpus=8 --num-gpus=1"
  resources:
    cpu: 8
    gpu: 1
    memory: "16GB"
  use_case: "Development with GPU acceleration, medium datasets"
  notes: "Good for GPU development and testing. Single GPU limits parallelism."

# Production CPU Cluster
production_cpu:
  description: "Production cluster with CPU-only resources"
  ray_start_command: "ray start --head --num-cpus=32 --num-gpus=0"
  resources:
    cpu: 32
    gpu: 0
    memory: "128GB"
  use_case: "Production workloads, large datasets, cost-sensitive deployments"
  notes: "High parallelism, good for CPU-optimized workloads. Lower cost than GPU."

# Production GPU Cluster
production_gpu:
  description: "Production cluster with GPU resources"
  ray_start_command: "ray start --head --num-cpus=64 --num-gpus=8"
  resources:
    cpu: 64
    gpu: 8
    memory: "256GB"
  use_case: "Production AI workloads, real-time inference, large-scale processing"
  notes: "High performance for AI workloads. Higher cost but better throughput."

# Hybrid Cluster (Mixed GPU/CPU)
hybrid_cluster:
  description: "Hybrid cluster with mixed GPU and CPU resources"
  ray_start_command: "ray start --head --num-cpus=32 --num-gpus=4"
  resources:
    cpu: 32
    gpu: 4
    memory: "128GB"
  use_case: "Mixed workloads, GPU-intensive tasks with CPU preprocessing"
  notes: "Balanced approach. GPU for inference, CPU for preprocessing."

# Cloud Cluster Examples

# AWS EC2 Cluster
aws_cluster:
  description: "AWS EC2 cluster configuration"
  instance_types:
    head: "m5.2xlarge"  # 8 vCPU, 32GB RAM
    workers: "m5.4xlarge"  # 16 vCPU, 64GB RAM
    gpu_workers: "g4dn.xlarge"  # 4 vCPU, 16GB RAM, 1 GPU
  resources:
    cpu: "16-64 vCPU"
    gpu: "0-8 GPU"
    memory: "64-256GB"
  use_case: "Cloud deployment, auto-scaling, cost optimization"
  notes: "Use spot instances for cost savings. Auto-scaling based on workload."

# GCP Cluster
gcp_cluster:
  description: "Google Cloud Platform cluster configuration"
  instance_types:
    head: "n2-standard-8"  # 8 vCPU, 32GB RAM
    workers: "n2-standard-16"  # 16 vCPU, 64GB RAM
    gpu_workers: "n1-standard-4"  # 4 vCPU, 15GB RAM + GPU
  resources:
    cpu: "16-64 vCPU"
    gpu: "0-8 GPU"
    memory: "64-256GB"
  use_case: "GCP deployment, integration with other GCP services"
  notes: "Good integration with BigQuery, Cloud Storage. Preemptible instances for cost savings."

# Azure Cluster
azure_cluster:
  description: "Microsoft Azure cluster configuration"
  instance_types:
    head: "Standard_D8s_v3"  # 8 vCPU, 32GB RAM
    workers: "Standard_D16s_v3"  # 16 vCPU, 64GB RAM
    gpu_workers: "Standard_NC6s_v3"  # 6 vCPU, 112GB RAM + GPU
  resources:
    cpu: "16-64 vCPU"
    gpu: "0-8 GPU"
    memory: "64-256GB"
  use_case: "Azure deployment, integration with Azure ML services"
  notes: "Good integration with Azure ML, Data Factory. Hybrid benefit eligible."

# Configuration Recommendations

recommendations:
  development:
    - "Start with local_cpu for learning and development"
    - "Use local_gpu if you have GPU hardware available"
    - "Keep cluster size small for quick iteration"
  
  production:
    - "Use production_gpu for AI/ML workloads with high throughput requirements"
    - "Use production_cpu for cost-sensitive deployments or CPU-optimized workloads"
    - "Consider hybrid_cluster for mixed workloads"
  
  cloud:
    - "Use spot/preemptible instances for cost savings on non-critical workloads"
    - "Implement auto-scaling based on workload demands"
    - "Choose regions close to your data for better I/O performance"
  
  optimization:
    - "GPU clusters: Focus on batch size and GPU memory optimization"
    - "CPU clusters: Focus on parallelism and memory efficiency"
    - "All clusters: Monitor resource utilization and adjust configuration accordingly"

# Environment Variables for Configuration
environment_variables:
  RAY_HEAD_NODE_IP: "127.0.0.1"  # Change for remote clusters
  RAY_REDIS_PASSWORD: ""  # Set for production security
  RAY_OBJECT_STORE_MEMORY: "1000000000"  # 1GB in bytes
  RAY_OBJECT_STORE_ALLOW_SLOW_STORAGE: "true"  # Allow disk spilling

# Performance Tuning Parameters
performance_tuning:
  batch_size:
    gpu: 128  # Optimal for GPU memory
    cpu: 64   # Optimal for CPU memory
  
  num_workers:
    gpu: "min(gpu_count * 2, cpu_count)"  # 2 workers per GPU
    cpu: "max(4, cpu_count // 2)"        # Half of available CPUs
  
  memory_per_worker:
    gpu: "2GB"  # Higher for GPU workers
    cpu: "1GB"  # Lower for CPU workers
  
  object_store_memory:
    percentage: "35%"  # 35% of total cluster memory
    min_gb: "2GB"      # Minimum 2GB
    max_gb: "64GB"     # Maximum 64GB
