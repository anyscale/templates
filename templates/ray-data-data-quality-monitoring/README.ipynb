{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e85502",
   "metadata": {},
   "source": [
    "# Data Quality Monitoring with Ray Data\n",
    "\n",
    "**Time to complete**: 25 min | **Difficulty**: Intermediate | **Prerequisites**: Data analysis experience, understanding of data quality concepts\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "Create an automated data quality monitoring system that continuously validates data, detects anomalies, and ensures your data pipelines produce reliable, trustworthy results - essential for any data-driven organization.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Data Quality Setup](#step-1-creating-test-data) (6 min)\n",
    "2. [Quality Validation](#step-2-automated-quality-checks) (8 min)\n",
    "3. [Anomaly Detection](#step-3-data-drift-monitoring) (7 min)\n",
    "4. [Quality Dashboard](#step-4-quality-reporting) (4 min)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this tutorial, you'll understand:\n",
    "\n",
    "- **Why data quality matters**: How poor data quality costs organizations millions annually\n",
    "- **Ray Data's quality capabilities**: Automate quality checks across large datasets using distributed processing\n",
    "- **Real-world applications**: How companies like Netflix and Airbnb ensure data reliability at scale\n",
    "- **Quality frameworks**: Implement comprehensive data validation and monitoring systems\n",
    "\n",
    "## Overview\n",
    "\n",
    "**The Challenge**: Poor data quality significantly impacts business decisions and organizational efficiency. Data quality issues can lead to incorrect insights and operational problems.\n",
    "\n",
    "**The Solution**: Ray Data enables continuous, automated data quality monitoring at scale, catching issues before they impact business decisions.\n",
    "\n",
    "**Real-world Impact**:\n",
    "- **Financial Services**: Banks prevent fraud by monitoring transaction data quality in real-time\n",
    "- **E-commerce**: Retailers ensure product catalog accuracy for better customer experience\n",
    "- **Healthcare**: Hospitals validate patient data quality for accurate diagnosis and treatment\n",
    "- **Analytics**: Data teams ensure reliable insights by monitoring data pipeline quality\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites Checklist\n",
    "\n",
    "Before starting, ensure you have:\n",
    "- [ ] Understanding of data quality concepts (completeness, accuracy, consistency)\n",
    "- [ ] Experience with data validation and testing\n",
    "- [ ] Familiarity with statistical concepts for anomaly detection\n",
    "- [ ] Python environment with data processing libraries\n",
    "\n",
    "## Quick Start (3 minutes)\n",
    "\n",
    "Want to see data quality monitoring immediately? This section demonstrates core data quality concepts in just a few minutes.\n",
    "\n",
    "### Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9655fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize Ray for distributed processing\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b21d1b",
   "metadata": {},
   "source": [
    "### Create Sample Dataset with Quality Issues\n",
    "\n",
    "We'll create a realistic dataset that contains common data quality issues found in real-world data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5617829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data generation parameters\n",
    "print(\"Creating sample dataset with quality issues...\")\n",
    "np.random.seed(42)  # For reproducible results\n",
    "\n",
    "# Define data quality issue rates (realistic percentages)\n",
    "MISSING_AGE_RATE = 0.10      # 10% missing ages\n",
    "INVALID_INCOME_RATE = 0.05   # 5% invalid income values\n",
    "INVALID_EMAIL_RATE = 0.08    # 8% invalid email formats\n",
    "MISSING_CATEGORY_RATE = 0.12 # 12% missing categories\n",
    "OUTLIER_SCORE_RATE = 0.03    # 3% score outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eca71bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate realistic customer data with quality issues\n",
    "data = []\n",
    "for i in range(10000):  # 10K records for meaningful analysis\n",
    "    record = {\n",
    "        \"customer_id\": f\"CUST_{i:05d}\",\n",
    "        \"age\": np.random.randint(18, 80) if np.random.random() > MISSING_AGE_RATE else None,\n",
    "        \"income\": np.random.normal(50000, 20000) if np.random.random() > INVALID_INCOME_RATE else -1,\n",
    "        \"email\": f\"user{i}@example.com\" if np.random.random() > INVALID_EMAIL_RATE else \"invalid_email\",\n",
    "        \"category\": np.random.choice([\"A\", \"B\", \"C\"]) if np.random.random() > MISSING_CATEGORY_RATE else None,\n",
    "        \"score\": np.random.uniform(0, 100) if np.random.random() > OUTLIER_SCORE_RATE else 999,\n",
    "        \"timestamp\": pd.Timestamp.now() - pd.Timedelta(days=np.random.randint(0, 365))\n",
    "    }\n",
    "    data.append(record)\n",
    "\n",
    "print(f\"Generated {len(data):,} customer records with intentional quality issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7696a048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Ray Dataset using native from_items operation\n",
    "ds = ray.data.from_items(data)\n",
    "print(f\"Created Ray Dataset with {ds.count():,} records for quality monitoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0619a4a",
   "metadata": {},
   "source": [
    "**What we created:**\n",
    "- 10,000 customer records with realistic data patterns\n",
    "- Intentional quality issues: missing values, invalid data, outliers  \n",
    "- Ray Dataset ready for distributed quality analysis\n",
    "\n",
    "### Quick Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9d3b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick data quality assessment\n",
    "sample_data = ds.take(5)\n",
    "\n",
    "print(\"Sample Data Quality Preview:\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"{'Customer ID':<12} {'Age':<5} {'Income':<10} {'Email':<20} {'Category':<10} {'Score':<8}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for record in sample_data:\n",
    "    age = str(record.get('age', 'NULL'))\n",
    "    income = f\"${record.get('income', 0):,.0f}\" if record.get('income', 0) > 0 else \"INVALID\"\n",
    "    email = record.get('email', 'NULL')[:18] + \"...\" if len(record.get('email', '')) > 20 else record.get('email', 'NULL')\n",
    "    category = record.get('category', 'NULL')\n",
    "    score = f\"{record.get('score', 0):.1f}\"\n",
    "    \n",
    "    print(f\"{record['customer_id']:<12} {age:<5} {income:<10} {email:<20} {category:<10} {score:<8}\")\n",
    "\n",
    "print(\"-\" * 100)\n",
    "print(\"Notice: Some records have missing (NULL) or invalid values - this is intentional for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e61fd78",
   "metadata": {},
   "source": [
    "### Import Visualization Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96407e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import visualization libraries for quality dashboards\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up plotting style for professional visualizations\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Visualization libraries loaded - ready for quality analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e0c2f5",
   "metadata": {},
   "source": [
    "## Why Data Quality Monitoring is Critical\n",
    "\n",
    "**The Cost of Poor Data Quality**:\n",
    "- **Business Impact**: 1 in 3 business leaders don't trust their data for decision-making\n",
    "- **Financial Loss**: Organizations lose $12.9M annually on average due to bad data\n",
    "- **Operational Issues**: 40% of business decisions use stale or outdated data\n",
    "- **Customer Impact**: Poor data quality leads to bad recommendations and customer churn\n",
    "\n",
    "**The Scale of the Problem:**\n",
    "- **Data Volume Growth**: Enterprise data grows 40-60% annually\n",
    "- **Source Proliferation**: Average enterprise has 400+ data sources\n",
    "- **Quality Degradation**: Data quality degrades 2% monthly without active monitoring\n",
    "- **Business Impact**: 1 in 3 business leaders don't trust their data for decision-making\n",
    "\n",
    "**Common Data Quality Issues:**\n",
    "- **Completeness**: 15-25% of enterprise data has missing values\n",
    "- **Accuracy**: 10-20% of data contains errors or inconsistencies\n",
    "- **Consistency**: Schema changes break 30% of downstream applications\n",
    "- **Timeliness**: 40% of business decisions use stale or outdated data\n",
    "\n",
    "### **Ray Data's Data Quality Advantages**\n",
    "\n",
    "Ray Data revolutionizes data quality monitoring by providing:\n",
    "\n",
    "| Traditional Approach | Ray Data Approach | Key Difference |\n",
    "|---------------------|-------------------|----------------|\n",
    "| **Batch quality checks** | Continuous monitoring | Real-time quality insights |\n",
    "| **Single-machine validation** | Distributed validation | Horizontal scalability |\n",
    "| **Manual rule creation** | Automated pattern detection | Streamlined rule development |\n",
    "| **Point-in-time analysis** | Historical trend tracking | Comprehensive quality management |\n",
    "| **Siloed quality tools** | Integrated data pipeline | Unified data operations |\n",
    "\n",
    "### **Enterprise Data Quality Framework**\n",
    "\n",
    "This template implements a comprehensive data quality framework based on industry best practices:\n",
    "\n",
    "**The Six Pillars of Data Quality:**\n",
    "\n",
    "1. **Completeness** (25% of quality score)\n",
    "   - Missing value detection and analysis\n",
    "   - Coverage assessment across data sources\n",
    "   - Null pattern identification and trends\n",
    "\n",
    "2. **Accuracy** (25% of quality score)\n",
    "   - Business rule validation and enforcement\n",
    "   - Format and range validation\n",
    "   - Cross-reference verification\n",
    "\n",
    "3. **Consistency** (20% of quality score)\n",
    "   - Schema compliance monitoring\n",
    "   - Data type validation\n",
    "   - Referential integrity checks\n",
    "\n",
    "4. **Timeliness** (15% of quality score)\n",
    "   - Data freshness monitoring\n",
    "   - Update frequency analysis\n",
    "   - Staleness detection and alerting\n",
    "\n",
    "5. **Validity** (10% of quality score)\n",
    "   - Domain-specific validation rules\n",
    "   - Constraint checking\n",
    "   - Business logic compliance\n",
    "\n",
    "6. **Uniqueness** (5% of quality score)\n",
    "   - Duplicate detection and analysis\n",
    "   - Primary key validation\n",
    "   - Record deduplication\n",
    "\n",
    "### **Business Impact and ROI**\n",
    "\n",
    "Organizations implementing comprehensive data quality monitoring see:\n",
    "\n",
    "| Quality Aspect | Traditional Approach | Ray Data Approach | Key Benefit |\n",
    "|---------------|---------------------|-------------------|-------------|\n",
    "| **Monitoring Scope** | Limited sample checking | Comprehensive full-dataset analysis | Complete visibility |\n",
    "| **Detection Method** | Manual rule checking | Automated pattern detection | Systematic identification |\n",
    "| **Processing Scale** | Single-machine analysis | Distributed processing | Horizontal scalability |\n",
    "| **Issue Response** | Reactive detection | Proactive monitoring | Earlier intervention |\n",
    "| **Resource Focus** | Manual quality tasks | Automated workflows | Strategic optimization |\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this template, you'll understand:\n",
    "- How to implement automated data quality checks\n",
    "- Schema validation and business rule enforcement\n",
    "- Statistical anomaly detection and monitoring\n",
    "- Building scalable data quality pipelines\n",
    "- Integration with monitoring and alerting systems\n",
    "\n",
    "## Use Case: Enterprise Data Quality Monitoring\n",
    "\n",
    "We'll build a pipeline that monitors:\n",
    "- **Data Completeness**: Missing values, null rates, data coverage\n",
    "- **Data Accuracy**: Value validation, range checks, format compliance\n",
    "- **Data Consistency**: Schema compliance, data type validation\n",
    "- **Data Freshness**: Timeliness, update frequency, staleness detection\n",
    "- **Data Integrity**: Referential integrity, constraint validation\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "Data Sources → Ray Data → Quality Checks → Validation Engine → Monitoring → Alerts\n",
    "     ↓           ↓           ↓              ↓                ↓          ↓\n",
    "  Databases   Parallel    Schema Check    Business Rules   Metrics    Notifications\n",
    "  Files       Processing  Completeness    Anomaly Detection  Scoring   Dashboards\n",
    "  APIs        GPU Workers  Accuracy       Drift Detection   Reports   APIs\n",
    "  Streams     Validation   Consistency    Integrity Check   Trends    Actions\n",
    "```\n",
    "\n",
    "## Key Components\n",
    "\n",
    "### 1. **Data Quality Checks**\n",
    "- Schema validation and type checking\n",
    "- Completeness and coverage analysis\n",
    "- Accuracy and range validation\n",
    "- Consistency and integrity verification\n",
    "\n",
    "### 2. **Statistical Monitoring**\n",
    "- Distribution analysis and drift detection\n",
    "- Outlier detection and anomaly identification\n",
    "- Trend analysis and pattern recognition\n",
    "- Statistical significance testing\n",
    "\n",
    "### 3. **Business Rule Engine**\n",
    "- Custom validation rules and constraints\n",
    "- Domain-specific quality requirements\n",
    "- Regulatory compliance checking\n",
    "- Automated rule generation and testing\n",
    "\n",
    "### 4. **Quality Scoring and Reporting**\n",
    "- Comprehensive quality metrics\n",
    "- Trend analysis and historical tracking\n",
    "- Automated alerting and notifications\n",
    "- Quality improvement recommendations\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Ray cluster with data processing capabilities\n",
    "- Python 3.8+ with data quality libraries\n",
    "- Access to data sources for monitoring\n",
    "- Basic understanding of data quality concepts\n",
    "\n",
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc24142",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install ray[data] pandas numpy great-expectations\n",
    "pip install scikit-learn scipy statsmodels\n",
    "pip install plotly dash streamlit\n",
    "pip install pyarrow boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0539c528",
   "metadata": {},
   "source": [
    "## Quick Start\n",
    "\n",
    "### 1. **Load Real Enterprise Data for Quality Monitoring**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb737a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.data import read_parquet, read_csv\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Ray\n",
    "ray.init()\n",
    "\n",
    "# Load real enterprise datasets from public sources\n",
    "# NYC Taxi data - publicly available\n",
    "taxi_data = read_parquet(\"s3://anonymous@nyc-tlc/trip_data/yellow_tripdata_2023-01.parquet\")\n",
    "\n",
    "# US Government spending data - publicly available\n",
    "spending_data = read_csv(\"s3://anonymous@usaspending-gov/download_center/Custom_Account_Data.csv\")\n",
    "\n",
    "# Public company financial data - publicly available\n",
    "financial_data = read_parquet(\"s3://anonymous@sec-edgar/financial_statements/2023/\")\n",
    "\n",
    "# Healthcare provider data - publicly available (CMS)\n",
    "healthcare_data = read_csv(\"s3://anonymous@cms-gov/provider-data/Physician_Compare_National_Downloadable_File.csv\")\n",
    "\n",
    "print(f\"Taxi data: {taxi_data.count()}\")\n",
    "print(f\"Spending data: {spending_data.count()}\")\n",
    "print(f\"Financial data: {financial_data.count()}\")\n",
    "print(f\"Healthcare data: {healthcare_data.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d99674",
   "metadata": {},
   "source": [
    "### 2. **Schema Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e696de6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List\n",
    "import json\n",
    "\n",
    "class SchemaValidator:\n",
    "    \"\"\"Validate data schema and structure.\"\"\"\n",
    "    \n",
    "    def __init__(self, expected_schema: Dict[str, Any]):\n",
    "        self.expected_schema = expected_schema\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Validate schema for a batch of data.\"\"\"\n",
    "        validation_results = []\n",
    "        \n",
    "        for item in batch:\n",
    "            try:\n",
    "                # Check required columns\n",
    "                missing_columns = []\n",
    "                extra_columns = []\n",
    "                \n",
    "                for expected_col in self.expected_schema[\"required_columns\"]:\n",
    "                    if expected_col not in item:\n",
    "                        missing_columns.append(expected_col)\n",
    "                \n",
    "                for actual_col in item.keys():\n",
    "                    if actual_col not in self.expected_schema[\"allowed_columns\"]:\n",
    "                        extra_columns.append(actual_col)\n",
    "                \n",
    "                # Check data types\n",
    "                type_violations = []\n",
    "                for col, expected_type in self.expected_schema[\"column_types\"].items():\n",
    "                    if col in item:\n",
    "                        actual_value = item[col]\n",
    "                        if not self._check_type(actual_value, expected_type):\n",
    "                            type_violations.append({\n",
    "                                \"column\": col,\n",
    "                                \"expected_type\": expected_type,\n",
    "                                \"actual_value\": str(actual_value)[:100]\n",
    "                            })\n",
    "                \n",
    "                # Create validation result\n",
    "                validation_result = {\n",
    "                    \"record_id\": item.get(\"id\", \"unknown\"),\n",
    "                    \"schema_valid\": len(missing_columns) == 0 and len(type_violations) == 0,\n",
    "                    \"missing_columns\": missing_columns,\n",
    "                    \"extra_columns\": extra_columns,\n",
    "                    \"type_violations\": type_violations,\n",
    "                    \"validation_timestamp\": pd.Timestamp.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                validation_results.append(validation_result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                validation_results.append({\n",
    "                    \"record_id\": item.get(\"id\", \"unknown\"),\n",
    "                    \"schema_valid\": False,\n",
    "                    \"error\": str(e),\n",
    "                    \"validation_timestamp\": pd.Timestamp.now().isoformat()\n",
    "                })\n",
    "        \n",
    "        return {\"schema_validation\": validation_results}\n",
    "    \n",
    "    def _check_type(self, value, expected_type):\n",
    "        \"\"\"Check if value matches expected type.\"\"\"\n",
    "        try:\n",
    "            if expected_type == \"string\":\n",
    "                return isinstance(value, str)\n",
    "            elif expected_type == \"integer\":\n",
    "                return isinstance(value, int) or (isinstance(value, float) and value.is_integer())\n",
    "            elif expected_type == \"float\":\n",
    "                return isinstance(value, (int, float))\n",
    "            elif expected_type == \"boolean\":\n",
    "                return isinstance(value, bool)\n",
    "            elif expected_type == \"datetime\":\n",
    "                return pd.api.types.is_datetime64_any_dtype(value) or isinstance(value, pd.Timestamp)\n",
    "            else:\n",
    "                return True  # Unknown type, assume valid\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "# Define expected schema\n",
    "customer_schema = {\n",
    "    \"required_columns\": [\"customer_id\", \"name\", \"email\", \"registration_date\"],\n",
    "    \"allowed_columns\": [\"customer_id\", \"name\", \"email\", \"registration_date\", \"phone\", \"address\"],\n",
    "    \"column_types\": {\n",
    "        \"customer_id\": \"string\",\n",
    "        \"name\": \"string\",\n",
    "        \"email\": \"string\",\n",
    "        \"registration_date\": \"datetime\",\n",
    "        \"phone\": \"string\",\n",
    "        \"address\": \"string\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Apply schema validation\n",
    "schema_validation = customer_data.map_batches(\n",
    "    SchemaValidator(customer_schema),\n",
    "    batch_size=1000,\n",
    "    concurrency=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e89c95",
   "metadata": {},
   "source": [
    "### 3. **Data Completeness Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e0ea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompletenessAnalyzer:\n",
    "    \"\"\"Analyze data completeness and missing value patterns.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.completeness_metrics = {}\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Analyze completeness for a batch of data.\"\"\"\n",
    "        if not batch:\n",
    "            return {\"completeness_analysis\": {}}\n",
    "        \n",
    "        # Convert batch to DataFrame for easier analysis\n",
    "        df = pd.DataFrame(batch)\n",
    "        \n",
    "        # Calculate completeness metrics\n",
    "        total_records = len(df)\n",
    "        completeness_metrics = {}\n",
    "        \n",
    "        for column in df.columns:\n",
    "            non_null_count = df[column].notna().sum()\n",
    "            null_count = df[column].isna().sum()\n",
    "            \n",
    "            completeness_metrics[column] = {\n",
    "                \"total_records\": total_records,\n",
    "                \"non_null_count\": int(non_null_count),\n",
    "                \"null_count\": int(null_count),\n",
    "                \"completeness_rate\": float(non_null_count / total_records),\n",
    "                \"missing_rate\": float(null_count / total_records)\n",
    "            }\n",
    "        \n",
    "        # Calculate overall completeness\n",
    "        overall_completeness = np.mean([metrics[\"completeness_rate\"] for metrics in completeness_metrics.values()])\n",
    "        \n",
    "        return {\n",
    "            \"completeness_analysis\": {\n",
    "                \"overall_completeness\": overall_completeness,\n",
    "                \"column_metrics\": completeness_metrics,\n",
    "                \"analysis_timestamp\": pd.Timestamp.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Apply completeness analysis\n",
    "completeness_analysis = customer_data.map_batches(\n",
    "    CompletenessAnalyzer(),\n",
    "    batch_size=1000,\n",
    "    concurrency=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bb8884",
   "metadata": {},
   "source": [
    "### 4. **Data Accuracy Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df2257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "class AccuracyValidator:\n",
    "    \"\"\"Validate data accuracy and business rules.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.validation_rules = {\n",
    "            \"email\": r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\",\n",
    "            \"phone\": r\"^\\+?1?\\d{9,15}$\",\n",
    "            \"customer_id\": r\"^CUST\\d{6}$\"\n",
    "        }\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Validate data accuracy for a batch.\"\"\"\n",
    "        accuracy_results = []\n",
    "        \n",
    "        for item in batch:\n",
    "            try:\n",
    "                validation_errors = []\n",
    "                \n",
    "                # Email validation\n",
    "                if \"email\" in item and item[\"email\"]:\n",
    "                    if not re.match(self.validation_rules[\"email\"], str(item[\"email\"])):\n",
    "                        validation_errors.append(\"Invalid email format\")\n",
    "                \n",
    "                # Phone validation\n",
    "                if \"phone\" in item and item[\"phone\"]:\n",
    "                    if not re.match(self.validation_rules[\"phone\"], str(item[\"phone\"])):\n",
    "                        validation_errors.append(\"Invalid phone format\")\n",
    "                \n",
    "                # Customer ID validation\n",
    "                if \"customer_id\" in item and item[\"customer_id\"]:\n",
    "                    if not re.match(self.validation_rules[\"customer_id\"], str(item[\"customer_id\"])):\n",
    "                        validation_errors.append(\"Invalid customer ID format\")\n",
    "                \n",
    "                # Date validation\n",
    "                if \"registration_date\" in item and item[\"registration_date\"]:\n",
    "                    try:\n",
    "                        registration_date = pd.to_datetime(item[\"registration_date\"])\n",
    "                        if registration_date > pd.Timestamp.now():\n",
    "                            validation_errors.append(\"Registration date cannot be in the future\")\n",
    "                    except:\n",
    "                        validation_errors.append(\"Invalid date format\")\n",
    "                \n",
    "                # Business rule: Customer ID must be unique (simplified check)\n",
    "                # In production, you'd check against a reference dataset\n",
    "                \n",
    "                accuracy_result = {\n",
    "                    \"record_id\": item.get(\"id\", \"unknown\"),\n",
    "                    \"is_accurate\": len(validation_errors) == 0,\n",
    "                    \"validation_errors\": validation_errors,\n",
    "                    \"validation_timestamp\": pd.Timestamp.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                accuracy_results.append(accuracy_result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                accuracy_results.append({\n",
    "                    \"record_id\": item.get(\"id\", \"unknown\"),\n",
    "                    \"is_accurate\": False,\n",
    "                    \"error\": str(e),\n",
    "                    \"validation_timestamp\": pd.Timestamp.now().isoformat()\n",
    "                })\n",
    "        \n",
    "        return {\"accuracy_validation\": accuracy_results}\n",
    "\n",
    "# Apply accuracy validation\n",
    "accuracy_validation = customer_data.map_batches(\n",
    "    AccuracyValidator(),\n",
    "    batch_size=1000,\n",
    "    concurrency=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2208a3cc",
   "metadata": {},
   "source": [
    "### 5. **Statistical Anomaly Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f715683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "class AnomalyDetector:\n",
    "    \"\"\"Detect statistical anomalies in data.\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=3.0):\n",
    "        self.threshold = threshold\n",
    "        self.statistical_metrics = {}\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Detect anomalies in a batch of data.\"\"\"\n",
    "        if not batch:\n",
    "            return {\"anomaly_detection\": {}}\n",
    "        \n",
    "        # Convert batch to DataFrame\n",
    "        df = pd.DataFrame(batch)\n",
    "        \n",
    "        anomaly_results = {}\n",
    "        \n",
    "        for column in df.select_dtypes(include=[np.number]).columns:\n",
    "            values = df[column].dropna()\n",
    "            \n",
    "            if len(values) > 0:\n",
    "                # Calculate statistical metrics\n",
    "                mean_val = np.mean(values)\n",
    "                std_val = np.std(values)\n",
    "                \n",
    "                # Detect outliers using Z-score\n",
    "                z_scores = np.abs(stats.zscore(values))\n",
    "                outliers = values[z_scores > self.threshold]\n",
    "                \n",
    "                # Calculate outlier percentage\n",
    "                outlier_percentage = len(outliers) / len(values) * 100\n",
    "                \n",
    "                anomaly_results[column] = {\n",
    "                    \"mean\": float(mean_val),\n",
    "                    \"std\": float(std_val),\n",
    "                    \"outlier_count\": int(len(outliers)),\n",
    "                    \"outlier_percentage\": float(outlier_percentage),\n",
    "                    \"is_anomalous\": outlier_percentage > 5.0  # More than 5% outliers\n",
    "                }\n",
    "        \n",
    "        return {\n",
    "            \"anomaly_detection\": {\n",
    "                \"statistical_metrics\": anomaly_results,\n",
    "                \"detection_timestamp\": pd.Timestamp.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Apply anomaly detection\n",
    "anomaly_detection = customer_data.map_batches(\n",
    "    AnomalyDetector(threshold=2.5),\n",
    "    batch_size=1000,\n",
    "    concurrency=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263abad9",
   "metadata": {},
   "source": [
    "### 6. **Data Quality Scoring**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f183ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityScorer:\n",
    "    \"\"\"Calculate comprehensive data quality scores.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.quality_weights = {\n",
    "            \"completeness\": 0.3,\n",
    "            \"accuracy\": 0.3,\n",
    "            \"consistency\": 0.2,\n",
    "            \"timeliness\": 0.2\n",
    "        }\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Calculate quality scores for a batch.\"\"\"\n",
    "        # This would typically combine results from multiple validation steps\n",
    "        # For demonstration, we'll create sample quality scores\n",
    "        \n",
    "        quality_scores = {\n",
    "            \"overall_quality_score\": 0.85,\n",
    "            \"completeness_score\": 0.92,\n",
    "            \"accuracy_score\": 0.78,\n",
    "            \"consistency_score\": 0.88,\n",
    "            \"timeliness_score\": 0.82,\n",
    "            \"quality_grade\": \"B\",\n",
    "            \"recommendations\": [\n",
    "                \"Improve email format validation\",\n",
    "                \"Reduce missing phone numbers\",\n",
    "                \"Standardize customer ID format\"\n",
    "            ],\n",
    "            \"scoring_timestamp\": pd.Timestamp.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        return {\"quality_scoring\": quality_scores}\n",
    "\n",
    "# Apply quality scoring\n",
    "quality_scoring = customer_data.map_batches(\n",
    "    QualityScorer(),\n",
    "    batch_size=1000,\n",
    "    concurrency=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2113702e",
   "metadata": {},
   "source": [
    "## Advanced Features\n",
    "\n",
    "### **Ray Data Fault Tolerance**\n",
    "- **Automatic Task Retries**: Failed validation tasks are automatically retried\n",
    "- **Worker Failure Recovery**: Ray Data reschedules tasks when workers fail\n",
    "- **Data Block Replication**: Critical data blocks are replicated for reliability\n",
    "- **RayTurbo Checkpointing**: Job-level checkpointing on Anyscale for long-running quality jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cb3cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Fault-tolerant data quality pipeline\n",
    "from ray.data.context import DataContext\n",
    "\n",
    "# Configure fault tolerance\n",
    "ctx = DataContext.get_current()\n",
    "ctx.enable_auto_log_stats = True\n",
    "\n",
    "# Ray Data automatically handles:\n",
    "# - Task failures and retries\n",
    "# - Worker node failures\n",
    "# - Memory pressure recovery\n",
    "# - Network interruptions\n",
    "\n",
    "# On Anyscale with RayTurbo:\n",
    "# - Job-level checkpointing\n",
    "# - Automatic job recovery\n",
    "# - State preservation across failures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be095f0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Data Drift Detection**\n",
    "- Statistical drift detection methods\n",
    "- Distribution comparison techniques\n",
    "- Automated drift monitoring\n",
    "- Alert generation for significant changes\n",
    "\n",
    "### **Custom Validation Rules**\n",
    "- Domain-specific business rules\n",
    "- Regulatory compliance checking\n",
    "- Automated rule generation\n",
    "- Rule performance monitoring\n",
    "\n",
    "### **Quality Trend Analysis**\n",
    "- Historical quality tracking\n",
    "- Trend identification and forecasting\n",
    "- Quality improvement recommendations\n",
    "- Automated reporting and dashboards\n",
    "\n",
    "## Production Considerations\n",
    "\n",
    "### **Performance Optimization**\n",
    "- Efficient validation algorithms\n",
    "- Parallel processing strategies\n",
    "- Caching and incremental updates\n",
    "- Resource optimization\n",
    "\n",
    "### **Scalability**\n",
    "- Horizontal scaling across nodes\n",
    "- Load balancing for validation workloads\n",
    "- Distributed rule processing\n",
    "- Efficient data partitioning\n",
    "\n",
    "### **Monitoring and Alerting**\n",
    "- Real-time quality monitoring\n",
    "- Automated alert generation\n",
    "- Escalation procedures\n",
    "- Performance tracking\n",
    "\n",
    "## Example Workflows\n",
    "\n",
    "### **Customer Data Quality Monitoring**\n",
    "1. Load customer data from multiple sources\n",
    "2. Validate schema and data types\n",
    "3. Check completeness and accuracy\n",
    "4. Detect anomalies and outliers\n",
    "5. Generate quality reports and alerts\n",
    "\n",
    "### **Financial Data Validation**\n",
    "1. Process transaction and financial data\n",
    "2. Validate business rules and constraints\n",
    "3. Check for fraud indicators\n",
    "4. Monitor data consistency\n",
    "5. Generate compliance reports\n",
    "\n",
    "### **Product Data Quality**\n",
    "1. Validate product catalog data\n",
    "2. Check pricing and inventory accuracy\n",
    "3. Monitor data freshness\n",
    "4. Detect duplicate and invalid entries\n",
    "5. Generate quality improvement recommendations\n",
    "\n",
    "## Performance Analysis\n",
    "\n",
    "### **Data Quality Assessment Framework**\n",
    "\n",
    "| Quality Dimension | Validation Method | Measurement Output | Visualization |\n",
    "|------------------|-------------------|-------------------|---------------|\n",
    "| **Completeness** | Missing value analysis | Completeness scores | Heatmaps |\n",
    "| **Accuracy** | Business rule validation | Error rates | Error distribution |\n",
    "| **Consistency** | Schema compliance | Violation counts | Compliance charts |\n",
    "| **Timeliness** | Freshness analysis | Age metrics | Freshness trends |\n",
    "\n",
    "### **Quality Scoring Methodology**\n",
    "\n",
    "```\n",
    "Data Quality Score Calculation:\n",
    "┌─────────────────┐\n",
    "│ Raw Data Input  │\n",
    "└────────┬────────┘\n",
    "         │\n",
    "         ▼\n",
    "┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n",
    "│ Schema          │    │ Completeness    │    │ Accuracy        │\n",
    "│ Validation      │    │ Analysis        │    │ Validation      │\n",
    "│ (30% weight)    │    │ (25% weight)    │    │ (25% weight)    │\n",
    "└────────┬────────┘    └────────┬────────┘    └────────┬────────┘\n",
    "         │                      │                      │\n",
    "         └──────────────────────┼──────────────────────┘\n",
    "                                │\n",
    "         ┌─────────────────┐    │    ┌─────────────────┐\n",
    "         │ Consistency     │    │    │ Statistical     │\n",
    "         │ Checks          │    ▼    │ Anomaly         │\n",
    "         │ (10% weight)    │    │    │ Detection       │\n",
    "         └────────┬────────┘    │    │ (10% weight)    │\n",
    "                  │             │    └────────┬────────┘\n",
    "                  └─────────────┼─────────────┘\n",
    "                                ▼\n",
    "                      ┌─────────────────┐\n",
    "                      │ Overall Quality │\n",
    "                      │ Score (0-100)   │\n",
    "                      └─────────────────┘\n",
    "```\n",
    "\n",
    "### **Quality Monitoring Dashboard**\n",
    "\n",
    "The template generates comprehensive quality monitoring visualizations:\n",
    "\n",
    "| Dashboard Component | Chart Type | File Output |\n",
    "|-------------------|------------|-------------|\n",
    "| **Quality Scores** | Gauge charts | `quality_dashboard.html` |\n",
    "| **Trend Analysis** | Time series | `quality_trends.html` |\n",
    "| **Issue Distribution** | Bar charts | `quality_issues.html` |\n",
    "| **Data Profiling** | Statistical summaries | `data_profile.html` |\n",
    "\n",
    "### **Expected Quality Metrics Output**\n",
    "\n",
    "```\n",
    "Data Quality Report:\n",
    "┌─────────────────────────────────────────┐\n",
    "│ Overall Quality Score: [Calculated]     │\n",
    "├─────────────────────────────────────────┤\n",
    "│ Completeness Score: [%]                 │\n",
    "│ ├─ Missing Values: [Count]              │\n",
    "│ ├─ Null Percentage: [%]                 │\n",
    "│ └─ Coverage Rate: [%]                   │\n",
    "├─────────────────────────────────────────┤\n",
    "│ Accuracy Score: [%]                     │\n",
    "│ ├─ Format Violations: [Count]           │\n",
    "│ ├─ Range Violations: [Count]            │\n",
    "│ └─ Business Rule Failures: [Count]      │\n",
    "├─────────────────────────────────────────┤\n",
    "│ Consistency Score: [%]                  │\n",
    "│ ├─ Schema Violations: [Count]           │\n",
    "│ ├─ Type Mismatches: [Count]             │\n",
    "│ └─ Constraint Failures: [Count]         │\n",
    "└─────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### **Common Issues**\n",
    "1. **Performance Issues**: Optimize validation rules and batch sizes\n",
    "2. **Memory Issues**: Reduce batch size or optimize data structures\n",
    "3. **False Positives**: Adjust validation thresholds and rules\n",
    "4. **Scalability**: Optimize data partitioning and resource allocation\n",
    "\n",
    "### **Debug Mode**\n",
    "Enable detailed logging and validation debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e86321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# Enable validation debugging\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9db2af5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Customize Rules**: Implement domain-specific validation rules\n",
    "2. **Define Metrics**: Create quality metrics relevant to your use case\n",
    "3. **Build Dashboards**: Create monitoring and alerting systems\n",
    "4. **Scale Production**: Deploy to multi-node clusters\n",
    "\n",
    "### **Security Considerations** (rule #656)\n",
    "\n",
    "**Data Privacy and Security**:\n",
    "- Use encrypted connections when accessing external data sources\n",
    "- Implement proper authentication for data access\n",
    "- Consider data anonymization for sensitive datasets\n",
    "- Follow data retention policies and compliance requirements\n",
    "\n",
    "**Ray Cluster Security**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fb5d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Initialize Ray with security considerations\n",
    "ray.init(\n",
    "    dashboard_port=None,  # Disable dashboard for security\n",
    "    include_dashboard=False,\n",
    "    _temp_dir=\"/secure/temp/path\"  # Use secure temporary directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60d686b",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Ray Data Documentation](https://docs.ray.io/en/latest/data/index.html)\n",
    "- [Great Expectations Documentation](https://docs.greatexpectations.io/)\n",
    "- [pandas Data Validation](https://pandas.pydata.org/docs/user_guide/missing_data.html)\n",
    "- [Data Quality Best Practices](https://docs.ray.io/en/latest/data/best-practices.html)\n",
    "- [Ray Security Documentation](https://docs.ray.io/en/latest/ray-security.html)\n",
    "\n",
    "---\n",
    "\n",
    "*This template provides a foundation for building production-ready data quality monitoring pipelines with Ray Data. Start with the basic examples and gradually add complexity based on your specific data quality requirements.*"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
