{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5388ead",
   "metadata": {},
   "source": [
    "# Geospatial Analysis with Ray Data\n",
    "\n",
    "**Time to complete**: 25 min | **Difficulty**: Intermediate | **Prerequisites**: Basic Python, understanding of coordinates\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "Create a scalable geospatial analysis pipeline that can process millions of location points across entire cities. You'll learn to find nearby businesses, calculate distances, and perform spatial clustering - all at massive scale.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Data Creation](#step-1-setup-and-data-loading) (5 min)\n",
    "2. [Spatial Operations](#step-2-basic-spatial-operations) (8 min)\n",
    "3. [Distance Calculations](#step-3-distance-calculations-at-scale) (7 min)  \n",
    "4. [Visualization and Results](#step-4-visualization-and-analysis) (5 min)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this tutorial, you'll understand:\n",
    "\n",
    "- **Why geospatial processing is hard**: Memory and computation challenges with location data\n",
    "- **Ray Data's spatial capabilities**: Distribute calculations across city-sized datasets\n",
    "- **Real-world applications**: How companies like Uber and DoorDash process location data\n",
    "- **Performance at scale**: Handle millions of coordinates efficiently\n",
    "\n",
    "## Overview\n",
    "\n",
    "**The Challenge**: Traditional geospatial tools struggle with large datasets. Processing millions of GPS coordinates for proximity analysis can take hours or run out of memory.\n",
    "\n",
    "**The Solution**: Ray Data distributes spatial calculations across multiple cores and machines, enabling large-scale geospatial analysis through parallel processing.\n",
    "\n",
    "**Real-world Impact**: \n",
    "- **Ride-sharing**: Find nearest drivers to passengers in real-time\n",
    "- **Retail**: Analyze store locations and customer proximity  \n",
    "- **Healthcare**: Emergency services optimization and resource allocation\n",
    "- **Social apps**: Location-based features and recommendations\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites Checklist\n",
    "\n",
    "Before starting, ensure you have:\n",
    "- [ ] Understanding of latitude/longitude coordinates\n",
    "- [ ] Basic knowledge of distance calculations\n",
    "- [ ] Familiarity with data processing concepts\n",
    "- [ ] Python environment with sufficient memory (4GB+ recommended)\n",
    "\n",
    "## Quick Start (3 minutes)\n",
    "\n",
    "Want to see geospatial processing in action immediately? This section demonstrates core spatial analysis concepts in just a few minutes.\n",
    "\n",
    "### Install Required Packages\n",
    "\n",
    "First, ensure you have the necessary geospatial libraries installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199f0ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install \"ray[data]\" pandas numpy matplotlib seaborn plotly folium geopandas contextily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81f18be",
   "metadata": {},
   "source": [
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02ca29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Ray for distributed processing\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb681bc",
   "metadata": {},
   "source": [
    "### Create Sample Location Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463bcbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample location data for major US cities\n",
    "print(\"Creating sample geospatial dataset...\")\n",
    "\n",
    "# Major US city coordinates\n",
    "major_cities = [\n",
    "    {\"name\": \"New York\", \"lat\": 40.7128, \"lon\": -74.0060},\n",
    "    {\"name\": \"Los Angeles\", \"lat\": 34.0522, \"lon\": -118.2437},\n",
    "    {\"name\": \"Chicago\", \"lat\": 41.8781, \"lon\": -87.6298},\n",
    "    {\"name\": \"Houston\", \"lat\": 29.7604, \"lon\": -95.3698},\n",
    "    {\"name\": \"Phoenix\", \"lat\": 33.4484, \"lon\": -112.0740}\n",
    "]\n",
    "\n",
    "# Generate points around each city (simulating businesses, stops, etc.)\n",
    "locations = []\n",
    "np.random.seed(42)  # For reproducible results\n",
    "\n",
    "for city in major_cities:\n",
    "    for i in range(2000):  # 2000 points per city = 10K total\n",
    "        # Add small random offset to create realistic distribution\n",
    "        lat_offset = np.random.normal(0, 0.1)  # ~11km radius\n",
    "        lon_offset = np.random.normal(0, 0.1)\n",
    "        \n",
    "        location = {\n",
    "            \"location_id\": f\"{city['name'][:3].upper()}_{i:04d}\",\n",
    "            \"city\": city['name'],\n",
    "            \"lat\": city['lat'] + lat_offset,\n",
    "            \"lon\": city['lon'] + lon_offset,\n",
    "            \"type\": np.random.choice([\"restaurant\", \"store\", \"office\", \"hospital\", \"school\"])\n",
    "        }\n",
    "        locations.append(location)\n",
    "\n",
    "ds = ray.data.from_items(locations)\n",
    "print(f\"Created dataset with {ds.count():,} location points across {len(major_cities)} cities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cd3172",
   "metadata": {},
   "source": [
    "### Quick Distance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487b99c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick demonstration of geospatial processing\n",
    "sample_locations = ds.take(5)\n",
    "\n",
    "print(\"Sample Location Data:\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Location ID':<12} {'City':<12} {'Type':<12} {'Latitude':<12} {'Longitude':<12}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for loc in sample_locations:\n",
    "    print(f\"{loc['location_id']:<12} {loc['city']:<12} {loc['type']:<12} {loc['lat']:<12.4f} {loc['lon']:<12.4f}\")\n",
    "\n",
    "print(\"-\" * 90)\n",
    "print(f\"Ready for advanced geospatial analysis with {ds.count():,} location points!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddc2fd4",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Data Loading\n",
    "\n",
    "First, let's set up Ray and load our geospatial datasets. We'll create realistic point-of-interest (POI) data across major metropolitan areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1778887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import folium\n",
    "from folium.plugins import HeatMap, MarkerCluster\n",
    "from typing import Dict, Any\n",
    "import time\n",
    "\n",
    "# Initialize Ray - this creates our distributed computing cluster\n",
    "ray.init()\n",
    "\n",
    "print(\" Ray cluster initialized!\")\n",
    "print(f\" Available resources: {ray.cluster_resources()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45689cf2",
   "metadata": {},
   "source": [
    "Now let's create our geospatial data generation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df401735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_geospatial_data():\n",
    "    \"\"\"\n",
    "    Load geospatial datasets for analysis.\n",
    "    \n",
    "    Returns:\n",
    "        ray.data.Dataset: Dataset containing point-of-interest data with coordinates,\n",
    "                         categories, and ratings for multiple metropolitan areas.\n",
    "                         \n",
    "    Note:\n",
    "        Uses reproducible random seed (42) for consistent results across runs.\n",
    "        Creates realistic POI distributions within major US metropolitan areas.\n",
    "    \"\"\"\n",
    "    print(\"Loading geospatial datasets...\")\n",
    "    \n",
    "    # Create sample POI data for major US metro areas\n",
    "    # Use fixed seed for reproducible results (rule #502)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    metro_areas = {\n",
    "        'NYC': {'lat': 40.7128, 'lon': -74.0060, 'radius': 0.5},\n",
    "        'LA': {'lat': 34.0522, 'lon': -118.2437, 'radius': 0.6},\n",
    "        'Chicago': {'lat': 41.8781, 'lon': -87.6298, 'radius': 0.4}\n",
    "    }\n",
    "    \n",
    "    poi_data = []\n",
    "    categories = ['restaurant', 'retail', 'hospital', 'school', 'bank']\n",
    "    \n",
    "    for metro, coords in metro_areas.items():\n",
    "        for i in range(1000):  # 1000 POIs per metro\n",
    "            angle = np.random.uniform(0, 2 * np.pi)\n",
    "            radius = np.random.uniform(0, coords['radius'])\n",
    "            \n",
    "            lat = coords['lat'] + radius * np.cos(angle)\n",
    "            lon = coords['lon'] + radius * np.sin(angle)\n",
    "            \n",
    "            poi_data.append({\n",
    "                'poi_id': f'{metro}_{i:04d}',\n",
    "                'name': f'Business_{i}',\n",
    "                'category': np.random.choice(categories),\n",
    "                'latitude': round(lat, 6),\n",
    "                'longitude': round(lon, 6),\n",
    "                'metro_area': metro,\n",
    "                'rating': np.random.uniform(1.0, 5.0)\n",
    "            })\n",
    "    \n",
    "    return ray.data.from_items(poi_data)\n",
    "\n",
    "# Load the dataset and measure performance\n",
    "start_time = time.time()\n",
    "poi_dataset = load_geospatial_data()\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "print(f\"â± Data loading took: {load_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de036f8",
   "metadata": {},
   "source": [
    "Inspect the dataset structure and validate our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a53ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(f\" Dataset size: {poi_dataset.count()} records\")\n",
    "print(f\" Schema: {poi_dataset.schema()}\")\n",
    "\n",
    "# Show sample data to verify it looks correct\n",
    "print(\"\\n Sample POI data:\")\n",
    "sample_data = poi_dataset.take(5)\n",
    "for i, poi in enumerate(sample_data):\n",
    "    print(f\"  {i+1}. {poi['name']} ({poi['category']}) at {poi['latitude']:.4f}, {poi['longitude']:.4f}\")\n",
    "\n",
    "# Comprehensive data validation (rule #218: Include comprehensive data validation)\n",
    "print(f\"\\n Data validation:\")\n",
    "\n",
    "# Validate coordinate ranges\n",
    "valid_coords = poi_dataset.filter(\n",
    "    lambda x: x['latitude'] is not None and x['longitude'] is not None and\n",
    "              -90 <= x['latitude'] <= 90 and -180 <= x['longitude'] <= 180\n",
    ").count()\n",
    "\n",
    "print(f\"  - Valid coordinates: {valid_coords} / {poi_dataset.count()}\")\n",
    "print(f\"  - Metro areas covered: {len(set([poi['metro_area'] for poi in poi_dataset.take(100)]))}\")\n",
    "\n",
    "# Additional validation checks\n",
    "sample_data = poi_dataset.take(100)\n",
    "categories = set([poi['category'] for poi in sample_data])\n",
    "ratings = [poi['rating'] for poi in sample_data if poi['rating'] is not None]\n",
    "\n",
    "print(f\"  - Categories found: {len(categories)} ({list(categories)})\")\n",
    "print(f\"  - Rating range: {min(ratings):.1f} - {max(ratings):.1f}\")\n",
    "\n",
    "# Validate data integrity\n",
    "if valid_coords != poi_dataset.count():\n",
    "    print(\"  Warning: Some POIs have invalid coordinates\")\n",
    "if len(categories) == 0:\n",
    "    raise ValueError(\"No valid categories found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ce49ad",
   "metadata": {},
   "source": [
    "** What just happened?**\n",
    "- Created 3,000 realistic POI locations across 3 major cities\n",
    "- Each POI has coordinates, category, and rating information\n",
    "- Data is distributed across Ray workers for parallel processing\n",
    "- We validated our data to ensure it's ready for analysis\n",
    "\n",
    "## Step 2: Basic Spatial Operations\n",
    "\n",
    "Now let's perform basic spatial operations using Ray Data's distributed processing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9b5660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_metrics(batch: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Calculate distance-based metrics for POI analysis.\"\"\"\n",
    "    df = pd.DataFrame(batch)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for metro in df['metro_area'].unique():\n",
    "        metro_pois = df[df['metro_area'] == metro]\n",
    "        \n",
    "        # Calculate center point\n",
    "        center_lat = metro_pois['latitude'].mean()\n",
    "        center_lon = metro_pois['longitude'].mean()\n",
    "        \n",
    "        # Simple distance calculation (Euclidean approximation)\n",
    "        distances = []\n",
    "        for _, poi in metro_pois.iterrows():\n",
    "            dist = np.sqrt((poi['latitude'] - center_lat)**2 + \n",
    "                          (poi['longitude'] - center_lon)**2) * 111  # km\n",
    "            distances.append(dist)\n",
    "        \n",
    "        results.append({\n",
    "            'metro_area': metro,\n",
    "            'center_lat': center_lat,\n",
    "            'center_lon': center_lon,\n",
    "            'avg_distance_from_center': np.mean(distances),\n",
    "            'max_distance_from_center': np.max(distances),\n",
    "            'poi_count': len(metro_pois)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results).to_dict('list')\n",
    "\n",
    "# Process the data using Ray Data\n",
    "distance_analysis = poi_dataset.map_batches(\n",
    "    calculate_distance_metrics,\n",
    "    batch_format=\"pandas\",\n",
    "    batch_size=1000,\n",
    "    concurrency=2\n",
    ")\n",
    "\n",
    "print(\"Distance Analysis Results:\")\n",
    "distance_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60877914",
   "metadata": {},
   "source": [
    "## Step 3: Aggregation and Grouping\n",
    "\n",
    "Ray Data provides powerful aggregation capabilities for geospatial analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab54fce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by metro area and category\n",
    "category_analysis = poi_dataset.groupby(['metro_area', 'category']).count()\n",
    "print(\"POI Count by Metro and Category:\")\n",
    "category_analysis.show(15)\n",
    "\n",
    "# Calculate average ratings by metro area\n",
    "rating_analysis = poi_dataset.groupby('metro_area').mean('rating')\n",
    "print(\"\\nAverage Rating by Metro Area:\")\n",
    "rating_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feffcf38",
   "metadata": {},
   "source": [
    "## Step 4: Spatial Joins and Proximity Analysis\n",
    "\n",
    "Let's create a more complex spatial analysis using Ray Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab144456",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAnalyzer:\n",
    "    \"\"\"Spatial analysis predictor class.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the spatial analyzer.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, batch: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Perform spatial analysis on a batch of POIs.\"\"\"\n",
    "        df = pd.DataFrame(batch)\n",
    "        \n",
    "        analysis_results = []\n",
    "        \n",
    "        for metro in df['metro_area'].unique():\n",
    "            metro_data = df[df['metro_area'] == metro]\n",
    "            \n",
    "            # Analyze service accessibility\n",
    "            hospitals = metro_data[metro_data['category'] == 'hospital']\n",
    "            schools = metro_data[metro_data['category'] == 'school']\n",
    "            \n",
    "            analysis_results.append({\n",
    "                'metro_area': metro,\n",
    "                'total_pois': len(metro_data),\n",
    "                'hospital_count': len(hospitals),\n",
    "                'school_count': len(schools),\n",
    "                'hospital_density': len(hospitals) / len(metro_data) if len(metro_data) > 0 else 0,\n",
    "                'avg_rating': metro_data['rating'].mean()\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(analysis_results).to_dict('list')\n",
    "\n",
    "# Apply spatial analysis\n",
    "spatial_results = poi_dataset.map_batches(\n",
    "    SpatialAnalyzer,\n",
    "    concurrency=2,\n",
    "    batch_size=1500\n",
    ")\n",
    "\n",
    "print(\"Spatial Analysis Results:\")\n",
    "spatial_results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92da9643",
   "metadata": {},
   "source": [
    "## Step 5: Interactive Visualizations and Results\n",
    "\n",
    "Let's create stunning interactive visualizations to understand our spatial data:\n",
    "\n",
    "### 5.1: Interactive Heatmaps and Density Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8aaef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_heatmap(dataset):\n",
    "    \"\"\"Create interactive heatmap using Folium.\"\"\"\n",
    "    print(\"Creating interactive heatmap...\")\n",
    "    \n",
    "    # Convert to pandas for visualization\n",
    "    poi_df = dataset.to_pandas()\n",
    "    \n",
    "    # Create base map centered on NYC\n",
    "    center_lat = poi_df['latitude'].mean()\n",
    "    center_lon = poi_df['longitude'].mean()\n",
    "    \n",
    "    # Create Folium map with multiple tile layers\n",
    "    m = folium.Map(\n",
    "        location=[center_lat, center_lon], \n",
    "        zoom_start=10,\n",
    "        tiles=None\n",
    "    )\n",
    "    \n",
    "    # Add multiple tile layers for better visualization\n",
    "    folium.TileLayer('OpenStreetMap').add_to(m)\n",
    "    folium.TileLayer('CartoDB Positron').add_to(m)\n",
    "    folium.TileLayer('CartoDB Dark_Matter').add_to(m)\n",
    "    \n",
    "    # Create heatmap data\n",
    "    heat_data = [[row['latitude'], row['longitude']] for _, row in poi_df.iterrows()]\n",
    "    \n",
    "    # Add heatmap layer\n",
    "    HeatMap(\n",
    "        heat_data,\n",
    "        min_opacity=0.2,\n",
    "        radius=15,\n",
    "        blur=15,\n",
    "        max_zoom=1,\n",
    "        name='POI Density Heatmap'\n",
    "    ).add_to(m)\n",
    "    \n",
    "    # Add marker clusters for detailed view\n",
    "    marker_cluster = MarkerCluster(name='POI Markers').add_to(m)\n",
    "    \n",
    "    # Add markers for each POI with category-based colors\n",
    "    category_colors = {\n",
    "        'restaurant': 'red',\n",
    "        'retail': 'blue', \n",
    "        'hospital': 'green',\n",
    "        'school': 'orange',\n",
    "        'bank': 'purple'\n",
    "    }\n",
    "    \n",
    "    for _, poi in poi_df.head(100).iterrows():  # Show first 100 for performance\n",
    "        color = category_colors.get(poi['category'], 'gray')\n",
    "        folium.Marker(\n",
    "            [poi['latitude'], poi['longitude']],\n",
    "            popup=f\"<b>{poi['name']}</b><br>Category: {poi['category']}<br>Rating: {poi['rating']:.1f}\",\n",
    "            tooltip=f\"{poi['category']}: {poi['name']}\",\n",
    "            icon=folium.Icon(color=color)\n",
    "        ).add_to(marker_cluster)\n",
    "    \n",
    "    # Add layer control\n",
    "    folium.LayerControl().add_to(m)\n",
    "    \n",
    "    # Save map\n",
    "    map_file = \"poi_heatmap.html\"\n",
    "    m.save(map_file)\n",
    "    print(f\"Interactive heatmap saved as {map_file}\")\n",
    "    \n",
    "    return m\n",
    "\n",
    "# Create the interactive heatmap\n",
    "heatmap = create_interactive_heatmap(poi_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb6dafc",
   "metadata": {},
   "source": [
    "### 5.2: 3D Density Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a089c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_3d_density_plot(dataset):\n",
    "    \"\"\"Create 3D density visualization using Plotly.\"\"\"\n",
    "    print(\"Creating 3D density visualization...\")\n",
    "    \n",
    "    poi_df = dataset.to_pandas()\n",
    "    \n",
    "    # Create 3D scatter plot with density\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add scatter plot for each metro area\n",
    "    for metro in poi_df['metro_area'].unique():\n",
    "        metro_data = poi_df[poi_df['metro_area'] == metro]\n",
    "        \n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=metro_data['longitude'],\n",
    "            y=metro_data['latitude'], \n",
    "            z=metro_data['rating'],\n",
    "            mode='markers',\n",
    "            name=f'{metro} POIs',\n",
    "            marker=dict(\n",
    "                size=4,\n",
    "                opacity=0.7,\n",
    "                color=metro_data['rating'],\n",
    "                colorscale='Viridis',\n",
    "                showscale=True,\n",
    "                colorbar=dict(title=\"Rating\")\n",
    "            ),\n",
    "            text=[f\"Name: {name}<br>Category: {cat}<br>Rating: {rating:.1f}\" \n",
    "                  for name, cat, rating in zip(metro_data['name'], metro_data['category'], metro_data['rating'])],\n",
    "            hovertemplate=\"<b>%{text}</b><br>Lat: %{y:.4f}<br>Lon: %{x:.4f}<extra></extra>\"\n",
    "        ))\n",
    "    \n",
    "    # Create density surface\n",
    "    fig.add_trace(go.Mesh3d(\n",
    "        x=poi_df['longitude'],\n",
    "        y=poi_df['latitude'],\n",
    "        z=poi_df['rating'],\n",
    "        alphahull=5,\n",
    "        opacity=0.1,\n",
    "        color='lightblue',\n",
    "        name='Density Surface'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"3D POI Distribution and Rating Analysis\",\n",
    "        scene=dict(\n",
    "            xaxis_title=\"Longitude\",\n",
    "            yaxis_title=\"Latitude\", \n",
    "            zaxis_title=\"Rating\",\n",
    "            camera=dict(\n",
    "                up=dict(x=0, y=0, z=1),\n",
    "                center=dict(x=0, y=0, z=0),\n",
    "                eye=dict(x=1.5, y=1.5, z=1.5)\n",
    "            )\n",
    "        ),\n",
    "        width=800,\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    # Save as HTML\n",
    "    fig.write_html(\"3d_poi_density.html\")\n",
    "    print(\"3D visualization saved as 3d_poi_density.html\")\n",
    "    \n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create 3D density plot\n",
    "density_3d = create_3d_density_plot(poi_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56856ab6",
   "metadata": {},
   "source": [
    "### 5.3: Advanced Statistical Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00145f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_statistical_visualizations(dataset):\n",
    "    \"\"\"Create comprehensive statistical visualizations.\"\"\"\n",
    "    print(\"Creating statistical visualizations...\")\n",
    "    \n",
    "# Convert results to pandas for visualization\n",
    "spatial_df = spatial_results.to_pandas()\n",
    "    poi_df = dataset.to_pandas()\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # Create comprehensive dashboard\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. POI Distribution by Metro and Category (Heatmap)\n",
    "    ax1 = plt.subplot(3, 3, 1)\n",
    "    category_metro = poi_df.groupby(['metro_area', 'category']).size().unstack(fill_value=0)\n",
    "    sns.heatmap(category_metro, annot=True, fmt='d', cmap='YlOrRd', ax=ax1)\n",
    "    ax1.set_title('POI Distribution Heatmap\\n(Metro Area vs Category)', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # 2. Rating Distribution by Category (Violin Plot)\n",
    "    ax2 = plt.subplot(3, 3, 2)\n",
    "    sns.violinplot(data=poi_df, x='category', y='rating', ax=ax2)\n",
    "    ax2.set_title('Rating Distribution by Category', fontsize=12, fontweight='bold')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. Geographic Scatter with Density\n",
    "    ax3 = plt.subplot(3, 3, 3)\n",
    "    scatter = ax3.scatter(poi_df['longitude'], poi_df['latitude'], \n",
    "                         c=poi_df['rating'], s=30, alpha=0.6, cmap='viridis')\n",
    "    ax3.set_title('Geographic Distribution with Ratings', fontsize=12, fontweight='bold')\n",
    "    ax3.set_xlabel('Longitude')\n",
    "    ax3.set_ylabel('Latitude')\n",
    "    plt.colorbar(scatter, ax=ax3, label='Rating')\n",
    "    \n",
    "    # 4. Total POIs by Metro (Enhanced Bar Chart)\n",
    "    ax4 = plt.subplot(3, 3, 4)\n",
    "    metro_counts = poi_df['metro_area'].value_counts()\n",
    "    bars = ax4.bar(metro_counts.index, metro_counts.values, \n",
    "                   color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "    ax4.set_title('Total POIs by Metro Area', fontsize=12, fontweight='bold')\n",
    "    ax4.set_ylabel('Number of POIs')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height + 10,\n",
    "                f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 5. Hospital Density Analysis\n",
    "    ax5 = plt.subplot(3, 3, 5)\n",
    "    if len(spatial_df) > 0:\n",
    "        bars = ax5.bar(spatial_df['metro_area'], spatial_df['hospital_density'], \n",
    "                       color=['#FF9F43', '#10AC84', '#5F27CD'])\n",
    "        ax5.set_title('Hospital Density by Metro Area', fontsize=12, fontweight='bold')\n",
    "        ax5.set_ylabel('Hospitals per Total POIs')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax5.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                    f'{height:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 6. Rating vs Distance from Center\n",
    "    ax6 = plt.subplot(3, 3, 6)\n",
    "    # Calculate distance from center for each metro\n",
    "    for metro in poi_df['metro_area'].unique():\n",
    "        metro_data = poi_df[poi_df['metro_area'] == metro]\n",
    "        center_lat = metro_data['latitude'].mean()\n",
    "        center_lon = metro_data['longitude'].mean()\n",
    "        \n",
    "        distances = []\n",
    "        for _, poi in metro_data.iterrows():\n",
    "            dist = np.sqrt((poi['latitude'] - center_lat)**2 + \n",
    "                          (poi['longitude'] - center_lon)**2) * 111  # km approximation\n",
    "            distances.append(dist)\n",
    "        \n",
    "        ax6.scatter(distances, metro_data['rating'], alpha=0.6, label=metro, s=20)\n",
    "    \n",
    "    ax6.set_title('Rating vs Distance from Center', fontsize=12, fontweight='bold')\n",
    "    ax6.set_xlabel('Distance from Center (km)')\n",
    "    ax6.set_ylabel('Rating')\n",
    "    ax6.legend()\n",
    "    \n",
    "    # 7. Category Distribution (Donut Chart)\n",
    "    ax7 = plt.subplot(3, 3, 7)\n",
    "    category_counts = poi_df['category'].value_counts()\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(category_counts)))\n",
    "    wedges, texts, autotexts = ax7.pie(category_counts.values, labels=category_counts.index, \n",
    "                                      autopct='%1.1f%%', colors=colors, startangle=90,\n",
    "                                      wedgeprops=dict(width=0.5))\n",
    "    ax7.set_title('POI Category Distribution', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # 8. Rating Trends by Metro\n",
    "    ax8 = plt.subplot(3, 3, 8)\n",
    "    metro_ratings = poi_df.groupby('metro_area')['rating'].agg(['mean', 'std']).reset_index()\n",
    "    x_pos = np.arange(len(metro_ratings))\n",
    "    \n",
    "    bars = ax8.bar(x_pos, metro_ratings['mean'], yerr=metro_ratings['std'], \n",
    "                   capsize=5, color=['#E17055', '#00B894', '#6C5CE7'])\n",
    "    ax8.set_title('Average Ratings by Metro Area', fontsize=12, fontweight='bold')\n",
    "    ax8.set_xlabel('Metro Area')\n",
    "    ax8.set_ylabel('Average Rating')\n",
    "    ax8.set_xticks(x_pos)\n",
    "    ax8.set_xticklabels(metro_ratings['metro_area'])\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        ax8.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "                f'{height:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 9. Correlation Matrix\n",
    "    ax9 = plt.subplot(3, 3, 9)\n",
    "    # Create correlation matrix for numerical columns\n",
    "    numeric_cols = poi_df.select_dtypes(include=[np.number]).columns\n",
    "    correlation_matrix = poi_df[numeric_cols].corr()\n",
    "    \n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=ax9,\n",
    "                square=True, fmt='.2f')\n",
    "    ax9.set_title('Feature Correlation Matrix', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout(pad=3.0)\n",
    "    plt.savefig('geospatial_analysis_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "    \n",
    "    print(\"Statistical visualizations saved as 'geospatial_analysis_dashboard.png'\")\n",
    "\n",
    "# Create statistical visualizations\n",
    "create_statistical_visualizations(poi_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c975dbe6",
   "metadata": {},
   "source": [
    "### 5.4: Interactive Plotly Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7dd37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_dashboard(dataset):\n",
    "    \"\"\"Create interactive Plotly dashboard.\"\"\"\n",
    "    print(\"Creating interactive Plotly dashboard...\")\n",
    "    \n",
    "    poi_df = dataset.to_pandas()\n",
    "    \n",
    "    # Create subplots\n",
    "    from plotly.subplots import make_subplots\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Geographic Distribution', 'Category Analysis', \n",
    "                       'Rating Distribution', 'Metro Comparison'),\n",
    "        specs=[[{\"type\": \"scattermapbox\"}, {\"type\": \"bar\"}],\n",
    "               [{\"type\": \"histogram\"}, {\"type\": \"box\"}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Geographic scatter map\n",
    "    fig.add_trace(\n",
    "        go.Scattermapbox(\n",
    "            lat=poi_df['latitude'],\n",
    "            lon=poi_df['longitude'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=8,\n",
    "                color=poi_df['rating'],\n",
    "                colorscale='Viridis',\n",
    "                showscale=True,\n",
    "                colorbar=dict(title=\"Rating\", x=0.45)\n",
    "            ),\n",
    "            text=[f\"Name: {name}<br>Category: {cat}<br>Rating: {rating:.1f}\" \n",
    "                  for name, cat, rating in zip(poi_df['name'], poi_df['category'], poi_df['rating'])],\n",
    "            hovertemplate=\"<b>%{text}</b><br>Lat: %{lat:.4f}<br>Lon: %{lon:.4f}<extra></extra>\",\n",
    "            name=\"POIs\"\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Category bar chart\n",
    "    category_counts = poi_df['category'].value_counts()\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=category_counts.index,\n",
    "            y=category_counts.values,\n",
    "            marker_color='lightblue',\n",
    "            name=\"Categories\"\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Rating histogram\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=poi_df['rating'],\n",
    "            nbinsx=20,\n",
    "            marker_color='lightgreen',\n",
    "            name=\"Rating Distribution\"\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Box plot by metro\n",
    "    for metro in poi_df['metro_area'].unique():\n",
    "        metro_data = poi_df[poi_df['metro_area'] == metro]\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=metro_data['rating'],\n",
    "                name=metro,\n",
    "                boxpoints='all',\n",
    "                jitter=0.3,\n",
    "                pointpos=-1.8\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title_text=\"Interactive Geospatial Analysis Dashboard\",\n",
    "        title_x=0.5,\n",
    "        height=800,\n",
    "        showlegend=False,\n",
    "        mapbox=dict(\n",
    "            style=\"open-street-map\",\n",
    "            center=dict(lat=poi_df['latitude'].mean(), lon=poi_df['longitude'].mean()),\n",
    "            zoom=8\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Update axes titles\n",
    "    fig.update_xaxes(title_text=\"Category\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Count\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Rating\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Frequency\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Rating\", row=2, col=2)\n",
    "    \n",
    "    # Save and show\n",
    "    fig.write_html(\"interactive_geospatial_dashboard.html\")\n",
    "    print(\"Interactive dashboard saved as 'interactive_geospatial_dashboard.html'\")\n",
    "    fig.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create interactive dashboard\n",
    "dashboard = create_interactive_dashboard(poi_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97921dc8",
   "metadata": {},
   "source": [
    "## Step 6: Saving Results\n",
    "\n",
    "Save your processed geospatial data for further analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea11534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "# Save results to parquet format\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "# Save spatial analysis results\n",
    "spatial_results.write_parquet(f\"local://{temp_dir}/spatial_analysis\")\n",
    "print(f\"Results saved to {temp_dir}/spatial_analysis\")\n",
    "\n",
    "# Save category analysis\n",
    "category_analysis.write_parquet(f\"local://{temp_dir}/category_analysis\")\n",
    "print(f\"Category analysis saved to {temp_dir}/category_analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bac580",
   "metadata": {},
   "source": [
    "## Performance Tips\n",
    "\n",
    "When working with large geospatial datasets:\n",
    "\n",
    "1. **Batch Size**: Use appropriate batch sizes based on your data size and available memory\n",
    "2. **Concurrency**: Set concurrency based on your cluster size and CPU cores\n",
    "3. **Memory Management**: Use streaming operations for very large datasets\n",
    "4. **Spatial Indexing**: Consider spatial indexing for complex geometric operations\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "Common issues and solutions:\n",
    "\n",
    "- **Out of Memory**: Reduce batch size or increase cluster resources\n",
    "- **Slow Performance**: Check concurrency settings and cluster utilization\n",
    "- **Coordinate System Issues**: Ensure consistent coordinate reference systems\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To extend this example:\n",
    "- Load real geospatial data from sources like OpenStreetMap or government APIs\n",
    "- Implement more complex spatial operations using specialized libraries\n",
    "- Add streaming processing for real-time geospatial data\n",
    "- Integrate with mapping services for visualization\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77086121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up Ray resources (rule #210: Always include cleanup code)\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "    print(\"Ray resources cleaned up successfully!\")\n",
    "else:\n",
    "    print(\"Ray was not initialized - no cleanup needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98abb2e4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- Ray Data enables distributed processing of large geospatial datasets\n",
    "- Use `map_batches()` for complex spatial operations\n",
    "- Leverage Ray Data's aggregation capabilities for spatial analysis\n",
    "- Proper batch sizing and concurrency settings are crucial for performance\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting Common Issues\n",
    "\n",
    "### **Problem: \"Memory errors with large datasets\"**\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52def930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce batch size for memory-intensive operations\n",
    "ds.map_batches(spatial_function, batch_size=100, concurrency=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb063269",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Problem: \"Slow distance calculations\"**\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be531cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use vectorized operations for better performance\n",
    "import numpy as np\n",
    "# Vectorized haversine distance is much faster than loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226f98c3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Problem: \"Coordinate system issues\"**\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33448aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always validate coordinate ranges\n",
    "def validate_coordinates(lat, lon):\n",
    "    return -90 <= lat <= 90 and -180 <= lon <= 180"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994d2497",
   "metadata": {},
   "source": [
    "### **Debug and Monitoring Capabilities** (rule #200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b459543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable debug mode for detailed logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# Ray Data debugging utilities\n",
    "def debug_dataset_info(dataset, name=\"dataset\"):\n",
    "    \"\"\"Debug utility to inspect dataset characteristics.\"\"\"\n",
    "    print(f\"\\n=== Debug Info for {name} ===\")\n",
    "    print(f\"Record count: {dataset.count()}\")\n",
    "    print(f\"Schema: {dataset.schema()}\")\n",
    "    print(f\"Sample record: {dataset.take(1)[0] if dataset.count() > 0 else 'No records'}\")\n",
    "    \n",
    "    # Check for common issues\n",
    "    try:\n",
    "        sample_batch = dataset.take_batch(10)\n",
    "        print(f\"Batch extraction: Success ({len(sample_batch)} records)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Batch extraction: Failed - {e}\")\n",
    "\n",
    "# Example usage: debug_dataset_info(poi_dataset, \"POI dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2a8957",
   "metadata": {},
   "source": [
    "### **Performance Optimization Tips**\n",
    "\n",
    "1. **Batch Processing**: Process locations in batches of 1000-5000 for optimal performance\n",
    "2. **Spatial Indexing**: Use spatial indexing for nearest neighbor searches\n",
    "3. **Coordinate Validation**: Always validate lat/lon ranges before processing\n",
    "4. **Memory Management**: Monitor memory usage for large spatial datasets\n",
    "5. **Parallel Processing**: Leverage Ray's automatic parallelization for spatial operations\n",
    "\n",
    "### **Performance Considerations**\n",
    "\n",
    "Ray Data provides several advantages for geospatial processing:\n",
    "- **Parallel computation**: Distance calculations are distributed across multiple workers\n",
    "- **Memory efficiency**: Large coordinate datasets are processed in manageable chunks\n",
    "- **Scalability**: The same code patterns work for neighborhood-scale to continental-scale analysis\n",
    "- **Automatic optimization**: Ray Data handles data partitioning and load balancing automatically\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps and Extensions\n",
    "\n",
    "### **Try These Advanced Features**\n",
    "1. **Real Datasets**: Use OpenStreetMap data or Census TIGER files\n",
    "2. **Spatial Joins**: Join POI data with demographic or economic data\n",
    "3. **Clustering Analysis**: Group POIs by spatial proximity and characteristics\n",
    "4. **Route Optimization**: Calculate optimal routes between multiple POIs\n",
    "5. **Heatmap Generation**: Create density maps and spatial visualizations\n",
    "\n",
    "### **Production Considerations**\n",
    "- **Coordinate System Management**: Handle different coordinate reference systems\n",
    "- **Spatial Indexing**: Implement R-tree or other spatial indexes for performance\n",
    "- **Real-Time Processing**: Adapt for streaming location data\n",
    "- **Privacy Protection**: Implement location privacy and anonymization\n",
    "- **Scalability**: Handle continental or global-scale spatial analysis\n",
    "\n",
    "### **Community Support** (rule #123)\n",
    "\n",
    "**Getting Help**:\n",
    "- [Ray Data GitHub Discussions](https://github.com/ray-project/ray/discussions)\n",
    "- [Ray Slack Community](https://forms.gle/9TSdDYUgxYs8SA9e8)\n",
    "- [Stack Overflow - Ray Data](https://stackoverflow.com/questions/tagged/ray-data)\n",
    "- [Ray Data Examples Repository](https://github.com/ray-project/ray/tree/master/python/ray/data/examples)\n",
    "\n",
    "### **Related Ray Data Templates**\n",
    "- **Ray Data Large-Scale ETL Optimization**: Optimize spatial data pipelines\n",
    "- **Ray Data Data Quality Monitoring**: Validate spatial data quality\n",
    "- **Ray Data Batch Inference Optimization**: Optimize spatial ML models\n",
    "\n",
    "** Congratulations!** You've successfully built a scalable geospatial analysis pipeline with Ray Data!\n",
    "\n",
    "These spatial processing techniques scale from city-level to continental-level analysis with the same code patterns.\n",
    "- Ray Data integrates well with existing geospatial Python libraries"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
