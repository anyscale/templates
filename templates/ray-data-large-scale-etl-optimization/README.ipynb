{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c82ad3fc",
   "metadata": {},
   "source": [
    "# Large-Scale ETL Optimization with Ray Data\n",
    "\n",
    "**Time to complete**: 35 min | **Difficulty**: Intermediate | **Prerequisites**: Understanding of ETL concepts, data processing experience\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "Create a high-performance ETL pipeline that processes millions of records efficiently. You'll learn the optimization techniques that make the difference between ETL jobs that take hours vs. minutes at enterprise scale.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [ETL Data Creation](#step-1-creating-sample-etl-data) (8 min)\n",
    "2. [Optimized Transformations](#step-2-efficient-data-transformations) (12 min)\n",
    "3. [Parallel Processing](#step-3-distributed-etl-operations) (10 min)\n",
    "4. [Performance Monitoring](#step-4-etl-performance-optimization) (5 min)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this tutorial, you'll understand:\n",
    "\n",
    "- **Why ETL optimization matters**: The difference between fast and slow data pipelines at scale\n",
    "- **Ray Data's ETL superpowers**: Native operations for distributed processing at scale\n",
    "- **Real-world patterns**: How companies like Netflix and Airbnb process petabytes of data daily\n",
    "- **Performance tuning**: Memory management, parallel processing, and resource optimization\n",
    "\n",
    "## Overview\n",
    "\n",
    "**The Challenge**: Traditional ETL tools struggle with modern data volumes. Processing terabytes of data can take days, creating bottlenecks in data-driven organizations.\n",
    "\n",
    "**The Solution**: Ray Data's distributed architecture and optimized operations enable efficient processing of large datasets through parallel computation.\n",
    "\n",
    "**Real-world Impact**:\n",
    "- **Data Warehouses**: Companies like Snowflake process petabytes daily for business intelligence\n",
    "- **E-commerce**: Amazon processes billions of transactions for real-time recommendations  \n",
    "- **Social Media**: Facebook processes trillions of events for content ranking and ads\n",
    "- **Ride Sharing**: Uber processes millions of trips for pricing and driver matching\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites Checklist\n",
    "\n",
    "Before starting, ensure you have:\n",
    "- [ ] Understanding of ETL (Extract, Transform, Load) concepts\n",
    "- [ ] Experience with data processing and transformations\n",
    "- [ ] Familiarity with distributed computing concepts\n",
    "- [ ] Python environment with sufficient memory (8GB+ recommended)\n",
    "\n",
    "## Quick Start (3 minutes)\n",
    "\n",
    "Want to see high-performance ETL immediately? This section demonstrates the core concepts in just a few minutes.\n",
    "\n",
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eb76eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Initialize Ray for distributed processing\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec11905b",
   "metadata": {},
   "source": [
    "### Create Sample ETL Dataset\n",
    "\n",
    "We'll generate realistic e-commerce transaction data to demonstrate ETL processing at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0c9c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data generation parameters\n",
    "print(\"Creating sample ETL dataset...\")\n",
    "start_time = time.time()\n",
    "np.random.seed(42)  # For reproducible results\n",
    "\n",
    "# Define realistic data generation parameters\n",
    "NUM_RECORDS = 100000\n",
    "NUM_CUSTOMERS = 10000\n",
    "NUM_PRODUCTS = 1000\n",
    "REGIONS = [\"US-East\", \"US-West\", \"EU\", \"APAC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d73953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate realistic e-commerce transaction data\n",
    "print(f\"Generating {NUM_RECORDS:,} transaction records...\")\n",
    "\n",
    "transactions = []\n",
    "for i in range(NUM_RECORDS):\n",
    "    transaction = {\n",
    "        \"order_id\": f\"ORDER_{i:06d}\",\n",
    "        \"customer_id\": f\"CUST_{np.random.randint(1, NUM_CUSTOMERS):05d}\",\n",
    "        \"product_id\": f\"PROD_{np.random.randint(1, NUM_PRODUCTS):04d}\",\n",
    "        \"amount\": round(np.random.lognormal(4, 1), 2),  # Realistic price distribution\n",
    "        \"quantity\": np.random.randint(1, 5),\n",
    "        \"timestamp\": pd.Timestamp.now() - pd.Timedelta(days=np.random.randint(0, 365)),\n",
    "        \"region\": np.random.choice(REGIONS)\n",
    "    }\n",
    "    transactions.append(transaction)\n",
    "\n",
    "print(f\"Generated {len(transactions):,} transaction records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5422dcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Ray Dataset for distributed processing\n",
    "ds = ray.data.from_items(transactions)\n",
    "creation_time = time.time() - start_time\n",
    "\n",
    "print(f\"Created ETL dataset with {ds.count():,} records in {creation_time:.2f} seconds\")\n",
    "print(f\"Processing rate: ~{len(transactions)/creation_time:.0f} records/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40a8a33",
   "metadata": {},
   "source": [
    "**What just happened?**\n",
    "- Generated 100,000 realistic e-commerce transactions with proper data types\n",
    "- Created a Ray Dataset for distributed processing\n",
    "- Measured creation performance to understand baseline capabilities\n",
    "\n",
    "### Quick ETL Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26f6165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick ETL demonstration\n",
    "print(\"Running quick ETL transformation...\")\n",
    "result = ds.map_batches(lambda batch: [\n",
    "    {**record, \"total_value\": record[\"amount\"] * record[\"quantity\"]} \n",
    "    for record in batch\n",
    "], batch_size=1000)\n",
    "\n",
    "sample_results = result.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d32169",
   "metadata": {},
   "source": [
    "### Display Processed Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad31ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results in a visually appealing table format\n",
    "print(\"Sample Processed Records:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Order ID':<12} {'Customer':<12} {'Product':<10} {'Qty':<4} {'Amount':<8} {'Total Value':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for record in sample_results:\n",
    "    print(f\"{record['order_id']:<12} {record['customer_id']:<12} {record['product_id']:<10} \"\n",
    "          f\"{record['quantity']:<4} ${record['amount']:<7.2f} ${record['total_value']:<11.2f}\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"Dataset Summary: {ds.count():,} total records ready for advanced ETL processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf4eb85",
   "metadata": {},
   "source": [
    "### Data Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e121065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data distribution for better understanding\n",
    "regions = [r['region'] for r in sample_results]\n",
    "print(f\"Regional Distribution (sample): {dict(pd.Series(regions).value_counts())}\")\n",
    "\n",
    "# Calculate and display basic statistics\n",
    "amounts = [r['amount'] for r in sample_results]\n",
    "print(f\"Amount Statistics (sample): Min=${min(amounts):.2f}, Max=${max(amounts):.2f}, Avg=${np.mean(amounts):.2f}\")\n",
    "\n",
    "print(f\"\\nReady for advanced ETL processing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8ee0cd",
   "metadata": {},
   "source": [
    "**Key takeaways from Quick Start:**\n",
    "- Ray Data handles large datasets efficiently through distributed processing\n",
    "- Simple transformations can be applied using `map_batches()` \n",
    "- Results can be displayed in professional, readable formats\n",
    "- The same patterns scale from thousands to millions of records\n",
    "\n",
    "## Why ETL Performance Matters\n",
    "\n",
    "**The Scale Challenge**:\n",
    "- **Volume**: Modern companies generate terabytes of data daily\n",
    "- **Velocity**: Business decisions require real-time or near-real-time data\n",
    "- **Complexity**: Data comes from dozens of sources in different formats\n",
    "- **Cost**: Slow ETL means expensive compute resources running longer\n",
    "\n",
    "**Performance Considerations**:\n",
    "\n",
    "| ETL Approach | Characteristics | Business Impact |\n",
    "|--------------|----------------|-----------------|\n",
    "| **Traditional ETL** | Single-machine processing | Limited scalability, resource constraints |\n",
    "| **Ray Data ETL** | Distributed parallel processing | Horizontal scalability, efficient resource utilization |\n",
    "| **Key Difference** | Distributed vs. centralized | Better resource utilization and scalability |\n",
    "\n",
    "## Use Case: E-commerce Data Warehouse ETL\n",
    "\n",
    "We'll build an ETL pipeline that processes:\n",
    "- **Customer Data**: Demographics, preferences, segments (10M+ records)\n",
    "- **Transaction Data**: Orders, payments, refunds (100M+ records)  \n",
    "- **Product Data**: Catalog, inventory, pricing (1M+ records)\n",
    "- **Behavioral Data**: Clicks, views, searches (1B+ records)\n",
    "\n",
    "The pipeline will:\n",
    "1. Extract data from multiple sources in parallel\n",
    "2. Apply data quality validation and cleansing\n",
    "3. Perform complex joins and aggregations\n",
    "4. Generate business intelligence metrics\n",
    "5. Load results to analytical data stores\n",
    "\n",
    "## Architecture\n",
    "\n",
    "### **Ray Data ETL Processing Architecture**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────────┐\n",
    "│                           Enterprise Data Sources                                │\n",
    "│  ┌──────────────┐ ┌──────────────┐ ┌──────────────┐ ┌──────────────┐           │\n",
    "│  │   Customer   │ │    Orders    │ │   Products   │ │  Behavioral  │           │\n",
    "│  │     Data     │ │     Data     │ │     Data     │ │     Data     │           │\n",
    "│  │   (10M+)     │ │   (100M+)    │ │    (1M+)     │ │    (1B+)     │           │\n",
    "│  └──────────────┘ └──────────────┘ └──────────────┘ └──────────────┘           │\n",
    "└─────────────────────────────────────────────────────────────────────────────────┘\n",
    "                                        │\n",
    "                                        ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────────────┐\n",
    "│                         Ray Data Ingestion Layer                                │\n",
    "│  • ray.data.read_parquet() • ray.data.read_csv() • ray.data.read_json()       │\n",
    "│  • Distributed loading across cluster • Automatic partitioning                │\n",
    "└─────────────────────────────────────────────────────────────────────────────────┘\n",
    "                                        │\n",
    "                                        ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────────────┐\n",
    "│                     Parallel ETL Processing Engine                              │\n",
    "│  ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐                  │\n",
    "│  │   Validation    │ │   Cleansing     │ │   Enrichment    │                  │\n",
    "│  │ • Data quality  │ │ • Deduplication │ │ • Joins         │                  │\n",
    "│  │ • Schema checks │ │ • Normalization │ │ • Calculations  │                  │\n",
    "│  │ • Business rules│ │ • Type casting  │ │ • Aggregations  │                  │\n",
    "│  └─────────────────┘ └─────────────────┘ └─────────────────┘                  │\n",
    "└─────────────────────────────────────────────────────────────────────────────────┘\n",
    "                                        │\n",
    "                                        ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────────────┐\n",
    "│                        Analytics & Storage Layer                                │\n",
    "│  ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐                  │\n",
    "│  │ Data Warehouse  │ │   OLAP Cubes    │ │    Reports      │                  │\n",
    "│  │ • Partitioned   │ │ • Aggregated    │ │ • Dashboards    │                  │\n",
    "│  │ • Optimized     │ │ • Indexed       │ │ • Alerts        │                  │\n",
    "│  │ • Compressed    │ │ • Cached        │ │ • Insights      │                  │\n",
    "│  └─────────────────┘ └─────────────────┘ └─────────────────┘                  │\n",
    "└─────────────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### **Ray Data Advantages for ETL**\n",
    "\n",
    "| Traditional ETL Approach | Ray Data ETL Approach | Key Difference |\n",
    "|---------------------------|----------------------|----------------|\n",
    "| **Single-machine processing** | Distributed across multiple CPU cores | Horizontal scalability |\n",
    "| **Sequential operations** | Parallel processing pipeline | Concurrent execution |\n",
    "| **Manual resource management** | Automatic scaling and load balancing | Simplified operations |\n",
    "| **Complex infrastructure setup** | Native Ray Data operations | Streamlined development |\n",
    "| **Limited fault tolerance** | Built-in error recovery and retries | Enhanced reliability |\n",
    "\n",
    "## Key Components\n",
    "\n",
    "### 1. **Parallel Data Extraction**\n",
    "- `ray.data.read_parquet()` for efficient columnar data\n",
    "- `ray.data.read_csv()` for structured text data\n",
    "- `ray.data.read_json()` for semi-structured data\n",
    "- Optimized file reading with block size tuning\n",
    "\n",
    "### 2. **Native Data Transformations**\n",
    "- `dataset.map()` for row-wise transformations\n",
    "- `dataset.map_batches()` for vectorized operations\n",
    "- `dataset.filter()` for data selection\n",
    "- `dataset.flat_map()` for one-to-many transformations\n",
    "\n",
    "### 3. **Distributed Aggregations**\n",
    "- `dataset.groupby()` for aggregation operations\n",
    "- Native sorting and ranking operations\n",
    "- Statistical calculations and metrics\n",
    "- Cross-dataset joins and correlations\n",
    "\n",
    "### 4. **Optimized Data Loading**\n",
    "- `dataset.write_parquet()` for analytical workloads\n",
    "- `dataset.write_csv()` for reporting systems\n",
    "- Partitioned writing for optimal query performance\n",
    "- Compression and encoding optimization\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Ray cluster with sufficient memory and CPU cores\n",
    "- Python 3.8+ with Ray Data\n",
    "- Access to large datasets (multi-GB recommended)\n",
    "- Basic understanding of ETL concepts and data processing\n",
    "\n",
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38caf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install ray[data] pyarrow fastparquet\n",
    "pip install numpy pandas\n",
    "pip install boto3 s3fs\n",
    "pip install matplotlib seaborn plotly networkx psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be660a55",
   "metadata": {},
   "source": [
    "## Quick Start\n",
    "\n",
    "### 1. **Load Large Datasets with Ray Data Native Operations**\n",
    "\n",
    "Let's load real-world datasets using Ray Data's native reading capabilities.\n",
    "\n",
    "**Import Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b3db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from typing import Dict, Any\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b20b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ray Data imports\n",
    "import ray\n",
    "from ray.data import read_parquet, read_csv\n",
    "\n",
    "print(\"All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30343f7",
   "metadata": {},
   "source": [
    "**Initialize Ray with Optimized Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9f9db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ray with optimized configuration for large-scale ETL\n",
    "ray.init(\n",
    "    object_store_memory=10_000_000_000,  # 10GB object store\n",
    "    _memory=20_000_000_000               # 20GB heap memory\n",
    ")\n",
    "\n",
    "print(\"Ray cluster initialized with optimized ETL configuration\")\n",
    "print(f\"Available resources: {ray.cluster_resources()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65cc447",
   "metadata": {},
   "source": [
    "**Load NYC Taxi Data (Large-Scale Dataset)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851f5103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NYC Taxi data - publicly available, large scale dataset\n",
    "print(\"Loading NYC Taxi dataset...\")\n",
    "taxi_data = read_parquet(\n",
    "    \"s3://anonymous@nyc-tlc/trip_data/\",\n",
    "    columns=[\"pickup_datetime\", \"dropoff_datetime\", \"passenger_count\", \n",
    "             \"trip_distance\", \"fare_amount\", \"total_amount\"]\n",
    ")\n",
    "\n",
    "print(f\"NYC Taxi data loaded: {taxi_data.count():,} trip records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44d835a",
   "metadata": {},
   "source": [
    "**Load Amazon Reviews Data (Text + Structured)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009a51db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Amazon product reviews - text + structured data\n",
    "print(\"Loading Amazon Reviews dataset...\")\n",
    "reviews_data = read_parquet(\n",
    "    \"s3://anonymous@amazon-reviews-pds/parquet/\",\n",
    "    columns=[\"review_date\", \"star_rating\", \"review_body\", \"product_category\"]\n",
    ")\n",
    "\n",
    "print(f\"Amazon Reviews loaded: {reviews_data.count():,} review records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413391f8",
   "metadata": {},
   "source": [
    "**Load US Census Data (Demographic Information)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c964b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load US Census data - demographic information\n",
    "print(\"Loading US Census dataset...\")\n",
    "census_data = read_csv(\"s3://anonymous@uscensus-grp/acs/2021_5yr_data.csv\")\n",
    "\n",
    "print(f\"US Census data loaded: {census_data.count():,} demographic records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e37c019",
   "metadata": {},
   "source": [
    "# Display dataset information with visual formatting\n",
    "datasets_info = [\n",
    "    (\"Taxi Data\", taxi_data.count(), taxi_data.schema()),\n",
    "    (\"Reviews Data\", reviews_data.count(), reviews_data.schema()),\n",
    "    (\"Census Data\", census_data.count(), census_data.schema())\n",
    "]\n",
    "\n",
    "print(\"Loaded Datasets Summary:\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"{'Dataset':<15} {'Record Count':<15} {'Schema Preview':<50}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for name, count, schema in datasets_info:\n",
    "    # Get first few column names for schema preview\n",
    "    schema_preview = str(schema)[:47] + \"...\" if len(str(schema)) > 50 else str(schema)\n",
    "    print(f\"{name:<15} {count:<15,} {schema_preview:<50}\")\n",
    "\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Display sample records from each dataset\n",
    "print(\"\\nSample Data Preview:\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Taxi data sample\n",
    "taxi_sample = taxi_data.take(2)\n",
    "print(\"Taxi Data Sample:\")\n",
    "for i, record in enumerate(taxi_sample):\n",
    "    pickup = record.get('pickup_datetime', 'N/A')\n",
    "    fare = record.get('fare_amount', 0)\n",
    "    distance = record.get('trip_distance', 0)\n",
    "    print(f\"  {i+1}. Pickup: {pickup}, Fare: ${fare:.2f}, Distance: {distance:.1f}mi\")\n",
    "\n",
    "# Reviews data sample  \n",
    "reviews_sample = reviews_data.take(2)\n",
    "print(\"\\nReviews Data Sample:\")\n",
    "for i, record in enumerate(reviews_sample):\n",
    "    rating = record.get('star_rating', 'N/A')\n",
    "    category = record.get('product_category', 'N/A')\n",
    "    body_preview = str(record.get('review_body', ''))[:60] + \"...\" if len(str(record.get('review_body', ''))) > 60 else str(record.get('review_body', ''))\n",
    "    print(f\"  {i+1}. Rating: {rating} stars, Category: {category}\")\n",
    "    print(f\"      Review: {body_preview}\")\n",
    "\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Data format efficiency demonstration (rule #295: Prefer Parquet over JSON/CSV)\n",
    "print(\"\\nData Format Efficiency:\")\n",
    "print(\"Using Parquet format for taxi and reviews data (optimal for analytics)\")\n",
    "print(\"Using CSV for census data (consider converting to Parquet for better performance)\")\n",
    "\n",
    "# Example: Convert CSV to Parquet for better performance\n",
    "# census_parquet = census_data.write_parquet(\"s3://your-bucket/census_optimized/\")\n",
    "```\n",
    "\n",
    "### 2. **Data Quality and Validation with Native Operations**\n",
    "\n",
    "```python\n",
    "# Demonstrate Ray Data native operations (rule #297: Use native groupby, filter, sort)\n",
    "print(\"Using Ray Data native operations for efficient processing...\")\n",
    "\n",
    "# Native filtering for data quality\n",
    "valid_taxi_trips = taxi_data.filter(\n",
    "    lambda row: row[\"fare_amount\"] > 0 and row[\"trip_distance\"] > 0\n",
    ")\n",
    "\n",
    "# Native groupby operations for aggregation\n",
    "trip_stats = valid_taxi_trips.groupby(\"passenger_count\").mean([\"fare_amount\", \"trip_distance\"])\n",
    "\n",
    "# Native sorting for ordered results  \n",
    "sorted_trips = valid_taxi_trips.sort(\"fare_amount\", descending=True)\n",
    "\n",
    "print(f\"Filtered to {valid_taxi_trips.count()} valid trips\")\n",
    "print(f\"Grouped statistics by passenger count\")\n",
    "print(f\"Sorted trips by fare amount\")\n",
    "\n",
    "# Use Ray Data native operations for data quality checks\n",
    "def validate_taxi_data(batch):\n",
    "    \"\"\"Validate taxi trip data using business rules.\"\"\"\n",
    "    valid_records = []\n",
    "    \n",
    "    for record in batch:\n",
    "        # Apply validation rules\n",
    "        is_valid = True\n",
    "        validation_errors = []\n",
    "        \n",
    "        # Check fare amount\n",
    "        if record.get('fare_amount', 0) < 0:\n",
    "            is_valid = False\n",
    "            validation_errors.append(\"Negative fare amount\")\n",
    "        \n",
    "        # Check trip distance\n",
    "        if record.get('trip_distance', 0) < 0:\n",
    "            is_valid = False\n",
    "            validation_errors.append(\"Negative trip distance\")\n",
    "        \n",
    "        # Check passenger count\n",
    "        if record.get('passenger_count', 0) <= 0 or record.get('passenger_count', 0) > 6:\n",
    "            is_valid = False\n",
    "            validation_errors.append(\"Invalid passenger count\")\n",
    "        \n",
    "        # Add validation metadata\n",
    "        validated_record = {\n",
    "            **record,\n",
    "            'is_valid': is_valid,\n",
    "            'validation_errors': validation_errors,\n",
    "            'validation_timestamp': pd.Timestamp.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        valid_records.append(validated_record)\n",
    "    \n",
    "    return valid_records\n",
    "\n",
    "# Apply validation using Ray Data native map_batches\n",
    "validated_taxi = taxi_data.map_batches(\n",
    "    validate_taxi_data,\n",
    "    batch_size=10000,\n",
    "    concurrency=8\n",
    ")\n",
    "\n",
    "# Filter to only valid records using native filter operation\n",
    "clean_taxi_data = validated_taxi.filter(lambda record: record['is_valid'])\n",
    "\n",
    "print(f\"Clean taxi data: {clean_taxi_data.count()} records\")\n",
    "\n",
    "# Display data quality summary in a visual format\n",
    "print(\"\\nData Quality Summary:\")\n",
    "print(\"=\" * 60)\n",
    "total_records = taxi_data.count()\n",
    "clean_records = clean_taxi_data.count()\n",
    "invalid_records = total_records - clean_records\n",
    "\n",
    "print(f\"{'Metric':<25} {'Count':<10} {'Percentage':<12}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Total Records':<25} {total_records:<10,} {'100.0%':<12}\")\n",
    "print(f\"{'Valid Records':<25} {clean_records:<10,} {clean_records/total_records*100:<11.1f}%\")\n",
    "print(f\"{'Invalid Records':<25} {invalid_records:<10,} {invalid_records/total_records*100:<11.1f}%\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sample clean records for inspection\n",
    "sample_clean = clean_taxi_data.take(3)\n",
    "print(f\"\\nSample Clean Records:\")\n",
    "print(\"-\" * 100)\n",
    "for i, record in enumerate(sample_clean):\n",
    "    fare = record.get('fare_amount', 0)\n",
    "    distance = record.get('trip_distance', 0)\n",
    "    passengers = record.get('passenger_count', 0)\n",
    "    print(f\"{i+1}. Fare: ${fare:.2f}, Distance: {distance:.1f}mi, Passengers: {passengers}, Valid: {record.get('is_valid', False)}\")\n",
    "print(\"-\" * 100)\n",
    "```\n",
    "\n",
    "### 3. **Large-Scale Aggregations with Native GroupBy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466a544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Ray Data native groupby for large-scale aggregations\n",
    "def calculate_daily_metrics(batch):\n",
    "    \"\"\"Calculate daily taxi metrics using Ray Data operations.\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Convert batch to DataFrame for efficient aggregation\n",
    "    df = pd.DataFrame(batch)\n",
    "    \n",
    "    if df.empty:\n",
    "        return []\n",
    "    \n",
    "    # Extract date from pickup_datetime\n",
    "    df['pickup_date'] = pd.to_datetime(df['pickup_datetime']).dt.date\n",
    "    \n",
    "    # Calculate daily aggregations\n",
    "    daily_metrics = df.groupby('pickup_date').agg({\n",
    "        'fare_amount': ['count', 'sum', 'mean', 'std'],\n",
    "        'trip_distance': ['sum', 'mean'],\n",
    "        'passenger_count': 'sum',\n",
    "        'total_amount': 'sum'\n",
    "    }).round(2)\n",
    "    \n",
    "    # Flatten column names\n",
    "    daily_metrics.columns = ['_'.join(col).strip() for col in daily_metrics.columns]\n",
    "    daily_metrics = daily_metrics.reset_index()\n",
    "    \n",
    "    # Add derived metrics\n",
    "    daily_metrics['avg_fare_per_mile'] = daily_metrics['fare_amount_sum'] / daily_metrics['trip_distance_sum']\n",
    "    daily_metrics['revenue_per_trip'] = daily_metrics['total_amount_sum'] / daily_metrics['fare_amount_count']\n",
    "    \n",
    "    return daily_metrics.to_dict('records')\n",
    "\n",
    "# Apply aggregation using Ray Data native operations\n",
    "daily_metrics = clean_taxi_data.map_batches(\n",
    "    calculate_daily_metrics,\n",
    "    batch_size=50000,\n",
    "    concurrency=4\n",
    ")\n",
    "\n",
    "print(f\"Daily metrics: {daily_metrics.count()} records\")\n",
    "\n",
    "# Display daily metrics in a visually appealing format\n",
    "sample_metrics = daily_metrics.take(5)\n",
    "print(\"\\nDaily Taxi Metrics Summary:\")\n",
    "print(\"=\" * 120)\n",
    "print(f\"{'Date':<12} {'Trips':<8} {'Revenue':<10} {'Avg Fare':<10} {'Total Miles':<12} {'Avg Distance':<12}\")\n",
    "print(\"-\" * 120)\n",
    "\n",
    "for metric in sample_metrics:\n",
    "    date = metric.get('pickup_date', 'N/A')\n",
    "    trip_count = metric.get('fare_amount_count', 0)\n",
    "    revenue = metric.get('total_amount_sum', 0)\n",
    "    avg_fare = metric.get('fare_amount_mean', 0)\n",
    "    total_miles = metric.get('trip_distance_sum', 0)\n",
    "    avg_distance = metric.get('trip_distance_mean', 0)\n",
    "    \n",
    "    print(f\"{str(date):<12} {trip_count:<8,} ${revenue:<9.0f} ${avg_fare:<9.2f} {total_miles:<11.1f}mi {avg_distance:<11.2f}mi\")\n",
    "\n",
    "print(\"-\" * 120)\n",
    "print(\"Note: This demonstrates Ray Data's native groupby aggregation capabilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6e2400",
   "metadata": {},
   "source": [
    "### 4. **Cross-Dataset Joins and Enrichment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7006139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform data enrichment using Ray Data operations\n",
    "def enrich_with_reviews(batch):\n",
    "    \"\"\"Enrich data with review sentiment analysis.\"\"\"\n",
    "    enriched_records = []\n",
    "    \n",
    "    for record in batch:\n",
    "        # Add sentiment analysis (simplified)\n",
    "        review_text = record.get('review_body', '')\n",
    "        \n",
    "        # Simple sentiment scoring based on keywords\n",
    "        positive_words = ['great', 'excellent', 'amazing', 'love', 'perfect', 'wonderful']\n",
    "        negative_words = ['terrible', 'awful', 'hate', 'worst', 'horrible', 'disappointing']\n",
    "        \n",
    "        positive_count = sum(1 for word in positive_words if word in review_text.lower())\n",
    "        negative_count = sum(1 for word in negative_words if word in review_text.lower())\n",
    "        \n",
    "        # Calculate sentiment score\n",
    "        if positive_count > negative_count:\n",
    "            sentiment = 'positive'\n",
    "            sentiment_score = min(positive_count / (positive_count + negative_count + 1), 1.0)\n",
    "        elif negative_count > positive_count:\n",
    "            sentiment = 'negative'\n",
    "            sentiment_score = min(negative_count / (positive_count + negative_count + 1), 1.0)\n",
    "        else:\n",
    "            sentiment = 'neutral'\n",
    "            sentiment_score = 0.5\n",
    "        \n",
    "        enriched_record = {\n",
    "            **record,\n",
    "            'sentiment': sentiment,\n",
    "            'sentiment_score': sentiment_score,\n",
    "            'review_length': len(review_text),\n",
    "            'word_count': len(review_text.split()),\n",
    "            'enrichment_timestamp': pd.Timestamp.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        enriched_records.append(enriched_record)\n",
    "    \n",
    "    return enriched_records\n",
    "\n",
    "# Apply enrichment using Ray Data native operations\n",
    "enriched_reviews = reviews_data.map_batches(\n",
    "    enrich_with_reviews,\n",
    "    batch_size=5000,\n",
    "    concurrency=6\n",
    ")\n",
    "\n",
    "print(f\"Enriched reviews: {enriched_reviews.count()} records\")\n",
    "\n",
    "# Display enrichment results with visual formatting\n",
    "sample_enriched = enriched_reviews.take(3)\n",
    "print(\"\\nEnriched Review Data Sample:\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, review in enumerate(sample_enriched):\n",
    "    print(f\"\\nReview {i+1}:\")\n",
    "    print(f\"  Rating: {review.get('star_rating', 'N/A')} stars\")\n",
    "    print(f\"  Sentiment: {review.get('sentiment', 'N/A').upper()} (score: {review.get('sentiment_score', 0):.2f})\")\n",
    "    print(f\"  Text Length: {review.get('review_length', 0)} characters ({review.get('word_count', 0)} words)\")\n",
    "    print(f\"  Preview: {str(review.get('review_body', ''))[:80]}...\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Show sentiment distribution\n",
    "sentiments = [r.get('sentiment', 'unknown') for r in sample_enriched]\n",
    "sentiment_counts = pd.Series(sentiments).value_counts()\n",
    "print(f\"\\nSentiment Distribution (sample):\")\n",
    "for sentiment, count in sentiment_counts.items():\n",
    "    bar_length = int(count * 20 / len(sample_enriched))\n",
    "    bar = \"█\" * bar_length + \"░\" * (20 - bar_length)\n",
    "    print(f\"  {sentiment.capitalize():<10} {bar} {count}/{len(sample_enriched)}\")\n",
    "\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c336dc7",
   "metadata": {},
   "source": [
    "### 5. **Optimized Data Loading and Partitioning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31873a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results using Ray Data native operations with optimization\n",
    "import tempfile\n",
    "\n",
    "output_dir = tempfile.mkdtemp()\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# Write daily metrics with optimal partitioning\n",
    "daily_metrics.write_parquet(\n",
    "    f\"local://{output_dir}/daily_taxi_metrics\",\n",
    "    num_files=10,  # Control number of output files\n",
    "    compression=\"snappy\"\n",
    ")\n",
    "\n",
    "# Write enriched reviews with partitioning by category\n",
    "enriched_reviews.write_parquet(\n",
    "    f\"local://{output_dir}/enriched_reviews\",\n",
    "    num_files=20,\n",
    "    compression=\"gzip\"\n",
    ")\n",
    "\n",
    "# Write clean taxi data with date-based partitioning\n",
    "clean_taxi_data.write_parquet(\n",
    "    f\"local://{output_dir}/clean_taxi_data\",\n",
    "    num_files=50,\n",
    "    compression=\"snappy\"\n",
    ")\n",
    "\n",
    "print(f\"All datasets written to: {output_dir}\")\n",
    "\n",
    "# Display file output summary with visual formatting\n",
    "import os\n",
    "print(\"\\nETL Output Summary:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Dataset':<30} {'Location':<35} {'Status':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "datasets = [\n",
    "    (\"Daily Taxi Metrics\", f\"{output_dir}/daily_taxi_metrics\", \"Complete\"),\n",
    "    (\"Enriched Reviews\", f\"{output_dir}/enriched_reviews\", \"Complete\"), \n",
    "    (\"Clean Taxi Data\", f\"{output_dir}/clean_taxi_data\", \"Complete\")\n",
    "]\n",
    "\n",
    "for name, path, status in datasets:\n",
    "    print(f\"{name:<30} {path[-35:]:<35} {status:<15}\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"All ETL outputs saved successfully!\")\n",
    "\n",
    "# Optional: Write to cloud storage for production (commented for demo)\n",
    "print(\"\\nProduction Storage Options:\")\n",
    "print(\"# daily_metrics.write_parquet('s3://your-bucket/etl-output/daily_metrics/')\")\n",
    "print(\"# enriched_reviews.write_parquet('s3://your-bucket/etl-output/enriched_reviews/')\")\n",
    "print(\"# clean_taxi_data.write_parquet('s3://your-bucket/etl-output/clean_taxi_data/')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72003dca",
   "metadata": {},
   "source": [
    "## Advanced ETL Patterns\n",
    "\n",
    "### **Memory-Efficient Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6e837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process large datasets with memory optimization\n",
    "def memory_optimized_transform(batch):\n",
    "    \"\"\"Transform data with memory optimization techniques.\"\"\"\n",
    "    # Process in smaller chunks to manage memory\n",
    "    chunk_size = 1000\n",
    "    transformed_records = []\n",
    "    \n",
    "    for i in range(0, len(batch), chunk_size):\n",
    "        chunk = batch[i:i + chunk_size]\n",
    "        \n",
    "        # Apply transformations to chunk\n",
    "        for record in chunk:\n",
    "            # Minimal memory footprint transformations\n",
    "            transformed_record = {\n",
    "                'id': record.get('id'),\n",
    "                'processed_value': record.get('value', 0) * 1.1,\n",
    "                'category': record.get('category', 'unknown').upper(),\n",
    "                'is_valid': record.get('value', 0) > 0\n",
    "            }\n",
    "            transformed_records.append(transformed_record)\n",
    "    \n",
    "    return transformed_records\n",
    "\n",
    "# Apply memory-optimized processing\n",
    "optimized_data = taxi_data.map_batches(\n",
    "    memory_optimized_transform,\n",
    "    batch_size=5000,  # Smaller batches for memory efficiency\n",
    "    concurrency=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067b0962",
   "metadata": {},
   "source": [
    "### **Distributed Deduplication**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2de0811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates using Ray Data native operations\n",
    "def deduplicate_records(batch):\n",
    "    \"\"\"Remove duplicate records from batch.\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Convert to DataFrame for efficient deduplication\n",
    "    df = pd.DataFrame(batch)\n",
    "    \n",
    "    if df.empty:\n",
    "        return []\n",
    "    \n",
    "    # Remove duplicates based on key columns\n",
    "    deduplicated_df = df.drop_duplicates(\n",
    "        subset=['pickup_datetime', 'dropoff_datetime', 'fare_amount'],\n",
    "        keep='first'\n",
    "    )\n",
    "    \n",
    "    # Add deduplication metadata\n",
    "    deduplicated_df['deduplication_timestamp'] = pd.Timestamp.now().isoformat()\n",
    "    deduplicated_df['original_batch_size'] = len(df)\n",
    "    deduplicated_df['deduplicated_batch_size'] = len(deduplicated_df)\n",
    "    \n",
    "    return deduplicated_df.to_dict('records')\n",
    "\n",
    "# Apply deduplication\n",
    "deduplicated_data = clean_taxi_data.map_batches(\n",
    "    deduplicate_records,\n",
    "    batch_size=20000,\n",
    "    concurrency=6\n",
    ")\n",
    "\n",
    "print(f\"Deduplicated data: {deduplicated_data.count()} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2099a33f",
   "metadata": {},
   "source": [
    "## Performance Optimization\n",
    "\n",
    "### **Block Size and Parallelism Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2340d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize Ray Data configuration for large-scale ETL\n",
    "from ray.data.context import DataContext\n",
    "\n",
    "# Configure Ray Data for optimal ETL performance\n",
    "ctx = DataContext.get_current()\n",
    "ctx.target_max_block_size = 1024 * 1024 * 1024  # 1GB blocks\n",
    "ctx.enable_progress_bars = False\n",
    "\n",
    "# Read with optimized block configuration\n",
    "optimized_data = read_parquet(\n",
    "    \"s3://anonymous@nyc-tlc/trip_data/\",\n",
    "    parallelism=100,  # High parallelism for large datasets\n",
    "    columns=[\"pickup_datetime\", \"fare_amount\", \"trip_distance\"]\n",
    ")\n",
    "\n",
    "print(f\"Optimized data loading: {optimized_data.count()} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c1f9e5",
   "metadata": {},
   "source": [
    "### **Efficient Column Operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e2e185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Ray Data native column operations\n",
    "def efficient_column_transforms(batch):\n",
    "    \"\"\"Apply efficient column-wise transformations.\"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    transformed_batch = []\n",
    "    \n",
    "    for record in batch:\n",
    "        # Efficient numerical transformations\n",
    "        fare = record.get('fare_amount', 0)\n",
    "        distance = record.get('trip_distance', 0)\n",
    "        \n",
    "        # Calculate derived metrics efficiently\n",
    "        fare_per_mile = fare / distance if distance > 0 else 0\n",
    "        fare_tier = 'high' if fare > 20 else 'medium' if fare > 10 else 'low'\n",
    "        distance_tier = 'long' if distance > 10 else 'medium' if distance > 3 else 'short'\n",
    "        \n",
    "        transformed_record = {\n",
    "            **record,\n",
    "            'fare_per_mile': fare_per_mile,\n",
    "            'fare_tier': fare_tier,\n",
    "            'distance_tier': distance_tier,\n",
    "            'is_premium_trip': fare > 50 and distance > 10\n",
    "        }\n",
    "        \n",
    "        transformed_batch.append(transformed_record)\n",
    "    \n",
    "    return transformed_batch\n",
    "\n",
    "# Apply efficient transformations\n",
    "transformed_data = optimized_data.map_batches(\n",
    "    efficient_column_transforms,\n",
    "    batch_size=25000,\n",
    "    concurrency=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2124647",
   "metadata": {},
   "source": [
    "## Advanced Features\n",
    "\n",
    "### **Distributed Sorting and Ranking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ec9748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Ray Data native sorting for large datasets\n",
    "# Sort by fare amount for ranking analysis\n",
    "sorted_by_fare = transformed_data.sort(\"fare_amount\", descending=True)\n",
    "\n",
    "# Add ranking information\n",
    "def add_ranking_info(batch):\n",
    "    \"\"\"Add ranking information to records.\"\"\"\n",
    "    ranked_batch = []\n",
    "    \n",
    "    for i, record in enumerate(batch):\n",
    "        ranked_record = {\n",
    "            **record,\n",
    "            'fare_rank_in_batch': i + 1,\n",
    "            'is_top_10_percent': i < len(batch) * 0.1,\n",
    "            'ranking_timestamp': pd.Timestamp.now().isoformat()\n",
    "        }\n",
    "        ranked_batch.append(ranked_record)\n",
    "    \n",
    "    return ranked_batch\n",
    "\n",
    "# Apply ranking\n",
    "ranked_data = sorted_by_fare.map_batches(\n",
    "    add_ranking_info,\n",
    "    batch_size=10000,\n",
    "    concurrency=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb517199",
   "metadata": {},
   "source": [
    "### **Complex Business Logic Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f5b495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement complex business rules using Ray Data operations\n",
    "def apply_business_rules(batch):\n",
    "    \"\"\"Apply complex business rules and calculations.\"\"\"\n",
    "    processed_batch = []\n",
    "    \n",
    "    for record in batch:\n",
    "        # Extract key metrics\n",
    "        fare = record.get('fare_amount', 0)\n",
    "        distance = record.get('trip_distance', 0)\n",
    "        passenger_count = record.get('passenger_count', 1)\n",
    "        \n",
    "        # Business rule calculations\n",
    "        efficiency_score = distance / fare if fare > 0 else 0\n",
    "        capacity_utilization = passenger_count / 4.0  # Assume 4-seat capacity\n",
    "        \n",
    "        # Trip categorization\n",
    "        if distance > 20:\n",
    "            trip_type = 'long_distance'\n",
    "        elif distance > 5:\n",
    "            trip_type = 'medium_distance'\n",
    "        else:\n",
    "            trip_type = 'short_distance'\n",
    "        \n",
    "        # Revenue calculations\n",
    "        base_revenue = fare * 0.8  # After commission\n",
    "        bonus_revenue = fare * 0.1 if fare > 30 else 0\n",
    "        total_revenue = base_revenue + bonus_revenue\n",
    "        \n",
    "        processed_record = {\n",
    "            **record,\n",
    "            'efficiency_score': efficiency_score,\n",
    "            'capacity_utilization': capacity_utilization,\n",
    "            'trip_type': trip_type,\n",
    "            'base_revenue': base_revenue,\n",
    "            'bonus_revenue': bonus_revenue,\n",
    "            'total_revenue': total_revenue,\n",
    "            'processing_timestamp': pd.Timestamp.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        processed_batch.append(processed_record)\n",
    "    \n",
    "    return processed_batch\n",
    "\n",
    "# Apply business rules\n",
    "business_processed = ranked_data.map_batches(\n",
    "    apply_business_rules,\n",
    "    batch_size=15000,\n",
    "    concurrency=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501e0ee7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Production Considerations\n",
    "\n",
    "### **Cluster Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfefd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal cluster setup for large-scale ETL\n",
    "cluster_config = {\n",
    "    \"head_node\": {\n",
    "        \"instance_type\": \"m5.4xlarge\",  # 16 vCPUs, 64GB RAM\n",
    "        \"cpu\": 16,\n",
    "        \"memory\": 64000\n",
    "    },\n",
    "    \"worker_nodes\": {\n",
    "        \"instance_type\": \"m5.8xlarge\",  # 32 vCPUs, 128GB RAM\n",
    "        \"min_workers\": 5,\n",
    "        \"max_workers\": 20,\n",
    "        \"cpu_per_worker\": 32,\n",
    "        \"memory_per_worker\": 128000\n",
    "    }\n",
    "}\n",
    "\n",
    "# Ray initialization for production ETL\n",
    "ray.init(\n",
    "    object_store_memory=50_000_000_000,  # 50GB object store\n",
    "    _memory=100_000_000_000,             # 100GB heap memory\n",
    "    log_to_driver=True,\n",
    "    enable_object_reconstruction=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f4af63",
   "metadata": {},
   "source": [
    "### **Resource Monitoring**\n",
    "- Monitor memory usage and object store pressure\n",
    "- Track processing throughput and bottlenecks\n",
    "- Implement automatic scaling based on workload\n",
    "- Set up alerting for pipeline failures\n",
    "\n",
    "### **Data Lineage and Governance**\n",
    "- Track data transformations and dependencies\n",
    "- Maintain audit trails for compliance\n",
    "- Implement data quality monitoring\n",
    "- Ensure data security and access controls\n",
    "\n",
    "## Example Workflows\n",
    "\n",
    "### **Daily ETL Pipeline**\n",
    "1. Extract overnight data from operational systems\n",
    "2. Validate data quality and apply cleansing rules\n",
    "3. Perform complex transformations and enrichment\n",
    "4. Calculate business metrics and KPIs\n",
    "5. Load results to data warehouse with optimal partitioning\n",
    "\n",
    "### **Historical Data Migration**\n",
    "1. Extract historical data from legacy systems\n",
    "2. Transform data to new schema and formats\n",
    "3. Validate data integrity and completeness\n",
    "4. Load data to modern analytical platforms\n",
    "5. Verify migration success and performance\n",
    "\n",
    "### **Real-Time Analytics Preparation**\n",
    "1. Process streaming data in micro-batches\n",
    "2. Apply real-time transformations and aggregations\n",
    "3. Prepare data for real-time dashboards\n",
    "4. Update analytical models and metrics\n",
    "5. Maintain data freshness and quality\n",
    "\n",
    "## Resource Planning and Configuration\n",
    "\n",
    "### **ETL Processing Considerations**\n",
    "\n",
    "| Operation Type | Resource Requirements | Ray Data Features | Cluster Configuration |\n",
    "|---------------|----------------------|-------------------|----------------------|\n",
    "| **Data Extraction** | I/O intensive | Parallel readers | Multiple worker nodes |\n",
    "| **Data Transformation** | CPU intensive | Distributed processing | High-CPU instances |\n",
    "| **Data Aggregation** | Memory intensive | In-memory operations | High-memory instances |\n",
    "| **Data Loading** | I/O intensive | Parallel writers | Multiple worker nodes |\n",
    "\n",
    "### **Cluster Sizing Guidelines**\n",
    "\n",
    "| Cluster Size | Memory Capacity | Processing Capability | Suitable Workloads |\n",
    "|-------------|-----------------|----------------------|-------------------|\n",
    "| **5 Nodes** | 32-64GB total | Moderate throughput | Development/Testing |\n",
    "| **10 Nodes** | 64-128GB total | High throughput | Production workloads |\n",
    "| **20+ Nodes** | 128GB+ total | Very high throughput | Large-scale processing |\n",
    "\n",
    "### **Resource Utilization Patterns**\n",
    "\n",
    "| Workload Type | CPU Requirements | Memory Requirements | Storage Requirements | Recommended Instance |\n",
    "|--------------|------------------|-------------------|---------------------|---------------------|\n",
    "| **Light ETL** | 2-4 cores | 4-8GB | Standard | m5.xlarge |\n",
    "| **Heavy Transformations** | 4-8 cores | 6-12GB | Standard | c5.2xlarge |\n",
    "| **Complex Joins** | 2-4 cores | 8-16GB | High-memory | r5.xlarge |\n",
    "| **ML Feature Engineering** | 4-8 cores | 12-24GB | Standard | c5.4xlarge |\n",
    "\n",
    "## Interactive ETL Pipeline Visualizations\n",
    "\n",
    "Let's create comprehensive visualizations to monitor and analyze our ETL pipeline performance. These visualizations are designed to work excellently in Jupyter notebook environments:\n",
    "\n",
    "**Import Visualization Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32a9765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import visualization libraries for dashboard creation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Visualization libraries imported and configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bd7d15",
   "metadata": {},
   "source": [
    "**Create ETL Performance Dashboard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba0a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_etl_dashboard(processed_data):\n",
    "    \"\"\"Create a comprehensive ETL performance dashboard.\"\"\"\n",
    "    \n",
    "    print(\"Creating ETL performance dashboard...\")\n",
    "    \n",
    "    # Set up the dashboard layout\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('ETL Pipeline Performance Dashboard', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Convert Ray Dataset to pandas for visualization\n",
    "    sample_data = processed_data.take(1000)  # Sample for visualization\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    \n",
    "    return fig, axes, df\n",
    "\n",
    "# Initialize dashboard components\n",
    "dashboard_fig, dashboard_axes, dashboard_data = create_etl_dashboard(business_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef517c44",
   "metadata": {},
   "source": [
    "**Dashboard Panel 1: Data Volume Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ca6425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Volume by Region\n",
    "if 'region' in dashboard_data.columns:\n",
    "    region_counts = dashboard_data['region'].value_counts()\n",
    "    dashboard_axes[0, 0].bar(region_counts.index, region_counts.values, color='skyblue')\n",
    "    dashboard_axes[0, 0].set_title('Data Volume by Region')\n",
    "    dashboard_axes[0, 0].set_xlabel('Region')\n",
    "    dashboard_axes[0, 0].set_ylabel('Record Count')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(region_counts.values):\n",
    "        dashboard_axes[0, 0].text(i, v + 0.01*max(region_counts.values), str(v), ha='center')\n",
    "\n",
    "print(\"Data volume analysis panel created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2ef439",
   "metadata": {},
   "source": [
    "**Dashboard Panel 2: Processing Time Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89393f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Processing Time Distribution\n",
    "processing_times = np.random.normal(2.5, 0.8, len(dashboard_data))  # Simulated times\n",
    "dashboard_axes[0, 1].hist(processing_times, bins=20, color='lightgreen', alpha=0.7, edgecolor='black')\n",
    "dashboard_axes[0, 1].set_title('Processing Time Distribution')\n",
    "dashboard_axes[0, 1].set_xlabel('Processing Time (seconds)')\n",
    "dashboard_axes[0, 1].set_ylabel('Frequency')\n",
    "dashboard_axes[0, 1].axvline(np.mean(processing_times), color='red', linestyle='--', \n",
    "                           label=f'Mean: {np.mean(processing_times):.2f}s')\n",
    "dashboard_axes[0, 1].legend()\n",
    "\n",
    "print(\"Processing time analysis panel created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bedc9db",
   "metadata": {},
   "source": [
    "**Dashboard Panel 3: Data Quality Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cc85b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Data Quality Metrics\n",
    "quality_metrics = ['Valid Records', 'Invalid Records']\n",
    "quality_values = [85, 15]  # Example percentages\n",
    "colors = ['green', 'red']\n",
    "dashboard_axes[0, 2].pie(quality_values, labels=quality_metrics, colors=colors, \n",
    "                       autopct='%1.1f%%', startangle=90)\n",
    "dashboard_axes[0, 2].set_title('Data Quality Distribution')\n",
    "\n",
    "print(\"Data quality metrics panel created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaeb25d",
   "metadata": {},
   "source": [
    "**Display Complete Dashboard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae6fdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize and display the dashboard\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ETL Performance Dashboard created successfully!\")\n",
    "print(\"This dashboard provides insights into your ETL pipeline performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f39eee2",
   "metadata": {},
   "source": [
    "### **ETL Pipeline Status Monitoring**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d20955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_pipeline_status(datasets_dict):\n",
    "    \"\"\"Display comprehensive pipeline status in a visual format.\"\"\"\n",
    "    \n",
    "    print(\"ETL Pipeline Status Monitor\")\n",
    "    print(\"=\" * 90)\n",
    "    print(f\"{'Pipeline Stage':<25} {'Dataset':<20} {'Records':<12} {'Status':<15} {'Notes':<20}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    stages = [\n",
    "        (\"Data Extraction\", \"Raw Taxi Data\", taxi_data.count(), \"Complete\", \"From S3 Parquet\"),\n",
    "        (\"Data Validation\", \"Validated Data\", clean_taxi_data.count(), \"Complete\", \"Business rules applied\"),\n",
    "        (\"Data Aggregation\", \"Daily Metrics\", daily_metrics.count(), \"Complete\", \"Grouped by date\"),\n",
    "        (\"Data Enrichment\", \"Enriched Reviews\", enriched_reviews.count(), \"Complete\", \"Sentiment analysis\"),\n",
    "        (\"Data Storage\", \"Final Output\", \"Multiple\", \"Complete\", \"Parquet format\")\n",
    "    ]\n",
    "    \n",
    "    for stage, dataset, records, status, notes in stages:\n",
    "        record_str = f\"{records:,}\" if isinstance(records, int) else records\n",
    "        status_symbol = \"[OK]\" if status == \"Complete\" else \"[WARN]\"\n",
    "        print(f\"{stage:<25} {dataset:<20} {record_str:<12} {status_symbol} {status:<14} {notes:<20}\")\n",
    "    \n",
    "    print(\"-\" * 90)\n",
    "    print(\"Pipeline Status: All stages completed successfully\")\n",
    "    \n",
    "    # Resource utilization summary\n",
    "    cluster_resources = ray.cluster_resources()\n",
    "    print(f\"\\nCluster Resource Summary:\")\n",
    "    print(f\"  Available CPUs: {cluster_resources.get('CPU', 0)}\")\n",
    "    print(f\"  Available Memory: {cluster_resources.get('memory', 0) / 1e9:.1f}GB\")\n",
    "    print(f\"  Available GPUs: {cluster_resources.get('GPU', 0)}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Display the pipeline status\n",
    "pipeline_status = display_pipeline_status({\n",
    "    \"taxi_data\": taxi_data,\n",
    "    \"clean_taxi_data\": clean_taxi_data,\n",
    "    \"daily_metrics\": daily_metrics,\n",
    "    \"enriched_reviews\": enriched_reviews\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601bc3e3",
   "metadata": {},
   "source": [
    "### ETL Pipeline Flow Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb1ebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_etl_pipeline_diagram():\n",
    "    \"\"\"Create interactive ETL pipeline flow diagram.\"\"\"\n",
    "    print(\"Creating ETL pipeline flow diagram...\")\n",
    "    \n",
    "    # Create a network graph representing the ETL pipeline\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add nodes for different pipeline stages\n",
    "    pipeline_stages = {\n",
    "        'Data Sources': {'pos': (0, 2), 'color': 'lightblue', 'size': 3000},\n",
    "        'Extract': {'pos': (1, 2), 'color': 'lightgreen', 'size': 2500},\n",
    "        'Validate': {'pos': (2, 3), 'color': 'orange', 'size': 2000},\n",
    "        'Transform': {'pos': (2, 1), 'color': 'yellow', 'size': 2500},\n",
    "        'Aggregate': {'pos': (3, 2), 'color': 'lightcoral', 'size': 2000},\n",
    "        'Load': {'pos': (4, 2), 'color': 'lightpink', 'size': 2500},\n",
    "        'Data Warehouse': {'pos': (5, 2), 'color': 'lightgray', 'size': 3000}\n",
    "    }\n",
    "    \n",
    "    # Add nodes to graph\n",
    "    for stage, attrs in pipeline_stages.items():\n",
    "        G.add_node(stage, **attrs)\n",
    "    \n",
    "    # Add edges representing data flow\n",
    "    pipeline_edges = [\n",
    "        ('Data Sources', 'Extract'),\n",
    "        ('Extract', 'Validate'),\n",
    "        ('Extract', 'Transform'),\n",
    "        ('Validate', 'Aggregate'),\n",
    "        ('Transform', 'Aggregate'),\n",
    "        ('Aggregate', 'Load'),\n",
    "        ('Load', 'Data Warehouse')\n",
    "    ]\n",
    "    \n",
    "    G.add_edges_from(pipeline_edges)\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    pos = nx.get_node_attributes(G, 'pos')\n",
    "    colors = [pipeline_stages[node]['color'] for node in G.nodes()]\n",
    "    sizes = [pipeline_stages[node]['size'] for node in G.nodes()]\n",
    "    \n",
    "    # Draw the network\n",
    "    nx.draw(G, pos, with_labels=True, node_color=colors, node_size=sizes,\n",
    "            font_size=12, font_weight='bold', arrows=True, arrowsize=20,\n",
    "            edge_color='gray', linewidths=2, arrowstyle='->')\n",
    "    \n",
    "    plt.title('ETL Pipeline Flow Diagram', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('etl_pipeline_diagram.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"ETL pipeline diagram saved as 'etl_pipeline_diagram.png'\")\n",
    "\n",
    "# Create pipeline diagram\n",
    "create_etl_pipeline_diagram()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13ea1fc",
   "metadata": {},
   "source": [
    "### ETL Performance Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e220a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_etl_performance_dashboard():\n",
    "    \"\"\"Create comprehensive ETL performance monitoring dashboard.\"\"\"\n",
    "    print(\"Creating ETL performance dashboard...\")\n",
    "    \n",
    "    # Simulate performance metrics (in production, these would come from actual monitoring)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate sample performance data\n",
    "    time_points = pd.date_range(start='2024-01-01', periods=24, freq='H')\n",
    "    \n",
    "    performance_data = {\n",
    "        'timestamp': time_points,\n",
    "        'records_processed': np.random.randint(800000, 1200000, 24),\n",
    "        'processing_time': np.random.uniform(45, 90, 24),\n",
    "        'memory_usage': np.random.uniform(60, 85, 24),\n",
    "        'cpu_usage': np.random.uniform(70, 95, 24),\n",
    "        'error_rate': np.random.uniform(0, 2, 24),\n",
    "        'throughput': np.random.uniform(15000, 25000, 24)\n",
    "    }\n",
    "    \n",
    "    perf_df = pd.DataFrame(performance_data)\n",
    "    \n",
    "    # Create comprehensive dashboard\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=2,\n",
    "        subplot_titles=('Records Processed Over Time', 'Processing Time Trends',\n",
    "                       'Resource Usage', 'Throughput Analysis',\n",
    "                       'Error Rate Monitoring', 'Performance Correlation'),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": True}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Records processed over time\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=perf_df['timestamp'], y=perf_df['records_processed'],\n",
    "                  mode='lines+markers', name='Records Processed',\n",
    "                  line=dict(color='blue', width=3)),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Processing time trends\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=perf_df['timestamp'], y=perf_df['processing_time'],\n",
    "                  mode='lines+markers', name='Processing Time (min)',\n",
    "                  line=dict(color='green', width=3)),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Resource usage (dual axis)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=perf_df['timestamp'], y=perf_df['memory_usage'],\n",
    "                  mode='lines', name='Memory Usage (%)',\n",
    "                  line=dict(color='red', width=2)),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=perf_df['timestamp'], y=perf_df['cpu_usage'],\n",
    "                  mode='lines', name='CPU Usage (%)',\n",
    "                  line=dict(color='orange', width=2)),\n",
    "        row=2, col=1, secondary_y=True\n",
    "    )\n",
    "    \n",
    "    # 4. Throughput analysis\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=perf_df['timestamp'], y=perf_df['throughput'],\n",
    "               name='Throughput (records/sec)', marker_color='lightblue'),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # 5. Error rate monitoring\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=perf_df['timestamp'], y=perf_df['error_rate'],\n",
    "                  mode='lines+markers', name='Error Rate (%)',\n",
    "                  line=dict(color='red', width=3),\n",
    "                  fill='tozeroy', fillcolor='rgba(255,0,0,0.1)'),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    \n",
    "    # 6. Performance correlation heatmap\n",
    "    correlation_data = perf_df[['records_processed', 'processing_time', 'memory_usage', \n",
    "                               'cpu_usage', 'throughput']].corr()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Heatmap(z=correlation_data.values,\n",
    "                  x=correlation_data.columns,\n",
    "                  y=correlation_data.index,\n",
    "                  colorscale='RdBu',\n",
    "                  zmid=0,\n",
    "                  text=correlation_data.round(2).values,\n",
    "                  texttemplate=\"%{text}\",\n",
    "                  showscale=True),\n",
    "        row=3, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title_text=\"ETL Performance Monitoring Dashboard\",\n",
    "        height=1000,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Update axes\n",
    "    fig.update_xaxes(title_text=\"Time\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Records\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Time\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Minutes\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Time\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Memory %\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"CPU %\", row=2, col=1, secondary_y=True)\n",
    "    fig.update_xaxes(title_text=\"Time\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Records/sec\", row=2, col=2)\n",
    "    fig.update_xaxes(title_text=\"Time\", row=3, col=1)\n",
    "    fig.update_yaxes(title_text=\"Error %\", row=3, col=1)\n",
    "    \n",
    "    # Save and show\n",
    "    fig.write_html(\"etl_performance_dashboard.html\")\n",
    "    print(\"ETL performance dashboard saved as 'etl_performance_dashboard.html'\")\n",
    "    fig.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create performance dashboard\n",
    "performance_dashboard = create_etl_performance_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32893e54",
   "metadata": {},
   "source": [
    "### Data Quality Monitoring Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23792d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_quality_dashboard():\n",
    "    \"\"\"Create data quality monitoring dashboard.\"\"\"\n",
    "    print(\"Creating data quality monitoring dashboard...\")\n",
    "    \n",
    "    # Simulate data quality metrics\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create comprehensive quality metrics\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
    "    fig.suptitle('ETL Data Quality Monitoring Dashboard', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Data Completeness by Source\n",
    "    ax1 = axes[0, 0]\n",
    "    sources = ['Customer DB', 'Orders API', 'Product Feed', 'Analytics Events']\n",
    "    completeness = [95.2, 98.7, 92.1, 89.5]\n",
    "    colors = ['green' if x > 95 else 'orange' if x > 90 else 'red' for x in completeness]\n",
    "    \n",
    "    bars = ax1.bar(sources, completeness, color=colors, alpha=0.7)\n",
    "    ax1.set_title('Data Completeness by Source', fontweight='bold')\n",
    "    ax1.set_ylabel('Completeness (%)')\n",
    "    ax1.axhline(y=95, color='red', linestyle='--', alpha=0.5, label='Target: 95%')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, completeness):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{value}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Data Freshness Trends\n",
    "    ax2 = axes[0, 1]\n",
    "    hours = list(range(24))\n",
    "    freshness_delay = np.random.exponential(2, 24)  # Exponential distribution for realistic delays\n",
    "    \n",
    "    ax2.plot(hours, freshness_delay, 'b-o', linewidth=2, markersize=4)\n",
    "    ax2.fill_between(hours, freshness_delay, alpha=0.3)\n",
    "    ax2.set_title('Data Freshness (Delay in Hours)', fontweight='bold')\n",
    "    ax2.set_xlabel('Hour of Day')\n",
    "    ax2.set_ylabel('Delay (hours)')\n",
    "    ax2.axhline(y=1, color='red', linestyle='--', alpha=0.5, label='SLA: 1 hour')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Schema Validation Results\n",
    "    ax3 = axes[0, 2]\n",
    "    validation_results = ['Pass', 'Warning', 'Fail']\n",
    "    validation_counts = [850, 120, 30]\n",
    "    colors_validation = ['green', 'orange', 'red']\n",
    "    \n",
    "    wedges, texts, autotexts = ax3.pie(validation_counts, labels=validation_results, \n",
    "                                      autopct='%1.1f%%', colors=colors_validation,\n",
    "                                      startangle=90)\n",
    "    ax3.set_title('Schema Validation Results', fontweight='bold')\n",
    "    \n",
    "    # 4. Data Volume Trends\n",
    "    ax4 = axes[1, 0]\n",
    "    days = pd.date_range(start='2024-01-01', periods=30, freq='D')\n",
    "    daily_volumes = np.random.normal(1000000, 150000, 30)  # ~1M records per day\n",
    "    \n",
    "    ax4.plot(days, daily_volumes/1000000, 'g-', linewidth=2)\n",
    "    ax4.fill_between(days, daily_volumes/1000000, alpha=0.3, color='green')\n",
    "    ax4.set_title('Daily Data Volume Trends', fontweight='bold')\n",
    "    ax4.set_xlabel('Date')\n",
    "    ax4.set_ylabel('Volume (Millions of Records)')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Data Type Distribution\n",
    "    ax5 = axes[1, 1]\n",
    "    data_types = ['String', 'Integer', 'Float', 'Date', 'Boolean', 'JSON']\n",
    "    type_counts = [35, 25, 20, 10, 5, 5]\n",
    "    \n",
    "    bars = ax5.barh(data_types, type_counts, color='skyblue', alpha=0.7)\n",
    "    ax5.set_title('Data Type Distribution', fontweight='bold')\n",
    "    ax5.set_xlabel('Percentage of Columns')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, type_counts):\n",
    "        width = bar.get_width()\n",
    "        ax5.text(width + 0.5, bar.get_y() + bar.get_height()/2.,\n",
    "                f'{value}%', ha='left', va='center', fontweight='bold')\n",
    "    \n",
    "    # 6. Duplicate Detection\n",
    "    ax6 = axes[1, 2]\n",
    "    duplicate_sources = ['Customer', 'Product', 'Order', 'Event']\n",
    "    duplicate_rates = [2.1, 0.8, 1.5, 4.2]\n",
    "    colors_dup = ['red' if x > 3 else 'orange' if x > 1 else 'green' for x in duplicate_rates]\n",
    "    \n",
    "    bars = ax6.bar(duplicate_sources, duplicate_rates, color=colors_dup, alpha=0.7)\n",
    "    ax6.set_title('Duplicate Detection Rates', fontweight='bold')\n",
    "    ax6.set_ylabel('Duplicate Rate (%)')\n",
    "    ax6.axhline(y=1, color='orange', linestyle='--', alpha=0.5, label='Warning: 1%')\n",
    "    ax6.axhline(y=3, color='red', linestyle='--', alpha=0.5, label='Critical: 3%')\n",
    "    ax6.legend()\n",
    "    \n",
    "    # 7. Processing Error Trends\n",
    "    ax7 = axes[2, 0]\n",
    "    error_hours = list(range(24))\n",
    "    error_counts = np.random.poisson(5, 24)  # Poisson distribution for error counts\n",
    "    \n",
    "    ax7.bar(error_hours, error_counts, color='red', alpha=0.6, width=0.8)\n",
    "    ax7.set_title('Processing Errors by Hour', fontweight='bold')\n",
    "    ax7.set_xlabel('Hour of Day')\n",
    "    ax7.set_ylabel('Error Count')\n",
    "    ax7.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 8. Data Quality Score Over Time\n",
    "    ax8 = axes[2, 1]\n",
    "    quality_days = pd.date_range(start='2024-01-01', periods=30, freq='D')\n",
    "    quality_scores = np.random.normal(92, 3, 30)  # Quality scores around 92%\n",
    "    quality_scores = np.clip(quality_scores, 80, 100)  # Clip to realistic range\n",
    "    \n",
    "    ax8.plot(quality_days, quality_scores, 'purple', linewidth=2, marker='o', markersize=3)\n",
    "    ax8.fill_between(quality_days, quality_scores, alpha=0.3, color='purple')\n",
    "    ax8.set_title('Overall Data Quality Score', fontweight='bold')\n",
    "    ax8.set_xlabel('Date')\n",
    "    ax8.set_ylabel('Quality Score (%)')\n",
    "    ax8.axhline(y=90, color='green', linestyle='--', alpha=0.5, label='Target: 90%')\n",
    "    ax8.legend()\n",
    "    ax8.tick_params(axis='x', rotation=45)\n",
    "    ax8.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 9. ETL Stage Performance\n",
    "    ax9 = axes[2, 2]\n",
    "    etl_stages = ['Extract', 'Transform', 'Load']\n",
    "    avg_times = [12.5, 35.2, 18.7]  # Average processing times in minutes\n",
    "    std_times = [2.1, 5.8, 3.2]     # Standard deviations\n",
    "    \n",
    "    bars = ax9.bar(etl_stages, avg_times, yerr=std_times, capsize=5,\n",
    "                   color=['lightblue', 'lightgreen', 'lightcoral'], alpha=0.7)\n",
    "    ax9.set_title('ETL Stage Performance', fontweight='bold')\n",
    "    ax9.set_ylabel('Processing Time (minutes)')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, avg_times):\n",
    "        height = bar.get_height()\n",
    "        ax9.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{value:.1f}m', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('etl_data_quality_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"ETL data quality dashboard saved as 'etl_data_quality_dashboard.png'\")\n",
    "\n",
    "# Create data quality dashboard\n",
    "create_data_quality_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217587f8",
   "metadata": {},
   "source": [
    "### Real-time ETL Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b449be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_realtime_etl_monitor():\n",
    "    \"\"\"Create real-time ETL monitoring visualization.\"\"\"\n",
    "    print(\"Creating real-time ETL monitoring system...\")\n",
    "    \n",
    "    # Simulate real-time metrics\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Create streaming-style data\n",
    "    time_points = pd.date_range(start='2024-01-01 00:00:00', periods=100, freq='1min')\n",
    "    \n",
    "    # Simulate different metrics\n",
    "    throughput = 15000 + 5000 * np.sin(np.linspace(0, 4*np.pi, 100)) + np.random.normal(0, 1000, 100)\n",
    "    latency = 50 + 20 * np.sin(np.linspace(0, 2*np.pi, 100)) + np.random.normal(0, 5, 100)\n",
    "    error_rate = np.maximum(0, 1 + 0.5 * np.sin(np.linspace(0, 6*np.pi, 100)) + np.random.normal(0, 0.3, 100))\n",
    "    \n",
    "    # Create subplot with secondary y-axis\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "    \n",
    "    # Add throughput trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=time_points, y=throughput,\n",
    "                  mode='lines', name='Throughput (records/min)',\n",
    "                  line=dict(color='blue', width=2)),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "    \n",
    "    # Add latency trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=time_points, y=latency,\n",
    "                  mode='lines', name='Latency (ms)',\n",
    "                  line=dict(color='green', width=2)),\n",
    "        secondary_y=True,\n",
    "    )\n",
    "    \n",
    "    # Add error rate trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=time_points, y=error_rate,\n",
    "                  mode='lines', name='Error Rate (%)',\n",
    "                  line=dict(color='red', width=2),\n",
    "                  fill='tozeroy', fillcolor='rgba(255,0,0,0.1)'),\n",
    "        secondary_y=True,\n",
    "    )\n",
    "    \n",
    "    # Add threshold lines\n",
    "    fig.add_hline(y=10000, line_dash=\"dash\", line_color=\"blue\", \n",
    "                  annotation_text=\"Min Throughput\", secondary_y=False)\n",
    "    fig.add_hline(y=100, line_dash=\"dash\", line_color=\"orange\", \n",
    "                  annotation_text=\"Max Latency\", secondary_y=True)\n",
    "    fig.add_hline(y=5, line_dash=\"dash\", line_color=\"red\", \n",
    "                  annotation_text=\"Max Error Rate\", secondary_y=True)\n",
    "    \n",
    "    # Set y-axes titles\n",
    "    fig.update_yaxes(title_text=\"Throughput (records/min)\", secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"Latency (ms) / Error Rate (%)\", secondary_y=True)\n",
    "    \n",
    "    # Set x-axis title\n",
    "    fig.update_xaxes(title_text=\"Time\")\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_text=\"Real-time ETL Pipeline Monitoring\",\n",
    "        height=600,\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    \n",
    "    # Save and show\n",
    "    fig.write_html(\"realtime_etl_monitor.html\")\n",
    "    print(\"Real-time ETL monitor saved as 'realtime_etl_monitor.html'\")\n",
    "    fig.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create real-time monitor\n",
    "realtime_monitor = create_realtime_etl_monitor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6a5ee6",
   "metadata": {},
   "source": [
    "### System Resource Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e3b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_system_resource_dashboard():\n",
    "    \"\"\"Create system resource monitoring dashboard.\"\"\"\n",
    "    print(\"Creating system resource monitoring dashboard...\")\n",
    "    \n",
    "    # Get actual system information\n",
    "    cpu_percent = psutil.cpu_percent(interval=1, percpu=True)\n",
    "    memory = psutil.virtual_memory()\n",
    "    disk = psutil.disk_usage('/')\n",
    "    \n",
    "    # Create system resource visualization\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('System Resource Monitoring for ETL Pipeline', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. CPU Usage by Core\n",
    "    cores = [f'Core {i+1}' for i in range(len(cpu_percent))]\n",
    "    colors = ['red' if cpu > 80 else 'orange' if cpu > 60 else 'green' for cpu in cpu_percent]\n",
    "    \n",
    "    bars = ax1.bar(cores, cpu_percent, color=colors, alpha=0.7)\n",
    "    ax1.set_title('CPU Usage by Core', fontweight='bold')\n",
    "    ax1.set_ylabel('CPU Usage (%)')\n",
    "    ax1.axhline(y=80, color='red', linestyle='--', alpha=0.5, label='Critical: 80%')\n",
    "    ax1.axhline(y=60, color='orange', linestyle='--', alpha=0.5, label='Warning: 60%')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, cpu_percent):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Memory Usage\n",
    "    memory_data = [\n",
    "        ('Used', memory.used / (1024**3), 'red'),\n",
    "        ('Available', memory.available / (1024**3), 'green'),\n",
    "        ('Cached', (memory.cached if hasattr(memory, 'cached') else 0) / (1024**3), 'blue')\n",
    "    ]\n",
    "    \n",
    "    labels, values, colors_mem = zip(*memory_data)\n",
    "    ax2.pie(values, labels=labels, colors=colors_mem, autopct='%1.1f%%', startangle=90)\n",
    "    ax2.set_title(f'Memory Usage (Total: {memory.total / (1024**3):.1f} GB)', fontweight='bold')\n",
    "    \n",
    "    # 3. Disk Usage\n",
    "    disk_data = [\n",
    "        ('Used', disk.used / (1024**3), 'red'),\n",
    "        ('Free', disk.free / (1024**3), 'green')\n",
    "    ]\n",
    "    \n",
    "    labels_disk, values_disk, colors_disk = zip(*disk_data)\n",
    "    ax3.pie(values_disk, labels=labels_disk, colors=colors_disk, autopct='%1.1f%%', startangle=90)\n",
    "    ax3.set_title(f'Disk Usage (Total: {disk.total / (1024**3):.1f} GB)', fontweight='bold')\n",
    "    \n",
    "    # 4. Resource Trends (simulated)\n",
    "    hours = list(range(24))\n",
    "    cpu_trend = 30 + 20 * np.sin(np.linspace(0, 2*np.pi, 24)) + np.random.normal(0, 5, 24)\n",
    "    memory_trend = 60 + 15 * np.sin(np.linspace(0, 4*np.pi, 24)) + np.random.normal(0, 3, 24)\n",
    "    \n",
    "    ax4.plot(hours, cpu_trend, 'b-o', label='CPU Usage (%)', linewidth=2, markersize=4)\n",
    "    ax4.plot(hours, memory_trend, 'r-s', label='Memory Usage (%)', linewidth=2, markersize=4)\n",
    "    ax4.set_title('24-Hour Resource Trends', fontweight='bold')\n",
    "    ax4.set_xlabel('Hour of Day')\n",
    "    ax4.set_ylabel('Usage (%)')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('system_resource_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"System resource dashboard saved as 'system_resource_dashboard.png'\")\n",
    "    \n",
    "    # Print current system status\n",
    "    print(f\"\\nCurrent System Status:\")\n",
    "    print(f\"  CPU Usage: {psutil.cpu_percent():.1f}%\")\n",
    "    print(f\"  Memory Usage: {memory.percent:.1f}%\")\n",
    "    print(f\"  Disk Usage: {(disk.used / disk.total) * 100:.1f}%\")\n",
    "\n",
    "# Create system resource dashboard\n",
    "create_system_resource_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b040d1b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### **Common Issues**\n",
    "1. **Memory Pressure**: Reduce batch size or increase cluster memory\n",
    "2. **Slow Performance**: Optimize block size and parallelism settings\n",
    "3. **Data Skew**: Implement data redistribution strategies\n",
    "4. **Resource Contention**: Balance CPU and memory allocation\n",
    "\n",
    "### **Debug Mode**\n",
    "Enable detailed logging and performance monitoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529ad585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# Enable Ray Data debugging\n",
    "from ray.data.context import DataContext\n",
    "ctx = DataContext.get_current()\n",
    "ctx.enable_progress_bars = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045933c1",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Scale to Production**: Deploy to multi-node clusters with proper resource allocation\n",
    "2. **Add Data Sources**: Connect to your specific data systems and formats\n",
    "3. **Implement Monitoring**: Set up comprehensive pipeline monitoring and alerting\n",
    "4. **Optimize Performance**: Fine-tune based on your specific workload characteristics\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Ray Data Documentation](https://docs.ray.io/en/latest/data/index.html)\n",
    "- [Ray Data Performance Guide](https://docs.ray.io/en/latest/data/performance-tips.html)\n",
    "- [ETL Best Practices](https://docs.ray.io/en/latest/data/best-practices.html)\n",
    "- [Large-Scale Data Processing](https://docs.ray.io/en/latest/data/batch_inference.html)\n",
    "\n",
    "---\n",
    "\n",
    "*This template demonstrates Ray Data's native capabilities for large-scale ETL processing. Focus on using Ray Data's built-in operations for optimal performance and scalability.*"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
