{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM offline batch inference with Ray Data LLM APIs\n",
    "\n",
    "**⏱️ Time to complete**: 10 min\n",
    "\n",
    "\n",
    "This notebook shows you how to run batch inference for LLMs using [Ray Data LLM](https://docs.ray.io/en/latest/data/api/llm.html).\n",
    "\n",
    "**Note:** This tutorial runs within a workspace. Review the [Introduction to Workspaces](https://docs.anyscale.com/examples/intro-workspaces) template before this tutorial.\n",
    "\n",
    "\n",
    "### Deciding between online vs offline inference for LLM\n",
    "Use online LLM inference (e.g., Anyscale Endpoints) to get real-time responses for prompts or to interact with the LLM. Use online inference when you want to optimize latency of inference.\n",
    "\n",
    "On the other hand, use offline LLM inference, also referred to as batch inference, when you want to get responses for a large number of prompts within some time frame, but not necessarily in real-time, for example in minutes to hours. Use offline inference when you want to:\n",
    "1. Process large-scale datasets.\n",
    "2. Optimize inference throughput and resource usage. For example, maximizing GPU utilization.\n",
    "\n",
    "This tutorial focuses on the latter, using offline LLM inference for a summarization task using real-world news articles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare a Ray Data dataset\n",
    "\n",
    "Ray Data LLM runs batch inference for LLMs on Ray Data datasets. In this tutorial, we will perform batch inference with an LLM to reformat dates. Our source is a 2-million-row CSV file containing sample customer data.\n",
    "First, we load the data from a remote URL. Then, to ensure the workload can be distributed across multiple GPUs, we repartition the dataset. This step is crucial for achieving parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "# Define the path to the sample CSV file hosted on S3.\n",
    "# This dataset contains 2 million rows of synthetic customer data.\n",
    "path = \"https://llm-guide.s3.us-west-2.amazonaws.com/data/ray-data-llm/customers-2000000.csv\"\n",
    "\n",
    "# Load the CSV file into a Ray Dataset.\n",
    "print(\"Loading dataset from remote URL...\")\n",
    "ds = ray.data.read_csv(path)\n",
    "\n",
    "# You can inspect the dataset schema and a few rows to verify it loaded correctly.\n",
    "# print(ds.schema())\n",
    "# ds.show(limit=2)\n",
    "\n",
    "# For this example, we'll limit the dataset to 100,000 rows for faster processing.\n",
    "print(\"Limiting dataset to 100,000 rows.\")\n",
    "ds = ds.limit(100000)\n",
    "\n",
    "# Repartition the dataset to enable parallelism across multiple workers (e.g., GPUs).\n",
    "# By default, a large remote file might be read into a single block. Repartitioning\n",
    "# splits the data into a specified number of blocks, allowing Ray to process them\n",
    "# in parallel. \n",
    "num_partitions = 128\n",
    "print(f\"Repartitioning dataset into {num_partitions} blocks for parallelism...\")\n",
    "ds = ds.repartition(num_blocks=num_partitions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the processor config for the vLLM engine\n",
    "\n",
    "You also need to define the model configs for the LLM engine, which configures the model and compute resources needed for inference. \n",
    "\n",
    "Make sure to provide your [Hugging Face user access token](https://huggingface.co/docs/hub/en/security-tokens). Ray uses this token to authenticate and download the model and Hugging Face **requires the token for official LLaMA, Mistral, and Gemma models**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"Insert your Hugging Face token here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example uses the `meta-llama/Meta-Llama-3.1-8B-Instruct` model.\n",
    "You also need to define a configuration associated with the model you want to use to configure the compute resources, engine arguments, and other inference engine specific parameters. For more details on the configs you can pass to vLLM engine, see [vLLM doc](https://docs.vllm.ai/en/latest/serving/engine_args.html).\n",
    "\n",
    "Note that because our input prompts and expected output token lengths are small, we have set `batch_size=256` in this case. However, depending on your workload, a large batch size can lead to increased idle GPU time when decoding long sequences. Be sure to adjust this value to find the optimal trade-off between throughput and latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.llm import vLLMEngineProcessorConfig\n",
    "\n",
    "\n",
    "processor_config = vLLMEngineProcessorConfig(\n",
    "    model_source=\"unsloth/Llama-3.1-8B-Instruct\",\n",
    "    engine_kwargs=dict(\n",
    "        tensor_parallel_size=1,\n",
    "        pipeline_parallel_size=1,\n",
    "        max_model_len=4096,\n",
    "        enable_chunked_prefill=True,\n",
    "        max_num_batched_tokens=1024,\n",
    "        gpu_memory_utilization=0.85,\n",
    "    ),\n",
    "    # Override Ray's runtime env to include the Hugging Face token. Ray is being used under the hood to orchestrate the inference pipeline.\n",
    "    runtime_env=dict(\n",
    "        env_vars=dict(\n",
    "            HF_TOKEN=HF_TOKEN,\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=256,\n",
    "    accelerator_type=\"L4\",\n",
    "    concurrency=4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the preprocess and postprocess functions\n",
    "\n",
    "The task is to format the `Subscription Date`as the format `MM-DD-YYYY` using LLM. \n",
    "\n",
    "Define the preprocess function to prepare `messages` and `sampling_params` for vLLM engine, and the postprocessor function to consume `generated_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "# Preprocess function prepares `messages` and `sampling_params` for vLLM engine, and\n",
    "# all other fields will be ignored.\n",
    "def preprocess(row: dict[str, Any]) -> dict[str, Any]:\n",
    "    return dict(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Convert this date:\\n{row['Subscription Date']}\\n\\n as the format:MM-DD-YYYY\"\n",
    "            },\n",
    "        ],\n",
    "        sampling_params=dict(\n",
    "            temperature=0.3,\n",
    "            max_tokens=150,\n",
    "            detokenize=False,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "# Input row of postprocess function will have `generated_text`. Also `**row` syntax\n",
    "# can be used to return all the original columns in the input dataset.\n",
    "def postprocess(row: dict[str, Any]) -> dict[str, Any]:\n",
    "    return {\n",
    "        \"resp\": row[\"generated_text\"],\n",
    "        **row,  # This will return all the original columns in the dataset.\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build and run the processor\n",
    "\n",
    "\n",
    "With the processors and configs defined, you can now build then run the processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.llm import build_llm_processor\n",
    "from pprint import pprint\n",
    "\n",
    "processor = build_llm_processor(\n",
    "    processor_config,\n",
    "    preprocess=preprocess,\n",
    "    postprocess=postprocess,\n",
    ")\n",
    "\n",
    "processed_ds = processor(ds.limit(10_000))\n",
    "# Materialize the dataset to memory. User can also use writing APIs like\n",
    "# `write_parquet`(https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.write_parquet.html#ray.data.Dataset.write_parquet)\n",
    "# `write_csv`(https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.write_csv.html#ray.data.Dataset.write_csv)\n",
    "# to persist the dataset.\n",
    "processed_ds = processed_ds.materialize()\n",
    "\n",
    "\n",
    "# Peek the first 3 entries.\n",
    "sampled = processed_ds.take(3)\n",
    "print(\"==================GENERATED OUTPUT===============\")\n",
    "\n",
    "pprint(sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring the execution\n",
    "\n",
    "Use the Ray Dashboard to monitor the execution. In the **Ray Dashboard** tab, navigate to the **Job** page and open the **Ray Data Overview** section. Click the link for the running job, and open the **Ray Data Overview** section to view the details of the batch inference execution:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/templates/main/templates/batch-llm/assets/ray-data-jobs.png\" width=900px />\n",
    "\n",
    "### Handling GPU out-of-memory failures\n",
    "If you run into CUDA out of memory, your batch size is likely too large. Set an explicit small batch size or use a smaller model, or a larger GPU.\n",
    "\n",
    "## Advanced: Image query with a vision language model\n",
    "\n",
    "Ray Data LLM also supports running batch inference with vision language models. This example shows how\n",
    "to prepare a dataset with images and run batch inference with a vision language model.\n",
    "\n",
    "We applied 2 adjustments on top of the previous example:\n",
    "* set `has_image=True` in `vLLMEngineProcessorConfig`\n",
    "* prepare image input inside preprocessor\n",
    "\n",
    "**Restart your Anyscale Workspace**\n",
    "\n",
    "To free up GPU memory held by the previously loaded LLM and prevent out-of-memory (OOM) errors, please restart your workspace before running your next job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install datasets library.\n",
    "!pip install \"datasets<4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your HF token\n",
    "HF_TOKEN = \"Insert your Hugging Face token here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray \n",
    "import datasets\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from ray.data.llm import vLLMEngineProcessorConfig\n",
    "\n",
    "# Load \"LMMs-Eval-Lite\" dataset from Hugging Face.\n",
    "vision_dataset_llms_lite = datasets.load_dataset(\"lmms-lab/LMMs-Eval-Lite\", \"coco2017_cap_val\")\n",
    "vision_dataset = ray.data.from_huggingface(vision_dataset_llms_lite[\"lite\"])\n",
    "\n",
    "vision_processor_config = vLLMEngineProcessorConfig(\n",
    "    model_source=\"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    engine_kwargs=dict(\n",
    "        tensor_parallel_size=1,\n",
    "        pipeline_parallel_size=1,\n",
    "        max_model_len=4096,\n",
    "        enable_chunked_prefill=True,\n",
    "        max_num_batched_tokens=2048,\n",
    "    ),\n",
    "    # Override Ray's runtime env to include the Hugging Face token. Ray Data uses Ray under the hood to orchestrate the inference pipeline.\n",
    "    runtime_env=dict(\n",
    "        env_vars=dict(\n",
    "            # HF_TOKEN=HF_TOKEN, # Qwen model is not gated so HF token is not needed\n",
    "            VLLM_USE_V1=\"1\",\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=16,\n",
    "    accelerator_type=\"L4\",\n",
    "    concurrency=4,\n",
    "    has_image=True,\n",
    ")\n",
    "\n",
    "def vision_preprocess(row: dict) -> dict:\n",
    "    choice_indices = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n",
    "    return dict(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"Analyze the image and question carefully, using step-by-step reasoning.\n",
    "First, describe any image provided in detail. Then, present your reasoning. And finally your final answer in this format:\n",
    "Final Answer: <answer>\n",
    "where <answer> is:\n",
    "- The single correct letter choice A, B, C, D, E, F, etc. when options are provided. Only include the letter.\n",
    "- Your direct answer if no options are given, as a single phrase or number.\n",
    "- If your answer is a number, only include the number without any unit.\n",
    "- If your answer is a word or phrase, do not paraphrase or reformat the text you see in the image.\n",
    "- You cannot answer that the question is unanswerable. You must either pick an option or provide a direct answer.\n",
    "IMPORTANT: Remember, to end your answer with Final Answer: <answer>.\"\"\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": row[\"question\"] + \"\\n\\n\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        # Ray Data accepts PIL Image or image URL.\n",
    "                        \"image\": Image.open(BytesIO(row[\"image\"][\"bytes\"]))\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"\\n\\nChoices:\\n\" + \"\\n\".join([f\"{choice_indices[i]}. {choice}\" for i, choice in enumerate(row[\"answer\"])])\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "        ],\n",
    "        sampling_params=dict(\n",
    "            temperature=0.3,\n",
    "            max_tokens=150,\n",
    "            detokenize=False,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "def vision_postprocess(row: dict) -> dict:\n",
    "    return {\n",
    "        \"resp\": row[\"generated_text\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to previous example, build and run the processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.llm import build_llm_processor\n",
    "\n",
    "vision_processor = build_llm_processor(\n",
    "    vision_processor_config,\n",
    "    preprocess=vision_preprocess,\n",
    "    postprocess=vision_postprocess,\n",
    ")\n",
    "\n",
    "vision_processed_ds = vision_processor(vision_dataset).materialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to previous example, peek the first 3 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# Peek the first 3 entries.\n",
    "vision_sampled = vision_processed_ds.take(3)\n",
    "print(\"==================GENERATED OUTPUT===============\")\n",
    "pprint(vision_sampled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "- Created a custom processor for the CNN/DailyMail summarization task.\n",
    "- Defined the model configs for the Meta Llama 3.1 8B model.\n",
    "- Ran the batch inference through Ray Data LLM API and monitored the execution.\n",
    "- As an advanced usage, ran the batch vision query through Ray Data LLM API\n",
    "  * Constructed vision understanding task with COCO dataset\n",
    "  * Using Qwen2.5-VL-3B-Instruct model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
