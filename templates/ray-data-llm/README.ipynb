{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM offline batch inference with Ray Data LLM APIs\n",
    "\n",
    "**⏱️ Time to complete**: 10 min\n",
    "\n",
    "\n",
    "<!-- TODO: add a link for the API reference -->\n",
    "This template shows you how to run batch inference for LLMs using Ray Data LLM.\n",
    "\n",
    "**Note:** This tutorial runs within a workspace. Review the `Introduction to Workspaces` template before this tutorial.\n",
    "\n",
    "\n",
    "### How to decide between online vs offline inference for LLM\n",
    "Online LLM inference (e.g. Anyscale Endpoint) should be used when you want to get real-time response for prompt or to interact with the LLM. Use online inference when you want to optimize latency of inference to be as quick as possible.\n",
    "\n",
    "On the other hand, offline LLM inference (also referred to as batch inference) should be used when you want to get reponses for a large number of prompts within some time frame, but not required to be real-time (minutes to hours granularity). Use offline inference when you want to:\n",
    "1. Process large-scale datasets\n",
    "2. Optimize inference throughput and resource usage (for example, maximizing GPU utilization).\n",
    "\n",
    "In this tutorial, we will focus on the latter, using offline LLM inference for a summarization task using real-world news articles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare a Ray Data dataset\n",
    "\n",
    "Ray Data LLM runs batch inference for LLMs on Ray Data datasets. In this tutorial, we will run batch inference with an LLM that summarizes news articles from [`CNNDailyMail`](https://huggingface.co/datasets/abisee/cnn_dailymail) dataset, which is a collection of news articles. And we will summarize each article with our batch inferencing pipeline. We will cover more details on how to customize the pipeline in the later sections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the datasets library\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray \n",
    "import datasets\n",
    "\n",
    "# Load the dataset from Hugging Face into Ray Data. Refer to Ray Data APIs\n",
    "# https://docs.ray.io/en/latest/data/api/input_output.html for details.\n",
    "# For example, you can use ray.data.read_json(dataset_file) to load dataset in JSONL.\n",
    "\n",
    "df = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "ds = ray.data.from_huggingface(df[\"train\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the processor config for vLLM engine\n",
    "\n",
    "We will also need to define the model configs for the LLM engine, which configures the model and compute resources needed for inference. \n",
    "\n",
    "Make sure to provide your [Hugging Face user access token](https://huggingface.co/docs/hub/en/security-tokens). This will be used to authenticate/download the model and **is required for official LLaMA, Mistral, and Gemma models**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"insert your hugging face token here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will be using the `meta-llama/Meta-Llama-3.1-8B-Instruct` model.\n",
    "We will also need to define a configuration associated with the model we want to use to configure the compute resources, engine arguments and other inference engine specific parameters. For more details on the configs passed to vLLM engine, see [vLLM doc](https://docs.vllm.ai/en/latest/serving/engine_args.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.llm import vLLMEngineProcessorConfig\n",
    "\n",
    "\n",
    "processor_config = vLLMEngineProcessorConfig(\n",
    "    model_source=\"unsloth/Llama-3.1-8B-Instruct\",\n",
    "    engine_kwargs=dict(\n",
    "        tensor_parallel_size=1,\n",
    "        pipeline_parallel_size=1,\n",
    "        max_model_len=16384,\n",
    "        enable_chunked_prefill=True,\n",
    "        max_num_batched_tokens=2048,\n",
    "    ),\n",
    "    # Override Ray's runtime env to include the Hugging Face token. Ray is being used under the hood to orchestrate the inference pipeline.\n",
    "    runtime_env=dict(\n",
    "        env_vars=dict(\n",
    "            HF_TOKEN=HF_TOKEN,\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=16,\n",
    "    accelerator_type=\"L4\",\n",
    "    concurrency=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the preprocess and postprocess functions\n",
    "\n",
    "\n",
    "We will need to define the preprocess function to prepare `messages` and `sampling_params` for vLLM engine, and also postprocessor function to consume `generated_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "# Preprocess function prepares `messages` and `sampling_params` for vLLM engine, and\n",
    "# all other fields will be ignored.\n",
    "def preprocess(row: dict[str, Any]) -> dict[str, Any]:\n",
    "    return dict(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a commentator. Your task is to \"\n",
    "                \"summarize highlights from article.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"# Article:\\n{row['article']}\\n\\n\"\n",
    "                \"#Instructions:\\nIn clear and concise language, \"\n",
    "                \"summarize the highlights presented in the article.\",\n",
    "            },\n",
    "        ],\n",
    "        sampling_params=dict(\n",
    "            temperature=0.3,\n",
    "            max_tokens=150,\n",
    "            detokenize=False,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "# Input row of postprocess function will have `generated_text`. Alse `**row` syntax\n",
    "# can be used to return all the original columns in the input dataset.\n",
    "def postprocess(row: dict[str, Any]) -> dict[str, Any]:\n",
    "    return {\n",
    "        \"resp\": row[\"generated_text\"],\n",
    "        **row,  # This will return all the original columns in the dataset.\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build and run the processor\n",
    "\n",
    "\n",
    "With the processors and configs defined, we can now build then run the processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.llm import build_llm_processor\n",
    "\n",
    "processor = build_llm_processor(\n",
    "    processor_config,\n",
    "    preprocess=preprocess,\n",
    "    postprocess=postprocess,\n",
    ")\n",
    "\n",
    "ds = processor(ds)\n",
    "# Materialize the dataset to memory. User can also use writing APIs like\n",
    "# `write_parquet`(https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.write_parquet.html#ray.data.Dataset.write_parquet)\n",
    "# `write_csv`(https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.write_csv.html#ray.data.Dataset.write_csv)\n",
    "# to persist the dataset.\n",
    "ds = ds.materialize()\n",
    "\n",
    "\n",
    "# Peak the first 3 entries. \n",
    "sampled = ds.take(3)\n",
    "print(\"==================GENERATED OUTPUT===============\")\n",
    "print('\\n'.join(sampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Monitoring the execution\n",
    "\n",
    "We can use the Ray Dashboard to monitor the execution. In the Ray Dashboard tab, navigate to the Job page and open the \"Ray Data Overview\" section. Click on the link for the running job, and open the \"Ray Data Overview\" section to view the details of the batch inference execution:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/templates/main/templates/batch-llm/assets/ray-data-jobs.png\" width=900px />\n",
    "\n",
    "### Handling GPU out-of-memory failures\n",
    "If you run into CUDA out of memory, your batch size is likely too large. Set an explicit small batch size or use a smaller model (or a larger GPU).\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "- Created a custom processor for the CNN/DailyMail summarization task.\n",
    "- Defined the model configs for the Meta Llama 3.1 8B model.\n",
    "- Ran the batch inference through Ray Data LLM API and monitored the execution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
