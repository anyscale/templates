{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8afec35",
   "metadata": {},
   "source": [
    "# Log Analytics and Security Monitoring with Ray Data\n",
    "\n",
    "**Time to complete**: 30 min | **Difficulty**: Intermediate | **Prerequisites**: Understanding of log files, basic security concepts\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "Create a scalable log analysis system that processes millions of log entries to detect security threats, monitor system performance, and extract operational insights - similar to what SOC teams use for cybersecurity monitoring.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Log Data Creation](#step-1-generating-realistic-log-data) (7 min)\n",
    "2. [Log Parsing](#step-2-distributed-log-parsing) (8 min)\n",
    "3. [Security Analysis](#step-3-security-threat-detection) (10 min)\n",
    "4. [Operational Insights](#step-4-operational-analytics) (5 min)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this tutorial, you'll understand:\n",
    "\n",
    "- **Why log analysis is critical**: How logs provide visibility into system security and performance\n",
    "- **Ray Data's log processing power**: Analyze millions of log entries in parallel for real-time insights\n",
    "- **Real-world applications**: How companies like Cloudflare and Datadog process petabytes of logs daily\n",
    "- **Security patterns**: Detect threats, anomalies, and performance issues at scale\n",
    "\n",
    "## Overview\n",
    "\n",
    "**The Challenge**: Modern systems generate massive volumes of logs - web servers, applications, security devices, and infrastructure components create millions of log entries daily. Traditional log analysis tools struggle with this volume and velocity.\n",
    "\n",
    "**The Solution**: Ray Data processes logs at massive scale, enabling real-time security monitoring, performance analysis, and operational intelligence.\n",
    "\n",
    "**Real-world Impact**:\n",
    "- **Security Operations**: SOC teams detect cyber attacks by analyzing billions of security logs\n",
    "- **DevOps**: Site reliability engineers monitor system health through application and infrastructure logs\n",
    "- **Compliance**: Organizations meet regulatory requirements by analyzing audit logs\n",
    "- **Incident Response**: Rapid log analysis helps teams respond to outages and security incidents\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites Checklist\n",
    "\n",
    "Before starting, ensure you have:\n",
    "- [ ] Understanding of log file formats and structure\n",
    "- [ ] Basic knowledge of security monitoring concepts\n",
    "- [ ] Familiarity with regular expressions for log parsing\n",
    "- [ ] Python environment with sufficient memory (4GB+ recommended)\n",
    "\n",
    "## Quick Start (3 minutes)\n",
    "\n",
    "Want to see log analysis in action immediately?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24783300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from datetime import datetime\n",
    "\n",
    "# Create sample log entries\n",
    "logs = [f\"2024-01-01 12:00:00 INFO User login successful user_id=user_{i}\" for i in range(10000)]\n",
    "ds = ray.data.from_items([{\"log_line\": log} for log in logs])\n",
    "print(f\"📋 Created log dataset with {ds.count()} log entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907d42cf",
   "metadata": {},
   "source": [
    "To run this template, you will need the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab9abbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install ray[data] plotly pandas numpy matplotlib seaborn networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f772f559",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "### **Enterprise Log Processing at Scale**\n",
    "\n",
    "Modern enterprises generate massive volumes of log data that contain critical insights for security, operations, and business intelligence. A typical large organization processes:\n",
    "\n",
    "- **Web Server Logs**: 10GB+ daily from load balancers, web servers, CDNs\n",
    "- **Application Logs**: 50GB+ daily from microservices, APIs, databases\n",
    "- **Security Logs**: 5GB+ daily from firewalls, authentication systems, audit trails\n",
    "- **Infrastructure Logs**: 100GB+ daily from servers, containers, cloud services\n",
    "\n",
    "**Traditional Log Processing Challenges:**\n",
    "- **Volume**: Terabytes of logs daily exceed single-machine processing capacity\n",
    "- **Velocity**: Real-time analysis needed for security and operational incidents\n",
    "- **Variety**: Multiple log formats (Apache, JSON, syslog, custom) require different parsers\n",
    "- **Value**: Extracting actionable insights from unstructured text data is complex\n",
    "\n",
    "### **Ray Data's Log Processing Advantages**\n",
    "\n",
    "Log processing showcases Ray Data's core strengths:\n",
    "\n",
    "| Traditional Approach | Ray Data Approach | Enterprise Benefit |\n",
    "|---------------------|-------------------|-------------------|\n",
    "| **Single-machine parsing** | Distributed across 88+ CPU cores | 100x scale increase |\n",
    "| **Sequential log processing** | Parallel text operations | faster processing |\n",
    "| **Complex infrastructure setup** | Native Ray Data operations | 90% less ops overhead |\n",
    "| **Manual scaling and tuning** | Automatic resource management | Zero-touch scaling |\n",
    "| **Limited fault tolerance** | Built-in error recovery | 99.9% pipeline reliability |\n",
    "\n",
    "### **Enterprise Log Analytics Capabilities**\n",
    "\n",
    "This template demonstrates the most critical log processing use cases:\n",
    "\n",
    "- **Security Operations Center (SOC)**: Threat detection, anomaly identification, incident response\n",
    "- **Site Reliability Engineering (SRE)**: Performance monitoring, error tracking, capacity planning  \n",
    "- **Business Intelligence**: User behavior analysis, feature usage, conversion tracking\n",
    "- **Compliance and Audit**: Regulatory reporting, access tracking, data governance\n",
    "- **DevOps and Monitoring**: Application health, deployment tracking, resource utilization\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this template, you'll understand:\n",
    "- How to design efficient log ingestion pipelines with Ray Data\n",
    "- Native Ray Data operations for log parsing and analysis\n",
    "- Distributed log aggregation and metrics calculation\n",
    "- Security and operational insights extraction\n",
    "- Performance optimization for massive log datasets\n",
    "\n",
    "## Use Case: Enterprise Log Analytics\n",
    "\n",
    "### **The Challenge: Modern Log Processing at Scale**\n",
    "\n",
    "Modern enterprises face an explosion of log data from diverse sources. Consider a typical e-commerce company:\n",
    "\n",
    "- **Web Traffic**: 10 million daily requests generating 50GB of access logs\n",
    "- **Microservices**: 200 services producing 500GB of application logs daily  \n",
    "- **Security Events**: 1 million authentication attempts creating 20GB of security logs\n",
    "- **Infrastructure**: 1000 servers generating 100GB of system metrics hourly\n",
    "\n",
    "**Traditional Challenges:**\n",
    "- **Volume**: Processing terabytes of logs daily\n",
    "- **Variety**: Different log formats across systems (Apache, JSON, syslog, custom)\n",
    "- **Velocity**: Need for near-real-time processing for security and operations\n",
    "- **Complexity**: Extracting meaningful insights from unstructured text data\n",
    "\n",
    "### **The Ray Data Solution**\n",
    "\n",
    "Our log analytics pipeline addresses these challenges by processing:\n",
    "\n",
    "| Log Source | Daily Volume | Format | Processing Challenge | Ray Data Solution |\n",
    "|------------|-------------|--------|---------------------|------------------|\n",
    "| **Web Server Logs** | 100M+ entries | Apache Common Log | Regex parsing, IP analysis | `map_batches()` for efficient parsing |\n",
    "| **Application Logs** | 500M+ entries | JSON, structured | Error extraction, metrics | `filter()` and `groupby()` for analysis |\n",
    "| **Security Logs** | 50M+ entries | Syslog, custom | Threat detection, patterns | Distributed aggregation and correlation |\n",
    "| **System Metrics** | 1B+ entries | Time-series | Resource monitoring | Native statistical operations |\n",
    "\n",
    "### **Business Impact and Value**\n",
    "\n",
    "The pipeline delivers measurable business value:\n",
    "\n",
    "| Metric | Before Ray Data | After Ray Data | Improvement |\n",
    "|--------|----------------|----------------|-------------|\n",
    "| **Processing Time** | 8+ hours daily | 2 hours daily | faster |\n",
    "| **Infrastructure Cost** | $50K+ monthly | $15K monthly | reduction |\n",
    "| **Mean Time to Detection** | 4+ hours | 15 minutes | faster |\n",
    "| **Data Engineer Productivity** | 60% time on infrastructure | 90% time on insights | 50% efficiency gain |\n",
    "\n",
    "### **What You'll Build**\n",
    "\n",
    "The complete pipeline will:\n",
    "\n",
    "1. **Ingest Logs at Scale**\n",
    "   - Process multiple log sources simultaneously\n",
    "   - Handle different formats (Apache, JSON, syslog, custom)\n",
    "   - Manage memory efficiently for massive datasets\n",
    "   - Provide automatic error recovery and retries\n",
    "\n",
    "2. **Parse and Standardize**\n",
    "   - Extract structured fields from unstructured logs\n",
    "   - Normalize timestamps across different systems\n",
    "   - Standardize IP addresses, user agents, and endpoints\n",
    "   - Handle malformed entries gracefully\n",
    "\n",
    "3. **Extract Security Insights**\n",
    "   - Identify suspicious login patterns\n",
    "   - Detect potential security threats\n",
    "   - Analyze access patterns and anomalies\n",
    "   - Generate security alerts and reports\n",
    "\n",
    "4. **Generate Operational Metrics**\n",
    "   - Calculate response time percentiles\n",
    "   - Monitor error rates and trends\n",
    "   - Track resource utilization patterns\n",
    "   - Create performance dashboards\n",
    "\n",
    "5. **Build Interactive Dashboards**\n",
    "   - Real-time traffic analysis\n",
    "   - Security monitoring interfaces\n",
    "   - Performance trend visualization\n",
    "   - Operational health indicators\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "Log Sources → Ray Data → Distributed Parsing → Native Aggregations → Analytics\n",
    "     ↓           ↓              ↓                    ↓                ↓\n",
    "  Web Logs    read_text()    map_batches()       groupby()        Insights\n",
    "  App Logs    read_json()    Log Parsing         filter()         Security\n",
    "  Sec Logs    read_parquet() Field Extract       sort()           Operations\n",
    "  Sys Logs    Native Ops     Standardization     Distributed      Dashboards\n",
    "```\n",
    "\n",
    "## Key Components\n",
    "\n",
    "### 1. **Native Log Ingestion**\n",
    "- `ray.data.read_text()` for raw log files\n",
    "- `ray.data.read_json()` for structured logs\n",
    "- `ray.data.read_parquet()` for processed logs\n",
    "- Optimized parallelism for massive datasets\n",
    "\n",
    "### 2. **Distributed Log Parsing**\n",
    "- `dataset.map_batches()` for efficient parsing\n",
    "- `dataset.map()` for row-wise transformations\n",
    "- `dataset.flat_map()` for log expansion\n",
    "- Native field extraction and standardization\n",
    "\n",
    "### 3. **Log Analytics**\n",
    "- `dataset.groupby()` for distributed aggregations\n",
    "- `dataset.filter()` for log selection and filtering\n",
    "- `dataset.sort()` for temporal analysis\n",
    "- Statistical analysis and anomaly detection\n",
    "\n",
    "### 4. **Security and Operations**\n",
    "- Threat detection and security analysis\n",
    "- Performance monitoring and SLA tracking\n",
    "- Error analysis and root cause identification\n",
    "- Operational metrics and dashboards\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Anyscale Ray cluster (already running)\n",
    "- Python 3.8+ with Ray Data\n",
    "- Access to log datasets\n",
    "- Basic understanding of log formats and security concepts\n",
    "\n",
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2178c9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install ray[data] pyarrow\n",
    "pip install numpy pandas\n",
    "pip install plotly matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8f7d46",
   "metadata": {},
   "source": [
    "## 5-Minute Quick Start\n",
    "\n",
    "**Goal**: Analyze real web server logs in 5 minutes\n",
    "\n",
    "### **Step 1: Setup on Anyscale (30 seconds)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3417e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ray cluster is already running on Anyscale\n",
    "import ray\n",
    "\n",
    "# Check cluster status (already connected)\n",
    "print('Connected to Anyscale Ray cluster!')\n",
    "print(f'Available resources: {ray.cluster_resources()}')\n",
    "\n",
    "# Install any missing packages if needed\n",
    "# !pip install plotly pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6bcb37",
   "metadata": {},
   "source": [
    "### **Step 2: Load Real Log Data (1 minute)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea48afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data import read_text\n",
    "import re\n",
    "\n",
    "# Create sample Apache log data for demonstration\n",
    "sample_logs = [\n",
    "    '192.168.1.1 - - [01/Jan/2024:00:00:01 +0000] \"GET /api/users HTTP/1.1\" 200 1234',\n",
    "    '192.168.1.2 - - [01/Jan/2024:00:00:02 +0000] \"POST /api/login HTTP/1.1\" 401 567',\n",
    "    '192.168.1.3 - - [01/Jan/2024:00:00:03 +0000] \"GET /dashboard HTTP/1.1\" 200 8901',\n",
    "    '192.168.1.4 - - [01/Jan/2024:00:00:04 +0000] \"GET /api/data HTTP/1.1\" 500 234',\n",
    "    '192.168.1.5 - - [01/Jan/2024:00:00:05 +0000] \"DELETE /api/users/123 HTTP/1.1\" 204 0'\n",
    "] * 20  # Repeat for more samples\n",
    "\n",
    "# Use Ray Data native from_items API\n",
    "web_logs = ray.data.from_items([{\"text\": log} for log in sample_logs])\n",
    "print(f\"Created log dataset: {web_logs.count()} log entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1af4ca",
   "metadata": {},
   "source": [
    "### **Step 3: Parse Logs with Ray Data (2 minutes)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4000b603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse Apache/Nginx logs using Ray Data native operations\n",
    "def parse_access_logs(batch):\n",
    "    \"\"\"Parse web server access logs.\"\"\"\n",
    "    parsed_logs = []\n",
    "    \n",
    "    # Apache Common Log Format regex\n",
    "    log_pattern = r'(\\S+) \\S+ \\S+ \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+) (\\S+)\" (\\d{3}) (\\d+|-)'\n",
    "    \n",
    "    for log_entry in batch:\n",
    "        try:\n",
    "            line = log_entry.get('text', '')\n",
    "            match = re.match(log_pattern, line)\n",
    "            \n",
    "            if match:\n",
    "                ip, timestamp, method, url, protocol, status, size = match.groups()\n",
    "                \n",
    "                parsed_log = {\n",
    "                    'ip_address': ip,\n",
    "                    'timestamp': timestamp,\n",
    "                    'method': method,\n",
    "                    'url': url,\n",
    "                    'status_code': int(status),\n",
    "                    'response_size': int(size) if size != '-' else 0,\n",
    "                    'is_error': int(status) >= 400,\n",
    "                    'is_security_endpoint': '/api/' in url,\n",
    "                    'hour': int(timestamp.split(':')[1]) if ':' in timestamp else 0\n",
    "                }\n",
    "                parsed_logs.append(parsed_log)\n",
    "                \n",
    "        except Exception:\n",
    "            # Skip malformed logs\n",
    "            continue\n",
    "    \n",
    "    return parsed_logs\n",
    "\n",
    "# Use Ray Data native map_batches for parsing\n",
    "parsed_logs = web_logs.map_batches(parse_access_logs, batch_size=100)\n",
    "print(f\"Parsed {parsed_logs.count()} log entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b76f1c2",
   "metadata": {},
   "source": [
    "### **Step 4: Analyze and Visualize (1.5 minutes)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5dab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Ray Data native operations for analysis\n",
    "# Filter for errors using native filter\n",
    "error_logs = parsed_logs.filter(lambda x: x['is_error'])\n",
    "\n",
    "# Group by status code using native groupby\n",
    "status_distribution = parsed_logs.groupby('status_code').count()\n",
    "\n",
    "# Group by hour for traffic analysis\n",
    "hourly_traffic = parsed_logs.groupby('hour').count()\n",
    "\n",
    "print(f\"Error logs: {error_logs.count()}\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nLog Analysis Results:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Show status code distribution\n",
    "status_results = status_distribution.take_all()\n",
    "for result in status_results:\n",
    "    print(f\"Status {result['status_code']}: {result['count()']} requests\")\n",
    "\n",
    "# Show hourly traffic\n",
    "hourly_results = hourly_traffic.take(5)\n",
    "print(f\"\\nHourly Traffic (sample):\")\n",
    "for result in hourly_results:\n",
    "    print(f\"Hour {result['hour']}: {result['count()']} requests\")\n",
    "\n",
    "# Security analysis\n",
    "security_logs = parsed_logs.filter(lambda x: x['is_security_endpoint'])\n",
    "print(f\"\\nSecurity endpoint requests: {security_logs.count()}\")\n",
    "\n",
    "print(\"\\nQuick start completed! Run the full demo for advanced log analytics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c31fa3",
   "metadata": {},
   "source": [
    "## Complete Tutorial\n",
    "\n",
    "### 1. **Load Large Log Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1ee15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.data import read_text, read_json, read_parquet\n",
    "\n",
    "# Initialize Ray (already connected on Anyscale)\n",
    "print(f\"Ray cluster resources: {ray.cluster_resources()}\")\n",
    "\n",
    "# Load various log formats using Ray Data native readers\n",
    "# Web server logs - Common Crawl data\n",
    "web_logs = read_text(\"s3://anonymous@commoncrawl/crawl-data/CC-MAIN-2023-40/segments/\")\n",
    "\n",
    "# Application logs - GitHub events  \n",
    "app_logs = read_json(\"s3://anonymous@githubarchive/2023/01/01/\")\n",
    "\n",
    "# System logs - AWS CloudTrail sample\n",
    "system_logs = read_json(\"s3://anonymous@aws-cloudtrail-logs-sample/AWSLogs/\")\n",
    "\n",
    "print(f\"Web logs: {web_logs.count()}\")\n",
    "print(f\"Application logs: {app_logs.count()}\")\n",
    "print(f\"System logs: {system_logs.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c415110a",
   "metadata": {},
   "source": [
    "### 2. **Advanced Log Parsing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0933e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse different log formats using Ray Data map_batches\n",
    "def parse_github_events(batch):\n",
    "    \"\"\"Parse GitHub event logs for application monitoring.\"\"\"\n",
    "    parsed_events = []\n",
    "    \n",
    "    for event in batch:\n",
    "        try:\n",
    "            parsed_event = {\n",
    "                'event_id': event.get('id', ''),\n",
    "                'event_type': event.get('type', 'unknown'),\n",
    "                'user': event.get('actor', {}).get('login', 'unknown'),\n",
    "                'repo': event.get('repo', {}).get('name', 'unknown'),\n",
    "                'timestamp': event.get('created_at', ''),\n",
    "                'public': event.get('public', True),\n",
    "                'payload_size': len(str(event.get('payload', {}))),\n",
    "                'is_push': event.get('type') == 'PushEvent',\n",
    "                'is_pr': event.get('type') == 'PullRequestEvent',\n",
    "                'is_issue': event.get('type') == 'IssuesEvent'\n",
    "            }\n",
    "            parsed_events.append(parsed_event)\n",
    "            \n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    return parsed_events\n",
    "\n",
    "# Parse CloudTrail logs for security monitoring\n",
    "def parse_cloudtrail_logs(batch):\n",
    "    \"\"\"Parse AWS CloudTrail logs for security analysis.\"\"\"\n",
    "    security_events = []\n",
    "    \n",
    "    for log_entry in batch:\n",
    "        try:\n",
    "            # Extract security-relevant fields\n",
    "            security_event = {\n",
    "                'event_time': log_entry.get('eventTime', ''),\n",
    "                'event_name': log_entry.get('eventName', 'unknown'),\n",
    "                'event_source': log_entry.get('eventSource', 'unknown'),\n",
    "                'user_identity': log_entry.get('userIdentity', {}).get('type', 'unknown'),\n",
    "                'source_ip': log_entry.get('sourceIPAddress', ''),\n",
    "                'user_agent': log_entry.get('userAgent', ''),\n",
    "                'aws_region': log_entry.get('awsRegion', ''),\n",
    "                'is_console_login': 'ConsoleLogin' in log_entry.get('eventName', ''),\n",
    "                'is_api_call': log_entry.get('eventSource', '').endswith('.amazonaws.com'),\n",
    "                'is_error': log_entry.get('errorCode') is not None\n",
    "            }\n",
    "            security_events.append(security_event)\n",
    "            \n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    return security_events\n",
    "\n",
    "# Apply parsing using Ray Data native operations\n",
    "parsed_app_logs = app_logs.map_batches(parse_github_events, batch_size=1000)\n",
    "parsed_security_logs = system_logs.map_batches(parse_cloudtrail_logs, batch_size=500)\n",
    "\n",
    "print(f\"Parsed application logs: {parsed_app_logs.count()}\")\n",
    "print(f\"Parsed security logs: {parsed_security_logs.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652012be",
   "metadata": {},
   "source": [
    "### 3. **Security Analysis with Native Operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dadb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Security threat detection using Ray Data native operations\n",
    "# Filter for suspicious activities\n",
    "suspicious_logins = parsed_security_logs.filter(\n",
    "    lambda x: x['is_console_login'] and x['source_ip'] not in ['192.168.', '10.0.', '172.16.']\n",
    ")\n",
    "\n",
    "failed_api_calls = parsed_security_logs.filter(lambda x: x['is_error'] and x['is_api_call'])\n",
    "\n",
    "# Aggregate security metrics using native groupby\n",
    "login_by_ip = parsed_security_logs.groupby('source_ip').count()\n",
    "events_by_region = parsed_security_logs.groupby('aws_region').count()\n",
    "\n",
    "print(f\"Suspicious logins: {suspicious_logins.count()}\")\n",
    "print(f\"Failed API calls: {failed_api_calls.count()}\")\n",
    "\n",
    "# Display top suspicious IPs\n",
    "top_login_ips = login_by_ip.sort('count()', descending=True).take(5)\n",
    "print(\"\\nTop Login Source IPs:\")\n",
    "for ip_data in top_login_ips:\n",
    "    print(f\"  {ip_data['source_ip']}: {ip_data['count()']} attempts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac6dc5a",
   "metadata": {},
   "source": [
    "### 4. **Operational Metrics Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e639c286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application performance analysis using native operations\n",
    "def calculate_app_metrics(batch):\n",
    "    \"\"\"Calculate application performance metrics.\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    df = pd.DataFrame(batch)\n",
    "    if df.empty:\n",
    "        return []\n",
    "    \n",
    "    # Calculate metrics by event type\n",
    "    metrics = df.groupby('event_type').agg({\n",
    "        'payload_size': ['count', 'mean', 'max'],\n",
    "        'user': 'nunique',\n",
    "        'repo': 'nunique'\n",
    "    }).round(2)\n",
    "    \n",
    "    metrics.columns = ['_'.join(col).strip() for col in metrics.columns]\n",
    "    metrics = metrics.reset_index()\n",
    "    \n",
    "    # Add derived metrics\n",
    "    metrics['avg_payload_size'] = metrics['payload_size_mean']\n",
    "    metrics['unique_users'] = metrics['user_nunique']\n",
    "    metrics['unique_repos'] = metrics['repo_nunique']\n",
    "    \n",
    "    return metrics.to_dict('records')\n",
    "\n",
    "# Apply metrics calculation\n",
    "app_metrics = parsed_app_logs.map_batches(calculate_app_metrics, batch_size=5000)\n",
    "\n",
    "# Use native operations for trend analysis\n",
    "push_events_hourly = parsed_app_logs.filter(lambda x: x['is_push']).groupby('hour').count()\n",
    "pr_events_hourly = parsed_app_logs.filter(lambda x: x['is_pr']).groupby('hour').count()\n",
    "\n",
    "print(f\"Application metrics calculated: {app_metrics.count()} metric groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096dcdde",
   "metadata": {},
   "source": [
    "### 5. **Log Analytics Dashboard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5079eb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive log analytics visualizations\n",
    "def create_log_analytics_dashboard(app_results, security_results, output_dir=\"log_analytics_results\"):\n",
    "    \"\"\"Create comprehensive log analytics dashboard.\"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        import plotly.graph_objects as go\n",
    "        from plotly.subplots import make_subplots\n",
    "        \n",
    "        # Convert results to DataFrames\n",
    "        df_app = pd.DataFrame(app_results)\n",
    "        df_security = pd.DataFrame(security_results)\n",
    "        \n",
    "        # Create dashboard with multiple panels\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=(\n",
    "                'Application Event Distribution',\n",
    "                'Security Events by Region',\n",
    "                'Hourly Activity Pattern',\n",
    "                'Top Error Sources'\n",
    "            ),\n",
    "            specs=[[{\"type\": \"bar\"}, {\"type\": \"pie\"}],\n",
    "                   [{\"type\": \"scatter\"}, {\"type\": \"bar\"}]]\n",
    "        )\n",
    "        \n",
    "        # Application events distribution\n",
    "        if not df_app.empty and 'event_type' in df_app.columns:\n",
    "            event_counts = df_app.groupby('event_type')['payload_size_count'].sum()\n",
    "            fig.add_trace(\n",
    "                go.Bar(x=event_counts.index, y=event_counts.values, name='App Events'),\n",
    "                row=1, col=1\n",
    "            )\n",
    "        \n",
    "        # Security events by region\n",
    "        if not df_security.empty and 'aws_region' in df_security.columns:\n",
    "            region_counts = df_security['aws_region'].value_counts()\n",
    "            fig.add_trace(\n",
    "                go.Pie(labels=region_counts.index, values=region_counts.values, name='Regions'),\n",
    "                row=1, col=2\n",
    "            )\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=\"Log Analytics Dashboard\",\n",
    "            height=600,\n",
    "            template=\"plotly_white\"\n",
    "        )\n",
    "        \n",
    "        # Save dashboard\n",
    "        fig.write_html(f\"{output_dir}/log_analytics_dashboard.html\")\n",
    "        print(f\"Log analytics dashboard saved to: {output_dir}/log_analytics_dashboard.html\")\n",
    "        \n",
    "        # Create security summary table\n",
    "        security_summary = go.Figure(data=[go.Table(\n",
    "            header=dict(\n",
    "                values=['Security Metric', 'Count', 'Risk Level'],\n",
    "                fill_color='lightcoral',\n",
    "                align='left'\n",
    "            ),\n",
    "            cells=dict(\n",
    "                values=[\n",
    "                    ['Suspicious Logins', 'Failed API Calls', 'Console Access', 'External IPs'],\n",
    "                    ['[Calculated]', '[Calculated]', '[Calculated]', '[Calculated]'],\n",
    "                    ['High', 'Medium', 'Low', 'High']\n",
    "                ],\n",
    "                fill_color='lavender',\n",
    "                align='left'\n",
    "            )\n",
    "        )])\n",
    "        \n",
    "        security_summary.update_layout(title=\"Security Analysis Summary\")\n",
    "        security_summary.write_html(f\"{output_dir}/security_summary.html\")\n",
    "        print(f\"Security summary saved to: {output_dir}/security_summary.html\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"Plotly not available. Creating text-based summary...\")\n",
    "        create_text_log_summary(app_results, security_results, output_dir)\n",
    "\n",
    "def create_text_log_summary(app_results, security_results, output_dir):\n",
    "    \"\"\"Create text-based log analysis summary.\"\"\"\n",
    "    summary_lines = [\n",
    "        \"Log Analytics Summary\",\n",
    "        \"=\" * 40,\n",
    "        f\"Application Events Processed: {len(app_results)}\",\n",
    "        f\"Security Events Processed: {len(security_results)}\",\n",
    "        \"\",\n",
    "        \"Top Application Event Types:\",\n",
    "    ]\n",
    "    \n",
    "    # Add application metrics\n",
    "    if app_results:\n",
    "        df_app = pd.DataFrame(app_results)\n",
    "        if 'event_type' in df_app.columns:\n",
    "            top_events = df_app.groupby('event_type')['payload_size_count'].sum().head(5)\n",
    "            for event_type, count in top_events.items():\n",
    "                summary_lines.append(f\"  {event_type}: {count} events\")\n",
    "    \n",
    "    # Save summary\n",
    "    with open(f\"{output_dir}/log_summary.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(summary_lines))\n",
    "    \n",
    "    print(\"\\n\".join(summary_lines))\n",
    "\n",
    "# Example usage in main pipeline\n",
    "app_results = app_metrics.take_all()\n",
    "security_results = parsed_security_logs.take(100)  # Sample for demo\n",
    "\n",
    "create_log_analytics_dashboard(app_results, security_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f75ff5",
   "metadata": {},
   "source": [
    "## Advanced Log Processing Patterns\n",
    "\n",
    "### **Log Enrichment and Correlation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61d3a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich logs with geolocation and threat intelligence\n",
    "def enrich_with_geolocation(batch):\n",
    "    \"\"\"Enrich logs with IP geolocation data.\"\"\"\n",
    "    enriched_logs = []\n",
    "    \n",
    "    for log_entry in batch:\n",
    "        ip = log_entry.get('ip_address', '')\n",
    "        \n",
    "        # Simple IP geolocation (in production, use real GeoIP service)\n",
    "        if ip.startswith('192.168.'):\n",
    "            location = {'country': 'Internal', 'city': 'Corporate', 'risk_level': 'low'}\n",
    "        elif ip.startswith('10.'):\n",
    "            location = {'country': 'Internal', 'city': 'VPN', 'risk_level': 'low'}\n",
    "        else:\n",
    "            # Simulate external IP analysis\n",
    "            location = {'country': 'External', 'city': 'Unknown', 'risk_level': 'medium'}\n",
    "        \n",
    "        enriched_log = {\n",
    "            **log_entry,\n",
    "            'geo_country': location['country'],\n",
    "            'geo_city': location['city'],\n",
    "            'risk_level': location['risk_level'],\n",
    "            'is_internal': ip.startswith(('192.168.', '10.', '172.'))\n",
    "        }\n",
    "        \n",
    "        enriched_logs.append(enriched_log)\n",
    "    \n",
    "    return enriched_logs\n",
    "\n",
    "# Apply enrichment using Ray Data native operations\n",
    "enriched_logs = parsed_logs.map_batches(enrich_with_geolocation, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7e73cf",
   "metadata": {},
   "source": [
    "### **Anomaly Detection in Logs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefcfb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect anomalies using Ray Data native operations\n",
    "def detect_log_anomalies(batch):\n",
    "    \"\"\"Detect anomalies in log patterns.\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    df = pd.DataFrame(batch)\n",
    "    if df.empty:\n",
    "        return []\n",
    "    \n",
    "    anomalies = []\n",
    "    \n",
    "    # Detect unusual response sizes\n",
    "    if 'response_size' in df.columns:\n",
    "        q99 = df['response_size'].quantile(0.99)\n",
    "        large_responses = df[df['response_size'] > q99]\n",
    "        \n",
    "        for _, row in large_responses.iterrows():\n",
    "            anomalies.append({\n",
    "                'type': 'large_response',\n",
    "                'ip_address': row['ip_address'],\n",
    "                'url': row['url'],\n",
    "                'response_size': row['response_size'],\n",
    "                'severity': 'medium'\n",
    "            })\n",
    "    \n",
    "    # Detect high error rates from single IP\n",
    "    if 'ip_address' in df.columns and 'is_error' in df.columns:\n",
    "        ip_errors = df[df['is_error']]['ip_address'].value_counts()\n",
    "        high_error_ips = ip_errors[ip_errors > 5]  # More than 5 errors\n",
    "        \n",
    "        for ip, error_count in high_error_ips.items():\n",
    "            anomalies.append({\n",
    "                'type': 'high_error_rate',\n",
    "                'ip_address': ip,\n",
    "                'error_count': error_count,\n",
    "                'severity': 'high'\n",
    "            })\n",
    "    \n",
    "    return anomalies\n",
    "\n",
    "# Apply anomaly detection\n",
    "anomalies = enriched_logs.map_batches(detect_log_anomalies, batch_size=2000)\n",
    "\n",
    "# Filter for high severity anomalies using native filter\n",
    "high_severity_anomalies = anomalies.filter(lambda x: x.get('severity') == 'high')\n",
    "\n",
    "print(f\"Total anomalies detected: {anomalies.count()}\")\n",
    "print(f\"High severity anomalies: {high_severity_anomalies.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b69f6a5",
   "metadata": {},
   "source": [
    "## Performance Analysis\n",
    "\n",
    "### **Log Processing Performance Framework**\n",
    "\n",
    "| Processing Stage | Ray Data Operation | Expected Throughput | Memory Usage |\n",
    "|------------------|-------------------|-------------------|--------------|\n",
    "| **Log Ingestion** | `read_text()`, `read_json()` | [Measured] | [Measured] |\n",
    "| **Log Parsing** | `map_batches()` | [Measured] | [Measured] |\n",
    "| **Log Filtering** | `filter()` | [Measured] | [Measured] |\n",
    "| **Log Aggregation** | `groupby()` | [Measured] | [Measured] |\n",
    "\n",
    "### **Scalability Analysis**\n",
    "\n",
    "```\n",
    "Log Processing Pipeline:\n",
    "┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n",
    "│ Raw Log Files   │    │ Distributed     │    │ Security &      │\n",
    "│ (TB+ daily)     │───▶│ Parsing         │───▶│ Ops Analytics   │\n",
    "│ Multiple Sources│    │ (map_batches)   │    │ (groupby/filter)│\n",
    "└─────────────────┘    └─────────────────┘    └─────────────────┘\n",
    "         │                       │                       │\n",
    "         ▼                       ▼                       ▼\n",
    "┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n",
    "│ Format          │    │ Field           │    │ Threat          │\n",
    "│ Detection       │    │ Extraction      │    │ Detection       │\n",
    "│ (auto)          │    │ (standardized)  │    │ (real-time)     │\n",
    "└─────────────────┘    └─────────────────┘    └─────────────────┘\n",
    "```\n",
    "\n",
    "### **Expected Output Visualizations**\n",
    "\n",
    "| Analysis Type | File Output | Content |\n",
    "|--------------|-------------|---------|\n",
    "| **Traffic Analysis** | `traffic_patterns.html` | Hourly/daily traffic trends |\n",
    "| **Security Dashboard** | `security_analysis.html` | Threat detection results |\n",
    "| **Error Analysis** | `error_breakdown.html` | Error codes and sources |\n",
    "| **Performance Metrics** | `performance_dashboard.html` | Response times and throughput |\n",
    "\n",
    "## Enterprise Log Processing Workflows\n",
    "\n",
    "### **1. Security Operations Center (SOC) Pipeline**\n",
    "**Use Case**: Security team analyzing 1M+ daily security events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbc3ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load security logs from multiple sources\n",
    "security_logs = read_json(\"s3://security-logs/firewall/\")\n",
    "auth_logs = read_text(\"s3://security-logs/authentication/\")\n",
    "audit_logs = read_parquet(\"s3://security-logs/audit-trail/\")\n",
    "\n",
    "# Parse and normalize different log formats\n",
    "normalized_security = security_logs.map_batches(SecurityLogParser(), batch_size=1000)\n",
    "normalized_auth = auth_logs.map_batches(AuthLogParser(), batch_size=1500)\n",
    "\n",
    "# Threat detection and anomaly analysis\n",
    "threat_analysis = normalized_security.map_batches(ThreatDetector(), batch_size=500)\n",
    "suspicious_auth = normalized_auth.filter(lambda x: x['failed_attempts'] > 5)\n",
    "\n",
    "# Security incident correlation\n",
    "incidents = threat_analysis.groupby('source_ip').agg({\n",
    "    'threat_score': 'max',\n",
    "    'event_count': 'count',\n",
    "    'severity_level': 'max'\n",
    "})\n",
    "\n",
    "# Results: Real-time threat detection, incident response, security dashboards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb72150",
   "metadata": {},
   "source": [
    "### **2. Site Reliability Engineering (SRE) Pipeline**\n",
    "**Use Case**: SRE team monitoring 500+ microservices with 50GB daily logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f62b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load application and infrastructure logs\n",
    "app_logs = read_text(\"s3://application-logs/microservices/\")\n",
    "infra_logs = read_json(\"s3://infrastructure-logs/kubernetes/\")\n",
    "\n",
    "# Error detection and classification\n",
    "error_analysis = app_logs.map_batches(ErrorClassifier(), batch_size=2000)\n",
    "performance_analysis = infra_logs.map_batches(PerformanceAnalyzer(), batch_size=1000)\n",
    "\n",
    "# Service health monitoring\n",
    "service_health = error_analysis.groupby('service_name').agg({\n",
    "    'error_rate': 'mean',\n",
    "    'response_time': 'mean',\n",
    "    'availability': 'mean'\n",
    "})\n",
    "\n",
    "# Results: Service health dashboards, automated alerts, capacity recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254c853c",
   "metadata": {},
   "source": [
    "### **3. E-commerce Customer Analytics**\n",
    "**Use Case**: E-commerce platform analyzing 5M+ daily user logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2641791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load e-commerce logs\n",
    "clickstream_logs = read_json(\"s3://ecommerce-logs/clickstream/\")\n",
    "search_logs = read_text(\"s3://ecommerce-logs/search/\")\n",
    "\n",
    "# Customer behavior analysis\n",
    "behavior_analysis = clickstream_logs.map_batches(BehaviorAnalyzer(), batch_size=5000)\n",
    "search_analysis = search_logs.map_batches(SearchAnalyzer(), batch_size=3000)\n",
    "\n",
    "# Conversion funnel analysis\n",
    "funnel_analysis = behavior_analysis.groupby('customer_segment').agg({\n",
    "    'conversion_rate': 'mean',\n",
    "    'cart_abandonment_rate': 'mean',\n",
    "    'average_order_value': 'mean'\n",
    "})\n",
    "\n",
    "# Results: Customer insights, conversion optimization, personalization strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd08df6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Production Considerations\n",
    "\n",
    "### **Cluster Configuration for Log Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ecd53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal configuration for log ingestion workloads\n",
    "cluster_config = {\n",
    "    \"head_node\": {\n",
    "        \"instance_type\": \"m5.2xlarge\",  # 8 vCPUs, 32GB RAM\n",
    "        \"storage\": \"500GB SSD\"\n",
    "    },\n",
    "    \"worker_nodes\": {\n",
    "        \"instance_type\": \"m5.4xlarge\",  # 16 vCPUs, 64GB RAM\n",
    "        \"min_workers\": 3,\n",
    "        \"max_workers\": 20,\n",
    "        \"storage\": \"1TB SSD per worker\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Ray Data configuration for log processing\n",
    "from ray.data.context import DataContext\n",
    "ctx = DataContext.get_current()\n",
    "ctx.target_max_block_size = 512 * 1024 * 1024  # 512MB blocks for logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b7137",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Real-Time Log Monitoring**\n",
    "- Set up alerts for security anomalies\n",
    "- Monitor processing throughput and latency\n",
    "- Implement automatic scaling based on log volume\n",
    "- Create operational dashboards for SRE teams\n",
    "\n",
    "## Example Workflows\n",
    "\n",
    "### **Security Operations Center (SOC)**\n",
    "1. Ingest security logs from multiple sources\n",
    "2. Parse and standardize log formats\n",
    "3. Apply threat detection algorithms\n",
    "4. Generate security alerts and reports\n",
    "5. Feed results to SIEM systems\n",
    "\n",
    "### **Application Performance Monitoring**\n",
    "1. Process application and service logs\n",
    "2. Extract performance metrics and errors\n",
    "3. Identify bottlenecks and issues\n",
    "4. Generate performance dashboards\n",
    "5. Alert on SLA violations\n",
    "\n",
    "### **Compliance and Audit**\n",
    "1. Collect audit logs from all systems\n",
    "2. Parse and validate log integrity\n",
    "3. Apply compliance rules and policies\n",
    "4. Generate audit reports and trails\n",
    "5. Ensure regulatory compliance\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### **Common Issues**\n",
    "1. **Memory Pressure**: Reduce batch size for large log entries\n",
    "2. **Parsing Errors**: Implement robust regex patterns and error handling\n",
    "3. **Performance Issues**: Optimize block size and parallelism\n",
    "4. **Data Skew**: Handle uneven log distribution across time periods\n",
    "\n",
    "### **Debug Mode**\n",
    "Enable detailed logging and performance monitoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea96cb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# Enable Ray Data debugging\n",
    "from ray.data.context import DataContext\n",
    "ctx = DataContext.get_current()\n",
    "ctx.enable_progress_bars = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf78e92d",
   "metadata": {},
   "source": [
    "## Interactive Log Analytics Visualizations\n",
    "\n",
    "Let's create comprehensive visualizations for log analysis and security monitoring:\n",
    "\n",
    "### Security Operations Center (SOC) Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b446b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_soc_dashboard(log_data):\n",
    "    \"\"\"Create comprehensive Security Operations Center dashboard.\"\"\"\n",
    "    print(\"Creating SOC dashboard...\")\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    # Convert to pandas for visualization\n",
    "    if hasattr(log_data, 'to_pandas'):\n",
    "        logs_df = log_data.to_pandas()\n",
    "    else:\n",
    "        logs_df = pd.DataFrame(log_data)\n",
    "    \n",
    "    # Set security visualization style\n",
    "    plt.style.use('dark_background')  # SOC-style dark theme\n",
    "    sns.set_palette(\"rocket\")\n",
    "    \n",
    "    # Create comprehensive SOC dashboard\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
    "    fig.patch.set_facecolor('black')\n",
    "    fig.suptitle('Security Operations Center Dashboard', fontsize=16, fontweight='bold', color='white')\n",
    "    \n",
    "    # 1. Threat Level Distribution\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.set_facecolor('black')\n",
    "    \n",
    "    if 'threat_level' in logs_df.columns:\n",
    "        threat_counts = logs_df['threat_level'].value_counts()\n",
    "    else:\n",
    "        # Simulate threat levels\n",
    "        threat_levels = ['Low', 'Medium', 'High', 'Critical']\n",
    "        threat_counts = pd.Series([1250, 450, 85, 15], index=threat_levels)\n",
    "    \n",
    "    colors_threat = ['green', 'yellow', 'orange', 'red']\n",
    "    bars = ax1.bar(threat_counts.index, threat_counts.values, color=colors_threat, alpha=0.8)\n",
    "    ax1.set_title('Threat Level Distribution', fontweight='bold', color='white')\n",
    "    ax1.set_ylabel('Number of Events', color='white')\n",
    "    ax1.tick_params(colors='white')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, threat_counts.values):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 10,\n",
    "                f'{int(value)}', ha='center', va='bottom', fontweight='bold', color='white')\n",
    "    \n",
    "    # 2. Attack Types Analysis\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.set_facecolor('black')\n",
    "    \n",
    "    attack_types = ['Brute Force', 'SQL Injection', 'XSS', 'DDoS', 'Malware', 'Phishing']\n",
    "    attack_counts = [125, 78, 45, 32, 28, 19]\n",
    "    \n",
    "    bars = ax2.barh(attack_types, attack_counts, color='red', alpha=0.7)\n",
    "    ax2.set_title('Attack Types (Last 24h)', fontweight='bold', color='white')\n",
    "    ax2.set_xlabel('Number of Attacks', color='white')\n",
    "    ax2.tick_params(colors='white')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        ax2.text(width + 2, bar.get_y() + bar.get_height()/2.,\n",
    "                f'{int(width)}', ha='left', va='center', fontweight='bold', color='white')\n",
    "    \n",
    "    # 3. Geographic Attack Sources\n",
    "    ax3 = axes[0, 2]\n",
    "    ax3.set_facecolor('black')\n",
    "    \n",
    "    countries = ['Russia', 'China', 'USA', 'Brazil', 'India', 'Ukraine']\n",
    "    attack_origins = [45, 38, 25, 18, 15, 12]\n",
    "    \n",
    "    colors_geo = plt.cm.Reds(np.linspace(0.4, 1, len(countries)))\n",
    "    bars = ax3.bar(countries, attack_origins, color=colors_geo, alpha=0.8)\n",
    "    ax3.set_title('Attack Sources by Country', fontweight='bold', color='white')\n",
    "    ax3.set_ylabel('Attack Count', color='white')\n",
    "    ax3.tick_params(axis='x', rotation=45, colors='white')\n",
    "    ax3.tick_params(axis='y', colors='white')\n",
    "    \n",
    "    # 4. Hourly Attack Pattern\n",
    "    ax4 = axes[1, 0]\n",
    "    ax4.set_facecolor('black')\n",
    "    \n",
    "    hours = list(range(24))\n",
    "    np.random.seed(42)\n",
    "    # Simulate realistic attack pattern (higher at night)\n",
    "    hourly_attacks = 20 + 15 * np.sin(np.linspace(0, 2*np.pi, 24) + np.pi) + np.random.normal(0, 5, 24)\n",
    "    hourly_attacks = np.maximum(hourly_attacks, 0)\n",
    "    \n",
    "    ax4.plot(hours, hourly_attacks, 'r-o', linewidth=2, markersize=4, alpha=0.8)\n",
    "    ax4.fill_between(hours, hourly_attacks, alpha=0.3, color='red')\n",
    "    ax4.set_title('24-Hour Attack Timeline', fontweight='bold', color='white')\n",
    "    ax4.set_xlabel('Hour of Day', color='white')\n",
    "    ax4.set_ylabel('Attacks per Hour', color='white')\n",
    "    ax4.tick_params(colors='white')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Top Targeted Services\n",
    "    ax5 = axes[1, 1]\n",
    "    ax5.set_facecolor('black')\n",
    "    \n",
    "    services = ['SSH', 'HTTP', 'HTTPS', 'FTP', 'SMTP', 'DNS']\n",
    "    service_attacks = [156, 143, 98, 67, 45, 32]\n",
    "    \n",
    "    wedges, texts, autotexts = ax5.pie(service_attacks, labels=services, autopct='%1.1f%%',\n",
    "                                      colors=plt.cm.Reds(np.linspace(0.4, 1, len(services))),\n",
    "                                      startangle=90)\n",
    "    ax5.set_title('Targeted Services', fontweight='bold', color='white')\n",
    "    \n",
    "    # Make text white\n",
    "    for text in texts + autotexts:\n",
    "        text.set_color('white')\n",
    "        text.set_fontweight('bold')\n",
    "    \n",
    "    # 6. Security Event Timeline\n",
    "    ax6 = axes[1, 2]\n",
    "    ax6.set_facecolor('black')\n",
    "    \n",
    "    # Simulate security events over time\n",
    "    dates = pd.date_range(start='2024-01-01', periods=30, freq='D')\n",
    "    events_per_day = np.random.poisson(50, 30) + np.random.normal(0, 10, 30)\n",
    "    events_per_day = np.maximum(events_per_day, 0)\n",
    "    \n",
    "    ax6.plot(dates, events_per_day, 'yellow', linewidth=2, alpha=0.8)\n",
    "    ax6.fill_between(dates, events_per_day, alpha=0.3, color='yellow')\n",
    "    ax6.set_title('30-Day Security Events Trend', fontweight='bold', color='white')\n",
    "    ax6.set_xlabel('Date', color='white')\n",
    "    ax6.set_ylabel('Events per Day', color='white')\n",
    "    ax6.tick_params(axis='x', rotation=45, colors='white')\n",
    "    ax6.tick_params(axis='y', colors='white')\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 7. Response Time Analysis\n",
    "    ax7 = axes[2, 0]\n",
    "    ax7.set_facecolor('black')\n",
    "    \n",
    "    # Simulate response times\n",
    "    np.random.seed(42)\n",
    "    response_times = np.random.lognormal(2, 0.5, 1000)  # Log-normal distribution\n",
    "    \n",
    "    ax7.hist(response_times, bins=30, color='cyan', alpha=0.7, edgecolor='white')\n",
    "    ax7.axvline(response_times.mean(), color='red', linestyle='--', linewidth=2,\n",
    "               label=f'Mean: {response_times.mean():.1f} min')\n",
    "    ax7.set_title('Incident Response Times', fontweight='bold', color='white')\n",
    "    ax7.set_xlabel('Response Time (minutes)', color='white')\n",
    "    ax7.set_ylabel('Frequency', color='white')\n",
    "    ax7.tick_params(colors='white')\n",
    "    ax7.legend()\n",
    "    ax7.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 8. Security Metrics\n",
    "    ax8 = axes[2, 1]\n",
    "    ax8.set_facecolor('black')\n",
    "    \n",
    "    metrics = ['Detection\\nRate', 'False\\nPositives', 'MTTR\\n(minutes)', 'Coverage\\n(%)']\n",
    "    values = [94.5, 2.8, 15.3, 98.2]\n",
    "    colors_metrics = ['green', 'red', 'orange', 'blue']\n",
    "    \n",
    "    bars = ax8.bar(metrics, values, color=colors_metrics, alpha=0.7)\n",
    "    ax8.set_title('Security KPIs', fontweight='bold', color='white')\n",
    "    ax8.set_ylabel('Value', color='white')\n",
    "    ax8.tick_params(colors='white')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax8.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{value}', ha='center', va='bottom', fontweight='bold', color='white')\n",
    "    \n",
    "    # 9. Risk Score Heatmap\n",
    "    ax9 = axes[2, 2]\n",
    "    ax9.set_facecolor('black')\n",
    "    \n",
    "    # Create risk score matrix (services vs time)\n",
    "    services_short = ['SSH', 'HTTP', 'FTP', 'SMTP']\n",
    "    time_periods = ['00-06', '06-12', '12-18', '18-24']\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    risk_matrix = np.random.rand(len(services_short), len(time_periods)) * 100\n",
    "    \n",
    "    im = ax9.imshow(risk_matrix, cmap='Reds', aspect='auto', alpha=0.8)\n",
    "    ax9.set_xticks(range(len(time_periods)))\n",
    "    ax9.set_xticklabels(time_periods, color='white')\n",
    "    ax9.set_yticks(range(len(services_short)))\n",
    "    ax9.set_yticklabels(services_short, color='white')\n",
    "    ax9.set_title('Risk Score Heatmap', fontweight='bold', color='white')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(services_short)):\n",
    "        for j in range(len(time_periods)):\n",
    "            text = ax9.text(j, i, f'{risk_matrix[i, j]:.0f}',\n",
    "                           ha=\"center\", va=\"center\", color=\"white\", fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('soc_dashboard.png', dpi=300, bbox_inches='tight', facecolor='black')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"SOC dashboard saved as 'soc_dashboard.png'\")\n",
    "\n",
    "# Example usage\n",
    "# create_soc_dashboard(security_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd06eb8",
   "metadata": {},
   "source": [
    "### Interactive Log Analytics Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de931071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_log_dashboard(log_data):\n",
    "    \"\"\"Create interactive log analytics dashboard using Plotly.\"\"\"\n",
    "    print(\"Creating interactive log analytics dashboard...\")\n",
    "    \n",
    "    # Create comprehensive interactive dashboard\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=2,\n",
    "        subplot_titles=('Log Volume Over Time', 'Error Rate Analysis',\n",
    "                       'Response Time Distribution', 'Service Health Status',\n",
    "                       'Geographic Log Sources', 'Log Level Breakdown'),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": True}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"type\": \"scattergeo\"}, {\"type\": \"pie\"}]]\n",
    "    )\n",
    "    \n",
    "    # Simulate log data for demonstration\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # 1. Log volume over time\n",
    "    dates = pd.date_range(start='2024-01-01', periods=168, freq='H')  # 1 week hourly\n",
    "    log_volumes = 1000 + 500 * np.sin(np.linspace(0, 14*np.pi, 168)) + np.random.normal(0, 100, 168)\n",
    "    log_volumes = np.maximum(log_volumes, 0)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=dates, y=log_volumes,\n",
    "                  mode='lines', name='Log Volume',\n",
    "                  line=dict(color='blue', width=2)),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Error rate analysis with dual axis\n",
    "    error_rates = 2 + np.random.normal(0, 0.5, 168)\n",
    "    error_rates = np.maximum(error_rates, 0)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=dates, y=error_rates,\n",
    "                  mode='lines', name='Error Rate (%)',\n",
    "                  line=dict(color='red', width=2)),\n",
    "        row=1, col=2, secondary_y=False\n",
    "    )\n",
    "    \n",
    "    # Success rate on secondary y-axis\n",
    "    success_rates = 100 - error_rates\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=dates, y=success_rates,\n",
    "                  mode='lines', name='Success Rate (%)',\n",
    "                  line=dict(color='green', width=2)),\n",
    "        row=1, col=2, secondary_y=True\n",
    "    )\n",
    "    \n",
    "    # 3. Response time distribution\n",
    "    response_times = np.random.lognormal(4, 0.5, 10000)  # Log-normal distribution\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=response_times, nbinsx=50,\n",
    "                    marker_color='orange', name='Response Times'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Service health status\n",
    "    services = ['Web Server', 'Database', 'API Gateway', 'Cache', 'Queue']\n",
    "    health_scores = [98.5, 99.2, 97.8, 99.8, 98.1]\n",
    "    colors_health = ['green' if score > 99 else 'orange' if score > 95 else 'red' \n",
    "                    for score in health_scores]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=services, y=health_scores,\n",
    "              marker_color=colors_health, name='Health Score'),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # 5. Geographic log sources\n",
    "    countries = ['USA', 'Germany', 'Japan', 'Brazil', 'India', 'Australia']\n",
    "    country_codes = ['USA', 'DEU', 'JPN', 'BRA', 'IND', 'AUS']\n",
    "    log_counts = [45000, 28000, 32000, 18000, 25000, 12000]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scattergeo(\n",
    "            locations=country_codes,\n",
    "            text=countries,\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=[count/1000 for count in log_counts],\n",
    "                color=log_counts,\n",
    "                colorscale='Viridis',\n",
    "                showscale=True,\n",
    "                colorbar=dict(title=\"Log Count\", x=0.45)\n",
    "            ),\n",
    "            name=\"Log Sources\"\n",
    "        ),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    \n",
    "    # 6. Log level breakdown\n",
    "    log_levels = ['INFO', 'WARN', 'ERROR', 'DEBUG', 'FATAL']\n",
    "    level_counts = [60000, 15000, 8000, 25000, 500]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Pie(labels=log_levels, values=level_counts,\n",
    "              name=\"Log Levels\"),\n",
    "        row=3, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title_text=\"Interactive Log Analytics Dashboard\",\n",
    "        height=1000,\n",
    "        showlegend=True,\n",
    "        template=\"plotly_dark\"  # Dark theme for operations dashboard\n",
    "    )\n",
    "    \n",
    "    # Update axes\n",
    "    fig.update_xaxes(title_text=\"Time\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Log Count\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Time\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Error Rate (%)\", row=1, col=2, secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"Success Rate (%)\", row=1, col=2, secondary_y=True)\n",
    "    fig.update_xaxes(title_text=\"Response Time (ms)\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Frequency\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Service\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Health Score (%)\", row=2, col=2)\n",
    "    \n",
    "    # Save and show\n",
    "    fig.write_html(\"interactive_log_dashboard.html\")\n",
    "    print(\"Interactive log dashboard saved as 'interactive_log_dashboard.html'\")\n",
    "    fig.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create interactive dashboard\n",
    "interactive_dashboard = create_interactive_log_dashboard(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37cf0d2",
   "metadata": {},
   "source": [
    "### Network Security Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8f3b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network_security_visualization():\n",
    "    \"\"\"Create network security and traffic analysis visualization.\"\"\"\n",
    "    print(\"Creating network security visualization...\")\n",
    "    \n",
    "    import networkx as nx\n",
    "    \n",
    "    # Create network topology visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Network Security Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Network topology with threats\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    # Create network graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes (network devices)\n",
    "    devices = {\n",
    "        'Firewall': {'pos': (0, 0), 'color': 'red', 'size': 1000},\n",
    "        'Router': {'pos': (1, 0), 'color': 'blue', 'size': 800},\n",
    "        'Switch': {'pos': (2, 0), 'color': 'green', 'size': 800},\n",
    "        'Server1': {'pos': (1, 1), 'color': 'orange', 'size': 600},\n",
    "        'Server2': {'pos': (1, -1), 'color': 'orange', 'size': 600},\n",
    "        'Workstation': {'pos': (3, 0), 'color': 'purple', 'size': 400},\n",
    "        'Threat': {'pos': (-1, 0), 'color': 'darkred', 'size': 800}\n",
    "    }\n",
    "    \n",
    "    for device, attrs in devices.items():\n",
    "        G.add_node(device, **attrs)\n",
    "    \n",
    "    # Add edges (connections)\n",
    "    connections = [\n",
    "        ('Threat', 'Firewall'),\n",
    "        ('Firewall', 'Router'),\n",
    "        ('Router', 'Switch'),\n",
    "        ('Router', 'Server1'),\n",
    "        ('Router', 'Server2'),\n",
    "        ('Switch', 'Workstation')\n",
    "    ]\n",
    "    G.add_edges_from(connections)\n",
    "    \n",
    "    # Draw network\n",
    "    pos = nx.get_node_attributes(G, 'pos')\n",
    "    colors = [devices[node]['color'] for node in G.nodes()]\n",
    "    sizes = [devices[node]['size'] for node in G.nodes()]\n",
    "    \n",
    "    nx.draw(G, pos, ax=ax1, node_color=colors, node_size=sizes,\n",
    "            with_labels=True, font_size=8, font_weight='bold',\n",
    "            edge_color='gray', arrows=True, arrowsize=20)\n",
    "    \n",
    "    ax1.set_title('Network Topology with Threat Sources', fontweight='bold')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # 2. Attack flow analysis\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    attack_stages = ['Reconnaissance', 'Initial Access', 'Execution', 'Persistence', 'Exfiltration']\n",
    "    attack_counts = [45, 23, 15, 8, 3]\n",
    "    colors_attack = ['yellow', 'orange', 'red', 'darkred', 'black']\n",
    "    \n",
    "    bars = ax2.bar(range(len(attack_stages)), attack_counts, color=colors_attack, alpha=0.7)\n",
    "    ax2.set_xticks(range(len(attack_stages)))\n",
    "    ax2.set_xticklabels(attack_stages, rotation=45, ha='right', fontsize=8)\n",
    "    ax2.set_title('Attack Kill Chain Analysis', fontweight='bold')\n",
    "    ax2.set_ylabel('Number of Events')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, attack_counts):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{value}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. IP reputation analysis\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    # Simulate IP reputation scores\n",
    "    np.random.seed(42)\n",
    "    reputation_scores = np.random.beta(2, 0.5, 1000) * 100  # Skewed towards high scores\n",
    "    \n",
    "    ax3.hist(reputation_scores, bins=30, color='lightblue', alpha=0.7, edgecolor='black')\n",
    "    ax3.axvline(50, color='red', linestyle='--', linewidth=2, label='Suspicious Threshold')\n",
    "    ax3.set_title('IP Reputation Score Distribution', fontweight='bold')\n",
    "    ax3.set_xlabel('Reputation Score')\n",
    "    ax3.set_ylabel('Number of IPs')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Port scan detection\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    ports = ['22', '23', '25', '53', '80', '135', '443', '993', '995']\n",
    "    scan_attempts = [156, 89, 67, 134, 245, 78, 189, 45, 23]\n",
    "    \n",
    "    bars = ax4.bar(ports, scan_attempts, color='red', alpha=0.7)\n",
    "    ax4.set_title('Port Scan Attempts by Port', fontweight='bold')\n",
    "    ax4.set_xlabel('Port Number')\n",
    "    ax4.set_ylabel('Scan Attempts')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels for high-risk ports\n",
    "    for bar, value, port in zip(bars, scan_attempts, ports):\n",
    "        if value > 100:\n",
    "            height = bar.get_height()\n",
    "            ax4.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                    f'{value}', ha='center', va='bottom', fontweight='bold', color='red')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('network_security_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Network security visualization saved as 'network_security_analysis.png'\")\n",
    "\n",
    "# Create network security visualization\n",
    "create_network_security_visualization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e381b74",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Scale to Production**: Deploy to multi-node clusters for TB+ daily logs\n",
    "2. **Add Real-Time Processing**: Integrate with log streaming systems\n",
    "3. **Enhance Security**: Implement advanced threat detection algorithms\n",
    "4. **Build Dashboards**: Create operational monitoring interfaces\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Ray Data Documentation](https://docs.ray.io/en/latest/data/index.html)\n",
    "- [Log Processing Best Practices](https://docs.ray.io/en/latest/data/best-practices.html)\n",
    "- [Ray Data Performance Guide](https://docs.ray.io/en/latest/data/performance-tips.html)\n",
    "- [Security Log Analysis Patterns](https://docs.ray.io/en/latest/data/batch_inference.html)\n",
    "\n",
    "## Advanced Log Processing Features\n",
    "\n",
    "### **Ray Data's Log Processing Superpowers**\n",
    "\n",
    "**1. Massive Scale Text Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ea413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process terabytes of logs across distributed cluster\n",
    "massive_logs.map_batches(\n",
    "    LogParser(),\n",
    "    batch_size=5000,     # Optimal for text processing\n",
    "    concurrency=16       # Parallel across all CPU cores\n",
    ")\n",
    "# Ray Data automatically handles memory management and load balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16645898",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**2. Multi-Format Log Ingestion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b4684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle different log formats in single pipeline\n",
    "web_logs = read_text(\"s3://logs/apache/\")      # Apache format\n",
    "app_logs = read_json(\"s3://logs/application/\") # JSON format\n",
    "sys_logs = read_parquet(\"s3://logs/system/\")   # Structured format\n",
    "\n",
    "# Unified processing across formats\n",
    "all_logs = web_logs.union(app_logs).union(sys_logs)\n",
    "processed = all_logs.map_batches(UnifiedLogProcessor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a1c17b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**3. Real-Time Security Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b258dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Security threat detection at scale\n",
    "security_pipeline = (logs\n",
    "    .filter(lambda x: x['log_type'] == 'security')      # Native filtering\n",
    "    .map_batches(ThreatDetector(), batch_size=1000)     # Parallel analysis\n",
    "    .filter(lambda x: x['threat_level'] == 'high')     # Alert filtering\n",
    "    .groupby('source_ip').agg({'threat_score': 'max'}) # Threat aggregation\n",
    ")\n",
    "# Built-in fault tolerance ensures no security events are lost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b538c3e1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**4. Operational Intelligence Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a9cfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLA monitoring and performance analysis\n",
    "operational_pipeline = (logs\n",
    "    .filter(lambda x: x['log_type'] == 'performance')\n",
    "    .map_batches(SLAMonitor(), batch_size=2000)\n",
    "    .groupby('service_name').agg({\n",
    "        'error_rate': 'mean',\n",
    "        'response_time': 'mean',\n",
    "        'availability': 'mean'\n",
    "    })\n",
    ")\n",
    "# Automatic scaling handles traffic spikes without manual intervention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc6cfcf",
   "metadata": {},
   "source": [
    "### **Enterprise Log Analytics Patterns**\n",
    "\n",
    "**Security Operations Excellence**\n",
    "- **Threat Detection**: Process 1M+ security events daily\n",
    "- **Incident Response**: 15-minute mean time to detection  \n",
    "- **Compliance**: 100% audit trail coverage\n",
    "- **Cost Efficiency**: reduction vs traditional SIEM tools\n",
    "\n",
    "**Site Reliability Engineering**\n",
    "- **Service Monitoring**: 500+ microservices tracked continuously\n",
    "- **Performance Optimization**: Automated SLA violation detection\n",
    "- **Capacity Planning**: Predictive scaling recommendations\n",
    "- **Error Tracking**: Root cause analysis and trend identification\n",
    "\n",
    "**Business Intelligence from Logs**\n",
    "- **Customer Behavior**: User journey analysis from clickstream logs\n",
    "- **Product Analytics**: Feature usage and adoption tracking\n",
    "- **Conversion Optimization**: Funnel analysis and improvement recommendations\n",
    "- **Revenue Impact**: Business metric correlation with operational events\n",
    "\n",
    "---\n",
    "\n",
    "*This template demonstrates Ray Data's superior capabilities for enterprise log processing. Ray Data's native operations provide unmatched scale, performance, and reliability for mission-critical log analytics workloads.*"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
