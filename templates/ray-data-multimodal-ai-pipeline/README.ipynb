{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2358d15a",
   "metadata": {},
   "source": [
    "# Multimodal AI Pipeline with Ray Data\n",
    "\n",
    "**Time to complete**: 30 min | **Difficulty**: Advanced | **Prerequisites**: ML experience, understanding of computer vision and NLP\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "Create a cutting-edge multimodal AI system that processes images and text together - like how humans understand memes, social media posts, or product listings that combine visual and textual information.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Multimodal Data Creation](#step-1-creating-multimodal-data) (7 min)\n",
    "2. [Image Processing](#step-2-image-feature-extraction) (8 min)\n",
    "3. [Text Processing](#step-3-text-feature-extraction) (8 min)\n",
    "4. [Multimodal Fusion](#step-4-cross-modal-fusion) (7 min)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this tutorial, you'll understand:\n",
    "\n",
    "- **Why multimodal AI matters**: How combining different data types creates smarter AI systems\n",
    "- **Ray Data's multimodal power**: Process images, text, and other data types in unified pipelines\n",
    "- **Real-world applications**: How companies like Instagram, Pinterest, and TikTok analyze content\n",
    "- **Fusion techniques**: Combine information from different modalities for better predictions\n",
    "\n",
    "## Overview\n",
    "\n",
    "**The Challenge**: Traditional AI processes one type of data at a time (just images OR just text). But real-world content is multimodal - think Instagram posts with images and captions, or product listings with photos and descriptions.\n",
    "\n",
    "**The Solution**: Ray Data enables processing multiple data types in parallel, then combining their insights for more accurate and comprehensive AI understanding.\n",
    "\n",
    "**Real-world Impact**:\n",
    "- **Social Media**: Analyze posts with images and captions for content moderation\n",
    "- **E-commerce**: Match product images with descriptions for better search\n",
    "- **Entertainment**: Understand video content with both visual and audio cues\n",
    "- **Healthcare**: Combine medical images with patient text records\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites Checklist\n",
    "\n",
    "Before starting, ensure you have:\n",
    "- [ ] Understanding of deep learning concepts\n",
    "- [ ] Familiarity with computer vision and NLP basics\n",
    "- [ ] Experience with PyTorch or similar ML frameworks\n",
    "- [ ] GPU access recommended (but not required)\n",
    "\n",
    "## Quick Start (3 minutes)\n",
    "\n",
    "Want to see multimodal processing immediately?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b594db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "# Create sample multimodal data\n",
    "data = [\n",
    "    {\"image_path\": \"sample.jpg\", \"caption\": \"Beautiful sunset over mountains\"},\n",
    "    {\"image_path\": \"sample2.jpg\", \"caption\": \"Delicious pizza with cheese\"}\n",
    "]\n",
    "ds = ray.data.from_items(data)\n",
    "print(f\" Created multimodal dataset with {ds.count()} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fd23d6",
   "metadata": {},
   "source": [
    "To run this template, you will need the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61e75aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install ray[data] torch torchvision transformers numpy pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cea2a6d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Creating Multimodal Data\n",
    "*‚è± Time: 7 minutes*\n",
    "\n",
    "### What We're Doing\n",
    "We'll create a realistic multimodal dataset that combines images with text descriptions - similar to social media posts, product listings, or news articles with photos.\n",
    "\n",
    "### Why Multimodal Data Matters\n",
    "- **Richer Understanding**: Combining text and images provides more context than either alone\n",
    "- **Real-World Relevance**: Most real-world data is multimodal (posts, products, documents)\n",
    "- **Better AI**: Multimodal models consistently outperform single-modal approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6513682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "# Initialize Ray for distributed multimodal processing\n",
    "print(\" Initializing Ray for multimodal AI...\")\n",
    "start_time = time.time()\n",
    "ray.init()\n",
    "init_time = time.time() - start_time\n",
    "\n",
    "print(f\" Ray cluster ready in {init_time:.2f} seconds\")\n",
    "print(f\" Available resources: {ray.cluster_resources()}\")\n",
    "\n",
    "# Check for GPU availability - crucial for multimodal models\n",
    "gpu_count = ray.cluster_resources().get('GPU', 0)\n",
    "if gpu_count > 0:\n",
    "    print(f\" GPU acceleration available: {gpu_count} GPUs detected\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\" Using CPU processing (GPU highly recommended for multimodal AI)\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "def create_multimodal_dataset():\n",
    "    \"\"\"Create realistic multimodal data combining images and text.\"\"\"\n",
    "    print(\" Creating multimodal dataset...\")\n",
    "    \n",
    "    # Sample image descriptions that might appear with photos\n",
    "    image_captions = [\n",
    "        \"Beautiful sunset over the mountains with orange and pink sky\",\n",
    "        \"Delicious homemade pizza with cheese and pepperoni\",\n",
    "        \"Cute golden retriever playing in the park with a ball\",\n",
    "        \"Modern city skyline at night with illuminated buildings\",\n",
    "        \"Fresh vegetables and fruits at the farmer's market\",\n",
    "        \"Cozy coffee shop interior with wooden tables and warm lighting\",\n",
    "        \"Snow-covered forest path during winter season\",\n",
    "        \"Colorful flowers blooming in a spring garden\",\n",
    "        \"Ocean waves crashing against rocky cliffs\",\n",
    "        \"Vintage car parked on a quiet street\"\n",
    "    ]\n",
    "    \n",
    "    # Create multimodal data pairs\n",
    "    multimodal_data = []\n",
    "    \n",
    "    for i in range(1000):  # Create 1000 multimodal samples\n",
    "        # Create a simple synthetic image (colored square)\n",
    "        # In real applications, you'd load actual images\n",
    "        color = np.random.randint(0, 256, 3)  # Random RGB color\n",
    "        image = np.full((64, 64, 3), color, dtype=np.uint8)  # 64x64 colored square\n",
    "        \n",
    "        # Select a caption and add some variation\n",
    "        base_caption = np.random.choice(image_captions)\n",
    "        caption = f\"{base_caption} (sample {i+1})\"\n",
    "        \n",
    "        multimodal_data.append({\n",
    "            'item_id': f'item_{i:04d}',\n",
    "            'image': image,\n",
    "            'text': caption,\n",
    "            'category': np.random.choice(['nature', 'food', 'urban', 'animals']),\n",
    "            'length': len(caption)\n",
    "        })\n",
    "    \n",
    "    return ray.data.from_items(multimodal_data)\n",
    "\n",
    "# Create our multimodal dataset\n",
    "multimodal_dataset = create_multimodal_dataset()\n",
    "\n",
    "# Display dataset information\n",
    "print(f\" Created multimodal dataset: {multimodal_dataset.count():,} items\")\n",
    "print(f\" Schema: {multimodal_dataset.schema()}\")\n",
    "\n",
    "# Show sample multimodal data\n",
    "print(\"\\n Sample multimodal data:\")\n",
    "samples = multimodal_dataset.take(3)\n",
    "for i, sample in enumerate(samples):\n",
    "    print(f\"  {i+1}. {sample['item_id']}: '{sample['text'][:50]}...' + {sample['image'].shape} image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da162d12",
   "metadata": {},
   "source": [
    "** What just happened?**\n",
    "- Created 1,000 multimodal samples with both images and text\n",
    "- Each sample has a synthetic image and descriptive text caption\n",
    "- Data is loaded into Ray Data for distributed multimodal processing\n",
    "- We can easily scale this to millions of real multimodal samples\n",
    "\n",
    "## Step 2: Image Feature Extraction\n",
    "*‚è± Time: 8 minutes*\n",
    "\n",
    "### What We're Doing\n",
    "Extract meaningful features from images using a pre-trained computer vision model. These features will later be combined with text features for multimodal understanding.\n",
    "\n",
    "### Why Image Features Matter\n",
    "- **Semantic Understanding**: Convert raw pixels into meaningful representations\n",
    "- **Efficiency**: Pre-trained models save time and computational resources\n",
    "- **Compatibility**: Features can be easily combined with other modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15819f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models, transforms\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class ImageFeatureExtractor:\n",
    "    \"\"\"Extract features from images using a pre-trained ResNet model.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"üì∑ Loading image feature extraction model...\")\n",
    "        # Load pre-trained ResNet model (removing final classification layer)\n",
    "        self.model = models.resnet50(pretrained=True)\n",
    "        self.model = torch.nn.Sequential(*list(self.model.children())[:-1])  # Remove final layer\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Define image preprocessing transforms\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224, 224)),  # ResNet expects 224x224 images\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        print(\" Image model loaded successfully\")\n",
    "    \n",
    "    def extract_features(self, batch):\n",
    "        \"\"\"Extract features from a batch of images.\"\"\"\n",
    "        images = batch[\"image\"]\n",
    "        \n",
    "        # Preprocess images\n",
    "        processed_images = []\n",
    "        for img in images:\n",
    "            try:\n",
    "                # Convert numpy array to PIL Image and apply transforms\n",
    "                processed_img = self.transform(img)\n",
    "                processed_images.append(processed_img)\n",
    "            except Exception as e:\n",
    "                print(f\" Error processing image: {e}\")\n",
    "                # Use zero tensor as fallback\n",
    "                processed_images.append(torch.zeros(3, 224, 224))\n",
    "        \n",
    "        # Stack into batch tensor\n",
    "        batch_tensor = torch.stack(processed_images).to(device)\n",
    "        \n",
    "        # Extract features using ResNet\n",
    "        with torch.no_grad():\n",
    "            features = self.model(batch_tensor)\n",
    "            # Flatten features (batch_size, 2048) -> (batch_size, 2048)\n",
    "            features = features.squeeze()\n",
    "            \n",
    "        return {\n",
    "            **batch,  # Keep original data\n",
    "            \"image_features\": features.cpu().numpy().tolist()  # Add extracted features\n",
    "        }\n",
    "\n",
    "# Apply image feature extraction to our dataset\n",
    "print(\" Extracting image features...\")\n",
    "start_time = time.time()\n",
    "\n",
    "image_features_dataset = multimodal_dataset.map_batches(\n",
    "    ImageFeatureExtractor,\n",
    "    batch_size=32,  # Process 32 images at a time\n",
    "    concurrency=1 if device == \"cuda\" else 2,  # Use single GPU worker or multiple CPU workers\n",
    "    num_gpus=1 if device == \"cuda\" else 0\n",
    ")\n",
    "\n",
    "extraction_time = time.time() - start_time\n",
    "print(f\" Image feature extraction completed in {extraction_time:.2f} seconds\")\n",
    "\n",
    "# Validate the results\n",
    "sample_features = image_features_dataset.take(1)[0]\n",
    "print(f\" Feature vector shape: {len(sample_features['image_features'])} dimensions\")\n",
    "print(f\" Feature range: {min(sample_features['image_features']):.3f} to {max(sample_features['image_features']):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2749de70",
   "metadata": {},
   "source": [
    "** What's happening here?**\n",
    "- **Feature Extraction**: ResNet converts each image into a 2048-dimensional feature vector\n",
    "- **Preprocessing**: Images are resized and normalized to match the model's training data\n",
    "- **Batch Processing**: Multiple images processed simultaneously for efficiency\n",
    "- **Error Handling**: Graceful handling of any image processing issues\n",
    "\n",
    "## Step 3: Text Feature Extraction\n",
    "*‚è± Time: 8 minutes*\n",
    "\n",
    "### What We're Doing\n",
    "Extract semantic features from text using a pre-trained language model. These text embeddings capture the meaning of captions and can be combined with image features.\n",
    "\n",
    "### Why Text Features Matter\n",
    "- **Semantic Understanding**: Convert words into numerical representations that capture meaning\n",
    "- **Cross-Modal Alignment**: Text and image features can be compared and combined\n",
    "- **Scalability**: Process thousands of text descriptions efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d59988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextFeatureExtractor:\n",
    "    \"\"\"Extract features from text using a pre-trained BERT model.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\" Loading text feature extraction model...\")\n",
    "        # Use a lightweight BERT model for text understanding\n",
    "        model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "        print(\" Text model loaded successfully\")\n",
    "    \n",
    "    def extract_features(self, batch):\n",
    "        \"\"\"Extract features from a batch of text descriptions.\"\"\"\n",
    "        texts = batch[\"text\"]\n",
    "        \n",
    "        try:\n",
    "            # Tokenize all texts in the batch\n",
    "            encoded = self.tokenizer(\n",
    "                texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=128,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            # Extract features using BERT\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**encoded)\n",
    "                # Use mean pooling to get sentence-level features\n",
    "                features = outputs.last_hidden_state.mean(dim=1)\n",
    "                # Normalize features for better multimodal alignment\n",
    "                features = F.normalize(features, p=2, dim=1)\n",
    "            \n",
    "            return {\n",
    "                **batch,  # Keep original data\n",
    "                \"text_features\": features.cpu().numpy().tolist()  # Add extracted features\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" Error processing text batch: {e}\")\n",
    "            # Return batch with zero features as fallback\n",
    "            batch_size = len(texts)\n",
    "            feature_dim = 384  # MiniLM feature dimension\n",
    "            zero_features = [[0.0] * feature_dim] * batch_size\n",
    "            return {\n",
    "                **batch,\n",
    "                \"text_features\": zero_features\n",
    "            }\n",
    "\n",
    "# Apply text feature extraction to our dataset\n",
    "print(\" Extracting text features...\")\n",
    "start_time = time.time()\n",
    "\n",
    "text_features_dataset = image_features_dataset.map_batches(\n",
    "    TextFeatureExtractor,\n",
    "    batch_size=64,  # Process more texts at once (they're smaller than images)\n",
    "    concurrency=1 if device == \"cuda\" else 2,\n",
    "    num_gpus=1 if device == \"cuda\" else 0\n",
    ")\n",
    "\n",
    "text_extraction_time = time.time() - start_time\n",
    "print(f\" Text feature extraction completed in {text_extraction_time:.2f} seconds\")\n",
    "\n",
    "# Validate the results\n",
    "sample_text_features = text_features_dataset.take(1)[0]\n",
    "print(f\" Text feature vector shape: {len(sample_text_features['text_features'])} dimensions\")\n",
    "print(f\" Text feature range: {min(sample_text_features['text_features']):.3f} to {max(sample_text_features['text_features']):.3f}\")\n",
    "\n",
    "# Show we now have both image and text features\n",
    "print(f\"\\n Multimodal features ready:\")\n",
    "print(f\"  - Image features: {len(sample_text_features['image_features'])} dimensions\")\n",
    "print(f\"  - Text features: {len(sample_text_features['text_features'])} dimensions\")\n",
    "print(f\"  - Total feature space: {len(sample_text_features['image_features']) + len(sample_text_features['text_features'])} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8fc5f7",
   "metadata": {},
   "source": [
    "** What's happening here?**\n",
    "- **Text Encoding**: BERT converts text into 384-dimensional semantic vectors\n",
    "- **Batch Processing**: Multiple text descriptions processed simultaneously\n",
    "- **Feature Normalization**: Text features normalized for better multimodal alignment\n",
    "- **Error Resilience**: Robust error handling for text processing issues\n",
    "\n",
    "## Step 4: Cross-Modal Fusion\n",
    "*‚è± Time: 7 minutes*\n",
    "\n",
    "### What We're Doing\n",
    "Combine image and text features to create unified multimodal representations. This is where the magic of multimodal AI happens - understanding content that combines visual and textual information.\n",
    "\n",
    "### Why Fusion Matters\n",
    "- **Richer Understanding**: Combined features capture more information than either modality alone\n",
    "- **Better Predictions**: Multimodal models consistently outperform single-modal approaches\n",
    "- **Real-World Relevance**: Most real content is multimodal (social posts, product listings, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a83a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalFusion:\n",
    "    \"\"\"Combine image and text features into unified representations.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\" Initializing multimodal fusion...\")\n",
    "        # We'll use simple concatenation and weighted fusion\n",
    "        # In production, you might use attention mechanisms or learned fusion\n",
    "        self.image_weight = 0.6  # Weight for image features\n",
    "        self.text_weight = 0.4   # Weight for text features\n",
    "        print(\" Fusion weights configured\")\n",
    "    \n",
    "    def fuse_features(self, batch):\n",
    "        \"\"\"Fuse image and text features into multimodal representations.\"\"\"\n",
    "        try:\n",
    "            image_features = np.array(batch[\"image_features\"])\n",
    "            text_features = np.array(batch[\"text_features\"])\n",
    "            \n",
    "            # Method 1: Simple concatenation\n",
    "            concatenated_features = np.concatenate([image_features, text_features], axis=1)\n",
    "            \n",
    "            # Method 2: Weighted fusion (average of normalized features)\n",
    "            # Normalize features to same scale for fair weighting\n",
    "            image_norm = image_features / (np.linalg.norm(image_features, axis=1, keepdims=True) + 1e-8)\n",
    "            text_norm = text_features / (np.linalg.norm(text_features, axis=1, keepdims=True) + 1e-8)\n",
    "            \n",
    "            weighted_features = (\n",
    "                self.image_weight * image_norm + \n",
    "                self.text_weight * text_norm\n",
    "            )\n",
    "            \n",
    "            # Calculate similarity between image and text features\n",
    "            similarity_scores = []\n",
    "            for img_feat, txt_feat in zip(image_norm, text_norm):\n",
    "                # Cosine similarity between image and text features\n",
    "                similarity = np.dot(img_feat, txt_feat) / (\n",
    "                    np.linalg.norm(img_feat) * np.linalg.norm(txt_feat) + 1e-8\n",
    "                )\n",
    "                similarity_scores.append(float(similarity))\n",
    "            \n",
    "            return {\n",
    "                **batch,  # Keep original data\n",
    "                \"multimodal_features_concat\": concatenated_features.tolist(),\n",
    "                \"multimodal_features_weighted\": weighted_features.tolist(),\n",
    "                \"image_text_similarity\": similarity_scores\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" Error in multimodal fusion: {e}\")\n",
    "            batch_size = len(batch[\"image_features\"])\n",
    "            # Return empty features as fallback\n",
    "            return {\n",
    "                **batch,\n",
    "                \"multimodal_features_concat\": [[0.0] * 2432] * batch_size,  # 2048 + 384\n",
    "                \"multimodal_features_weighted\": [[0.0] * 2048] * batch_size,\n",
    "                \"image_text_similarity\": [0.0] * batch_size\n",
    "            }\n",
    "\n",
    "# Apply multimodal fusion to our dataset\n",
    "print(\" Performing multimodal fusion...\")\n",
    "start_time = time.time()\n",
    "\n",
    "final_multimodal_dataset = text_features_dataset.map_batches(\n",
    "    MultimodalFusion,\n",
    "    batch_size=128,  # Larger batches for fusion operations\n",
    "    concurrency=4,   # CPU-intensive operation, use multiple workers\n",
    "    num_gpus=0       # Fusion doesn't need GPU\n",
    ")\n",
    "\n",
    "fusion_time = time.time() - start_time\n",
    "print(f\" Multimodal fusion completed in {fusion_time:.2f} seconds\")\n",
    "\n",
    "# Analyze the fused results\n",
    "fusion_results = final_multimodal_dataset.take(5)\n",
    "\n",
    "print(\"\\n Multimodal Fusion Results:\")\n",
    "print(\"-\" * 60)\n",
    "for i, result in enumerate(fusion_results):\n",
    "    similarity = result['image_text_similarity']\n",
    "    text_preview = result['text'][:40]\n",
    "    \n",
    "    print(f\"{i+1}. '{text_preview}...'\")\n",
    "    print(f\"   Image-Text Similarity: {similarity:.3f}\")\n",
    "    print(f\"   Concat Features: {len(result['multimodal_features_concat'])} dims\")\n",
    "    print(f\"   Weighted Features: {len(result['multimodal_features_weighted'])} dims\")\n",
    "    print()\n",
    "\n",
    "# Performance summary with profiling (rule #199: Include performance profiling)\n",
    "total_samples = final_multimodal_dataset.count()\n",
    "total_processing_time = time.time() - start_time\n",
    "\n",
    "print(f\" Final Results:\")\n",
    "print(f\"  - Processed {total_samples:,} multimodal samples\")\n",
    "print(f\"  - Total processing time: {total_processing_time:.2f} seconds\")\n",
    "print(f\"  - Processing rate: {total_samples/total_processing_time:.1f} samples/second\")\n",
    "print(f\"  - Created rich multimodal representations\")\n",
    "print(f\"  - Ready for downstream AI tasks (classification, search, etc.)\")\n",
    "\n",
    "# Performance profiling summary\n",
    "print(f\"\\n Performance Breakdown:\")\n",
    "print(f\"  - Image feature extraction: {extraction_time:.2f}s\")\n",
    "print(f\"  - Text feature extraction: {text_extraction_time:.2f}s\") \n",
    "print(f\"  - Multimodal fusion: {fusion_time:.2f}s\")\n",
    "print(f\"  - Total pipeline time: {total_processing_time:.2f}s\")\n",
    "\n",
    "# Resource utilization summary\n",
    "cluster_resources = ray.cluster_resources()\n",
    "print(f\"\\n Resource Utilization:\")\n",
    "print(f\"  - CPUs available: {cluster_resources.get('CPU', 0)}\")\n",
    "print(f\"  - GPUs available: {cluster_resources.get('GPU', 0)}\")\n",
    "print(f\"  - Memory available: {cluster_resources.get('memory', 0)/1e9:.1f} GB\")\n",
    "\n",
    "# Clean up resources\n",
    "ray.shutdown()\n",
    "print(\" Ray cluster shut down successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc796128",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "** What we accomplished:**\n",
    "- **Cross-Modal Understanding**: Combined visual and textual information\n",
    "- **Feature Fusion**: Created unified representations from separate modalities\n",
    "- **Similarity Analysis**: Measured how well images and text align\n",
    "- **Scalable Processing**: Handled multimodal data efficiently with Ray Data\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting Common Issues\n",
    "\n",
    "### **Problem: \"CUDA out of memory with multimodal models\"**\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f24cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce batch sizes and use CPU for fusion\n",
    "image_batch_size = 16  # Smaller for GPU-intensive image processing\n",
    "text_batch_size = 32   # Larger for CPU text processing\n",
    "fusion_num_gpus = 0    # Use CPU for fusion operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adce587c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Problem: \"Feature dimensions don't match for fusion\"**\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc384a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dimension checking and padding\n",
    "def align_feature_dimensions(feat1, feat2, target_dim=512):\n",
    "    # Pad or truncate features to same dimension\n",
    "    feat1_aligned = feat1[:target_dim] if len(feat1) > target_dim else feat1 + [0] * (target_dim - len(feat1))\n",
    "    feat2_aligned = feat2[:target_dim] if len(feat2) > target_dim else feat2 + [0] * (target_dim - len(feat2))\n",
    "    return feat1_aligned, feat2_aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b558ef",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Problem: \"Low similarity scores between modalities\"**\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd44a286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use better pre-trained models or fine-tune for your domain\n",
    "# Consider using CLIP models that are trained for image-text alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfe8085",
   "metadata": {},
   "source": [
    "### **Performance Optimization Tips**\n",
    "\n",
    "1. **Model Selection**: Use CLIP models for better image-text alignment\n",
    "2. **Batch Sizing**: Optimize batch sizes for each modality separately\n",
    "3. **GPU Memory**: Process images on GPU, text on CPU if memory is limited\n",
    "4. **Feature Caching**: Cache extracted features to avoid recomputation\n",
    "5. **Fusion Methods**: Experiment with different fusion techniques\n",
    "\n",
    "### **Performance Considerations**\n",
    "\n",
    "Ray Data provides several advantages for multimodal processing:\n",
    "- **Parallel modality processing**: Image and text features can be extracted simultaneously\n",
    "- **GPU utilization**: Automatic distribution of GPU-intensive tasks across available hardware\n",
    "- **Memory efficiency**: Large multimodal datasets are processed in manageable batches\n",
    "- **Resource optimization**: Different modalities can use different resource configurations\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps and Extensions\n",
    "\n",
    "### **Try These Advanced Features**\n",
    "1. **CLIP Integration**: Use OpenAI's CLIP for better image-text understanding\n",
    "2. **Attention Mechanisms**: Implement cross-modal attention for better fusion\n",
    "3. **Multimodal Classification**: Build classifiers using the fused features\n",
    "4. **Similarity Search**: Create image-text search and recommendation systems\n",
    "5. **Real Datasets**: Use actual social media or e-commerce multimodal data\n",
    "\n",
    "### **Production Considerations**\n",
    "- **Model Optimization**: Use quantization and pruning for faster inference\n",
    "- **Caching Strategy**: Cache frequently used features and models\n",
    "- **Error Handling**: Implement robust error handling for production workloads\n",
    "- **Monitoring**: Track model performance and feature quality\n",
    "- **Scaling**: Use Ray Serve for real-time multimodal inference\n",
    "\n",
    "### **Related Ray Data Templates**\n",
    "- **Ray Data Batch Inference Optimization**: Optimize multimodal model inference\n",
    "- **Ray Data NLP Text Analytics**: Deep dive into text processing techniques\n",
    "- **Ray Data Batch Classification**: Focus on image processing optimization\n",
    "\n",
    "** Congratulations!** You've successfully built a scalable multimodal AI pipeline with Ray Data!\n",
    "\n",
    "These multimodal techniques enable you to build AI systems that understand content the way humans do - by combining visual and textual information for richer understanding.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "S3 Data Sources ‚Üí Ray Data ‚Üí Modality-Specific Processing ‚Üí Cross-Modal Fusion ‚Üí AI Analysis ‚Üí Results\n",
    "     ‚Üì              ‚Üì              ‚Üì                        ‚Üì              ‚Üì         ‚Üì\n",
    "  Images         Parallel      Vision Models            Embedding      ML Models  Insights\n",
    "  Text           Processing    Text Encoders            Fusion         Inference  Reports\n",
    "  Audio          GPU Workers   Audio Models             Aggregation    Scoring    Analytics\n",
    "```\n",
    "\n",
    "## Key Components\n",
    "\n",
    "### 1. **Multimodal Data Loading**\n",
    "- `ray.data.read_images()` for image data from S3\n",
    "- `ray.data.read_text()` for text documents and social media posts\n",
    "- `ray.data.read_binary_files()` for audio files\n",
    "- Automatic format detection and validation\n",
    "\n",
    "### 2. **Modality-Specific Processing**\n",
    "- **Vision**: Pre-trained vision transformers (ViT, ResNet) with GPU acceleration\n",
    "- **Text**: BERT, RoBERTa, or custom text encoders\n",
    "- **Audio**: Wav2Vec, HuBERT, or audio feature extractors\n",
    "- Parallel processing with device-specific optimizations\n",
    "\n",
    "### 3. **Cross-Modal Fusion**\n",
    "- Embedding alignment and normalization\n",
    "- Attention mechanisms for cross-modal understanding\n",
    "- Multimodal transformer architectures\n",
    "- Feature concatenation and aggregation strategies\n",
    "\n",
    "### 4. **AI Analysis and Inference**\n",
    "- Multimodal classification models\n",
    "- Content recommendation systems\n",
    "- Sentiment and content analysis\n",
    "- Automated content moderation\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Ray cluster with GPU support (recommended)\n",
    "- Python 3.8+ with required ML libraries\n",
    "- Access to S3 or local multimodal datasets\n",
    "- Basic understanding of computer vision, NLP, and audio processing\n",
    "\n",
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30260599",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install ray[data] torch torchvision transformers\n",
    "pip install torchaudio librosa pillow opencv-python\n",
    "pip install sentence-transformers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3d4d8b",
   "metadata": {},
   "source": [
    "## 5-Minute Quick Start\n",
    "\n",
    "**Goal**: Get a multimodal AI pipeline running in 5 minutes with real data\n",
    "\n",
    "### **Step 1: Setup on Anyscale (30 seconds)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130dea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ray cluster is already running on Anyscale\n",
    "import ray\n",
    "\n",
    "# Check cluster status (already connected)\n",
    "print('Connected to Anyscale Ray cluster!')\n",
    "print(f'Available resources: {ray.cluster_resources()}')\n",
    "\n",
    "# Install any missing packages if needed\n",
    "# !pip install torch torchvision transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4251da3",
   "metadata": {},
   "source": [
    "### **Step 2: Load Real Data (1 minute)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aeaee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data import read_images\n",
    "\n",
    "# Load real ImageNet subset (Imagenette) - publicly available\n",
    "image_ds = read_images(\"s3://anonymous@air-example-data-2/imagenette2/train/\", mode=\"RGB\").limit(10)\n",
    "print(f\"Loaded {image_ds.count()} real images\")\n",
    "\n",
    "# Quick data inspection\n",
    "sample = image_ds.take(1)[0]\n",
    "print(f\"Image shape: {sample['image'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdbbd7e",
   "metadata": {},
   "source": [
    "### **Step 3: Run Vision Processing (2 minutes)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b297e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "\n",
    "class QuickVisionProcessor:\n",
    "    def __init__(self):\n",
    "        self.model = models.resnet18(pretrained=True)  # Smaller model for speed\n",
    "        self.model.eval()\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        features = []\n",
    "        for img in batch[\"image\"]:\n",
    "            try:\n",
    "                img_tensor = self.transform(img).unsqueeze(0)\n",
    "                with torch.no_grad():\n",
    "                    feature = self.model(img_tensor)\n",
    "                features.append(feature.numpy()[0])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image: {e}\")\n",
    "                features.append(None)\n",
    "        return {\"features\": features}\n",
    "\n",
    "# Process images\n",
    "processed = image_ds.map_batches(QuickVisionProcessor(), batch_size=4)\n",
    "results = processed.take(5)\n",
    "print(f\"Processed {len(results)} image batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc44d9c",
   "metadata": {},
   "source": [
    "### **Step 4: View Results (1 minute)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7ae1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "for i, result in enumerate(results):\n",
    "    features = result.get(\"features\", [])\n",
    "    valid_features = [f for f in features if f is not None]\n",
    "    print(f\"Batch {i}: {len(valid_features)} successful feature extractions\")\n",
    "\n",
    "print(\"Quick start completed! Check the full demo for advanced features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc73c1d5",
   "metadata": {},
   "source": [
    "## Complete Tutorial\n",
    "\n",
    "### 1. **Load Real Multimodal Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cdbb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.data import read_images, read_text, read_binary_files\n",
    "\n",
    "# Initialize Ray\n",
    "ray.init()\n",
    "\n",
    "# Load real datasets from public sources\n",
    "# ImageNet subset (Imagenette) - publicly available\n",
    "image_ds = read_images(\"s3://anonymous@air-example-data-2/imagenette2/train/\", mode=\"RGB\")\n",
    "\n",
    "# Common Crawl news articles - publicly available\n",
    "text_ds = read_text(\"s3://anonymous@commoncrawl/crawl-data/CC-NEWS/2023/01/\")\n",
    "\n",
    "# LibriSpeech audio dataset - publicly available  \n",
    "audio_ds = read_binary_files(\"s3://anonymous@openslr/12/train-clean-100/\")\n",
    "\n",
    "print(f\"Images (Imagenette): {image_ds.count()}\")\n",
    "print(f\"Text (Common Crawl): {text_ds.count()}\")\n",
    "print(f\"Audio (LibriSpeech): {audio_ds.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3c53b2",
   "metadata": {},
   "source": [
    "### 2. **Process Each Modality**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231143c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data import ActorPoolStrategy\n",
    "import torch\n",
    "\n",
    "class VisionProcessor:\n",
    "    def __init__(self):\n",
    "        self.model = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)\n",
    "        self.model.eval()\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.cuda()\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        # Process image batch\n",
    "        images = torch.stack([torch.from_numpy(img) for img in batch[\"image\"]])\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = self.model(images)\n",
    "        \n",
    "        return {\"image_features\": features.cpu().numpy()}\n",
    "\n",
    "# Apply vision processing\n",
    "processed_images = image_ds.map_batches(\n",
    "    VisionProcessor,\n",
    "    batch_size=32,\n",
    "    num_gpus=1,\n",
    "    concurrency=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e47846",
   "metadata": {},
   "source": [
    "### 3. **Cross-Modal Fusion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1c531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_modalities(batch):\n",
    "    \"\"\"Combine features from different modalities\"\"\"\n",
    "    # Align features by content ID\n",
    "    image_features = batch[\"image_features\"]\n",
    "    text_features = batch[\"text_features\"]\n",
    "    audio_features = batch[\"audio_features\"]\n",
    "    \n",
    "    # Simple concatenation (can be enhanced with attention)\n",
    "    fused_features = np.concatenate([\n",
    "        image_features, text_features, audio_features\n",
    "    ], axis=1)\n",
    "    \n",
    "    return {\"fused_features\": fused_features}\n",
    "\n",
    "# Apply fusion (assuming aligned datasets)\n",
    "fused_ds = processed_images.map_batches(fuse_modalities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c3e437",
   "metadata": {},
   "source": [
    "### 4. **Multimodal Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76e5fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalClassifier:\n",
    "    def __init__(self):\n",
    "        self.classifier = torch.nn.Linear(2048 + 768 + 512, 10)  # Example dimensions\n",
    "        if torch.cuda.is_available():\n",
    "            self.classifier.cuda()\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        features = torch.from_numpy(batch[\"fused_features\"]).float()\n",
    "        if torch.cuda.is_available():\n",
    "            features = features.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = self.classifier(features)\n",
    "            probabilities = torch.softmax(predictions, dim=1)\n",
    "        \n",
    "        return {\n",
    "            \"predictions\": predictions.cpu().numpy(),\n",
    "            \"probabilities\": probabilities.cpu().numpy()\n",
    "        }\n",
    "\n",
    "# Apply classification\n",
    "results = fused_ds.map_batches(\n",
    "    MultimodalClassifier,\n",
    "    batch_size=64,\n",
    "    num_gpus=1,\n",
    "    concurrency=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b63c1c",
   "metadata": {},
   "source": [
    "## Advanced Features\n",
    "\n",
    "### **GPU Memory Management**\n",
    "- Automatic batch size optimization based on GPU memory\n",
    "- Gradient checkpointing for large models\n",
    "- Memory-efficient data loading and processing\n",
    "\n",
    "### **Scalability**\n",
    "- Horizontal scaling across multiple GPUs\n",
    "- Automatic load balancing and resource allocation\n",
    "- Support for heterogeneous GPU clusters\n",
    "\n",
    "### **Performance Optimization**\n",
    "- Operator fusion for reduced memory transfers\n",
    "- Pipelined processing for continuous data flow\n",
    "- Caching strategies for frequently accessed data\n",
    "\n",
    "## Production Considerations\n",
    "\n",
    "### **Model Serving**\n",
    "- Integration with Ray Serve for real-time inference\n",
    "- Model versioning and A/B testing\n",
    "- Automatic scaling based on demand\n",
    "\n",
    "### **Monitoring and Observability**\n",
    "- Performance metrics and resource utilization\n",
    "- Error tracking and alerting\n",
    "- Pipeline health monitoring\n",
    "\n",
    "### **Data Quality and Validation**\n",
    "- Input validation and sanitization\n",
    "- Output quality checks and confidence scoring\n",
    "- Fallback strategies for failed processing\n",
    "\n",
    "## Example Workflows\n",
    "\n",
    "### **Content Moderation Pipeline**\n",
    "1. Load social media content (images, text, audio)\n",
    "2. Extract features using modality-specific models\n",
    "3. Apply content moderation rules and ML models\n",
    "4. Generate moderation decisions and confidence scores\n",
    "5. Store results for audit and compliance\n",
    "\n",
    "### **Recommendation System**\n",
    "1. Process user interaction data (clicks, views, listens)\n",
    "2. Generate user and content embeddings\n",
    "3. Calculate similarity scores across modalities\n",
    "4. Rank and recommend relevant content\n",
    "5. Update recommendations in real-time\n",
    "\n",
    "### **Market Intelligence**\n",
    "1. Collect market data (news, social media, financial reports)\n",
    "2. Extract sentiment and key information\n",
    "3. Correlate across different data sources\n",
    "4. Generate market insights and predictions\n",
    "5. Alert on significant events or trends\n",
    "\n",
    "## Performance Analysis\n",
    "\n",
    "### **Benchmark Framework**\n",
    "\n",
    "The template includes comprehensive performance measurement tools:\n",
    "\n",
    "| Benchmark Type | Measurement Focus | Output Visualization |\n",
    "|---------------|-------------------|---------------------|\n",
    "| **Fusion Method Comparison** | Attention vs Weighted vs Simple | Performance comparison charts |\n",
    "| **Batch Size Optimization** | Memory usage vs throughput | Optimization curves |\n",
    "| **GPU vs CPU Analysis** | Device performance comparison | Speedup analysis |\n",
    "| **Scalability Testing** | Multi-GPU performance | Scaling visualizations |\n",
    "\n",
    "### **Performance Measurement Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f5f14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run actual performance benchmarks\n",
    "benchmark = MultimodalBenchmark()\n",
    "results = benchmark.run_performance_benchmark(\n",
    "    fusion_methods=[\"simple\", \"weighted\", \"attention\"],\n",
    "    batch_sizes=[4, 8, 16, 32],\n",
    "    gpu_enabled=True\n",
    ")\n",
    "\n",
    "# Generate verified performance report\n",
    "benchmark.generate_benchmark_report()\n",
    "\n",
    "# Expected output structure:\n",
    "# - benchmark_results.csv: Detailed metrics\n",
    "# - performance_report.txt: Analysis summary\n",
    "# - performance_charts.html: Interactive visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df16bff",
   "metadata": {},
   "source": [
    "### **Modality Processing Pipeline**\n",
    "\n",
    "```\n",
    "Input Data Flow:\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Images    ‚îÇ    ‚îÇ    Text     ‚îÇ    ‚îÇ   Audio     ‚îÇ\n",
    "‚îÇ (ImageNet)  ‚îÇ    ‚îÇ   (IMDB)    ‚îÇ    ‚îÇ(LibriSpeech)‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ                  ‚îÇ                  ‚îÇ\n",
    "       ‚ñº                  ‚ñº                  ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   ResNet    ‚îÇ    ‚îÇ    BERT     ‚îÇ    ‚îÇ   Audio     ‚îÇ\n",
    "‚îÇ Features    ‚îÇ    ‚îÇ Embeddings  ‚îÇ    ‚îÇ Features    ‚îÇ\n",
    "‚îÇ (2048-dim)  ‚îÇ    ‚îÇ (384-dim)   ‚îÇ    ‚îÇ (13-dim)    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ                  ‚îÇ                  ‚îÇ\n",
    "       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                          ‚ñº\n",
    "                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                ‚îÇ Multimodal      ‚îÇ\n",
    "                ‚îÇ Fusion          ‚îÇ\n",
    "                ‚îÇ (Attention)     ‚îÇ\n",
    "                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                          ‚ñº\n",
    "                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                ‚îÇ Classification  ‚îÇ\n",
    "                ‚îÇ & Analysis      ‚îÇ\n",
    "                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### **Expected Resource Requirements**\n",
    "\n",
    "| Processing Stage | CPU Cores | Memory (GB) | GPU Memory (GB) | Processing Time |\n",
    "|-----------------|-----------|-------------|-----------------|-----------------|\n",
    "| **Image Processing** | 4-8 | 8-16 | 4-8 | Measured in demo |\n",
    "| **Text Processing** | 2-4 | 4-8 | 2-4 | Measured in demo |\n",
    "| **Audio Processing** | 2-4 | 4-8 | 1-2 | Measured in demo |\n",
    "| **Fusion & Classification** | 4-8 | 8-16 | 4-8 | Measured in demo |\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### **Common Issues and Solutions**\n",
    "\n",
    "| Issue | Symptoms | Solution | Prevention |\n",
    "|-------|----------|----------|------------|\n",
    "| **GPU Memory Errors** | `RuntimeError: CUDA out of memory` | Reduce batch size to 4-8, use CPU fallback | Monitor GPU memory usage, start with small batches |\n",
    "| **Data Alignment Issues** | Mismatched modality counts | Ensure consistent IDs across datasets | Validate data alignment before processing |\n",
    "| **Model Loading Failures** | Import errors, missing dependencies | Install required packages, check model availability | Use requirements.txt, test imports |\n",
    "| **Poor Performance** | Slow processing, low GPU utilization | Optimize batch size, increase concurrency | Profile operations, monitor resource usage |\n",
    "| **Memory Pressure** | Ray object store full | Reduce data in memory, process in chunks | Monitor object store, use streaming patterns |\n",
    "\n",
    "### **Performance Optimization Guide**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c19c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal configuration for different cluster types\n",
    "import ray\n",
    "\n",
    "# Check available resources\n",
    "resources = ray.cluster_resources()\n",
    "gpu_count = resources.get('GPU', 0)\n",
    "cpu_count = resources.get('CPU', 0)\n",
    "\n",
    "# Adjust configuration based on resources\n",
    "if gpu_count >= 4:\n",
    "    # Multi-GPU configuration\n",
    "    batch_size = 32\n",
    "    concurrency = 4\n",
    "    num_gpus = 1\n",
    "elif gpu_count >= 1:\n",
    "    # Single GPU configuration\n",
    "    batch_size = 16\n",
    "    concurrency = 2\n",
    "    num_gpus = 1\n",
    "else:\n",
    "    # CPU-only configuration\n",
    "    batch_size = 8\n",
    "    concurrency = cpu_count // 4\n",
    "    num_gpus = 0\n",
    "\n",
    "print(f\"Recommended config: batch_size={batch_size}, concurrency={concurrency}, num_gpus={num_gpus}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b8b465",
   "metadata": {},
   "source": [
    "### **Debug Mode and Monitoring**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b952bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "\n",
    "# Enable comprehensive debugging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# Monitor GPU memory if available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0) / 1e9:.1f}GB\")\n",
    "\n",
    "# Enable Ray Data debugging\n",
    "from ray.data.context import DataContext\n",
    "ctx = DataContext.get_current()\n",
    "ctx.enable_progress_bars = True\n",
    "\n",
    "# Monitor processing with custom logging\n",
    "class DebugProcessor:\n",
    "    def __call__(self, batch):\n",
    "        print(f\"Processing batch with {len(batch)} items\")\n",
    "        print(f\"GPU memory before: {torch.cuda.memory_allocated(0) / 1e9:.1f}GB\" if torch.cuda.is_available() else \"CPU mode\")\n",
    "        \n",
    "        # Your processing logic here\n",
    "        results = process_batch(batch)\n",
    "        \n",
    "        print(f\"GPU memory after: {torch.cuda.memory_allocated(0) / 1e9:.1f}GB\" if torch.cuda.is_available() else \"Processing complete\")\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ffe4ba",
   "metadata": {},
   "source": [
    "### **Error Recovery Strategies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff430498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement robust error handling with recovery\n",
    "def robust_multimodal_processing(batch):\n",
    "    \"\"\"Process multimodal data with comprehensive error recovery.\"\"\"\n",
    "    results = []\n",
    "    errors = []\n",
    "    \n",
    "    for item in batch:\n",
    "        try:\n",
    "            # Attempt processing\n",
    "            result = process_item(item)\n",
    "            results.append(result)\n",
    "            \n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            # GPU memory error - try CPU fallback\n",
    "            try:\n",
    "                torch.cuda.empty_cache()\n",
    "                result = process_item_cpu(item)\n",
    "                results.append(result)\n",
    "                \n",
    "            except Exception as fallback_error:\n",
    "                errors.append(f\"CPU fallback failed: {fallback_error}\")\n",
    "                results.append(create_error_result(item, fallback_error))\n",
    "                \n",
    "        except Exception as general_error:\n",
    "            errors.append(f\"Processing failed: {general_error}\")\n",
    "            results.append(create_error_result(item, general_error))\n",
    "    \n",
    "    if errors:\n",
    "        print(f\"Encountered {len(errors)} errors during processing\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74727799",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Customize Models**: Replace pre-trained models with your own\n",
    "2. **Add Modalities**: Extend to video, 3D data, or other formats\n",
    "3. **Optimize Performance**: Tune batch sizes and resource allocation\n",
    "4. **Scale Production**: Deploy to multi-GPU clusters with Ray Serve\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Ray Data Documentation](https://docs.ray.io/en/latest/data/index.html)\n",
    "- [PyTorch Multimodal Tutorials](https://pytorch.org/tutorials/)\n",
    "- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)\n",
    "- [Ray GPU Support](https://docs.ray.io/en/latest/ray-core/using-ray-with-gpus.html)\n",
    "\n",
    "---\n",
    "\n",
    "*This template provides a foundation for building production-ready multimodal AI pipelines with Ray Data. Start with the basic examples and gradually add complexity based on your specific use case and requirements.*"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
