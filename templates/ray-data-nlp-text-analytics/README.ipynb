{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e676e8a7",
   "metadata": {},
   "source": [
    "# Text Analytics with Ray Data\n",
    "\n",
    "**â± Time to complete**: 30 min | **Difficulty**: Intermediate | **Prerequisites**: Basic Python, familiarity with text processing\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "Create a scalable text processing pipeline that analyzes thousands of text documents in parallel. You'll learn sentiment analysis, text classification, and how to process large text datasets efficiently.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Text Data Loading](#step-1-loading-text-data) (5 min)\n",
    "2. [Text Preprocessing](#step-2-text-preprocessing-at-scale) (8 min)\n",
    "3. [Sentiment Analysis](#step-3-distributed-sentiment-analysis) (10 min)\n",
    "4. [Results and Insights](#step-4-analyzing-results) (7 min)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this tutorial, you'll understand:\n",
    "\n",
    "- **Why text processing is challenging**: Memory and computation issues with large text datasets\n",
    "- **Ray Data's text capabilities**: Distribute NLP tasks across multiple workers\n",
    "- **Real-world applications**: How companies process millions of reviews, comments, and documents\n",
    "- **Performance benefits**: Process 100x more text in the same time\n",
    "\n",
    "## Overview\n",
    "\n",
    "**The Challenge**: Processing large text datasets (reviews, social media, documents) with traditional tools is slow and often runs out of memory.\n",
    "\n",
    "**The Solution**: Ray Data distributes text processing across multiple cores, making it possible to analyze millions of documents quickly.\n",
    "\n",
    "**Real-world Impact**:\n",
    "-  **E-commerce**: Analyze millions of product reviews for insights\n",
    "-  **Social media**: Process tweets and posts for sentiment trends  \n",
    "- ðŸ“° **News**: Classify and analyze thousands of articles daily\n",
    "- ðŸ’¬ **Customer support**: Automatically categorize and route support tickets\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites Checklist\n",
    "\n",
    "Before starting, ensure you have:\n",
    "- [ ] Basic understanding of text processing concepts\n",
    "- [ ] Familiarity with sentiment analysis\n",
    "- [ ] Python environment with sufficient memory (4GB+ recommended)\n",
    "- [ ] Understanding of machine learning basics\n",
    "\n",
    "## Quick Start (3 minutes)\n",
    "\n",
    "Want to see text processing in action immediately?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417bd293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "# Create sample text data\n",
    "texts = [\"I love this product!\", \"This is terrible\", \"Pretty good overall\"]\n",
    "ds = ray.data.from_items([{\"text\": t} for t in texts * 1000])\n",
    "print(f\" Created dataset with {ds.count()} text samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64ba5c2",
   "metadata": {},
   "source": [
    "To run this template, you will need the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62ec9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install ray[data] transformers torch nltk wordcloud matplotlib seaborn plotly textstat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380506d6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Loading Text Data\n",
    "*â± Time: 5 minutes*\n",
    "\n",
    "### What We're Doing\n",
    "We'll create a realistic text dataset similar to product reviews or social media posts. This gives us something meaningful to analyze without requiring huge downloads.\n",
    "\n",
    "### Why This Matters\n",
    "- **Realistic data**: Learn with data that resembles real-world text\n",
    "- **Scalable patterns**: Techniques that work for thousands will work for millions\n",
    "- **Memory efficiency**: Handle large text datasets without running out of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1deabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from wordcloud import WordCloud\n",
    "import textstat\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Initialize Ray for distributed processing\n",
    "ray.init()\n",
    "\n",
    "def create_sample_text_data():\n",
    "    \"\"\"Create realistic sample text data for analysis.\"\"\"\n",
    "    print(\" Creating sample text dataset...\")\n",
    "    \n",
    "    # Sample review texts with different sentiments\n",
    "    positive_reviews = [\n",
    "        \"This product is absolutely amazing! Best purchase ever.\",\n",
    "        \"Fantastic quality and fast shipping. Highly recommend!\",\n",
    "        \"Love it! Exactly what I was looking for.\",\n",
    "        \"Outstanding customer service and great product.\",\n",
    "        \"Perfect! Works exactly as described.\"\n",
    "    ]\n",
    "    \n",
    "    negative_reviews = [\n",
    "        \"Terrible quality, broke after one day.\",\n",
    "        \"Worst purchase ever. Complete waste of money.\",\n",
    "        \"Poor customer service and defective product.\",\n",
    "        \"Not as described. Very disappointed.\",\n",
    "        \"Cheaply made and doesn't work properly.\"\n",
    "    ]\n",
    "    \n",
    "    neutral_reviews = [\n",
    "        \"It's okay, nothing special but does the job.\",\n",
    "        \"Average product, met basic expectations.\",\n",
    "        \"Decent quality for the price point.\",\n",
    "        \"Works fine, no major complaints.\",\n",
    "        \"Pretty standard, what you'd expect.\"\n",
    "    ]\n",
    "    \n",
    "    # Create a larger dataset by combining and repeating\n",
    "    all_reviews = []\n",
    "    \n",
    "    # Add multiple copies with slight variations\n",
    "    for i in range(1000):  # Create 1000 reviews total\n",
    "        # Randomly select review type\n",
    "        review_type = np.random.choice(['positive', 'negative', 'neutral'])\n",
    "        \n",
    "        if review_type == 'positive':\n",
    "            text = np.random.choice(positive_reviews)\n",
    "            sentiment = 'positive'\n",
    "        elif review_type == 'negative':\n",
    "            text = np.random.choice(negative_reviews) \n",
    "            sentiment = 'negative'\n",
    "        else:\n",
    "            text = np.random.choice(neutral_reviews)\n",
    "            sentiment = 'neutral'\n",
    "        \n",
    "        all_reviews.append({\n",
    "            'review_id': f'review_{i:04d}',\n",
    "            'text': text,\n",
    "            'true_sentiment': sentiment,  # We'll use this to check our analysis\n",
    "            'length': len(text)\n",
    "        })\n",
    "    \n",
    "    return ray.data.from_items(all_reviews)\n",
    "\n",
    "# Create our text dataset\n",
    "text_dataset = create_sample_text_data()\n",
    "\n",
    "# Display basic information about our dataset\n",
    "print(f\" Created dataset with {text_dataset.count()} text samples\")\n",
    "print(f\" Schema: {text_dataset.schema()}\")\n",
    "\n",
    "# Show a few sample reviews\n",
    "print(\"\\n Sample reviews:\")\n",
    "samples = text_dataset.take(3)\n",
    "for i, sample in enumerate(samples):\n",
    "    print(f\"{i+1}. {sample['text'][:50]}... (sentiment: {sample['true_sentiment']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c614073",
   "metadata": {},
   "source": [
    "** What just happened?**\n",
    "- Created 1,000 realistic text samples (reviews)\n",
    "- Each sample has text content and known sentiment\n",
    "- Data is loaded into Ray Data for distributed processing\n",
    "- We can easily scale this to millions of real reviews\n",
    "\n",
    "## Use Case: Enterprise Content Intelligence Platform\n",
    "\n",
    "### **Real-World Business Scenario**\n",
    "\n",
    "A large e-commerce company receives 100,000+ pieces of text content daily from multiple sources and needs to extract actionable business insights at scale. Traditional NLP tools can't handle this volume efficiently.\n",
    "\n",
    "**Content Sources and Volumes:**\n",
    "- **Customer Reviews**: 50,000+ daily product and service reviews\n",
    "- **Support Tickets**: 15,000+ daily customer service interactions  \n",
    "- **Social Media**: 25,000+ daily mentions, posts, and comments\n",
    "- **Internal Documents**: 10,000+ daily emails, reports, and documentation\n",
    "\n",
    "**Business Challenges:**\n",
    "- **Manual Processing**: Takes 40+ hours daily with human analysts\n",
    "- **Inconsistent Analysis**: Different analysts provide varying insights\n",
    "- **Delayed Response**: 24-48 hour delay for sentiment analysis and issue identification\n",
    "- **Limited Scale**: Can only process 10% of available content\n",
    "- **High Cost**: $200K+ monthly for external NLP services\n",
    "\n",
    "### **Ray Data Solution Benefits**\n",
    "\n",
    "The comprehensive NLP pipeline delivers:\n",
    "\n",
    "| Business Metric | Before Ray Data | After Ray Data | Improvement |\n",
    "|----------------|----------------|----------------|-------------|\n",
    "| **Processing Time** | 40+ hours | 2 hours | Much faster |\n",
    "| **Content Coverage** | 10% processed | 100% processed | Complete coverage |\n",
    "| **Analysis Consistency** | Variable quality | Standardized insights | Much more consistent |\n",
    "| **Response Time** | 24-48 hours | Real-time | Much faster response |\n",
    "| **Monthly Cost** | $200K+ | $20K | Significant cost reduction |\n",
    "| **Insight Quality** | Basic sentiment | 10+ NLP functions | Comprehensive analysis |\n",
    "\n",
    "### **Enterprise NLP Pipeline Capabilities**\n",
    "\n",
    "The pipeline provides comprehensive text intelligence:\n",
    "\n",
    "1. **Content Classification and Routing**\n",
    "   - Automatically categorize incoming content by type and urgency\n",
    "   - Route high-priority issues to appropriate teams\n",
    "   - Identify trending topics and emerging issues\n",
    "\n",
    "2. **Customer Experience Analytics**\n",
    "   - Real-time sentiment monitoring across all channels\n",
    "   - Product satisfaction scoring and trend analysis\n",
    "   - Customer pain point identification and escalation\n",
    "\n",
    "3. **Competitive Intelligence**\n",
    "   - Brand mention analysis and competitive comparison\n",
    "   - Market sentiment tracking and trend identification\n",
    "   - Product feature feedback and improvement suggestions\n",
    "\n",
    "4. **Operational Efficiency**\n",
    "   - Automated content summarization for executive reports\n",
    "   - Key entity extraction for CRM enrichment\n",
    "   - Multi-language content processing for global operations\n",
    "\n",
    "## Architecture\n",
    "\n",
    "### **Ray Data NLP Processing Architecture**\n",
    "\n",
    "```\n",
    "Enterprise Text Sources (100K+ daily)\n",
    "â”œâ”€â”€ Customer Reviews (50K)\n",
    "â”œâ”€â”€ Support Tickets (15K) \n",
    "â”œâ”€â”€ Social Media (25K)\n",
    "â””â”€â”€ Documents (10K)\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           Ray Data Ingestion            â”‚\n",
    "â”‚  â€¢ read_text() â€¢ read_parquet()        â”‚\n",
    "â”‚  â€¢ from_huggingface() â€¢ read_json()    â”‚\n",
    "â”‚  â€¢ Distributed loading across cluster  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                  â”‚\n",
    "                  â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚     Distributed Text Processing         â”‚\n",
    "â”‚  â€¢ map_batches() for vectorized ops    â”‚\n",
    "â”‚  â€¢ Parallel preprocessing across nodes â”‚\n",
    "â”‚  â€¢ Memory-efficient text cleaning      â”‚\n",
    "â”‚  â€¢ Automatic load balancing           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                  â”‚\n",
    "                  â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚        Multi-Model NLP Analysis         â”‚\n",
    "â”‚  â€¢ BERT embeddings (GPU accelerated)   â”‚\n",
    "â”‚  â€¢ Sentiment analysis (transformer)    â”‚\n",
    "â”‚  â€¢ Topic modeling (LDA + clustering)   â”‚\n",
    "â”‚  â€¢ Named entity recognition (spaCy)    â”‚\n",
    "â”‚  â€¢ Text summarization (BART)          â”‚\n",
    "â”‚  â€¢ Language detection (multilingual)   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                  â”‚\n",
    "                  â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚      Ray Data LLM Integration          â”‚ (Optional)\n",
    "â”‚  â€¢ Production LLM inference           â”‚\n",
    "â”‚  â€¢ Batch processing optimization      â”‚\n",
    "â”‚  â€¢ Structured prompt engineering      â”‚\n",
    "â”‚  â€¢ GPU resource management           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                  â”‚\n",
    "                  â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚      Business Intelligence Layer        â”‚\n",
    "â”‚  â€¢ Aggregated insights and metrics     â”‚\n",
    "â”‚  â€¢ Interactive dashboards             â”‚\n",
    "â”‚  â€¢ Real-time alerts and notifications â”‚\n",
    "â”‚  â€¢ Executive reporting and analytics  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### **Ray Data Advantages for NLP**\n",
    "\n",
    "| Traditional NLP Approach | Ray Data NLP Approach | Business Impact |\n",
    "|---------------------------|----------------------|-----------------|\n",
    "| **Single-machine processing** | Distributed across 88+ CPU cores | 50x scale increase |\n",
    "| **Sequential model inference** | Parallel GPU acceleration | faster processing |\n",
    "| **Manual pipeline orchestration** | Native Ray Data operations | 80% less infrastructure code |\n",
    "| **Complex resource management** | Automatic scaling and load balancing | Zero ops overhead |\n",
    "| **Limited fault tolerance** | Built-in error recovery and retries | 99.9% pipeline reliability |\n",
    "\n",
    "## Key Components\n",
    "\n",
    "### 1. **Text Data Loading**\n",
    "- `ray.data.read_text()` for text files\n",
    "- `ray.data.read_parquet()` for structured text data\n",
    "- Custom readers for specific text formats\n",
    "- Text data validation and schema management\n",
    "\n",
    "### 2. **Text Preprocessing**\n",
    "- Text cleaning and normalization\n",
    "- Tokenization and stemming\n",
    "- Stop word removal and lemmatization\n",
    "- Language detection and encoding handling\n",
    "\n",
    "### 3. **NLP Model Integration**\n",
    "- Pre-trained language models (BERT, RoBERTa, GPT)\n",
    "- Custom model training and fine-tuning\n",
    "- Embedding generation and similarity analysis\n",
    "- Multi-language support and localization\n",
    "\n",
    "### 4. **Text Analytics**\n",
    "- Sentiment analysis and emotion detection\n",
    "- Topic modeling and clustering\n",
    "- Named entity recognition\n",
    "- Text classification and categorization\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Ray cluster with GPU support (recommended)\n",
    "- Python 3.8+ with NLP libraries\n",
    "- Access to text datasets\n",
    "- Basic understanding of NLP concepts and techniques\n",
    "\n",
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fe1eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install ray[data] transformers torch\n",
    "pip install nltk spacy textblob\n",
    "pip install sentence-transformers scikit-learn\n",
    "pip install pandas numpy pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3619eba2",
   "metadata": {},
   "source": [
    "## 5-Minute Quick Start\n",
    "\n",
    "**Goal**: Analyze sentiment of real text data in 5 minutes\n",
    "\n",
    "### **Step 1: Setup on Anyscale (30 seconds)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7815b369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ray cluster is already running on Anyscale\n",
    "import ray\n",
    "\n",
    "# Check cluster status (already connected)\n",
    "print('Connected to Anyscale Ray cluster!')\n",
    "print(f'Available resources: {ray.cluster_resources()}')\n",
    "\n",
    "# Install any missing packages if needed\n",
    "# !pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b35849e",
   "metadata": {},
   "source": [
    "### **Step 2: Load Real Text Data (1 minute)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e2e20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "# Create sample real movie reviews for quick demo\n",
    "real_reviews = [\n",
    "    \"This movie was absolutely fantastic! Great acting and plot.\",\n",
    "    \"Terrible film. Waste of time and money. Very disappointed.\",\n",
    "    \"Amazing cinematography and outstanding performances throughout.\",\n",
    "    \"The movie was okay, nothing special but entertaining enough.\",\n",
    "    \"Brilliant storytelling and incredible attention to detail.\"\n",
    "]\n",
    "\n",
    "# Convert to Ray dataset\n",
    "text_ds = ray.data.from_items([{\"text\": review, \"id\": i} for i, review in enumerate(real_reviews)])\n",
    "print(f\"Loaded {text_ds.count()} real movie reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd83d7b",
   "metadata": {},
   "source": [
    "### **Step 3: Run Sentiment Analysis (2 minutes)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa620f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "class QuickSentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.sentiment_pipeline = pipeline(\"sentiment-analysis\", device=-1)  # CPU for speed\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        results = []\n",
    "        for item in batch:\n",
    "            try:\n",
    "                text = item[\"text\"]\n",
    "                sentiment = self.sentiment_pipeline(text[:512])[0]\n",
    "                results.append({\n",
    "                    **item,\n",
    "                    \"sentiment\": sentiment[\"label\"],\n",
    "                    \"confidence\": sentiment[\"score\"]\n",
    "                })\n",
    "            except Exception as e:\n",
    "                results.append({**item, \"error\": str(e)})\n",
    "        return results\n",
    "\n",
    "# Analyze sentiment\n",
    "sentiment_results = text_ds.map_batches(QuickSentimentAnalyzer(), batch_size=5)\n",
    "final_results = sentiment_results.take_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68249f8f",
   "metadata": {},
   "source": [
    "### **Step 4: View Results (1 minute)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20564085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sentiment analysis results\n",
    "print(\"\\nSentiment Analysis Results:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for result in final_results:\n",
    "    text = result[\"text\"][:50] + \"...\" if len(result[\"text\"]) > 50 else result[\"text\"]\n",
    "    sentiment = result.get(\"sentiment\", \"ERROR\")\n",
    "    confidence = result.get(\"confidence\", 0)\n",
    "    \n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {sentiment} (confidence: {confidence:.2f})\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ae7c68",
   "metadata": {},
   "source": [
    "## Interactive Text Analytics Visualizations\n",
    "\n",
    "Let's create stunning visualizations to analyze our text data:\n",
    "\n",
    "### Word Clouds and Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea0216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_visualizations(dataset):\n",
    "    \"\"\"Create comprehensive text analytics visualizations.\"\"\"\n",
    "    print(\"Creating text analytics visualizations...\")\n",
    "    \n",
    "    # Convert to pandas for visualization\n",
    "    text_df = dataset.to_pandas()\n",
    "    \n",
    "    # Get sentiment results\n",
    "    sentiment_df = pd.DataFrame(final_results)\n",
    "    \n",
    "    # Create comprehensive dashboard\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
    "    fig.suptitle('Text Analytics Dashboard', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Sentiment Distribution\n",
    "    ax1 = axes[0, 0]\n",
    "    if 'sentiment' in sentiment_df.columns:\n",
    "        sentiment_counts = sentiment_df['sentiment'].value_counts()\n",
    "        colors = ['green' if s == 'positive' else 'red' if s == 'negative' else 'gray' \n",
    "                 for s in sentiment_counts.index]\n",
    "        bars = ax1.bar(sentiment_counts.index, sentiment_counts.values, color=colors, alpha=0.7)\n",
    "        ax1.set_title('Sentiment Distribution', fontweight='bold')\n",
    "        ax1.set_ylabel('Number of Texts')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                    f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Text Length Distribution\n",
    "    ax2 = axes[0, 1]\n",
    "    text_lengths = text_df['length'].values\n",
    "    ax2.hist(text_lengths, bins=30, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "    ax2.set_title('Text Length Distribution', fontweight='bold')\n",
    "    ax2.set_xlabel('Character Count')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.axvline(np.mean(text_lengths), color='red', linestyle='--', \n",
    "               label=f'Mean: {np.mean(text_lengths):.1f}')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # 3. Word Cloud for Positive Sentiment\n",
    "    ax3 = axes[0, 2]\n",
    "    if 'sentiment' in sentiment_df.columns:\n",
    "        positive_texts = sentiment_df[sentiment_df['sentiment'] == 'positive']['text'].tolist()\n",
    "        if positive_texts:\n",
    "            positive_text = ' '.join(positive_texts)\n",
    "            wordcloud_pos = WordCloud(width=400, height=300, background_color='white',\n",
    "                                    colormap='Greens').generate(positive_text)\n",
    "            ax3.imshow(wordcloud_pos, interpolation='bilinear')\n",
    "            ax3.set_title('Positive Sentiment Word Cloud', fontweight='bold')\n",
    "            ax3.axis('off')\n",
    "    \n",
    "    # 4. Word Cloud for Negative Sentiment\n",
    "    ax4 = axes[1, 0]\n",
    "    if 'sentiment' in sentiment_df.columns:\n",
    "        negative_texts = sentiment_df[sentiment_df['sentiment'] == 'negative']['text'].tolist()\n",
    "        if negative_texts:\n",
    "            negative_text = ' '.join(negative_texts)\n",
    "            wordcloud_neg = WordCloud(width=400, height=300, background_color='white',\n",
    "                                    colormap='Reds').generate(negative_text)\n",
    "            ax4.imshow(wordcloud_neg, interpolation='bilinear')\n",
    "            ax4.set_title('Negative Sentiment Word Cloud', fontweight='bold')\n",
    "            ax4.axis('off')\n",
    "    \n",
    "    # 5. Most Common Words\n",
    "    ax5 = axes[1, 1]\n",
    "    all_text = ' '.join(text_df['text'].tolist())\n",
    "    # Simple word extraction (remove punctuation and convert to lowercase)\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', all_text.lower())\n",
    "    # Filter out common stop words\n",
    "    stop_words = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them', 'my', 'your', 'his', 'her', 'its', 'our', 'their'}\n",
    "    filtered_words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    if filtered_words:\n",
    "        word_counts = Counter(filtered_words).most_common(10)\n",
    "        words_list, counts_list = zip(*word_counts)\n",
    "        \n",
    "        bars = ax5.barh(range(len(words_list)), counts_list, color='lightcoral')\n",
    "        ax5.set_yticks(range(len(words_list)))\n",
    "        ax5.set_yticklabels(words_list)\n",
    "        ax5.set_title('Top 10 Most Common Words', fontweight='bold')\n",
    "        ax5.set_xlabel('Frequency')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, bar in enumerate(bars):\n",
    "            width = bar.get_width()\n",
    "            ax5.text(width + 0.5, bar.get_y() + bar.get_height()/2.,\n",
    "                    f'{int(width)}', ha='left', va='center', fontweight='bold')\n",
    "    \n",
    "    # 6. Text Complexity Analysis\n",
    "    ax6 = axes[1, 2]\n",
    "    if text_df['text'].notna().any():\n",
    "        # Calculate readability scores for a sample of texts\n",
    "        sample_texts = text_df['text'].dropna().head(100).tolist()\n",
    "        readability_scores = []\n",
    "        \n",
    "        for text in sample_texts:\n",
    "            try:\n",
    "                # Flesch Reading Ease Score (higher = easier to read)\n",
    "                score = textstat.flesch_reading_ease(text)\n",
    "                readability_scores.append(score)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if readability_scores:\n",
    "            ax6.hist(readability_scores, bins=20, color='lightgreen', alpha=0.7, edgecolor='black')\n",
    "            ax6.set_title('Text Readability Distribution', fontweight='bold')\n",
    "            ax6.set_xlabel('Flesch Reading Ease Score')\n",
    "            ax6.set_ylabel('Frequency')\n",
    "            ax6.axvline(np.mean(readability_scores), color='red', linestyle='--',\n",
    "                       label=f'Mean: {np.mean(readability_scores):.1f}')\n",
    "            ax6.legend()\n",
    "    \n",
    "    # 7. Sentiment by Text Length\n",
    "    ax7 = axes[2, 0]\n",
    "    if 'sentiment' in sentiment_df.columns and 'length' in text_df.columns:\n",
    "        # Merge sentiment with original text data\n",
    "        merged_df = pd.merge(sentiment_df, text_df, left_on='text', right_on='text', how='inner')\n",
    "        \n",
    "        sentiment_colors = {'positive': 'green', 'negative': 'red', 'neutral': 'gray'}\n",
    "        for sentiment in merged_df['sentiment'].unique():\n",
    "            sentiment_data = merged_df[merged_df['sentiment'] == sentiment]\n",
    "            ax7.scatter(sentiment_data['length'], [sentiment]*len(sentiment_data), \n",
    "                       c=sentiment_colors.get(sentiment, 'blue'), alpha=0.6, \n",
    "                       label=sentiment, s=30)\n",
    "        \n",
    "        ax7.set_title('Sentiment vs Text Length', fontweight='bold')\n",
    "        ax7.set_xlabel('Text Length (characters)')\n",
    "        ax7.set_ylabel('Sentiment')\n",
    "        ax7.legend()\n",
    "    \n",
    "    # 8. Character Distribution\n",
    "    ax8 = axes[2, 1]\n",
    "    char_counts = {}\n",
    "    for text in text_df['text'].head(100):  # Sample for performance\n",
    "        for char in text.lower():\n",
    "            if char.isalpha():\n",
    "                char_counts[char] = char_counts.get(char, 0) + 1\n",
    "    \n",
    "    if char_counts:\n",
    "        sorted_chars = sorted(char_counts.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "        chars, counts = zip(*sorted_chars)\n",
    "        \n",
    "        bars = ax8.bar(chars, counts, color='lightblue', alpha=0.7)\n",
    "        ax8.set_title('Character Frequency Distribution', fontweight='bold')\n",
    "        ax8.set_xlabel('Characters')\n",
    "        ax8.set_ylabel('Frequency')\n",
    "        ax8.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 9. Sentiment Confidence (if available)\n",
    "    ax9 = axes[2, 2]\n",
    "    if 'confidence' in sentiment_df.columns:\n",
    "        confidence_scores = sentiment_df['confidence'].dropna()\n",
    "        ax9.hist(confidence_scores, bins=20, color='orange', alpha=0.7, edgecolor='black')\n",
    "        ax9.set_title('Sentiment Confidence Distribution', fontweight='bold')\n",
    "        ax9.set_xlabel('Confidence Score')\n",
    "        ax9.set_ylabel('Frequency')\n",
    "    else:\n",
    "        # Show text categories distribution instead\n",
    "        if 'true_sentiment' in text_df.columns:\n",
    "            category_counts = text_df['true_sentiment'].value_counts()\n",
    "            ax9.pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%',\n",
    "                   colors=['lightgreen', 'lightcoral', 'lightgray'])\n",
    "            ax9.set_title('True Sentiment Distribution', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('text_analytics_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Text analytics dashboard saved as 'text_analytics_dashboard.png'\")\n",
    "\n",
    "# Create text visualizations\n",
    "create_text_visualizations(text_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a75852",
   "metadata": {},
   "source": [
    "### Interactive Plotly Text Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc4561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_text_dashboard(dataset):\n",
    "    \"\"\"Create interactive text analytics dashboard using Plotly.\"\"\"\n",
    "    print(\"Creating interactive text analytics dashboard...\")\n",
    "    \n",
    "    text_df = dataset.to_pandas()\n",
    "    sentiment_df = pd.DataFrame(final_results)\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=3,\n",
    "        subplot_titles=('Sentiment Distribution', 'Text Length Analysis', 'Word Frequency',\n",
    "                       'Sentiment vs Length', 'Readability Scores', 'Text Categories'),\n",
    "        specs=[[{\"type\": \"bar\"}, {\"type\": \"histogram\"}, {\"type\": \"bar\"}],\n",
    "               [{\"type\": \"scatter\"}, {\"type\": \"histogram\"}, {\"type\": \"pie\"}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Sentiment Distribution\n",
    "    if 'sentiment' in sentiment_df.columns:\n",
    "        sentiment_counts = sentiment_df['sentiment'].value_counts()\n",
    "        colors = ['green' if s == 'positive' else 'red' if s == 'negative' else 'orange' \n",
    "                 for s in sentiment_counts.index]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=sentiment_counts.index, y=sentiment_counts.values,\n",
    "                  marker_color=colors, name=\"Sentiment\"),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # 2. Text Length Distribution\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=text_df['length'], nbinsx=30, marker_color='skyblue', \n",
    "                    name=\"Text Length\"),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Top Words Frequency\n",
    "    all_text = ' '.join(text_df['text'].tolist())\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', all_text.lower())\n",
    "    stop_words = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were'}\n",
    "    filtered_words = [word for word in words if word not in stop_words and len(word) > 3]\n",
    "    \n",
    "    if filtered_words:\n",
    "        word_counts = Counter(filtered_words).most_common(10)\n",
    "        words_list, counts_list = zip(*word_counts)\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=list(words_list), y=list(counts_list), \n",
    "                  marker_color='lightcoral', name=\"Word Frequency\"),\n",
    "            row=1, col=3\n",
    "        )\n",
    "    \n",
    "    # 4. Sentiment vs Text Length Scatter\n",
    "    if 'sentiment' in sentiment_df.columns:\n",
    "        merged_df = pd.merge(sentiment_df, text_df, left_on='text', right_on='text', how='inner')\n",
    "        \n",
    "        for sentiment in merged_df['sentiment'].unique():\n",
    "            sentiment_data = merged_df[merged_df['sentiment'] == sentiment]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=sentiment_data['length'], \n",
    "                          y=[sentiment]*len(sentiment_data),\n",
    "                          mode='markers', name=sentiment,\n",
    "                          marker=dict(size=8, opacity=0.6)),\n",
    "                row=2, col=1\n",
    "            )\n",
    "    \n",
    "    # 5. Readability Scores\n",
    "    sample_texts = text_df['text'].dropna().head(50).tolist()\n",
    "    readability_scores = []\n",
    "    \n",
    "    for text in sample_texts:\n",
    "        try:\n",
    "            score = textstat.flesch_reading_ease(text)\n",
    "            readability_scores.append(score)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if readability_scores:\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=readability_scores, nbinsx=15, marker_color='lightgreen',\n",
    "                        name=\"Readability\"),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # 6. Text Categories Pie Chart\n",
    "    if 'true_sentiment' in text_df.columns:\n",
    "        category_counts = text_df['true_sentiment'].value_counts()\n",
    "        fig.add_trace(\n",
    "            go.Pie(labels=category_counts.index, values=category_counts.values,\n",
    "                  name=\"Categories\"),\n",
    "            row=2, col=3\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title_text=\"Interactive Text Analytics Dashboard\",\n",
    "        height=800,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Update axes\n",
    "    fig.update_xaxes(title_text=\"Sentiment\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Text Length\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Frequency\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Words\", row=1, col=3)\n",
    "    fig.update_yaxes(title_text=\"Frequency\", row=1, col=3)\n",
    "    fig.update_xaxes(title_text=\"Text Length\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Sentiment\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Readability Score\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Frequency\", row=2, col=2)\n",
    "    \n",
    "    # Save and show\n",
    "    fig.write_html(\"interactive_text_dashboard.html\")\n",
    "    print(\"Interactive text dashboard saved as 'interactive_text_dashboard.html'\")\n",
    "    fig.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create interactive dashboard\n",
    "interactive_dashboard = create_interactive_text_dashboard(text_dataset)\n",
    "\n",
    "print(\"Quick start completed! Run the full demo for advanced NLP features.\")\n",
    "\n",
    "# Expected Output:\n",
    "# Sentiment Analysis Results:\n",
    "# --------------------------------------------------\n",
    "# Text: This movie was absolutely fantastic! Great acting...\n",
    "# Sentiment: POSITIVE (confidence: 0.95)\n",
    "# ------------------------------\n",
    "# Text: Terrible film. Waste of time and money. Very dis...\n",
    "# Sentiment: NEGATIVE (confidence: 0.92)\n",
    "# ------------------------------\n",
    "# Text: Amazing cinematography and outstanding performanc...\n",
    "# Sentiment: POSITIVE (confidence: 0.88)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b986f128",
   "metadata": {},
   "source": [
    "## Complete Tutorial\n",
    "\n",
    "### 1. **Load Real Text Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b012f8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.data import read_text, read_parquet, from_huggingface\n",
    "\n",
    "# Ray cluster is already running on Anyscale\n",
    "print(f'Ray cluster resources: {ray.cluster_resources()}')\n",
    "\n",
    "# Load real text datasets using Ray Data native APIs\n",
    "# Use Ray Data's native Hugging Face integration\n",
    "imdb_reviews = from_huggingface(\"imdb\", split=\"train[:1000]\")\n",
    "print(f\"IMDB Reviews: {imdb_reviews.count()}\")\n",
    "\n",
    "# Load Amazon reviews using native read_parquet\n",
    "amazon_reviews = read_parquet(\n",
    "    \"s3://amazon-reviews-pds/parquet/product_category=Books/\",\n",
    "    columns=[\"review_body\", \"star_rating\"]\n",
    ").limit(500)\n",
    "print(f\"Amazon Reviews: {amazon_reviews.count()}\")\n",
    "\n",
    "# Inspect sample data\n",
    "sample_review = imdb_reviews.take(1)[0]\n",
    "print(f\"Sample review: {sample_review['text'][:100]}...\")\n",
    "print(f\"Sample label: {sample_review['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb29cfc",
   "metadata": {},
   "source": [
    "### 2. **Text Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93d657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"Preprocess text data for NLP analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Download required NLTK data\n",
    "        try:\n",
    "            nltk.download('punkt', quiet=True)\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            nltk.download('wordnet', quiet=True)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Preprocess a batch of text documents.\"\"\"\n",
    "        processed_texts = []\n",
    "        \n",
    "        for text in batch[\"text\"]:\n",
    "            try:\n",
    "                # Clean and normalize text\n",
    "                cleaned_text = self._clean_text(text)\n",
    "                \n",
    "                # Tokenize\n",
    "                tokens = self._tokenize(cleaned_text)\n",
    "                \n",
    "                # Remove stop words and lemmatize\n",
    "                processed_tokens = self._process_tokens(tokens)\n",
    "                \n",
    "                # Join tokens back into text\n",
    "                processed_text = \" \".join(processed_tokens)\n",
    "                \n",
    "                processed_texts.append({\n",
    "                    \"original_text\": text,\n",
    "                    \"processed_text\": processed_text,\n",
    "                    \"token_count\": len(processed_tokens),\n",
    "                    \"cleaned_length\": len(processed_text)\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error preprocessing text: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return {\"processed_texts\": processed_texts}\n",
    "    \n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"Clean and normalize text.\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters and extra whitespace\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Remove numbers (optional)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def _tokenize(self, text):\n",
    "        \"\"\"Tokenize text into words.\"\"\"\n",
    "        return nltk.word_tokenize(text)\n",
    "    \n",
    "    def _process_tokens(self, tokens):\n",
    "        \"\"\"Remove stop words and lemmatize tokens.\"\"\"\n",
    "        processed = []\n",
    "        for token in tokens:\n",
    "            if token not in self.stop_words and len(token) > 2:\n",
    "                lemmatized = self.lemmatizer.lemmatize(token)\n",
    "                processed.append(lemmatized)\n",
    "        return processed\n",
    "\n",
    "# Apply text preprocessing\n",
    "processed_texts = reviews_ds.map_batches(\n",
    "    TextPreprocessor(),\n",
    "    batch_size=100,\n",
    "    concurrency=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9480b0",
   "metadata": {},
   "source": [
    "### 3. **Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec4538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    \"\"\"Perform sentiment analysis using pre-trained models.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Load sentiment analysis pipeline\n",
    "        self.sentiment_pipeline = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "            device=0 if torch.cuda.is_available() else -1\n",
    "        )\n",
    "        \n",
    "        # Load emotion detection pipeline\n",
    "        self.emotion_pipeline = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
    "            device=0 if torch.cuda.is_available() else -1\n",
    "        )\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Analyze sentiment and emotions in text batch.\"\"\"\n",
    "        sentiment_results = []\n",
    "        \n",
    "        for item in batch[\"processed_texts\"]:\n",
    "            try:\n",
    "                text = item[\"processed_text\"]\n",
    "                \n",
    "                # Perform sentiment analysis\n",
    "                sentiment_result = self.sentiment_pipeline(text[:512])[0]\n",
    "                \n",
    "                # Perform emotion detection\n",
    "                emotion_result = self.emotion_pipeline(text[:512])[0]\n",
    "                \n",
    "                # Combine results\n",
    "                result = {\n",
    "                    \"id\": item.get(\"id\"),\n",
    "                    \"original_text\": item[\"original_text\"],\n",
    "                    \"processed_text\": text,\n",
    "                    \"sentiment\": sentiment_result[\"label\"],\n",
    "                    \"sentiment_score\": sentiment_result[\"score\"],\n",
    "                    \"emotion\": emotion_result[\"label\"],\n",
    "                    \"emotion_score\": emotion_result[\"score\"],\n",
    "                    \"token_count\": item[\"token_count\"]\n",
    "                }\n",
    "                \n",
    "                sentiment_results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in sentiment analysis: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return {\"sentiment_results\": sentiment_results}\n",
    "\n",
    "# Apply sentiment analysis\n",
    "sentiment_analysis = processed_texts.map_batches(\n",
    "    SentimentAnalyzer(),\n",
    "    batch_size=32,\n",
    "    num_gpus=1 if ray.cluster_resources().get(\"GPU\", 0) > 0 else 0,\n",
    "    concurrency=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b711df",
   "metadata": {},
   "source": [
    "### 4. **Topic Modeling and Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7864abf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "class TopicModeler:\n",
    "    \"\"\"Perform topic modeling and text clustering.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_topics=10, num_clusters=5):\n",
    "        self.num_topics = num_topics\n",
    "        self.num_clusters = num_clusters\n",
    "        \n",
    "        # Initialize TF-IDF vectorizer\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=1000,\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 2)\n",
    "        )\n",
    "        \n",
    "        # Initialize topic model\n",
    "        self.lda_model = LatentDirichletAllocation(\n",
    "            n_components=num_topics,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Initialize clustering model\n",
    "        self.kmeans_model = KMeans(\n",
    "            n_clusters=num_clusters,\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Perform topic modeling and clustering on text batch.\"\"\"\n",
    "        try:\n",
    "            texts = [item[\"processed_text\"] for item in batch[\"sentiment_results\"]]\n",
    "            \n",
    "            if not texts:\n",
    "                return {\"topic_results\": []}\n",
    "            \n",
    "            # Vectorize texts\n",
    "            tfidf_matrix = self.vectorizer.fit_transform(texts)\n",
    "            \n",
    "            # Topic modeling\n",
    "            topic_distributions = self.lda_model.fit_transform(tfidf_matrix)\n",
    "            \n",
    "            # Clustering\n",
    "            cluster_labels = self.kmeans_model.fit_predict(tfidf_matrix)\n",
    "            \n",
    "            # Get feature names for topic interpretation\n",
    "            feature_names = self.vectorizer.get_feature_names_out()\n",
    "            \n",
    "            # Extract top words for each topic\n",
    "            top_words_per_topic = []\n",
    "            for topic_idx, topic in enumerate(self.lda_model.components_):\n",
    "                top_words_idx = topic.argsort()[-10:][::-1]\n",
    "                top_words = [feature_names[i] for i in top_words_idx]\n",
    "                top_words_per_topic.append(top_words)\n",
    "            \n",
    "            # Combine results\n",
    "            topic_results = []\n",
    "            for i, item in enumerate(batch[\"sentiment_results\"]):\n",
    "                result = {\n",
    "                    **item,\n",
    "                    \"topic_distribution\": topic_distributions[i].tolist(),\n",
    "                    \"dominant_topic\": int(np.argmax(topic_distributions[i])),\n",
    "                    \"topic_confidence\": float(np.max(topic_distributions[i])),\n",
    "                    \"cluster_label\": int(cluster_labels[i]),\n",
    "                    \"top_topic_words\": top_words_per_topic[np.argmax(topic_distributions[i])]\n",
    "                }\n",
    "                topic_results.append(result)\n",
    "            \n",
    "            return {\"topic_results\": topic_results}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in topic modeling: {e}\")\n",
    "            return {\"topic_results\": []}\n",
    "\n",
    "# Apply topic modeling\n",
    "topic_modeling = sentiment_analysis.map_batches(\n",
    "    TopicModeler(num_topics=8, num_clusters=4),\n",
    "    batch_size=100,\n",
    "    concurrency=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f14bb5",
   "metadata": {},
   "source": [
    "### 5. **Named Entity Recognition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f2c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import defaultdict\n",
    "\n",
    "class NERExtractor:\n",
    "    \"\"\"Extract named entities from text.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Load English language model\n",
    "        try:\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        except:\n",
    "            # Download if not available\n",
    "            import os\n",
    "            os.system(\"python -m spacy download en_core_web_sm\")\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Extract named entities from text batch.\"\"\"\n",
    "        ner_results = []\n",
    "        \n",
    "        for item in batch[\"topic_results\"]:\n",
    "            try:\n",
    "                text = item[\"processed_text\"]\n",
    "                \n",
    "                # Process text with spaCy\n",
    "                doc = self.nlp(text)\n",
    "                \n",
    "                # Extract entities by type\n",
    "                entities = defaultdict(list)\n",
    "                for ent in doc.ents:\n",
    "                    entities[ent.label_].append({\n",
    "                        \"text\": ent.text,\n",
    "                        \"start\": ent.start_char,\n",
    "                        \"end\": ent.end_char,\n",
    "                        \"confidence\": ent.label_\n",
    "                    })\n",
    "                \n",
    "                # Count entities\n",
    "                entity_counts = {label: len(ents) for label, ents in entities.items()}\n",
    "                \n",
    "                # Add NER results\n",
    "                result = {\n",
    "                    **item,\n",
    "                    \"entities\": dict(entities),\n",
    "                    \"entity_counts\": entity_counts,\n",
    "                    \"total_entities\": sum(entity_counts.values())\n",
    "                }\n",
    "                \n",
    "                ner_results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in NER extraction: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return {\"ner_results\": ner_results}\n",
    "\n",
    "# Apply NER extraction\n",
    "ner_extraction = topic_modeling.map_batches(\n",
    "    NERExtractor(),\n",
    "    batch_size=50,\n",
    "    concurrency=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b90507",
   "metadata": {},
   "source": [
    "### 6. **Advanced NLP Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a60e9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "class AdvancedNLPProcessor:\n",
    "    \"\"\"Perform advanced NLP tasks including summarization, language detection, and classification.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize multiple NLP pipelines\n",
    "        self.summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=-1)\n",
    "        self.classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\", device=-1)\n",
    "        self.language_detector = pipeline(\"text-classification\", model=\"papluca/xlm-roberta-base-language-detection\", device=-1)\n",
    "        \n",
    "        # Question answering pipeline\n",
    "        self.qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\", device=-1)\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Apply advanced NLP processing to text batch.\"\"\"\n",
    "        advanced_results = []\n",
    "        \n",
    "        for item in batch[\"ner_results\"]:\n",
    "            try:\n",
    "                text = item[\"processed_text\"]\n",
    "                original_text = item[\"original_text\"]\n",
    "                \n",
    "                # Text summarization (for longer texts)\n",
    "                summary = \"\"\n",
    "                if len(original_text) > 500:\n",
    "                    try:\n",
    "                        summary_result = self.summarizer(original_text[:1024], max_length=150, min_length=30, do_sample=False)\n",
    "                        summary = summary_result[0]['summary_text']\n",
    "                    except Exception as e:\n",
    "                        summary = f\"Summarization failed: {str(e)}\"\n",
    "                \n",
    "                # Language detection\n",
    "                try:\n",
    "                    language_result = self.language_detector(text[:512])\n",
    "                    detected_language = language_result[0]['label']\n",
    "                    language_confidence = language_result[0]['score']\n",
    "                except Exception as e:\n",
    "                    detected_language = \"unknown\"\n",
    "                    language_confidence = 0.0\n",
    "                \n",
    "                # Text classification (additional classification beyond sentiment)\n",
    "                try:\n",
    "                    classification_result = self.classifier(text[:512])\n",
    "                    text_category = classification_result[0]['label']\n",
    "                    category_confidence = classification_result[0]['score']\n",
    "                except Exception as e:\n",
    "                    text_category = \"unknown\"\n",
    "                    category_confidence = 0.0\n",
    "                \n",
    "                # Question answering (example questions)\n",
    "                qa_results = []\n",
    "                sample_questions = [\n",
    "                    \"What is the main topic?\",\n",
    "                    \"What is the sentiment?\",\n",
    "                    \"Who is mentioned?\"\n",
    "                ]\n",
    "                \n",
    "                for question in sample_questions:\n",
    "                    try:\n",
    "                        qa_result = self.qa_pipeline(question=question, context=original_text[:512])\n",
    "                        qa_results.append({\n",
    "                            \"question\": question,\n",
    "                            \"answer\": qa_result['answer'],\n",
    "                            \"confidence\": qa_result['score']\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        qa_results.append({\n",
    "                            \"question\": question,\n",
    "                            \"answer\": \"N/A\",\n",
    "                            \"confidence\": 0.0\n",
    "                        })\n",
    "                \n",
    "                # Text readability and complexity metrics\n",
    "                word_count = len(text.split())\n",
    "                sentence_count = len([s for s in original_text.split('.') if s.strip()])\n",
    "                avg_word_length = sum(len(word) for word in text.split()) / word_count if word_count > 0 else 0\n",
    "                \n",
    "                advanced_result = {\n",
    "                    **item,\n",
    "                    \"summary\": summary,\n",
    "                    \"detected_language\": detected_language,\n",
    "                    \"language_confidence\": language_confidence,\n",
    "                    \"text_category\": text_category,\n",
    "                    \"category_confidence\": category_confidence,\n",
    "                    \"qa_results\": qa_results,\n",
    "                    \"readability_metrics\": {\n",
    "                        \"word_count\": word_count,\n",
    "                        \"sentence_count\": sentence_count,\n",
    "                        \"avg_word_length\": avg_word_length,\n",
    "                        \"avg_sentence_length\": word_count / sentence_count if sentence_count > 0 else 0\n",
    "                    },\n",
    "                    \"advanced_processing_timestamp\": pd.Timestamp.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                advanced_results.append(advanced_result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in advanced NLP processing: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return {\"advanced_nlp_results\": advanced_results}\n",
    "\n",
    "# Apply advanced NLP processing\n",
    "advanced_nlp = ner_extraction.map_batches(\n",
    "    AdvancedNLPProcessor(),\n",
    "    batch_size=16,\n",
    "    concurrency=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5343e4cf",
   "metadata": {},
   "source": [
    "### 7. **Ray Data LLM Package Integration (Optional)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4837137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Use Ray Data LLM package for large-scale language model inference\n",
    "try:\n",
    "    from ray.data.llm import LLMPredictor\n",
    "    \n",
    "    class LLMTextAnalyzer:\n",
    "        \"\"\"Use Ray Data LLM package for advanced text analysis.\"\"\"\n",
    "        \n",
    "        def __init__(self, model_name=\"microsoft/DialoGPT-medium\"):\n",
    "            \"\"\"Initialize LLM predictor for text analysis.\"\"\"\n",
    "            self.model_name = model_name\n",
    "            \n",
    "            # Initialize LLM predictor with Ray Data LLM package\n",
    "            self.llm_predictor = LLMPredictor.from_checkpoint(\n",
    "                checkpoint=model_name,\n",
    "                torch_dtype=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "        \n",
    "        def __call__(self, batch):\n",
    "            \"\"\"Perform LLM-based text analysis.\"\"\"\n",
    "            llm_results = []\n",
    "            \n",
    "            for item in batch[\"advanced_nlp_results\"]:\n",
    "                try:\n",
    "                    original_text = item[\"original_text\"]\n",
    "                    \n",
    "                    # Create prompts for different analysis tasks\n",
    "                    analysis_prompts = [\n",
    "                        f\"Analyze the sentiment and key themes in this text: {original_text[:500]}\",\n",
    "                        f\"Extract the main topics and entities from: {original_text[:500]}\",\n",
    "                        f\"Provide a brief summary of: {original_text[:500]}\"\n",
    "                    ]\n",
    "                    \n",
    "                    llm_analyses = []\n",
    "                    for prompt in analysis_prompts:\n",
    "                        try:\n",
    "                            # Use LLM predictor for inference\n",
    "                            response = self.llm_predictor.predict(prompt)\n",
    "                            llm_analyses.append({\n",
    "                                \"prompt_type\": prompt.split(':')[0],\n",
    "                                \"response\": response,\n",
    "                                \"prompt_length\": len(prompt)\n",
    "                            })\n",
    "                        except Exception as e:\n",
    "                            llm_analyses.append({\n",
    "                                \"prompt_type\": prompt.split(':')[0],\n",
    "                                \"response\": f\"LLM inference failed: {str(e)}\",\n",
    "                                \"error\": True\n",
    "                            })\n",
    "                    \n",
    "                    llm_result = {\n",
    "                        **item,\n",
    "                        \"llm_analyses\": llm_analyses,\n",
    "                        \"llm_model_used\": self.model_name,\n",
    "                        \"llm_processing_timestamp\": pd.Timestamp.now().isoformat()\n",
    "                    }\n",
    "                    \n",
    "                    llm_results.append(llm_result)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in LLM processing: {e}\")\n",
    "                    llm_results.append({\n",
    "                        **item,\n",
    "                        \"llm_error\": str(e)\n",
    "                    })\n",
    "            \n",
    "            return {\"llm_results\": llm_results}\n",
    "    \n",
    "    # Apply LLM analysis (optional - requires Ray Data LLM package)\n",
    "    llm_analysis = advanced_nlp.map_batches(\n",
    "        LLMTextAnalyzer(),\n",
    "        batch_size=8,  # Smaller batch for LLM processing\n",
    "        num_gpus=1 if ray.cluster_resources().get(\"GPU\", 0) > 0 else 0,\n",
    "        concurrency=1  # Single concurrency for LLM to avoid resource conflicts\n",
    "    )\n",
    "    \n",
    "    print(\"LLM analysis completed using Ray Data LLM package\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Ray Data LLM package not available. Skipping LLM analysis.\")\n",
    "    print(\"To use LLM features, install with: pip install ray[data,llm]\")\n",
    "    llm_analysis = advanced_nlp\n",
    "\n",
    "# Alternative: Simple LLM integration without Ray Data LLM package\n",
    "class SimpleLLMProcessor:\n",
    "    \"\"\"Simple LLM integration using transformers directly.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        from transformers import pipeline\n",
    "        \n",
    "        # Initialize text generation pipeline\n",
    "        self.text_generator = pipeline(\n",
    "            \"text-generation\", \n",
    "            model=\"gpt2\", \n",
    "            device=-1,\n",
    "            max_length=100\n",
    "        )\n",
    "        \n",
    "        # Initialize summarization pipeline\n",
    "        self.summarizer = pipeline(\n",
    "            \"summarization\",\n",
    "            model=\"facebook/bart-large-cnn\",\n",
    "            device=-1\n",
    "        )\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Apply simple LLM processing.\"\"\"\n",
    "        simple_llm_results = []\n",
    "        \n",
    "        for item in batch[\"advanced_nlp_results\"]:\n",
    "            try:\n",
    "                text = item[\"original_text\"]\n",
    "                \n",
    "                # Text summarization\n",
    "                summary = \"\"\n",
    "                if len(text) > 200:\n",
    "                    try:\n",
    "                        summary_result = self.summarizer(text[:1024], max_length=100, min_length=30)\n",
    "                        summary = summary_result[0]['summary_text']\n",
    "                    except Exception as e:\n",
    "                        summary = f\"Summarization error: {str(e)}\"\n",
    "                \n",
    "                # Simple text generation\n",
    "                generation_prompt = f\"Based on this text: {text[:100]}... the key insight is\"\n",
    "                try:\n",
    "                    generation_result = self.text_generator(generation_prompt, max_length=50, num_return_sequences=1)\n",
    "                    generated_insight = generation_result[0]['generated_text'][len(generation_prompt):].strip()\n",
    "                except Exception as e:\n",
    "                    generated_insight = f\"Generation error: {str(e)}\"\n",
    "                \n",
    "                simple_llm_result = {\n",
    "                    **item,\n",
    "                    \"text_summary\": summary,\n",
    "                    \"generated_insight\": generated_insight,\n",
    "                    \"llm_processing_method\": \"transformers_direct\",\n",
    "                    \"llm_timestamp\": pd.Timestamp.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                simple_llm_results.append(simple_llm_result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in simple LLM processing: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return {\"simple_llm_results\": simple_llm_results}\n",
    "\n",
    "# Apply simple LLM processing as alternative\n",
    "simple_llm_analysis = advanced_nlp.map_batches(\n",
    "    SimpleLLMProcessor(),\n",
    "    batch_size=8,\n",
    "    concurrency=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1f1f9d",
   "metadata": {},
   "source": [
    "### 8. **Text Similarity and Semantic Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fab6120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "class TextSimilarityAnalyzer:\n",
    "    \"\"\"Analyze text similarity and enable semantic search.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.similarity_threshold = 0.7\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Calculate text similarity within batch.\"\"\"\n",
    "        similarity_results = []\n",
    "        \n",
    "        # Extract embeddings from batch\n",
    "        embeddings = []\n",
    "        texts = []\n",
    "        items = []\n",
    "        \n",
    "        for item in batch.get(\"embedding_results\", []):\n",
    "            if \"embeddings\" in item and item[\"embeddings\"]:\n",
    "                embeddings.append(np.array(item[\"embeddings\"]))\n",
    "                texts.append(item[\"processed_text\"])\n",
    "                items.append(item)\n",
    "        \n",
    "        if len(embeddings) < 2:\n",
    "            return {\"similarity_results\": items}\n",
    "        \n",
    "        # Calculate pairwise similarities\n",
    "        embeddings_matrix = np.array(embeddings)\n",
    "        similarity_matrix = cosine_similarity(embeddings_matrix)\n",
    "        \n",
    "        # Find similar text pairs\n",
    "        for i, item in enumerate(items):\n",
    "            similar_texts = []\n",
    "            \n",
    "            for j, other_item in enumerate(items):\n",
    "                if i != j and similarity_matrix[i][j] > self.similarity_threshold:\n",
    "                    similar_texts.append({\n",
    "                        \"similar_text_id\": other_item.get(\"document_id\", j),\n",
    "                        \"similarity_score\": float(similarity_matrix[i][j]),\n",
    "                        \"similar_text_preview\": other_item[\"processed_text\"][:100]\n",
    "                    })\n",
    "            \n",
    "            # Sort by similarity score\n",
    "            similar_texts.sort(key=lambda x: x[\"similarity_score\"], reverse=True)\n",
    "            \n",
    "            result = {\n",
    "                **item,\n",
    "                \"similar_texts\": similar_texts[:5],  # Top 5 similar texts\n",
    "                \"similarity_count\": len(similar_texts),\n",
    "                \"has_similar_content\": len(similar_texts) > 0\n",
    "            }\n",
    "            \n",
    "            similarity_results.append(result)\n",
    "        \n",
    "        return {\"similarity_results\": similarity_results}\n",
    "\n",
    "# Apply similarity analysis\n",
    "similarity_analysis = ner_extraction.map_batches(\n",
    "    TextSimilarityAnalyzer(),\n",
    "    batch_size=20,\n",
    "    concurrency=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619c6562",
   "metadata": {},
   "source": [
    "### 9. **Language Detection and Multilingual Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fee54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "class MultilingualProcessor:\n",
    "    \"\"\"Handle multilingual text processing.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Language detection pipeline\n",
    "        self.language_detector = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=\"papluca/xlm-roberta-base-language-detection\",\n",
    "            device=-1\n",
    "        )\n",
    "        \n",
    "        # Multilingual sentiment analysis\n",
    "        self.multilingual_sentiment = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n",
    "            device=-1\n",
    "        )\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Process multilingual text.\"\"\"\n",
    "        multilingual_results = []\n",
    "        \n",
    "        for item in batch[\"similarity_results\"]:\n",
    "            try:\n",
    "                text = item[\"processed_text\"]\n",
    "                original_text = item[\"original_text\"]\n",
    "                \n",
    "                # Detect language\n",
    "                try:\n",
    "                    lang_result = self.language_detector(text[:512])\n",
    "                    detected_language = lang_result[0]['label']\n",
    "                    language_confidence = lang_result[0]['score']\n",
    "                except Exception as e:\n",
    "                    detected_language = \"unknown\"\n",
    "                    language_confidence = 0.0\n",
    "                \n",
    "                # Multilingual sentiment analysis\n",
    "                try:\n",
    "                    multilingual_sentiment = self.multilingual_sentiment(text[:512])\n",
    "                    ml_sentiment = multilingual_sentiment[0]['label']\n",
    "                    ml_sentiment_score = multilingual_sentiment[0]['score']\n",
    "                except Exception as e:\n",
    "                    ml_sentiment = \"unknown\"\n",
    "                    ml_sentiment_score = 0.0\n",
    "                \n",
    "                # Text complexity analysis\n",
    "                complexity_metrics = {\n",
    "                    \"character_count\": len(original_text),\n",
    "                    \"word_count\": len(text.split()),\n",
    "                    \"unique_words\": len(set(text.split())),\n",
    "                    \"lexical_diversity\": len(set(text.split())) / len(text.split()) if len(text.split()) > 0 else 0,\n",
    "                    \"avg_word_length\": sum(len(word) for word in text.split()) / len(text.split()) if len(text.split()) > 0 else 0\n",
    "                }\n",
    "                \n",
    "                multilingual_result = {\n",
    "                    **item,\n",
    "                    \"detected_language\": detected_language,\n",
    "                    \"language_confidence\": language_confidence,\n",
    "                    \"multilingual_sentiment\": ml_sentiment,\n",
    "                    \"multilingual_sentiment_score\": ml_sentiment_score,\n",
    "                    \"complexity_metrics\": complexity_metrics,\n",
    "                    \"is_english\": detected_language.lower() in ['en', 'english'],\n",
    "                    \"is_complex_text\": complexity_metrics[\"lexical_diversity\"] > 0.6,\n",
    "                    \"multilingual_timestamp\": pd.Timestamp.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                multilingual_results.append(multilingual_result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in multilingual processing: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return {\"multilingual_results\": multilingual_results}\n",
    "\n",
    "# Apply multilingual processing\n",
    "multilingual_analysis = similarity_analysis.map_batches(\n",
    "    MultilingualProcessor(),\n",
    "    batch_size=16,\n",
    "    concurrency=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9969844",
   "metadata": {},
   "source": [
    "### 10. **Ray Data LLM Package for Production Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba7840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production-scale LLM inference using Ray Data LLM package\n",
    "try:\n",
    "    from ray.data.llm import LLMPredictor\n",
    "    \n",
    "    class ProductionLLMAnalyzer:\n",
    "        \"\"\"Production-scale LLM analysis using Ray Data LLM package.\"\"\"\n",
    "        \n",
    "        def __init__(self, model_name=\"microsoft/DialoGPT-small\"):\n",
    "            \"\"\"Initialize production LLM analyzer.\"\"\"\n",
    "            self.model_name = model_name\n",
    "            \n",
    "            # Configure LLM predictor for production use\n",
    "            self.llm_predictor = LLMPredictor.from_checkpoint(\n",
    "                checkpoint=model_name,\n",
    "                torch_dtype=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                max_batch_size=16,\n",
    "                max_concurrent_requests=4\n",
    "            )\n",
    "        \n",
    "        def __call__(self, batch):\n",
    "            \"\"\"Perform production LLM inference.\"\"\"\n",
    "            production_results = []\n",
    "            \n",
    "            # Prepare prompts for batch inference\n",
    "            prompts = []\n",
    "            items = []\n",
    "            \n",
    "            for item in batch[\"multilingual_results\"]:\n",
    "                text = item[\"original_text\"]\n",
    "                \n",
    "                # Create structured prompt for analysis\n",
    "                analysis_prompt = f\"\"\"\n",
    "                Analyze this text and provide insights:\n",
    "                \n",
    "                Text: {text[:800]}\n",
    "                \n",
    "                Please provide:\n",
    "                1. Main theme\n",
    "                2. Key entities\n",
    "                3. Emotional tone\n",
    "                4. Business relevance\n",
    "                \"\"\"\n",
    "                \n",
    "                prompts.append(analysis_prompt)\n",
    "                items.append(item)\n",
    "            \n",
    "            # Batch LLM inference using Ray Data LLM package\n",
    "            try:\n",
    "                llm_responses = self.llm_predictor.predict_batch(prompts)\n",
    "                \n",
    "                for item, response in zip(items, llm_responses):\n",
    "                    production_result = {\n",
    "                        **item,\n",
    "                        \"llm_analysis\": response,\n",
    "                        \"llm_model\": self.model_name,\n",
    "                        \"llm_method\": \"ray_data_llm_package\",\n",
    "                        \"production_timestamp\": pd.Timestamp.now().isoformat()\n",
    "                    }\n",
    "                    production_results.append(production_result)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"LLM batch inference failed: {e}\")\n",
    "                # Fallback to individual processing\n",
    "                for item in items:\n",
    "                    production_results.append({\n",
    "                        **item,\n",
    "                        \"llm_analysis\": \"LLM processing unavailable\",\n",
    "                        \"llm_error\": str(e)\n",
    "                    })\n",
    "            \n",
    "            return {\"production_llm_results\": production_results}\n",
    "    \n",
    "    # Apply production LLM analysis\n",
    "    production_llm = multilingual_analysis.map_batches(\n",
    "        ProductionLLMAnalyzer(),\n",
    "        batch_size=8,\n",
    "        num_gpus=1 if ray.cluster_resources().get(\"GPU\", 0) > 0 else 0,\n",
    "        concurrency=1\n",
    "    )\n",
    "    \n",
    "    print(\"Production LLM analysis completed using Ray Data LLM package\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Ray Data LLM package not available.\")\n",
    "    print(\"To use production LLM features, install with: pip install ray[data] vllm\")\n",
    "    production_llm = multilingual_analysis\n",
    "\n",
    "# Final results collection\n",
    "final_nlp_results = production_llm.take_all()\n",
    "print(f\"Complete NLP pipeline processed {len(final_nlp_results)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c22741",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Advanced Features\n",
    "\n",
    "### **Ray Data's NLP Superpowers**\n",
    "\n",
    "**1. Distributed Model Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbc9d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process 100K documents across cluster with automatic load balancing\n",
    "large_dataset.map_batches(\n",
    "    TransformerModel(), \n",
    "    batch_size=64,      # Optimal GPU utilization\n",
    "    concurrency=8,      # Parallel model instances\n",
    "    num_gpus=1          # GPU acceleration per instance\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565417bc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**2. Memory-Efficient Text Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53079929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle datasets larger than cluster memory\n",
    "massive_text_dataset.map_batches(\n",
    "    TextProcessor(),\n",
    "    batch_size=1000,    # Process in manageable chunks\n",
    "    concurrency=16      # Maximize CPU utilization\n",
    ")\n",
    "# Ray Data automatically manages memory and spilling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00503846",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**3. Fault-Tolerant NLP Pipelines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc5d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in error recovery for production NLP\n",
    "nlp_pipeline = (dataset\n",
    "    .map_batches(TextCleaner())      # Automatic retry on failures\n",
    "    .map_batches(SentimentAnalyzer()) # Worker failure recovery\n",
    "    .map_batches(TopicModeler())     # Data block replication\n",
    ")\n",
    "# No additional fault tolerance code needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7057141e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**4. Seamless Model Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45213116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale from single GPU to multi-GPU automatically\n",
    "if ray.cluster_resources().get(\"GPU\", 0) >= 4:\n",
    "    # Multi-GPU configuration\n",
    "    concurrency = 4\n",
    "    num_gpus = 1\n",
    "else:\n",
    "    # CPU fallback\n",
    "    concurrency = 8\n",
    "    num_gpus = 0\n",
    "\n",
    "# Ray Data handles resource allocation automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f6e2cd",
   "metadata": {},
   "source": [
    "### **Enterprise NLP Use Case Patterns**\n",
    "\n",
    "**Customer Feedback Analysis Pipeline**\n",
    "- **Scale**: Process 50K+ daily reviews\n",
    "- **Speed**: 2-hour processing vs 40+ hours manual\n",
    "- **Accuracy**: Consistent analysis across all content\n",
    "- **Insights**: Product satisfaction, feature requests, issue identification\n",
    "\n",
    "**Support Ticket Intelligence**\n",
    "- **Automation**: 90% tickets auto-classified and routed\n",
    "- **Response Time**: 15-minute vs 24-hour issue identification\n",
    "- **Efficiency**: reduction in manual ticket triage\n",
    "- **Quality**: Consistent urgency and intent classification\n",
    "\n",
    "**Brand Monitoring at Scale**\n",
    "- **Coverage**: 100% social media mention analysis\n",
    "- **Real-time**: Immediate sentiment tracking and alerts\n",
    "- **Competitive**: Multi-brand comparison and positioning\n",
    "- **Actionable**: Trend identification and response recommendations\n",
    "\n",
    "## Production Considerations\n",
    "\n",
    "### **Model Management**\n",
    "- Model versioning and deployment\n",
    "- A/B testing for model performance\n",
    "- Model monitoring and retraining\n",
    "\n",
    "### **Scalability**\n",
    "- Horizontal scaling across multiple nodes\n",
    "- Load balancing for NLP workloads\n",
    "- Resource optimization for different model types\n",
    "\n",
    "### **Quality Assurance**\n",
    "- Text data validation\n",
    "- Model performance monitoring\n",
    "- Output quality checks\n",
    "\n",
    "## Example Workflows\n",
    "\n",
    "### **1. Customer Experience Intelligence**\n",
    "**Use Case**: E-commerce company analyzing 50K daily reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc936aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load customer reviews at scale\n",
    "reviews = from_huggingface(\"amazon_reviews_multi\", split=\"train[:50000]\")\n",
    "\n",
    "# Multi-dimensional sentiment analysis\n",
    "sentiment_pipeline = reviews.map_batches(SentimentAnalyzer(), batch_size=100, concurrency=8)\n",
    "\n",
    "# Product feature extraction\n",
    "feature_pipeline = sentiment_pipeline.map_batches(ProductFeatureExtractor(), batch_size=50)\n",
    "\n",
    "# Business insights generation\n",
    "insights = feature_pipeline.groupby('product_category').agg({\n",
    "    'sentiment_score': 'mean',\n",
    "    'feature_mentions': 'count',\n",
    "    'recommendation_score': 'mean'\n",
    "})\n",
    "\n",
    "# Results: Product satisfaction by category, feature improvement priorities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd756d5",
   "metadata": {},
   "source": [
    "### **2. Brand Monitoring and Competitive Analysis**\n",
    "**Use Case**: Marketing team tracking 25K daily social mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f6de81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load social media mentions\n",
    "social_data = read_text(\"s3://social-media-feeds/mentions/\")\n",
    "\n",
    "# Brand sentiment tracking\n",
    "brand_analysis = social_data.map_batches(BrandSentimentAnalyzer(), batch_size=200)\n",
    "\n",
    "# Competitive comparison\n",
    "competitor_analysis = brand_analysis.map_batches(CompetitorMentionTracker(), batch_size=100)\n",
    "\n",
    "# Trend identification\n",
    "trending_topics = competitor_analysis.groupby('mention_type').agg({\n",
    "    'sentiment_score': 'mean',\n",
    "    'engagement_rate': 'mean',\n",
    "    'virality_score': 'max'\n",
    "})\n",
    "\n",
    "# Results: Real-time brand health, competitive positioning insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a900d1fe",
   "metadata": {},
   "source": [
    "### **3. Support Ticket Intelligence**\n",
    "**Use Case**: Customer service team processing 15K daily tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3018fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load support tickets\n",
    "tickets = read_parquet(\"s3://support-system/tickets/\")\n",
    "\n",
    "# Urgency classification and routing\n",
    "classified_tickets = tickets.map_batches(TicketClassifier(), batch_size=150)\n",
    "\n",
    "# Issue categorization and solution matching\n",
    "categorized_tickets = classified_tickets.map_batches(IssueCategorizer(), batch_size=100)\n",
    "\n",
    "# Knowledge base enhancement\n",
    "knowledge_updates = categorized_tickets.map_batches(KnowledgeExtractor(), batch_size=75)\n",
    "\n",
    "# Results: Automated ticket routing, solution recommendations, knowledge base updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19f7cad",
   "metadata": {},
   "source": [
    "### **4. Content Moderation at Scale**\n",
    "**Use Case**: Social platform moderating 100K daily posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8105a051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load user-generated content\n",
    "user_content = read_json(\"s3://platform-content/posts/\")\n",
    "\n",
    "# Multi-layer content analysis\n",
    "safety_analysis = user_content.map_batches(ContentSafetyAnalyzer(), batch_size=500)\n",
    "\n",
    "# Toxicity and harmful content detection\n",
    "toxicity_analysis = safety_analysis.map_batches(ToxicityDetector(), batch_size=300)\n",
    "\n",
    "# Automated moderation decisions\n",
    "moderation_actions = toxicity_analysis.map_batches(ModerationDecisionEngine(), batch_size=200)\n",
    "\n",
    "# Results: Automated content moderation, safety scoring, action recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f994ffd",
   "metadata": {},
   "source": [
    "### **5. Document Intelligence and Compliance**\n",
    "**Use Case**: Legal firm processing 10K daily documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5bec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load legal documents\n",
    "legal_docs = read_text(\"s3://legal-documents/filings/\")\n",
    "\n",
    "# Contract analysis and clause extraction\n",
    "contract_analysis = legal_docs.map_batches(ContractAnalyzer(), batch_size=50)\n",
    "\n",
    "# Compliance checking and risk assessment\n",
    "compliance_check = contract_analysis.map_batches(ComplianceValidator(), batch_size=25)\n",
    "\n",
    "# Key information extraction for case management\n",
    "case_intelligence = compliance_check.map_batches(CaseIntelligenceExtractor(), batch_size=30)\n",
    "\n",
    "# Results: Contract risk assessment, compliance validation, case insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f8c52d",
   "metadata": {},
   "source": [
    "### **6. Multilingual Customer Support**\n",
    "**Use Case**: Global company handling 30K daily multilingual inquiries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb115823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multilingual customer inquiries\n",
    "inquiries = read_text(\"s3://global-support/inquiries/\")\n",
    "\n",
    "# Language detection and routing\n",
    "language_analysis = inquiries.map_batches(LanguageDetector(), batch_size=300)\n",
    "\n",
    "# Multilingual sentiment and intent analysis\n",
    "intent_analysis = language_analysis.map_batches(MultilingualIntentAnalyzer(), batch_size=150)\n",
    "\n",
    "# Automated response generation\n",
    "response_generation = intent_analysis.map_batches(ResponseGenerator(), batch_size=100)\n",
    "\n",
    "# Results: Automated multilingual support, intent classification, response suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc750713",
   "metadata": {},
   "source": [
    "## Performance Analysis\n",
    "\n",
    "### **NLP Pipeline Processing Flow**\n",
    "\n",
    "```\n",
    "Text Data Processing Pipeline:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Raw Text Data  â”‚ (IMDB, Amazon, News)\n",
    "â”‚  1M+ documents  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Text Cleaning   â”‚    â”‚ BERT Embeddings â”‚    â”‚ Sentiment       â”‚\n",
    "â”‚ & Preprocessing â”‚â”€â”€â”€â–¶â”‚ Generation      â”‚â”€â”€â”€â–¶â”‚ Analysis        â”‚\n",
    "â”‚                 â”‚    â”‚ (384-dim)       â”‚    â”‚ (Pos/Neg/Neu)  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                                       â”‚\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
    "         â”‚ Named Entity    â”‚    â”‚ Topic Modeling  â”‚    â”‚\n",
    "         â”‚ Recognition     â”‚â—€â”€â”€â”€â”‚ & Clustering    â”‚â—€â”€â”€â”€â”˜\n",
    "         â”‚ (Persons/Orgs)  â”‚    â”‚ (LDA/K-means)   â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### **Performance Measurement Framework**\n",
    "\n",
    "| Analysis Type | Benchmark Method | Visualization Output |\n",
    "|--------------|------------------|---------------------|\n",
    "| **Text Processing** | Throughput measurement | Processing speed charts |\n",
    "| **Model Inference** | GPU utilization tracking | Resource usage graphs |\n",
    "| **Sentiment Analysis** | Accuracy vs speed | Performance trade-offs |\n",
    "| **Topic Modeling** | Convergence analysis | Topic quality metrics |\n",
    "\n",
    "### **Expected Output Visualizations**\n",
    "\n",
    "The demo generates comprehensive analysis charts:\n",
    "\n",
    "| Chart Type | File Output | Content Description |\n",
    "|-----------|-------------|-------------------|\n",
    "| **Sentiment Distribution** | `sentiment_analysis.html` | Positive/Negative/Neutral breakdown |\n",
    "| **Topic Modeling Results** | `topic_visualization.html` | Word clouds and topic clusters |\n",
    "| **Performance Metrics** | `nlp_performance.html` | Throughput and resource usage |\n",
    "| **Entity Analysis** | `entity_extraction.html` | Named entity frequency charts |\n",
    "\n",
    "### **Sample Output Structure**\n",
    "\n",
    "```\n",
    "NLP Analysis Results:\n",
    "â”œâ”€â”€ Text Statistics\n",
    "â”‚   â”œâ”€â”€ Document count: [Actual count]\n",
    "â”‚   â”œâ”€â”€ Average length: [Measured]\n",
    "â”‚   â””â”€â”€ Language distribution: [Detected]\n",
    "â”œâ”€â”€ Sentiment Analysis\n",
    "â”‚   â”œâ”€â”€ Positive: [%]\n",
    "â”‚   â”œâ”€â”€ Negative: [%]\n",
    "â”‚   â””â”€â”€ Neutral: [%]\n",
    "â”œâ”€â”€ Topic Modeling\n",
    "â”‚   â”œâ”€â”€ Topics discovered: [Number]\n",
    "â”‚   â”œâ”€â”€ Top keywords per topic\n",
    "â”‚   â””â”€â”€ Document-topic assignments\n",
    "â””â”€â”€ Named Entities\n",
    "    â”œâ”€â”€ Persons: [Count]\n",
    "    â”œâ”€â”€ Organizations: [Count]\n",
    "    â””â”€â”€ Locations: [Count]\n",
    "```\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### **Common Issues and Solutions**\n",
    "\n",
    "| Issue | Symptoms | Solution | Prevention |\n",
    "|-------|----------|----------|------------|\n",
    "| **Model Download Failures** | `OSError: Can't load model` | Check internet connection, try different model | Use local model cache, verify model names |\n",
    "| **GPU Memory Issues** | `CUDA out of memory` | Reduce batch size to 8-16, use CPU fallback | Monitor GPU memory, start with small batches |\n",
    "| **Text Encoding Errors** | `UnicodeDecodeError` | Add encoding handling, clean text | Validate text encoding, use UTF-8 |\n",
    "| **Slow Processing** | Long processing times | Use GPU acceleration, optimize batch size | Profile operations, check resource utilization |\n",
    "| **Model Compatibility** | Version conflicts | Update transformers library | Pin dependency versions in requirements.txt |\n",
    "\n",
    "### **Performance Optimization for Different Text Sizes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524e0658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive batch sizing based on text characteristics\n",
    "def get_optimal_batch_size(sample_texts):\n",
    "    \"\"\"Calculate optimal batch size based on text characteristics.\"\"\"\n",
    "    avg_length = sum(len(text) for text in sample_texts) / len(sample_texts)\n",
    "    \n",
    "    if avg_length > 1000:\n",
    "        return 4   # Long texts - small batches\n",
    "    elif avg_length > 500:\n",
    "        return 8   # Medium texts - medium batches\n",
    "    else:\n",
    "        return 16  # Short texts - larger batches\n",
    "\n",
    "# Memory-efficient text processing\n",
    "def process_with_memory_management(dataset, processor_class):\n",
    "    \"\"\"Process text with automatic memory management.\"\"\"\n",
    "    import torch\n",
    "    \n",
    "    # Start with conservative batch size\n",
    "    batch_size = 8\n",
    "    \n",
    "    try:\n",
    "        result = dataset.map_batches(\n",
    "            processor_class(),\n",
    "            batch_size=batch_size,\n",
    "            num_gpus=1 if torch.cuda.is_available() else 0\n",
    "        )\n",
    "        return result\n",
    "        \n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        print(\"GPU memory error, reducing batch size and retrying...\")\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Retry with smaller batch\n",
    "        return dataset.map_batches(\n",
    "            processor_class(),\n",
    "            batch_size=batch_size // 2,\n",
    "            num_gpus=0  # Use CPU fallback\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1b5122",
   "metadata": {},
   "source": [
    "### **Text Data Quality Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05ac015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate text data quality before processing\n",
    "def validate_text_quality(batch):\n",
    "    \"\"\"Validate text data quality and clean problematic entries.\"\"\"\n",
    "    clean_texts = []\n",
    "    \n",
    "    for item in batch:\n",
    "        text = item.get('text', '')\n",
    "        \n",
    "        # Quality checks\n",
    "        issues = []\n",
    "        \n",
    "        if not text or len(text.strip()) == 0:\n",
    "            issues.append(\"empty_text\")\n",
    "        \n",
    "        if len(text) > 10000:\n",
    "            issues.append(\"text_too_long\")\n",
    "            text = text[:10000]  # Truncate\n",
    "        \n",
    "        if not text.isascii():\n",
    "            try:\n",
    "                text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "                issues.append(\"non_ascii_characters\")\n",
    "            except:\n",
    "                issues.append(\"encoding_error\")\n",
    "                continue\n",
    "        \n",
    "        clean_item = {\n",
    "            **item,\n",
    "            'text': text,\n",
    "            'quality_issues': issues,\n",
    "            'is_clean': len(issues) == 0\n",
    "        }\n",
    "        \n",
    "        clean_texts.append(clean_item)\n",
    "    \n",
    "    return clean_texts\n",
    "\n",
    "# Apply validation before processing\n",
    "validated_texts = text_dataset.map_batches(validate_text_quality, batch_size=100)\n",
    "clean_texts = validated_texts.filter(lambda x: x['is_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49403067",
   "metadata": {},
   "source": [
    "### **Debug Mode and Monitoring**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e6d061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "from transformers import logging as transformers_logging\n",
    "\n",
    "# Enable comprehensive debugging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "transformers_logging.set_verbosity_info()\n",
    "\n",
    "# Monitor system resources\n",
    "def monitor_resources():\n",
    "    \"\"\"Monitor system resources during processing.\"\"\"\n",
    "    import psutil\n",
    "    \n",
    "    # CPU and memory monitoring\n",
    "    cpu_percent = psutil.cpu_percent(interval=1)\n",
    "    memory = psutil.virtual_memory()\n",
    "    \n",
    "    print(f\"CPU Usage: {cpu_percent:.1f}%\")\n",
    "    print(f\"Memory Usage: {memory.percent:.1f}% ({memory.used / 1e9:.1f}GB / {memory.total / 1e9:.1f}GB)\")\n",
    "    \n",
    "    # GPU monitoring if available\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.memory_allocated(0) / 1e9\n",
    "        gpu_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"GPU Memory: {gpu_memory:.1f}GB / {gpu_total:.1f}GB ({gpu_memory/gpu_total*100:.1f}%)\")\n",
    "\n",
    "# Enable progress tracking\n",
    "class ProgressTracker:\n",
    "    def __init__(self, total_items):\n",
    "        self.total_items = total_items\n",
    "        self.processed_items = 0\n",
    "    \n",
    "    def update(self, batch_size):\n",
    "        self.processed_items += batch_size\n",
    "        progress = (self.processed_items / self.total_items) * 100\n",
    "        print(f\"Progress: {progress:.1f}% ({self.processed_items:,}/{self.total_items:,})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5c5b8c",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Customize Models**: Use domain-specific models and fine-tuning\n",
    "2. **Define Analytics**: Implement your specific NLP requirements\n",
    "3. **Build Pipelines**: Create end-to-end text processing workflows\n",
    "4. **Scale Production**: Deploy to multi-node clusters\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Ray Data Documentation](https://docs.ray.io/en/latest/data/index.html)\n",
    "- [Transformers Documentation](https://huggingface.co/docs/transformers/)\n",
    "- [spaCy Documentation](https://spacy.io/usage)\n",
    "- [NLTK Documentation](https://www.nltk.org/)\n",
    "\n",
    "---\n",
    "\n",
    "*This template provides a foundation for building production-ready NLP and text analytics pipelines with Ray Data. Start with the basic examples and gradually add complexity based on your specific text processing requirements.*"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
