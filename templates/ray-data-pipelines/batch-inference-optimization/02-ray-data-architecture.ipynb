{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2: Ray Data Architecture and Optimization\n",
        "\n",
        "**Time to complete**: 25 min | **Difficulty**: Advanced | **Prerequisites**: Complete Part 1 and Part 2\n",
        "\n",
        "**[← Back to Part 1](01-inference-fundamentals.ipynb)** | **[Return to Overview](README.md)**\n",
        "\n",
        "---\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "Understanding Ray Data's architecture is critical for making informed optimization decisions. In this part, you'll learn:\n",
        "\n",
        "1. How Ray Data's streaming execution model enables efficient batch inference\n",
        "2. How blocks and the object store affect memory management\n",
        "3. How operators and planning impact performance\n",
        "4. How resource management affects optimization strategies\n",
        "5. Why these architectural choices matter for your inference workloads\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Complete [Part 1: Inference Fundamentals](01-inference-fundamentals.md) and [Part 2: Advanced Optimization](02-advanced-optimization.md) before starting this part.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Streaming Execution Model](#streaming-execution-model)\n",
        "2. [Datasets and Blocks](#datasets-and-blocks)\n",
        "3. [Ray Memory Model](#ray-memory-model)\n",
        "4. [Operators and Planning](#operators-and-planning)\n",
        "5. [Resource Management](#resource-management-and-backpressure)\n",
        "6. [Optimization Implications](#optimization-implications)\n",
        "\n",
        "---\n",
        "\n",
        "## Streaming Execution Model\n",
        "\n",
        "## Streaming execution\n",
        "\n",
        "Ray Data processes large datasets efficiently using a streaming model, which works with **blocks** as the basic units of data.\n",
        "\n",
        "This approach replaces traditional bulk processing, where the entire dataset and intermediate results had to fit in the cluster's memory.\n",
        "\n",
        "Ray Data's streaming execution combines the **best of batch processing techniques** (logical and physical plan optimizations) along with the **best of data stream processin**g (high scalability and concurrent stages)\n",
        "\n",
        "\n",
        "### Why Streaming Execution Matters for Batch Inference\n",
        "\n",
        "Traditional batch processing loads entire datasets into memory before processing. For batch inference with 1M+ images or documents, this approach fails:\n",
        "\n",
        "**Traditional Batch Processing Problems:**\n",
        "- **Memory explosion**: Loading 1M images × 3MB each = 3TB memory required\n",
        "- **No pipeline parallelism**: Model loading, preprocessing, inference all sequential, GPUs just waiting idle\n",
        "- **Long time to first result**: Wait for all data to load before any inference\n",
        "- **OOM errors**: Cluster runs out of memory frequently\n",
        "\n",
        "**Ray Data Streaming Execution Solution:**\n",
        "- **Constant memory**: Process 128MB blocks at a time, not full dataset\n",
        "- **Pipeline parallelism**: Load, preprocess, and infer simultaneously\n",
        "- **Fast time to first result**: Start inferring as soon as first block loads, fully saturating your GPUs\n",
        "- **Automatic backpressure**: Prevents memory overflow dynamically\n",
        "\n",
        "\n",
        "### Visualizing the Difference\n",
        "\n",
        "**Traditional Batch Processing:**\n",
        "\n",
        "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/cko-2025-q1/batch-processing.png\" width=\"800\" alt=\"Traditional Batch Processing\">\n",
        "\n",
        "**Problems with traditional approach:**\n",
        "- High memory - requires loading entire dataset\n",
        "- No parallelism - stages run sequentially  \n",
        "- Long latency - wait for complete load before processing\n",
        "- Wasted resources - GPUs idle during load/write stages\n",
        "\n",
        "**Ray Data Streaming Execution:**\n",
        "\n",
        "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/cko-2025-q1/pipelining.png\" width=\"800\" alt=\"Ray Data Streaming Execution\">\n",
        "\n",
        "**Benefits of streaming execution:**\n",
        "- Low memory - constant predidctable blocks regardless of dataset size\n",
        "- Pipeline parallelism - all stages active simultaneously\n",
        "- Fast first result - inference starts immediately\n",
        "- Maximum throughput - all resources utilized continuously\n",
        "\n",
        "### How This Affects Your Optimization Decisions\n",
        "\n",
        "Understanding streaming execution helps you make better optimization choices:\n",
        "\n",
        "**1. Batch Size Selection:**\n",
        "- **Don't make batch_size too large**: Risk memory overflow\n",
        "- **Don't make batch_size too small**: Waste GPU capacity\n",
        "- **Sweet spot**: Match GPU memory and throughput needs\n",
        "\n",
        "**2. Concurrency Configuration:**\n",
        "- **Too many actors**: Backpressure kicks in, actors idle waiting for resources\n",
        "- **Too few actors**: Underutilized cluster, low throughput\n",
        "- **Optimal**: Match available GPUs and memory constraints\n",
        "\n",
        "**3. Model Loading Strategy:**\n",
        "- **Why actors work**: Model loads once, reused across many blocks\n",
        "- **Why tasks fail**: Model reloads for every block, massive overhead\n",
        "- **Architecture enables**: Stateful processing without memory bloat\n",
        "\n",
        "### Practical Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-23 17:03:04,172\tINFO worker.py:1833 -- Connecting to existing Ray cluster at address: 10.0.35.66:6379...\n",
            "2025-10-23 17:03:04,184\tINFO worker.py:2004 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-d1zksaq9ieevgqna5eqcl23rjk.i.anyscaleuserdata-staging.com \u001b[39m\u001b[22m\n",
            "2025-10-23 17:03:04,187\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_3bb37a88b8e9fec528627c1380803c9e37219f36.zip' (0.29MiB) to Ray cluster...\n",
            "2025-10-23 17:03:04,188\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_3bb37a88b8e9fec528627c1380803c9e37219f36.zip'.\n",
            "/home/ray/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py:2052: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
            "  warnings.warn(\n",
            "2025-10-23 17:03:09,059\tINFO logging.py:293 -- Registered dataset logger for dataset dataset_3_0\n",
            "2025-10-23 17:03:09,130\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_3_0. Full logs are in /tmp/ray/session_2025-10-23_16-34-47_857542_2333/logs/ray-data\n",
            "2025-10-23 17:03:09,131\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_3_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> LimitOperator[limit=1000] -> TaskPoolMapOperator[ReadFiles] -> ActorPoolMapOperator[Map(preprocess_image)->MapBatches(InferenceWorker)]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "926afad6d486450a97c419680f7087e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Running 0: 0.00 row [00:00, ? row/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "{\"asctime\":\"2025-10-23 17:03:09,190\",\"levelname\":\"E\",\"message\":\"Actor with class name: 'MapWorker(Map(preprocess_image)->MapBatches(InferenceWorker))' and ID: '27fd6720fbea830fe8023fee02000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\",\"filename\":\"core_worker.cc\",\"lineno\":2161}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7125b7e4acd1488985a436c29c843b61",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "- ListFiles 1: 0.00 row [00:00, ? row/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "726d9468805c40eaaa0a7334d1c7a2d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "- limit=1000 2: 0.00 row [00:00, ? row/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "987ce422d9be4a319605ae7dac243d1f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "- ReadFiles 3: 0.00 row [00:00, ? row/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b15a9a0cdf8f45a5a5f970ce78027b54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "- Map(preprocess_image)->MapBatches(InferenceWorker) 4: 0.00 row [00:00, ? row/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-23 17:03:09,299\tWARNING resource_manager.py:134 -- ⚠️  Ray's object store is configured to use only 27.9% of available memory (98.3GiB out of 352.0GiB total). For optimal Ray Data performance, we recommend setting the object store to at least 50% of available memory. You can do this by setting the 'object_store_memory' parameter when calling ray.init() or by setting the RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION environment variable.\n",
            "/home/ray/anaconda3/lib/python3.12/site-packages/ray/anyscale/data/_internal/cluster_autoscaler/productivity_calculator.py:174: RuntimeWarning: invalid value encountered in divide\n",
            "  gpu_fraction_per_op = (optimal_num_tasks_per_op * num_gpus_per_op) / np.sum(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(MapWorker(Map(preprocess_image)->MapBatches(InferenceWorker)) pid=3549, ip=10.0.39.125)\u001b[0m Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /home/ray/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0.00/230M [00:00<?, ?B/s]Batches(InferenceWorker)) pid=3549, ip=10.0.39.125)\u001b[0m \n",
            "  5%|▍         | 11.1M/230M [00:00<00:01, 117MB/s]InferenceWorker)) pid=3549, ip=10.0.39.125)\u001b[0m \n",
            " 10%|▉         | 22.2M/230M [00:00<00:02, 93.1MB/s]nferenceWorker)) pid=3549, ip=10.0.39.125)\u001b[0m \n",
            " 14%|█▎        | 31.5M/230M [00:00<00:02, 93.1MB/s]nferenceWorker)) pid=3549, ip=10.0.39.125)\u001b[0m \n",
            " 18%|█▊        | 40.6M/230M [00:00<00:02, 69.1MB/s]nferenceWorker)) pid=3549, ip=10.0.39.125)\u001b[0m \n",
            " 26%|██▌       | 59.8M/230M [00:00<00:01, 105MB/s] nferenceWorker)) pid=3549, ip=10.0.39.125)\u001b[0m \n",
            " 35%|███▌      | 81.2M/230M [00:00<00:01, 139MB/s]InferenceWorker)) pid=3549, ip=10.0.39.125)\u001b[0m \n",
            "100%|██████████| 230M/230M [00:01<00:00, 150MB/s](InferenceWorker)) pid=3549, ip=10.0.39.125)\u001b[0m \n",
            "100%|██████████| 230M/230M [00:01<00:00, 202MB/s](InferenceWorker)) pid=4448, ip=10.0.6.101)\u001b[0m \n",
            "100%|██████████| 230M/230M [00:01<00:00, 240MB/s](InferenceWorker)) pid=3473, ip=10.0.31.127)\u001b[0m \n",
            "\u001b[36m(MapWorker(Map(preprocess_image)->MapBatches(InferenceWorker)) pid=3549, ip=10.0.39.125)\u001b[0m /tmp/ray/session_2025-10-23_16-34-47_857542_2333/runtime_resources/pip/a191cadd5dd422d657139880a3a4b447e02336c4/virtualenv/lib/python3.12/site-packages/torchvision/transforms/functional.py:154: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
            "\u001b[36m(MapWorker(Map(preprocess_image)->MapBatches(InferenceWorker)) pid=3549, ip=10.0.39.125)\u001b[0m   img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(MapWorker(Map(preprocess_image)->MapBatches(InferenceWorker)) pid=3426, ip=10.0.3.18)\u001b[0m Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /home/ray/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0.00/230M [00:00<?, ?B/s]\u001b[32m [repeated 3x across cluster]\u001b[0m=10.0.3.18)\u001b[0m \n",
            " 89%|████████▉ | 206M/230M [00:00<00:00, 246MB/s]\u001b[32m [repeated 24x across cluster]\u001b[0m.127)\u001b[0m \n",
            "100%|██████████| 230M/230M [00:01<00:00, 204MB/s](InferenceWorker)) pid=3426, ip=10.0.3.18)\u001b[0m \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'predicted_label': array(['tench', 'tench', 'tench', 'English springer', 'English springer',\n",
            "       'English springer', 'English springer', 'English springer',\n",
            "       'radio', 'tape player'], dtype=object), 'confidence': array([0.99935967, 0.99985158, 0.99108666, 0.49943095, 0.99055171,\n",
            "       0.99199039, 0.85377097, 0.69636279, 0.36682165, 0.54299575])}\n",
            "{'predicted_label': array(['loudspeaker', 'CD player', 'sports car', 'chain saw', 'chain saw',\n",
            "       'chain saw', 'chain saw', 'chain saw', 'church', 'altar'],\n",
            "      dtype=object), 'confidence': array([0.84784979, 0.93608212, 0.44139817, 0.99995494, 0.98789829,\n",
            "       0.99997699, 0.98841596, 0.99988866, 0.23647654, 0.49457902])}\n",
            "{'predicted_label': array(['church', 'church', 'French horn', 'French horn', 'French horn',\n",
            "       'French horn', 'French horn', 'garbage truck', 'garbage truck',\n",
            "       'garbage truck'], dtype=object), 'confidence': array([0.99217391, 0.49143541, 0.80212873, 0.99511343, 0.99905521,\n",
            "       0.96243846, 0.99852824, 0.92528981, 0.99976498, 0.99908614])}\n",
            "{'predicted_label': array(['garbage truck', 'garbage truck', 'gas pump', 'gas pump',\n",
            "       'gas pump', 'gas pump', 'golf ball', 'golf ball', 'golf ball',\n",
            "       'golf ball'], dtype=object), 'confidence': array([0.99970573, 0.99865073, 0.96700221, 0.99999332, 0.99999988,\n",
            "       0.98631358, 0.99996531, 0.99995232, 0.99997175, 0.99997401])}\n",
            "{'predicted_label': array(['golf ball', 'parachute', 'parachute', 'maypole', 'parachute',\n",
            "       'parachute', 'tench', 'tench', 'coho', 'tench'], dtype=object), 'confidence': array([0.99987698, 0.57258463, 0.76674062, 0.32100627, 0.99992895,\n",
            "       0.22059849, 0.99088389, 0.99998736, 0.82265562, 0.9995442 ])}\n",
            "{'predicted_label': array(['English springer', 'English springer', 'English springer',\n",
            "       'English springer', 'English springer', 'tape player',\n",
            "       'tape player', 'cassette player', 'CD player', 'cassette player'],\n",
            "      dtype=object), 'confidence': array([0.96530926, 0.46694911, 0.87001896, 0.95391583, 0.91354901,\n",
            "       0.47834054, 0.45952678, 0.95370817, 0.37442911, 0.95454866])}\n",
            "{'predicted_label': array(['chain saw', 'chain saw', 'chain saw', 'fountain', 'chain saw',\n",
            "       'church', 'church', 'church', 'church', 'French horn'],\n",
            "      dtype=object), 'confidence': array([0.99998236, 0.99334538, 0.9993875 , 0.95875615, 0.99995649,\n",
            "       0.9929207 , 0.74598336, 0.95919091, 0.96174109, 0.86448598])}\n",
            "{'predicted_label': array(['French horn', 'French horn', 'French horn', 'French horn',\n",
            "       'garbage truck', 'garbage truck', 'Model T', 'garbage truck',\n",
            "       'garbage truck', 'cash machine'], dtype=object), 'confidence': array([0.99976951, 0.99887568, 0.99454957, 0.97791195, 0.93285614,\n",
            "       0.99785489, 0.44733241, 0.96622479, 0.98551673, 0.35762274])}\n",
            "{'predicted_label': array(['gas pump', 'gas pump', 'gas pump', 'golf ball', 'golf ball',\n",
            "       'golf ball', 'golf ball', 'golf ball', 'parachute', 'palace'],\n",
            "      dtype=object), 'confidence': array([0.7483533 , 0.99999952, 0.48844698, 0.99843425, 0.9994567 ,\n",
            "       0.99893397, 0.99992657, 0.99995089, 0.86895299, 0.1777254 ])}\n",
            "{'predicted_label': array(['parachute', 'balloon', 'parachute', 'tench', 'barracouta',\n",
            "       'tench', 'tench', 'English springer', 'English springer',\n",
            "       'English springer'], dtype=object), 'confidence': array([0.99992549, 0.55432242, 0.99994242, 0.99685717, 0.90060669,\n",
            "       0.99893171, 0.99791104, 0.83909506, 0.96231097, 0.84846103])}\n",
            "{'predicted_label': array(['English springer', 'English springer', 'space bar',\n",
            "       'cassette player', 'cassette player', 'cassette player',\n",
            "       'cassette', 'dogsled', 'chain saw', 'chain saw'], dtype=object), 'confidence': array([0.98835516, 0.96261871, 0.34081483, 0.51791048, 0.50386578,\n",
            "       0.6547932 , 0.6392566 , 0.35541376, 0.99999988, 0.86839175])}\n",
            "{'predicted_label': array(['chain saw', 'church', 'church', 'church', 'church', 'church',\n",
            "       'French horn', 'French horn', 'French horn', 'French horn'],\n",
            "      dtype=object), 'confidence': array([0.7305879 , 0.9387297 , 0.97280854, 0.9551636 , 0.87666476,\n",
            "       0.75593233, 0.96195829, 0.99671847, 0.9968161 , 0.9615159 ])}\n",
            "{'predicted_label': array(['French horn', 'garbage truck', 'garbage truck', 'garbage truck',\n",
            "       'garbage truck', 'garbage truck', 'gas pump', 'gas pump',\n",
            "       'gas pump', 'gas pump'], dtype=object), 'confidence': array([0.94235337, 0.99957854, 0.93546295, 0.9839685 , 0.98007101,\n",
            "       0.99643964, 0.99915338, 0.99999988, 0.9999963 , 0.99999952])}\n",
            "{'predicted_label': array(['carton', 'golf ball', 'golf ball', 'golf ball', 'golf ball',\n",
            "       'parachute', 'parachute', 'parachute', 'warplane', 'parachute'],\n",
            "      dtype=object), 'confidence': array([0.99845123, 0.99800843, 0.99999535, 0.99168479, 0.99491596,\n",
            "       0.9508819 , 0.58535862, 0.87744081, 0.80565494, 0.8874225 ])}\n",
            "{'predicted_label': array(['tench', 'tench', 'tench', 'tench', 'tench', 'English springer',\n",
            "       'English springer', 'English springer', 'English springer',\n",
            "       'radio'], dtype=object), 'confidence': array([0.99866712, 0.99984324, 0.98325998, 0.99731195, 0.99897754,\n",
            "       0.99418324, 0.88157743, 0.91828644, 0.68467021, 0.27602622])}\n",
            "{'predicted_label': array(['cassette player', 'cassette player', 'cassette player', 'minivan',\n",
            "       'chain saw', 'chain saw', 'chain saw', 'chain saw', 'chain saw',\n",
            "       'church'], dtype=object), 'confidence': array([0.42324293, 0.9559375 , 0.97052234, 0.4040792 , 0.99988794,\n",
            "       0.76936454, 0.99999964, 0.99999344, 0.99875116, 0.67848867])}\n",
            "{'predicted_label': array(['church', 'church', 'church', 'French horn', 'French horn',\n",
            "       'French horn', 'French horn', 'French horn', 'garbage truck',\n",
            "       'garbage truck'], dtype=object), 'confidence': array([0.9825514 , 0.96504164, 0.93236697, 0.99794525, 0.99701416,\n",
            "       0.99776244, 0.99949324, 0.93461519, 0.99441516, 0.88156503])}\n",
            "{'predicted_label': array(['garbage truck', 'garbage truck', 'garbage truck', 'gas pump',\n",
            "       'gas pump', 'gas pump', 'gas pump', 'golf ball', 'golf ball',\n",
            "       'golf ball'], dtype=object), 'confidence': array([0.60968351, 0.99472469, 0.96575725, 0.89301491, 0.99999917,\n",
            "       0.99987388, 0.9999851 , 0.9968971 , 0.99771571, 0.99624169])}\n",
            "{'predicted_label': array(['golf ball', 'golf ball', 'umbrella', 'parachute', 'parachute',\n",
            "       'parachute', 'parachute', 'tench', 'tench', 'reel'], dtype=object), 'confidence': array([0.98913479, 0.99968159, 0.54681081, 0.99960893, 0.99073529,\n",
            "       0.99999261, 0.99975926, 0.99978644, 0.998285  , 0.9515292 ])}\n",
            "{'predicted_label': array(['tench', 'English springer', 'English springer',\n",
            "       'English springer', 'English springer', 'cassette player',\n",
            "       'cassette', 'cassette player', 'cassette player', 'minivan'],\n",
            "      dtype=object), 'confidence': array([0.99989498, 0.81450784, 0.97347081, 0.97205091, 0.91751528,\n",
            "       0.99782515, 0.46281433, 0.84816885, 0.60795969, 0.89111131])}\n",
            "{'predicted_label': array(['chain saw', 'chain saw', 'chain saw', 'chain saw', 'chain saw',\n",
            "       'church', 'church', 'church', 'church', 'French horn'],\n",
            "      dtype=object), 'confidence': array([0.99999464, 0.99999082, 0.69763565, 0.99999988, 0.78777343,\n",
            "       0.98164934, 0.98550445, 0.99689293, 0.99685836, 0.93328434])}\n",
            "{'predicted_label': array(['French horn', 'French horn', 'French horn', 'cornet',\n",
            "       'garbage truck', 'garbage truck', 'garbage truck', 'garbage truck',\n",
            "       'garbage truck', 'gas pump'], dtype=object), 'confidence': array([0.97975391, 0.40142724, 0.99949288, 0.88590699, 0.99340832,\n",
            "       0.67745692, 0.99865937, 0.99998581, 0.99909937, 0.9999963 ])}\n",
            "{'predicted_label': array(['gas pump', 'gas pump', 'gas pump', 'golf ball', 'golf ball',\n",
            "       'golf ball', 'golf ball', 'golf ball', 'parachute', 'parachute'],\n",
            "      dtype=object), 'confidence': array([1.        , 0.99999857, 0.99999881, 0.99526781, 0.99999261,\n",
            "       0.95055449, 0.95733184, 0.9999429 , 0.99985576, 0.99986696])}\n",
            "{'predicted_label': array(['parachute', 'parachute', 'parachute', 'tench', 'tench', 'tench',\n",
            "       'English springer', 'English springer', 'English setter',\n",
            "       'English springer'], dtype=object), 'confidence': array([0.99917787, 0.95665532, 0.99996614, 0.99681771, 0.97203177,\n",
            "       0.99924314, 0.37748525, 0.91509509, 0.43959507, 0.95483357])}\n",
            "{'predicted_label': array(['tape player', 'tape player', 'cassette player', 'tape player',\n",
            "       'CD player', 'chain saw', 'chain saw', 'chain saw', 'chain saw',\n",
            "       'chain saw'], dtype=object), 'confidence': array([0.49580839, 0.51037204, 0.95050877, 0.35653245, 0.36480024,\n",
            "       1.        , 0.99795365, 0.99278086, 0.99999881, 0.99547571])}\n",
            "{'predicted_label': array(['church', 'church', 'church', 'church', 'French horn',\n",
            "       'French horn', 'French horn', 'French horn', 'French horn',\n",
            "       'garbage truck'], dtype=object), 'confidence': array([0.91590232, 0.44303447, 0.76611876, 0.84206176, 0.46831244,\n",
            "       0.99803585, 0.47501785, 0.99584645, 0.99826294, 0.99915802])}\n",
            "{'predicted_label': array(['garbage truck', 'garbage truck', 'garbage truck', 'garbage truck',\n",
            "       'gas pump', 'gas pump', 'gas pump', 'gas pump', 'gas pump',\n",
            "       'golf ball'], dtype=object), 'confidence': array([0.99949598, 0.4959892 , 0.99759966, 0.85763985, 0.99956709,\n",
            "       0.99999964, 0.99999988, 0.92931873, 0.99972051, 0.99995553])}\n",
            "{'predicted_label': array(['golf ball', 'golf ball', 'golf ball', 'parachute', 'parachute',\n",
            "       'parachute', 'parachute', 'parachute', 'tench', 'bittern'],\n",
            "      dtype=object), 'confidence': array([0.99999702, 0.99982327, 0.99803203, 0.99987841, 0.99642795,\n",
            "       0.99997485, 0.99978334, 0.996463  , 0.99770242, 0.854482  ])}\n",
            "{'predicted_label': array(['tench', 'tench', 'English springer', 'English springer',\n",
            "       'English springer', 'English springer', 'cassette', 'cassette',\n",
            "       'cassette player', 'cassette player'], dtype=object), 'confidence': array([0.99988973, 0.99997485, 0.63134313, 0.96637672, 0.98333609,\n",
            "       0.99425417, 0.32064462, 0.85414332, 0.82625365, 0.9586699 ])}\n",
            "{'predicted_label': array(['tape player', 'chain saw', 'chain saw', 'chain saw', 'chain saw',\n",
            "       'chain saw', 'church', 'dome', 'church', 'church'], dtype=object), 'confidence': array([0.44731018, 0.99999905, 0.82212722, 0.99975973, 0.99999988,\n",
            "       0.9973833 , 0.64361876, 0.90266758, 0.50233108, 0.56153965])}\n",
            "{'predicted_label': array(['French horn', 'French horn', 'French horn', 'French horn',\n",
            "       'French horn', 'garbage truck', 'garbage truck', 'garbage truck',\n",
            "       'garbage truck', 'garbage truck'], dtype=object), 'confidence': array([0.53744233, 0.99965274, 0.98127651, 0.99917346, 0.94260609,\n",
            "       0.99338138, 0.9066233 , 0.99038774, 0.99998856, 0.99932432])}\n",
            "{'predicted_label': array(['gas pump', 'gas pump', 'gas pump', 'gas pump', 'web site',\n",
            "       'golf ball', 'golf ball', 'golf ball', 'golf ball', 'parachute'],\n",
            "      dtype=object), 'confidence': array([0.99281996, 0.99856633, 0.9998216 , 0.99705517, 0.77476537,\n",
            "       0.99998987, 0.9944647 , 0.99973613, 0.99567801, 0.95316958])}\n",
            "{'predicted_label': array(['parachute', 'parachute', 'parachute', 'parachute', 'tench',\n",
            "       'tench', 'tench', 'tench', 'English springer', 'English springer'],\n",
            "      dtype=object), 'confidence': array([0.99975032, 0.99969923, 0.99999762, 0.90900874, 0.99980682,\n",
            "       0.99987388, 0.99908733, 0.99786842, 0.99653387, 0.90145832])}\n",
            "{'predicted_label': array(['English springer', 'English springer', 'cassette player',\n",
            "       'tape player', 'cassette player', 'cassette player',\n",
            "       'cassette player', 'chain saw', 'chain saw', 'chain saw'],\n",
            "      dtype=object), 'confidence': array([0.81383115, 0.89594382, 0.98636824, 0.48764592, 0.80873585,\n",
            "       0.91873187, 0.96991158, 0.99941492, 0.99999034, 0.9999218 ])}\n",
            "{'predicted_label': array(['chain saw', 'shovel', 'church', 'church', 'church', 'church',\n",
            "       'French horn', 'sax', 'French horn', 'French horn'], dtype=object), 'confidence': array([0.98237866, 0.4557749 , 0.98768008, 0.99211907, 0.99433833,\n",
            "       0.89527506, 0.98286855, 0.46259975, 0.9985733 , 0.99337667])}\n",
            "{'predicted_label': array(['French horn', 'garbage truck', 'garbage truck', 'garbage truck',\n",
            "       'garbage truck', 'garbage truck', 'gas pump', 'gas pump',\n",
            "       'gas pump', 'gas pump'], dtype=object), 'confidence': array([0.99864858, 0.97905821, 0.92516989, 0.85829395, 0.99888343,\n",
            "       0.72665268, 0.96757662, 1.        , 0.99999964, 0.99951148])}\n",
            "{'predicted_label': array(['golf ball', 'golf ball', 'golf ball', 'golf ball', 'golf ball',\n",
            "       'parachute', 'parachute', 'parachute', 'parachute', 'parachute'],\n",
            "      dtype=object), 'confidence': array([0.99998999, 0.99995947, 0.99979204, 0.99958092, 0.98698437,\n",
            "       0.99897635, 0.99928552, 0.99989474, 0.99775273, 0.99989319])}\n",
            "{'predicted_label': array(['tench', 'tench', 'tench', 'tench', 'English springer',\n",
            "       'English springer', 'English springer', 'English springer',\n",
            "       'CD player', 'cassette player'], dtype=object), 'confidence': array([0.99960142, 0.98994279, 0.98741871, 0.99902117, 0.97914392,\n",
            "       0.81125307, 0.99071771, 0.69976008, 0.49765843, 0.5536558 ])}\n",
            "{'predicted_label': array(['cassette player', 'cassette player', 'cassette player',\n",
            "       'chain saw', 'chain saw', 'chain saw', 'chain saw', 'chain saw',\n",
            "       'church', 'church'], dtype=object), 'confidence': array([0.87954432, 0.67989421, 0.54358578, 0.99999976, 0.93203741,\n",
            "       0.99225724, 1.        , 0.99973732, 0.59265751, 0.47959661])}\n",
            "{'predicted_label': array(['church', 'church', 'cornet', 'French horn', 'French horn',\n",
            "       'French horn', 'French horn', 'garbage truck', 'garbage truck',\n",
            "       'garbage truck'], dtype=object), 'confidence': array([0.19554839, 0.68525529, 0.44702274, 0.97030425, 0.94305295,\n",
            "       0.98875731, 0.99504399, 0.86678123, 0.9998467 , 0.99941206])}\n",
            "{'predicted_label': array(['garbage truck', 'garbage truck', 'gas pump', 'gas pump',\n",
            "       'gas pump', 'minivan', 'gas pump', 'golf ball', 'golf ball',\n",
            "       'golf ball'], dtype=object), 'confidence': array([0.68556875, 0.98688394, 0.99962246, 0.99635172, 0.9997496 ,\n",
            "       0.24173038, 0.99985218, 0.99940097, 0.9967159 , 0.99943525])}\n",
            "{'predicted_label': array(['golf ball', 'parachute', 'parachute', 'parachute', 'parachute',\n",
            "       'great white shark', 'tench', 'tench', 'tench', 'tench'],\n",
            "      dtype=object), 'confidence': array([0.9977715 , 0.95020258, 0.99885952, 0.9999578 , 0.98507404,\n",
            "       0.21947235, 0.99809855, 0.99990737, 0.99997783, 0.99994719])}\n",
            "{'predicted_label': array(['tench', 'tench', 'tench', 'English springer', 'English springer',\n",
            "       'English springer', 'English springer', 'English springer',\n",
            "       'CD player', 'tape player'], dtype=object), 'confidence': array([0.99989176, 0.99993515, 0.99996543, 0.97315061, 0.99709797,\n",
            "       0.44472018, 0.99640816, 0.90255374, 0.39717525, 0.55421299])}\n",
            "{'predicted_label': array(['cassette player', 'cassette player', 'dining table', 'chain saw',\n",
            "       'chain saw', 'chain saw', 'chain saw', 'chain saw', 'church',\n",
            "       'church'], dtype=object), 'confidence': array([0.894449  , 0.96124136, 0.31873336, 0.99784601, 0.96732146,\n",
            "       0.99856806, 0.99999881, 0.99998641, 0.91052276, 0.93421793])}\n",
            "{'predicted_label': array(['vault', 'church', 'French horn', 'French horn', 'French horn',\n",
            "       'French horn', 'French horn', 'garbage truck', 'garbage truck',\n",
            "       'garbage truck'], dtype=object), 'confidence': array([0.45783132, 0.94243401, 0.96054536, 0.99993873, 0.99482238,\n",
            "       0.9602316 , 0.99642086, 0.99528271, 0.99900204, 0.99714065])}\n",
            "{'predicted_label': array(['garbage truck', 'garbage truck', 'gas pump', 'gas pump',\n",
            "       'gas pump', 'gas pump', 'ping-pong ball', 'golf ball', 'golf ball',\n",
            "       'golf ball'], dtype=object), 'confidence': array([0.98294997, 0.99292833, 0.99996173, 0.80979437, 0.99967349,\n",
            "       0.80450761, 0.3227863 , 0.99995291, 0.96787107, 0.99494249])}\n",
            "{'predicted_label': array(['golf ball', 'parachute', 'parachute', 'parachute', 'parachute',\n",
            "       'parachute', 'tench', 'tench', 'tench', 'tench'], dtype=object), 'confidence': array([0.99978775, 0.92129898, 0.95972812, 0.99990535, 0.91960484,\n",
            "       0.99993396, 0.99987888, 0.99999213, 0.99961472, 0.99952495])}\n",
            "{'predicted_label': array(['tench', 'tench', 'tench', 'tench', 'English springer',\n",
            "       'English springer', 'English springer', 'English springer',\n",
            "       'Border collie', 'cassette player'], dtype=object), 'confidence': array([0.97776866, 0.99757582, 0.99924266, 0.99872786, 0.99487394,\n",
            "       0.97970265, 0.96449006, 0.86523747, 0.38882747, 0.41775966])}\n",
            "{'predicted_label': array(['cassette player', 'cassette player', 'cassette player',\n",
            "       'cassette player', 'chain saw', 'chain saw', 'chain saw',\n",
            "       'chain saw', 'church', 'church'], dtype=object), 'confidence': array([0.52573317, 0.97641569, 0.69791383, 0.46414968, 0.99847573,\n",
            "       0.93374217, 0.99986851, 0.99726403, 0.90447682, 0.84084737])}\n",
            "{'predicted_label': array(['church', 'church', 'church', 'electric fan', 'French horn',\n",
            "       'French horn', 'French horn', 'French horn', 'garbage truck',\n",
            "       'garbage truck'], dtype=object), 'confidence': array([0.63581651, 0.93114448, 0.98736572, 0.13959228, 0.99654657,\n",
            "       0.99986696, 0.99916399, 0.99003816, 0.98404551, 0.99802774])}\n",
            "{'predicted_label': array(['garbage truck', 'garbage truck', 'garbage truck', 'gas pump',\n",
            "       'gas pump', 'gas pump', 'gas pump', 'golf ball', 'golf ball',\n",
            "       'golf ball'], dtype=object), 'confidence': array([0.99619496, 0.99966216, 0.90114468, 0.99545395, 0.99998653,\n",
            "       0.99996185, 0.99999702, 0.99976927, 0.99222434, 0.9996953 ])}\n",
            "{'predicted_label': array(['golf ball', 'golf ball', 'parachute', 'parachute', 'parachute',\n",
            "       'parachute', 'parachute', 'tench', 'tench', 'tench'], dtype=object), 'confidence': array([0.99610823, 0.99996698, 0.99788183, 0.99625874, 0.89336103,\n",
            "       0.99991512, 0.98458046, 0.99860966, 0.99966145, 0.97668242])}\n",
            "{'predicted_label': array(['English springer', 'English springer', 'English springer',\n",
            "       'English springer', 'tape player', 'tape player',\n",
            "       'cassette player', 'tape player', 'CD player', 'chain saw'],\n",
            "      dtype=object), 'confidence': array([0.987445  , 0.46787775, 0.99088216, 0.91662771, 0.47679871,\n",
            "       0.44571346, 0.30719122, 0.31269324, 0.78988075, 0.9999696 ])}\n",
            "{'predicted_label': array(['chain saw', 'chain saw', 'chain saw', 'chain saw', 'church',\n",
            "       'church', 'church', 'church', 'French horn', 'French horn'],\n",
            "      dtype=object), 'confidence': array([0.99798977, 1.        , 0.99970251, 0.71939635, 0.64637148,\n",
            "       0.99963999, 0.52268898, 0.92334419, 0.99973065, 0.64638269])}\n",
            "{'predicted_label': array(['French horn', 'French horn', 'French horn', 'garbage truck',\n",
            "       'garbage truck', 'garbage truck', 'garbage truck', 'garbage truck',\n",
            "       'gas pump', 'gas pump'], dtype=object), 'confidence': array([0.99404389, 0.99508709, 0.99665052, 0.93386537, 0.99537945,\n",
            "       0.97532254, 0.89703476, 0.99994111, 0.99725705, 1.        ])}\n",
            "{'predicted_label': array(['gas pump', 'gas pump', 'golf ball', 'golf ball', 'golf ball',\n",
            "       'golf ball', 'golf ball', 'parachute', 'parachute', 'parachute'],\n",
            "      dtype=object), 'confidence': array([0.99991858, 1.        , 0.99997056, 0.99963427, 0.96794176,\n",
            "       0.99999809, 0.99671507, 0.38821375, 0.99913496, 0.99991691])}\n",
            "{'predicted_label': array(['parachute', 'parachute', 'tench', 'tench', 'coho', 'barracouta',\n",
            "       'English springer', 'English springer', 'English setter',\n",
            "       'English springer'], dtype=object), 'confidence': array([0.79809105, 0.99997973, 0.57833511, 0.96365952, 0.55129975,\n",
            "       0.5654493 , 0.992823  , 0.90437686, 0.96938288, 0.97182184])}\n",
            "{'predicted_label': array(['tape player', 'cassette player', 'cassette player',\n",
            "       'cassette player', 'CD player', 'binoculars', 'chain saw',\n",
            "       'chain saw', 'chain saw', 'chain saw'], dtype=object), 'confidence': array([0.40660375, 0.65008849, 0.59499222, 0.4934257 , 0.56491172,\n",
            "       0.1698425 , 0.99990988, 0.99999428, 0.99999881, 0.68745321])}\n",
            "{'predicted_label': array(['church', 'church', 'church', 'church', 'French horn',\n",
            "       'French horn', 'French horn', 'French horn', 'French horn',\n",
            "       'garbage truck'], dtype=object), 'confidence': array([0.59947461, 0.80170768, 0.57486475, 0.8707698 , 0.99874866,\n",
            "       0.99679488, 0.98995543, 0.68090081, 0.99857008, 0.96534193])}\n",
            "{'predicted_label': array(['garbage truck', 'garbage truck', 'garbage truck', 'garbage truck',\n",
            "       'gas pump', 'gas pump', 'gas pump', 'gas pump', 'gas pump',\n",
            "       'golf ball'], dtype=object), 'confidence': array([0.99999177, 0.99834442, 0.99350125, 0.99728262, 0.99963534,\n",
            "       0.99999928, 0.9999975 , 0.98882383, 0.99989367, 0.9781211 ])}\n",
            "{'predicted_label': array(['golf ball', 'golf ball', 'golf ball', 'parachute', 'parachute',\n",
            "       'parachute', 'parachute', 'parachute', 'loggerhead', 'tench'],\n",
            "      dtype=object), 'confidence': array([0.99970394, 0.99997842, 0.99656624, 0.99942362, 0.99900609,\n",
            "       0.99991536, 0.99684083, 0.37003952, 0.63020587, 0.97966605])}\n",
            "{'predicted_label': array(['tench', 'tench', 'English springer', 'English springer',\n",
            "       'English springer', 'English springer', 'English springer',\n",
            "       'home theater', 'cassette player', 'cassette player'], dtype=object), 'confidence': array([0.99965465, 0.99993134, 0.97036451, 0.98635662, 0.96293873,\n",
            "       0.9872579 , 0.76495832, 0.35488495, 0.54215574, 0.78766567])}\n",
            "{'predicted_label': array(['cassette player', 'CD player', 'chain saw', 'chain saw',\n",
            "       'chain saw', 'chain saw', 'chain saw', 'prison', 'church',\n",
            "       'church'], dtype=object), 'confidence': array([0.73077607, 0.76162714, 0.99999964, 0.99973184, 0.99999082,\n",
            "       0.99999964, 0.99993455, 0.80081475, 0.98304164, 0.86848104])}\n",
            "{'predicted_label': array(['church', 'French horn', 'French horn', 'French horn',\n",
            "       'French horn', 'French horn', 'garbage truck', 'garbage truck',\n",
            "       'garbage truck', 'garbage truck'], dtype=object), 'confidence': array([0.93890232, 0.99745005, 0.94779861, 0.99607038, 0.98660696,\n",
            "       0.99720001, 0.78158748, 0.96060729, 0.99998629, 0.98695904])}\n",
            "{'predicted_label': array(['garbage truck', 'gas pump', 'gas pump', 'gas pump', 'gas pump',\n",
            "       'golf ball', 'golf ball', 'golf ball', 'golf ball', 'golf ball'],\n",
            "      dtype=object), 'confidence': array([0.81931376, 0.94592845, 0.99997497, 0.99583697, 0.99986792,\n",
            "       0.99292266, 0.99992406, 0.99932551, 0.9999454 , 0.99997211])}\n",
            "{'predicted_label': array(['parachute', 'military uniform', 'parachute', 'parachute',\n",
            "       'parachute', 'tench', 'tench', 'tench', 'English springer',\n",
            "       'English springer'], dtype=object), 'confidence': array([0.99983728, 0.33619171, 0.96423054, 0.99992621, 0.99984884,\n",
            "       0.99982798, 0.99994624, 0.9987908 , 0.66911191, 0.98337793])}\n",
            "{'predicted_label': array(['English springer', 'English springer', 'tape player',\n",
            "       'cassette player', 'cassette player', 'cassette player', 'screw',\n",
            "       'chain saw', 'chain saw', 'chain saw'], dtype=object), 'confidence': array([0.9678266 , 0.95186865, 0.60992408, 0.39158008, 0.85144395,\n",
            "       0.49033368, 0.16349141, 0.99858475, 0.99989605, 0.97759604])}\n",
            "{'predicted_label': array(['chain saw', 'chain saw', 'vestment', 'fountain', 'church',\n",
            "       'monastery', 'French horn', 'French horn', 'French horn',\n",
            "       'French horn'], dtype=object), 'confidence': array([0.9995814 , 0.99999881, 0.46694386, 0.38507426, 0.53113174,\n",
            "       0.79634273, 0.9945538 , 0.94055796, 0.99373078, 0.99966419])}\n",
            "{'predicted_label': array(['French horn', 'garbage truck', 'garbage truck', 'garbage truck',\n",
            "       'garbage truck', 'garbage truck', 'gas pump', 'gas pump',\n",
            "       'gas pump', 'gas pump'], dtype=object), 'confidence': array([0.99760252, 1.        , 0.99893063, 0.9994486 , 0.99998593,\n",
            "       0.79988152, 0.9999994 , 0.99995828, 0.99750948, 0.99999678])}\n",
            "{'predicted_label': array(['gas pump', 'golf ball', 'golf ball', 'golf ball', 'golf ball',\n",
            "       'maypole', 'parachute', 'parachute', 'parachute', 'schooner'],\n",
            "      dtype=object), 'confidence': array([0.99985993, 0.98116285, 0.99994099, 0.99992192, 0.99997926,\n",
            "       0.4035244 , 0.99304867, 0.99407923, 0.99913222, 0.71986592])}\n",
            "{'predicted_label': array(['tench', 'tench', 'tench', 'tench', 'tench', 'tench', 'tench',\n",
            "       'tench', 'tench', 'tench'], dtype=object), 'confidence': array([0.99927956, 0.99973899, 0.99987137, 0.99994922, 0.99994278,\n",
            "       0.94332731, 0.93425268, 0.9852078 , 0.95207453, 0.99989533])}\n",
            "{'predicted_label': array(['English springer', 'English springer', 'English springer',\n",
            "       'English springer', 'English springer', 'cassette player',\n",
            "       'cassette player', 'cassette', 'cassette player', 'tape player'],\n",
            "      dtype=object), 'confidence': array([0.5886544 , 0.99410903, 0.99210817, 0.43752128, 0.76701397,\n",
            "       0.7135514 , 0.4021838 , 0.86870933, 0.92949265, 0.68597567])}\n",
            "{'predicted_label': array(['chain saw', 'chain saw', 'chain saw', 'chain saw', 'chain saw',\n",
            "       'church', 'church', 'church', 'church', 'French horn'],\n",
            "      dtype=object), 'confidence': array([1.        , 1.        , 0.99999654, 0.99997032, 0.99996209,\n",
            "       0.66856354, 0.91049993, 0.95867175, 0.9149884 , 0.99579573])}\n",
            "{'predicted_label': array(['French horn', 'French horn', 'French horn', 'French horn',\n",
            "       'garbage truck', 'garbage truck', 'garbage truck', 'garbage truck',\n",
            "       'garbage truck', 'gas pump'], dtype=object), 'confidence': array([0.99986017, 0.96105534, 0.95961326, 0.99969411, 0.99945551,\n",
            "       0.87164712, 0.98175907, 0.99998999, 0.99958438, 0.99995458])}\n",
            "{'predicted_label': array(['gas pump', 'gas pump', 'gas pump', 'golf ball', 'croquet ball',\n",
            "       'golf ball', 'golf ball', 'soccer ball', 'parachute', 'parachute'],\n",
            "      dtype=object), 'confidence': array([0.99884641, 0.99859673, 0.99997175, 0.99669349, 0.57041138,\n",
            "       0.99954009, 0.99984419, 0.52621049, 0.99235743, 0.99986327])}\n",
            "{'predicted_label': array(['parachute', 'parachute', 'parachute', 'tench', 'tench', 'tench',\n",
            "       'English springer', 'English springer', 'English springer',\n",
            "       'English springer'], dtype=object), 'confidence': array([0.99636048, 0.99894661, 0.99642169, 0.99998295, 0.99990082,\n",
            "       0.99999988, 0.81475687, 0.99587822, 0.95175695, 0.97030288])}\n",
            "{'predicted_label': array(['German short-haired pointer', 'entertainment center',\n",
            "       'tape player', 'CD player', 'cassette player', 'CD player',\n",
            "       'chain saw', 'chain saw', 'chain saw', 'chain saw'], dtype=object), 'confidence': array([0.66520458, 0.21999487, 0.4873673 , 0.69377404, 0.72660989,\n",
            "       0.32962343, 0.99880385, 1.        , 0.99999845, 0.99998808])}\n",
            "{'predicted_label': array(['chain saw', 'church', 'church', 'church', 'church', 'French horn',\n",
            "       'French horn', 'French horn', 'French horn', 'French horn'],\n",
            "      dtype=object), 'confidence': array([0.38511023, 0.92308939, 0.50637591, 0.85461235, 0.79553252,\n",
            "       0.92784381, 0.99958593, 0.12597138, 0.99811828, 0.99959069])}\n",
            "{'predicted_label': array(['garbage truck', 'garbage truck', 'trailer truck', 'garbage truck',\n",
            "       'garbage truck', 'gas pump', 'gas pump', 'gas pump', 'gas pump',\n",
            "       'golf ball'], dtype=object), 'confidence': array([0.99694628, 0.97972918, 0.97384441, 0.99999917, 0.98831034,\n",
            "       0.86345708, 0.99999988, 0.99006885, 0.99985349, 0.99986112])}\n",
            "{'predicted_label': array(['croquet ball', 'golf ball', 'golf ball', 'golf ball',\n",
            "       'bulletproof vest', 'parachute', 'parachute', 'parachute',\n",
            "       'parachute', 'tench'], dtype=object), 'confidence': array([0.36199445, 0.99984527, 0.90876639, 0.9999069 , 0.34047222,\n",
            "       0.99972814, 0.99963808, 0.98098606, 0.99950814, 0.99991512])}\n",
            "{'predicted_label': array(['tench', 'tench', 'tench', 'Brittany spaniel', 'English springer',\n",
            "       'English springer', 'English springer', 'English springer',\n",
            "       'cassette player', 'tape player'], dtype=object), 'confidence': array([0.9998405 , 0.99815804, 0.99902546, 0.47000027, 0.90671754,\n",
            "       0.74687821, 0.98757893, 0.9973489 , 0.3674022 , 0.60296112])}\n",
            "{'predicted_label': array(['cassette player', 'cassette player', 'cassette player',\n",
            "       'chain saw', 'chain saw', 'chain saw', 'chain saw', 'church',\n",
            "       'church', 'church'], dtype=object), 'confidence': array([0.64071542, 0.80128294, 0.72431415, 1.        , 0.99959439,\n",
            "       0.99997926, 1.        , 0.69907975, 0.99582338, 0.71613967])}\n",
            "{'predicted_label': array(['church', 'church', 'French horn', 'French horn', 'French horn',\n",
            "       'French horn', 'French horn', 'garbage truck', 'garbage truck',\n",
            "       'garbage truck'], dtype=object), 'confidence': array([0.99712974, 0.86030239, 0.99926704, 0.67178041, 0.41444734,\n",
            "       0.99895275, 0.99875951, 0.99820638, 0.99317223, 0.99985337])}\n",
            "{'predicted_label': array(['garbage truck', 'garbage truck', 'gas pump', 'gas pump',\n",
            "       'gas pump', 'gas pump', 'golf ball', 'golf ball', 'golf ball',\n",
            "       'golf ball'], dtype=object), 'confidence': array([0.99493909, 0.9990508 , 0.99963391, 1.        , 0.99999845,\n",
            "       0.99999905, 0.99997902, 0.99817336, 0.99996889, 0.99177402])}\n",
            "{'predicted_label': array(['golf ball', 'parachute', 'parachute', 'parachute', 'parachute',\n",
            "       'parachute', 'tench', 'tench', 'tench', 'coho'], dtype=object), 'confidence': array([0.99996495, 0.91025579, 0.99875355, 0.68167728, 0.99978548,\n",
            "       0.99773383, 0.88672256, 0.99997818, 0.99943417, 0.7403236 ])}\n",
            "{'predicted_label': array(['tench', 'English springer', 'English springer',\n",
            "       'English springer', 'English springer', 'tape player',\n",
            "       'tape player', 'cassette player', 'cassette player',\n",
            "       'cassette player'], dtype=object), 'confidence': array([0.98499358, 0.92425817, 0.97164726, 0.60876328, 0.9741208 ,\n",
            "       0.36802757, 0.51519191, 0.353488  , 0.54670221, 0.85374975])}\n",
            "{'predicted_label': array(['chain saw', 'chain saw', 'chain saw', 'chain saw', 'chain saw',\n",
            "       'church', 'church', 'church', 'church', 'French horn'],\n",
            "      dtype=object), 'confidence': array([0.99997413, 0.90559179, 0.99999487, 0.63266551, 0.97090369,\n",
            "       0.97460824, 0.76853055, 0.95285106, 0.93124682, 0.85567588])}\n",
            "{'predicted_label': array(['French horn', 'French horn', 'French horn', 'French horn',\n",
            "       'garbage truck', 'garbage truck', 'garbage truck', 'garbage truck',\n",
            "       'garbage truck', 'gas pump'], dtype=object), 'confidence': array([0.86796504, 0.99831897, 0.96797413, 0.99899822, 0.98134929,\n",
            "       0.88983238, 0.80394113, 0.99604094, 0.99998987, 0.99997556])}\n",
            "{'predicted_label': array(['gas pump', 'gas pump', 'gas pump', 'golf ball', 'golf ball',\n",
            "       'golf ball', 'golf ball', 'golf ball', 'balloon', 'parachute'],\n",
            "      dtype=object), 'confidence': array([1.        , 0.99991918, 0.99998355, 0.99853778, 0.51474988,\n",
            "       0.99986863, 0.66377848, 0.99703264, 0.50762057, 0.97788948])}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-23 17:06:39,142\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_3_0 execution finished in 210.01 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'predicted_label': array(['parachute', 'parachute', 'parachute', 'tench', 'tench', 'tench',\n",
            "       'tench', 'tench', 'tench', 'English springer'], dtype=object), 'confidence': array([0.99843508, 0.99999845, 0.9997533 , 0.92295462, 0.99542964,\n",
            "       0.99858332, 0.9927972 , 0.9998436 , 0.97728777, 0.80834675])}\n",
            "{'predicted_label': array(['English springer', 'English springer', 'English springer',\n",
            "       'English springer', 'cassette', 'cassette player',\n",
            "       'cassette player', 'cassette player', 'switch', 'chain saw'],\n",
            "      dtype=object), 'confidence': array([0.72270048, 0.89827406, 0.94940645, 0.6293081 , 0.5389266 ,\n",
            "       0.55195016, 0.92570847, 0.85766459, 0.34444249, 0.9998554 ])}\n",
            "{'predicted_label': array(['chain saw', 'chain saw', 'chain saw', 'bell cote', 'church',\n",
            "       'church', 'church', 'church', 'French horn', 'French horn'],\n",
            "      dtype=object), 'confidence': array([0.99728942, 0.95580912, 0.99999905, 0.42472515, 0.96667236,\n",
            "       0.81282824, 0.78124791, 0.98568439, 0.97936821, 0.99788362])}\n",
            "{'predicted_label': array(['French horn', 'French horn', 'cornet', 'garbage truck',\n",
            "       'garbage truck', 'garbage truck', 'garbage truck', 'garbage truck',\n",
            "       'gas pump', 'gas pump'], dtype=object), 'confidence': array([0.81505096, 0.99945515, 0.66348225, 0.99999654, 0.92018569,\n",
            "       0.81608385, 0.23693511, 0.87861258, 0.98409814, 1.        ])}\n",
            "{'predicted_label': array(['gas pump', 'gas pump', 'ping-pong ball', 'golf ball', 'golf ball',\n",
            "       'golf ball', 'golf ball', 'parachute', 'parachute', 'parachute'],\n",
            "      dtype=object), 'confidence': array([0.99999726, 0.99999583, 0.99864429, 0.99991035, 0.9622559 ,\n",
            "       0.9357301 , 0.9996599 , 0.97522825, 0.99667442, 0.86580712])}\n",
            "{'predicted_label': array(['parachute', 'parachute', 'piggy bank', 'tench', 'tench', 'tench',\n",
            "       'English springer', 'English springer', 'English springer',\n",
            "       'tench'], dtype=object), 'confidence': array([0.99999547, 0.9899956 , 0.33963355, 0.99586248, 0.99994326,\n",
            "       0.99972659, 0.98474389, 0.95667011, 0.98228765, 0.98907626])}\n",
            "{'predicted_label': array(['tench', 'tench', 'tench', 'tench', 'tench', 'tench', 'tench',\n",
            "       'English springer', 'English springer', 'English springer'],\n",
            "      dtype=object), 'confidence': array([0.99977785, 0.82049477, 0.99998844, 0.99284899, 0.9998492 ,\n",
            "       0.99878103, 0.99134058, 0.81633329, 0.48870626, 0.85336673])}\n",
            "{'predicted_label': array(['English springer', 'Welsh springer spaniel', 'radio', 'CD player',\n",
            "       'cassette player', 'radio', 'tape player', 'chain saw',\n",
            "       'chain saw', 'chain saw'], dtype=object), 'confidence': array([0.98662913, 0.5286085 , 0.70633161, 0.59422141, 0.63860202,\n",
            "       0.4016256 , 0.52161777, 1.        , 0.62146336, 0.9916476 ])}\n",
            "{'predicted_label': array(['chain saw', 'pedestal', 'church', 'church', 'church', 'church',\n",
            "       'hook', 'cornet', 'French horn', 'French horn'], dtype=object), 'confidence': array([0.9564811 , 0.40111569, 0.82121199, 0.99268085, 0.677746  ,\n",
            "       0.81769812, 0.30259365, 0.50041908, 0.9989869 , 0.99965346])}\n",
            "{'predicted_label': array(['French horn', 'garbage truck', 'garbage truck', 'garbage truck',\n",
            "       'trailer truck', 'garbage truck', 'gas pump', 'gas pump',\n",
            "       'gas pump', 'slot'], dtype=object), 'confidence': array([0.56639129, 0.99921346, 0.99977916, 0.99986851, 0.60649461,\n",
            "       0.57021219, 1.        , 0.99136752, 0.99999917, 0.83543408])}\n",
            "{'predicted_label': array(['golf ball', 'golf ball', 'golf ball', 'golf ball', 'golf ball',\n",
            "       'parachute', 'parachute', 'shopping basket', 'parachute',\n",
            "       'parachute'], dtype=object), 'confidence': array([0.99983168, 0.99997759, 0.99984562, 0.90023416, 0.99983752,\n",
            "       0.99175465, 0.98826396, 0.285851  , 0.99899453, 0.985277  ])}\n"
          ]
        }
      ],
      "source": [
        "import ray\n",
        "from typing import Dict, Any\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision.models import ResNet152_Weights\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "import ray.data\n",
        "\n",
        "# Check if GPU is available\n",
        "HAS_GPU = torch.cuda.is_available()\n",
        "\n",
        "def preprocess_image(row: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
        "    weights = ResNet152_Weights.IMAGENET1K_V1\n",
        "    imagenet_transforms = weights.transforms()\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        imagenet_transforms\n",
        "    ])\n",
        "    transformed_tensor = transform(row[\"image\"])\n",
        "\n",
        "    return {\n",
        "        \"transformed_image\": transformed_tensor.numpy(),\n",
        "    }\n",
        "\n",
        "\n",
        "# InferenceWorker from Notebook 1\n",
        "class InferenceWorker:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.weights = ResNet152_Weights.IMAGENET1K_V1\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = models.resnet152(weights=self.weights).to(self.device)\n",
        "        self.model.eval()\n",
        "    \n",
        "        imagenet_transforms = self.weights.transforms()\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            imagenet_transforms\n",
        "        ])\n",
        "\n",
        "    def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, Any]:\n",
        "        torch_batch = torch.from_numpy(batch[\"transformed_image\"]).to(self.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            prediction = self.model(torch_batch)\n",
        "            predicted_classes = prediction.argmax(dim=1).detach().cpu()\n",
        "            predicted_labels = [\n",
        "                self.weights.meta[\"categories\"][i] for i in predicted_classes\n",
        "            ]\n",
        "        \n",
        "            probabilities = torch.nn.functional.softmax(prediction, dim=1)\n",
        "            confidence_scores = probabilities.max(dim=1).values.detach().cpu().numpy()\n",
        "        \n",
        "        return {\n",
        "            \"predicted_label\": predicted_labels,\n",
        "            \"confidence\": confidence_scores.tolist(),\n",
        "        }\n",
        "\n",
        "# Example: How streaming execution enables large-scale inference\n",
        "images = ray.data.read_images(\n",
        "    \"s3://anonymous@air-example-data-2/imagenette2/train/\",\n",
        "    mode=\"RGB\",\n",
        "    ray_remote_args={\"num_cpus\": 0.1},\n",
        ").limit(1000)\n",
        "\n",
        "# This works even if cluster only has 64GB memory!\n",
        "# Why? Streaming execution processes 128MB blocks at a time\n",
        "results = images.map(preprocess_image).map_batches(\n",
        "    InferenceWorker,  # Loads once per actor\n",
        "    batch_size=32,   # Small batches prevent memory overflow\n",
        "    # num_gpus=1,      # One model per GPU\n",
        "    concurrency=4    # 4 parallel actors\n",
        ")\n",
        "\n",
        "# As you iterate results, Ray Data:\n",
        "# 1. Loads blocks from S3 (streaming)\n",
        "# 2. Preprocesses in parallel (pipeline parallelism)\n",
        "# 3. Runs inference on GPUs (distributed)\n",
        "# 4. Writes results (continuous output)\n",
        "# All while maintaining constant memory footprint!\n",
        "\n",
        "for batch in results.iter_batches(batch_size=10):\n",
        "    # First results available immediately\n",
        "    # Don't need to wait for all 1M images\n",
        "    print(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Ray Data Architecture Deep Dive\n",
        "\n",
        "## Datasets and Blocks\n",
        "\n",
        "### What Are Blocks?\n",
        "\n",
        "A **block** is the fundamental unit of data in Ray Data. Understanding blocks is essential for optimization.\n",
        "\n",
        "**Block characteristics:**\n",
        "- **Size**: Typically 128MB (configurable via `target_max_block_size`). This is the industry standard for partition sizing for the average ratio of CPU to memory to I/O. This value can be adjust via the Ray Data context.\n",
        "- **Format**: Stored as PyArrow tables or pandas DataFrames internally. This allows for easy integrations with the rest of the Python ecosystem and lower serialization costs compared to JVM-based engines.\n",
        "- **Location**: Ray object store (shared memory), this builds off of the existing work in Ray Core for fast in-memory trasnfers and data spilling.\n",
        "- **Processing unit**: One block = one task typically when batch_size=None. Ray Data automatically tries to find the best batch sizing, but can also be configured by the user.\n",
        "\n",
        "### Why Block Size Matters for Batch Inference\n",
        "\n",
        "Block size directly impacts inference performance:\n",
        "\n",
        "**Block Size Too Small (e.g., 1MB):**\n",
        "- Too many tasks created (scheduling overhead)\n",
        "- Poor GPU utilization (small batches)\n",
        "- High network overhead (many small transfers)\n",
        "- Scheduler bottleneck (managing thousands of tasks)\n",
        "\n",
        "**Block Size Too Large (e.g., 10GB):**\n",
        "- Memory pressure (blocks don't fit in object store)\n",
        "- Poor parallelism (few blocks = few parallel tasks)\n",
        "- Spilling to disk (performance degradation)\n",
        "- Uneven load balancing (some workers idle)\n",
        "\n",
        "**Optimal Block Size (128MB default):**\n",
        "- Good parallelism (many blocks for distribution)\n",
        "- Low overhead (reasonable number of tasks)\n",
        "- Fits in memory (object store can hold multiple blocks)\n",
        "- Efficient transfer (network overhead manageable)\n",
        "\n",
        "### Configuring Block Size for Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-23 17:06:39,281\tINFO logging.py:293 -- Registered dataset logger for dataset dataset_6_0\n",
            "2025-10-23 17:06:39,286\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_6_0. Full logs are in /tmp/ray/session_2025-10-23_16-34-47_857542_2333/logs/ray-data\n",
            "2025-10-23 17:06:39,286\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_6_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> LimitOperator[limit=1000] -> TaskPoolMapOperator[ReadFiles]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Default max block size: 128MB\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f436b8e9591440cb13db89d76b6083a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Running 0: 0.00 row [00:00, ? row/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30bda2c6b90a4d6897b2676b5d3e2c85",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "- ListFiles 1: 0.00 row [00:00, ? row/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25f651ee5f484a198f803ac49136cb05",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "- limit=1000 2: 0.00 row [00:00, ? row/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95caacb8d70c47b2b722053e31b9b92a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "- ReadFiles 3: 0.00 row [00:00, ? row/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-23 17:06:43,642\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_6_0 execution finished in 4.35 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset blocks: 22\n",
            "Estimated blocks: 3\n",
            "Dataset stats: Operator 0 ReadFiles: \n",
            "\n",
            "Operator 1 ListFiles: 1 tasks executed, 200 blocks produced in 1.47s\n",
            "* Remote wall time: 96.02us min, 1.17s max, 5.96ms mean, 1.19s total\n",
            "* Remote cpu time: 95.41us min, 440.35ms max, 2.33ms mean, 465.25ms total\n",
            "* UDF time: 0us min, 0us max, 0.0us mean, 0us total\n",
            "* Peak heap memory usage (MiB): 179.93 min, 180.18 max, 180 mean\n",
            "* Output num rows per block: 43 min, 54 max, 47 mean, 9469 total\n",
            "* Output size bytes per block: 3929 min, 4923 max, 4303 mean, 860757 total\n",
            "* Output rows per task: 9469 min, 9469 max, 9469 mean, 1 tasks used\n",
            "* Tasks per node: 1 min, 1 max, 1 mean; 1 nodes used\n",
            "* Operator throughput:\n",
            "\t* Total input num rows: 0 rows\n",
            "\t* Total output num rows: 9469 rows\n",
            "\t* Ray Data throughput: 6431.857209243873 rows/s\n",
            "\t* Estimated single node throughput: 7944.014838581921 rows/s\n",
            "\n",
            "Operator 2 limit=1000: 1 tasks executed, 22 blocks produced in 1.2s\n",
            "* Remote wall time: 96.02us min, 1.17s max, 53.34ms mean, 1.17s total\n",
            "* Remote cpu time: 95.41us min, 440.35ms max, 20.14ms mean, 443.08ms total\n",
            "* UDF time: 0us min, 0us max, 0.0us mean, 0us total\n",
            "* Peak heap memory usage (MiB): 179.93 min, 180.18 max, 180 mean\n",
            "* Output num rows per block: 7 min, 53 max, 45 mean, 1000 total\n",
            "* Output size bytes per block: 644 min, 4824 max, 4150 mean, 91318 total\n",
            "* Output rows per task: 1000 min, 1000 max, 1000 mean, 1 tasks used\n",
            "* Tasks per node: 1 min, 1 max, 1 mean; 1 nodes used\n",
            "* Operator throughput:\n",
            "\t* Total input num rows: 9469 rows\n",
            "\t* Total output num rows: 1000 rows\n",
            "\t* Ray Data throughput: 830.7007892290928 rows/s\n",
            "\t* Estimated single node throughput: 852.1009403344207 rows/s\n",
            "\n",
            "Operator 3 ReadFiles: [execution cached]\n",
            "\n",
            "Dataset throughput:\n",
            "\t* Ray Data throughput: 210.7012149644982 rows/s\n",
            "\t* Estimated single node throughput: 69.64062078679629 rows/s\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import ray\n",
        "\n",
        "# Configure block size for your inference workload\n",
        "ctx = ray.data.DataContext.get_current()\n",
        "\n",
        "# Default: 128MB blocks\n",
        "print(f\"Default max block size: {ctx.target_max_block_size / 1024**2:.0f}MB\")\n",
        "\n",
        "# For image inference with large images:\n",
        "# Smaller blocks = more parallelism\n",
        "ctx.target_max_block_size = 64 * 1024**2  # 64MB blocks\n",
        "\n",
        "# For text inference with small documents:\n",
        "# Larger blocks = less overhead\n",
        "ctx.target_max_block_size = 256 * 1024**2  # 256MB blocks\n",
        "\n",
        "# Load images with configured block size\n",
        "images = ray.data.read_images(\n",
        "    \"s3://anonymous@air-example-data-2/imagenette2/train/\",\n",
        "    mode=\"RGB\",\n",
        "    ray_remote_args={\"num_cpus\": 1},\n",
        ").limit(1000).materialize()\n",
        "\n",
        "print(f\"Dataset blocks: {images.num_blocks()}\")\n",
        "print(f\"Estimated blocks: {images.size_bytes() / ctx.target_max_block_size:.0f}\")\n",
        "print(f\"Dataset stats: {images.stats()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How Blocks Flow Through Inference Pipeline\n",
        "\n",
        "Ray Data's block-based architecture enables parallelism at every stage:\n",
        "\n",
        "<img src=\"https://docs.ray.io/en/latest/_images/dataset-arch.svg\" width=\"700\" alt=\"Ray Data Block Architecture\">\n",
        "\n",
        "**Key insights:**\n",
        "- Each block contains a disjoint subset of rows\n",
        "- Blocks are processed in parallel across the cluster\n",
        "- Distributed object store enables efficient block transfer\n",
        "- Tasks operate on individual blocks for maximum parallelism\n",
        "\n",
        "---\n",
        "\n",
        "## Ray Memory Model\n",
        "\n",
        "### Object Store and Heap Memory\n",
        "\n",
        "Ray manages two types of memory that affect batch inference:\n",
        "\n",
        "<img src=\"https://docs.ray.io/en/latest/_images/memory.svg\" width=\"600\" alt=\"Ray Memory Model\">\n",
        "\n",
        "**1. Object Store Memory (30% of node memory by default, set to a higher ratio for Ray Data):**\n",
        "- **Purpose**: Shared memory for passing data between tasks\n",
        "- **Contents**: Blocks (PyArrow tables), task outputs, intermediate results\n",
        "- **Optimization impact**: Determines how many blocks can be in-flight\n",
        "- **When full**: Triggers spilling to disk (major performance hit)\n",
        "\n",
        "**2. Heap Memory (70% of node memory by default):**\n",
        "- **Purpose**: Task execution, model loading, preprocessing\n",
        "- **Contents**: Loaded models, batch data being processed, Python objects\n",
        "- **Optimization impact**: Determines how many models fit in memory\n",
        "- **When full**: Python out-of-memory errors, task failures\n",
        "\n",
        "<div style=\"border: 2px solid #ffeb3b; background: #fffde7; border-radius: 6px; padding: 18px 18px 12px 18px; margin-bottom: 18px;\">\n",
        "  <strong>💡 <span style=\"color:#555;\">Ray Data Optimization Tip</span>:</strong>\n",
        "  <ul style=\"margin:8px 0 8px 20px;\">\n",
        "    <li>\n",
        "      <b>Set the <code>object store memory</code> for Ray Data workloads to <span style=\"color:#002984;\">at least <u>50%</u> of your node memory</span></b>.\n",
        "    </li>\n",
        "    <li>\n",
        "      Ray's <code>object_store_memory</code> default (30%) is often <u>tuned for Ray Core, Train, or compute-heavy workloads</u> with large heap requirements.\n",
        "    </li>\n",
        "    <li>\n",
        "      <b>Ray Data</b> workloads depend on storing <b>blocks of data in the object store</b> for maximum parallel throughput and efficiency.\n",
        "    </li>\n",
        "    <li>\n",
        "      <b>Increase the object store allocation</b> for Ray Data so that your pipeline can hold more data in-memory without frequent spilling to disk or throughput collapse.\n",
        "    </li>\n",
        "  </ul>\n",
        "  <span style=\"color:#444;\">See <code>object_store_memory</code> and <code>DataContext</code> configuration for details.</span>\n",
        "</div>\n",
        "\n",
        "\n",
        "### How This Affects Batch Inference Optimization\n",
        "\n",
        "```text\n",
        "Node Memory: 64GB\n",
        "├── Object Store (30% = 19GB)\n",
        "│   ├── Block 1 (128MB)\n",
        "│   ├── Block 2 (128MB)\n",
        "│   ├── ...\n",
        "│   └── Block N (up to ~148 blocks fit)\n",
        "│\n",
        "└── Heap Memory (70% = 45GB)\n",
        "    ├── Model weights (5GB per model)\n",
        "    ├── Batch preprocessing (2GB per actor)\n",
        "    ├── Python overhead (1GB)\n",
        "    └── Available for actors: ~37GB\n",
        "        → Can fit ~7 model actors at 5GB each\n",
        "```\n",
        "\n",
        "**Optimization implications:**\n",
        "\n",
        "**Object Store Pressure:**\n",
        "- **Symptom**: \"Object store full\" warnings in logs\n",
        "- **Cause**: Too many blocks generated too fast\n",
        "- **Solution**: Reduce `concurrency` or modify `num_cpus` on the reader tasks\n",
        "\n",
        "**Heap Memory Pressure:**\n",
        "- **Symptom**: Out-of-memory or GRAM errors, task failures\n",
        "- **Cause**: Too many models loaded or batch_size too large\n",
        "- **Solution**: Reduce `concurrency` or `batch_size`\n",
        "\n",
        "### Practical Memory Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "import ray\n",
        "\n",
        "# Set up Ray with a custom object store size.\n",
        "# This reserves 4 GB (4 * 1024**3 bytes) exclusively for Ray's plasma object store.\n",
        "ray.init(object_store_memory=4 * 1024**3)\n",
        "\n",
        "# Set Ray Data to be allowed to use up to 75% of the object store for its block cache.\n",
        "# There are two ways to do this: via env var or programmatically.\n",
        "import ray.data\n",
        "\n",
        "# Option 1: Environment variable (do before process start, not here)\n",
        "# os.environ[\"RAY_DATA_OBJECT_STORE_MEMORY_LIMIT_FRACTION\"] = \"0.75\"\n",
        "\n",
        "# Option 2: Programmatically set the fraction (recommended in Ray 2.x+)\n",
        "ctx = ray.data.DataContext.get_current()\n",
        "ctx.object_store_memory_limit_fraction = 0.75\n",
        "\n",
        "# What does this mean?\n",
        "# - Total node memory:          (e.g., 64GB)\n",
        "# - Ray object store:           4GB  (from ray.init above)\n",
        "# - Ray Data can use up to 75%: 3GB  (0.75 * 4GB) for its block cache.\n",
        "# - Other Ray workloads (core, RLlib, actors, etc.) share the remaining object store (up to 1GB).\n",
        "\n",
        "# This ensures Ray Data can't overwhelm the object store and will apply backpressure if it fills its quota,\n",
        "# helping to prevent \"object store full\" errors and OOMs.\n",
        "\n",
        "# Example Ray Data pipeline using the above memory configuration:\n",
        "images = ray.data.read_images(\"s3://images/\", num_cpus=0.05)\n",
        "\n",
        "results = images.map_batches(\n",
        "    LargeModelInference,   # Example: Large model, may require lots of heap memory\n",
        "    batch_size=16,         # Small batches if GPU is a bottleneck\n",
        "    num_gpus=1,\n",
        "    concurrency=2          # Only 2 concurrent actors to avoid heap pressure\n",
        ")\n",
        "\n",
        "# Ray Data will:\n",
        "# - Backpressure file reads and ingestion when its allocated share (3GB here) fills up\n",
        "# - Limit concurrent tasks to avoid heap and object store OOM\n",
        "# - Enable more predictable, tunable memory usage for batch inference\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zero-Copy Optimization\n",
        "\n",
        "Ray Data uses **zero-copy deserialization** for efficiency:\n",
        "\n",
        "**What it means:**\n",
        "- Blocks stored in object store are PyArrow tables\n",
        "- Accessing a block doesn't copy data - just creates a pointer\n",
        "- Multiple tasks can read same block without duplication\n",
        "\n",
        "**Why it matters for inference:**\n",
        "- **Memory efficiency**: 10 actors can share same preprocessed block\n",
        "- **Performance**: No serialization overhead between stages\n",
        "- **Scalability**: Enables high-throughput pipelines\n",
        "\n",
        "**Practical impact:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Without zero-copy (hypothetical):\n",
        "# Block in object store: 128MB\n",
        "# Actor 1 reads block: +128MB copy → 256MB total\n",
        "# Actor 2 reads block: +128MB copy → 384MB total\n",
        "# Result: 3x memory usage!\n",
        "\n",
        "# With zero-copy (Ray Data actual):\n",
        "# Block in object store: 128MB\n",
        "# Actor 1 reads block: 0MB copy (pointer) → 128MB total\n",
        "# Actor 2 reads block: 0MB copy (pointer) → 128MB total\n",
        "# Result: Constant memory!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Visual representation of object store usage:**\n",
        "\n",
        "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-data-deep-dive/producer-consumer-object-store-v2.png\" width=\"700\" alt=\"Object Store Data Flow\">\n",
        "\n",
        "---\n",
        "\n",
        "## Operators and Planning\n",
        "\n",
        "### Logical vs Physical Plans\n",
        "\n",
        "Ray Data transforms your code into an optimized execution plan:\n",
        "\n",
        "**Your Code:**\n",
        "```python\n",
        "results = (\n",
        "    ray.data.read_images(\"s3://images/\")\n",
        "    .map_batches(preprocess_images)\n",
        "    .map_batches(InferenceModel, num_gpus=1)\n",
        ")\n",
        "```\n",
        "\n",
        "### Planning Steps\n",
        "\n",
        "Below are the steps that Ray Data takes to plan the execution of a dataset.\n",
        "\n",
        "1. Ray Data optimizes the logical plan performing a series of optimizations to produce an optimized logical plan. \n",
        "\n",
        "2. The plan then gets translated into a physical plan using a physical planner\n",
        "\n",
        "3. The physical plan is then further optimized (e.g. fusing operators) to produce an optimized physical plan.\n",
        "\n",
        "See the below diagram for the planning process:\n",
        "\n",
        "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-data-deep-dive/get_execution_plan.png\" width=\"600\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Logical Plan (What to do):**\n",
        "```\n",
        "ReadFiles → MapBatches[preprocess] → MapBatches[inference]\n",
        "```\n",
        "\n",
        "**Physical Plan (How to do it):**\n",
        "```\n",
        "TaskPoolMapOperator[ReadFiles→preprocess→inference]\n",
        "```\n",
        "\n",
        "**Note the fusion:** Ray Data combined all three operations into a single operator!\n",
        "\n",
        "### Operators\n",
        "\n",
        "The DAG shows that \"logically\" we require operations like `ReadFiles` and `MapBatches` to be run.\n",
        "\n",
        "Physical plans indicate \"how\" the logical operators will be executed.\n",
        "\n",
        "The DAG shows a plan of physical operators that will be executed:\n",
        "- `InputDataBuffer` is a placeholder for the input data.\n",
        "- `TaskPoolMapOperator` is a physical operator which will execute logical operations like `ReadFiles` and `MapBatches` using a \"pool\" of Ray Tasks. \n",
        "- The syntax is `{PhysicalOperator}[{LogicalOperator}]`. \n",
        "\n",
        "#### Operator Fusion\n",
        "Under certain conditions, Ray Data will fuse operators together to reduce data movement and improve execution efficiency.\n",
        "\n",
        "Here is the syntax for fusing operators:\n",
        "- `{PhysicalOperator}[{LogicalOperator1}->{LogicalOperator2}]`\n",
        "\n",
        "### Operator Fusion and Its Impact on Inference\n",
        "\n",
        "**Operator fusion** combines multiple operations into single tasks to reduce overhead.\n",
        "\n",
        "**Benefits for batch inference:**\n",
        "- **Reduced data movement**: Preprocessed images go straight to model (no object store roundtrip)\n",
        "- **Lower task overhead**: One task instead of three per block\n",
        "- **Better GPU utilization**: Continuous processing without gaps\n",
        "- **Memory efficiency**: Intermediate results stay in task memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**When fusion happens:**\n",
        "- Same compute configuration (both use tasks or both use actors)\n",
        "- Compatible batch sizes\n",
        "- Compatible resource requirements\n",
        "- No shuffle/repartition operations between them\n",
        "\n",
        "**When fusion doesn't happen:**\n",
        "- Different compute strategies (task vs actor)\n",
        "- Different resource requirements (CPU vs GPU)\n",
        "- Shuffle operations (groupby, sort, repartition)\n",
        "\n",
        "**Optimization strategy:**\n",
        "Keep preprocessing and inference configs compatible to enable fusion:\n",
        "```python\n",
        "# Good: Fusion enabled\n",
        "images.map_batches(preprocess, batch_size=32).map_batches(InferenceModel, batch_size=32\n",
        "\n",
        "# Suboptimal: Fusion disabled (different batch sizes)\n",
        "images.map_batches(preprocess, batch_size=64).map_batches(InferenceModel, batch_size=32)\n",
        "```\n",
        "\n",
        "## Streaming Topology\n",
        "\n",
        "After constructing the optimized DAG of physical operators, the execution plan is handed to the `StreamingExecutor` to execute.\n",
        "\n",
        "The first step is to build a streaming topology, here is a sample diagram of the streaming topology:\n",
        "\n",
        "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-data-deep-dive/build_streaming_topology.png\" width=\"1000\">\n",
        "\n",
        "Each physical operator will be wired to the next physical operator downstream in the execution plan. \n",
        "\n",
        "An upstream operator's external output queue will *refer to the same queue* as the input of the downstream operator.\n",
        "\n",
        "## Data flow within an operator\n",
        "\n",
        "Below is a diagram showing the data flow within an operator.\n",
        "\n",
        "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-data-deep-dive/data_flow_simplified_v4.png\" width=\"800\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----\n",
        "\n",
        "# Optimizing Batch Inference\n",
        "\n",
        "## Resource Management and Backpressure\n",
        "\n",
        "### Dynamic Resource Allocation\n",
        "\n",
        "Ray Data automatically manages resources across operators to maximize throughput. Understanding this helps you set optimal parameters.\n",
        "\n",
        "**The Challenge:**\n",
        "- **Too aggressive loading**: Object store fills up, spilling to disk\n",
        "- **Too conservative loading**: GPUs idle waiting for data\n",
        "- **Unbalanced pipeline**: Some stages bottleneck while others wait\n",
        "\n",
        "**Ray Data's Solution:**\n",
        "Dynamically allocates resources based on operator throughput and backpressure policies.\n",
        "\n",
        "### Backpressure Mechanisms\n",
        "\n",
        "**1. Submission-Based Backpressure:**\n",
        "Prevents operators from submitting new tasks when resource budgets exceeded.\n",
        "\n",
        "**Example scenario:**\n",
        "```\n",
        "GPU inference slower than data loading\n",
        "↓\n",
        "Object store filling with preprocessed images\n",
        "↓\n",
        "Ray Data backpressures data loading\n",
        "↓\n",
        "Loading slows down to match inference throughput\n",
        "↓\n",
        "Balanced pipeline - no memory overflow\n",
        "```\n",
        "\n",
        "**2. Output-Based Backpressure:**\n",
        "Limits how many task outputs move to operator queues based on memory availability.\n",
        "\n",
        "**Practical impact on inference:**\n",
        "\n",
        "```python\n",
        "# Scenario: Fast preprocessing, slow inference\n",
        "\n",
        "images = ray.data.read_images(\"s3://images/\", num_cpus=0.05)\n",
        "\n",
        "results = images.map_batches(\n",
        "    fast_preprocess,     # Processes 1000 images/sec\n",
        "    batch_size=64,\n",
        "    concurrency=16        # Many parallel preprocessors\n",
        ").map_batches(\n",
        "    SlowInferenceModel,  # Processes 100 images/sec\n",
        "    batch_size=16,\n",
        "    num_gpus=1,\n",
        "    concurrency=2         # Only 2 GPUs available\n",
        ")\n",
        "```\n",
        "\n",
        "What Ray Data does automatically:\n",
        "1. Preprocessing generates blocks faster than inference consumes\n",
        "2. Object store starts filling with preprocessed blocks\n",
        "3. Backpressure kicks in - preprocessing tasks not scheduled\n",
        "4. Pipeline balances - preprocessing matches inference rate\n",
        "5. Memory stays constant - no overflow despite throughput mismatch\n",
        "\n",
        "\n",
        "## Resource management and allocation\n",
        "\n",
        "Ray data will:\n",
        "- dynamically allocate resources across operators\n",
        "- backpressure operators that have exceeded their resource budgets\n",
        "\n",
        "### Operator backpressure in Ray Data\n",
        "\n",
        "Backpressure in Ray Data is essential for managing resource contention and ensuring fair resource utilization across operators. Its primary goal is to maximize execution plan throughput by controlling task flow.\n",
        "\n",
        "### How Backpressure Works\n",
        "\n",
        "Backpressure can be applied to each physical operator in an execution plan through two approaches:\n",
        "\n",
        "1. **Submission-Based Backpressure**: Prevents an operator from submitting new tasks if it exceeds its resource budget.\n",
        "2. **Output-Based Backpressure**: Prevents an operator from moving outputs to its out-queue if it produces more data than its resource budget allows.\n",
        "\n",
        "### Where is backpressure applied?\n",
        "\n",
        "Below is a diagram of the scheduling loop highlighting where backpressure is applied.\n",
        "\n",
        "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-data-deep-dive/scheduling_loop_with_backpressure.png\" width=\"1000\">\n",
        "\n",
        "<details>\n",
        "<summary>Here is how backpressure is implemented:</summary>\n",
        "\n",
        "##### Submission-based backpressure\n",
        "Submission-based backpressure is implemented in the `ResourceManager.can_submit_new_task()` method. It is used in the `select_operator_to_run()` method to determine if an operator can submit new tasks.\n",
        "\n",
        "##### Output-based backpressure\n",
        "\n",
        "Output-based backpressure is implemented in the `ResourceManager.max_task_output_bytes_to_read()` method. It is used in the `process_completed_tasks()` method to determine the maximum bytes of task outputs that can be read and moved to an external queue.\n",
        "\n",
        "<details>\n",
        "<summary>Here are the technical details where output-based backpressure is applied:</summary>\n",
        "\n",
        "The `scheduling_loop_step()` will invoke:\n",
        "\n",
        "- `process_completed_tasks()` to:\n",
        "  - Wait and gather completed task outputs from all operators\n",
        "  - Move completed task outputs to the operator's in-queue\n",
        "\n",
        "`process_completed_tasks` will make use of `max_task_output_bytes_to_read()` to determine the maximum bytes of task outputs that can be read and moved to an external queue.\n",
        "\n",
        "Here is the code for `process_completed_tasks()`:\n",
        "\n",
        "```python\n",
        "from ray.data._internal.execution.streaming_executor_state import process_completed_tasks\n",
        "\n",
        "%psource process_completed_tasks\n",
        "```\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Monitoring Resource Usage\n",
        "```python\n",
        "# Enable Ray Data resource manager debug logging\n",
        "import os\n",
        "os.environ['RAY_DATA_DEBUG_RESOURCE_MANAGER'] = '1'\n",
        "\n",
        "# Run your inference pipeline\n",
        "results = images.map_batches(InferenceModel, num_gpus=1, concurrency=4)\n",
        "```\n",
        "You'll see output like:\n",
        "```text\n",
        "[ResourceManager] Operator budgets:\n",
        "  ReadImages: object_store_memory=5.0GB, cpu=8.0\n",
        "  MapBatches: object_store_memory=5.0GB, cpu=0.0, gpu=4.0\n",
        "  MapBatches: object_store_memory=10.0GB, cpu=0.0, gpu=0.0\n",
        "```\n",
        "This shows:\n",
        "- How resources are allocated across operators\n",
        "- Where bottlenecks might occur\n",
        "- If backpressure is active"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Optimization Implications\n",
        "\n",
        "### How Architecture Informs Optimization Decisions\n",
        "\n",
        "Understanding Ray Data's architecture helps you make better optimization choices:\n",
        "\n",
        "#### 1. Choosing Batch Size\n",
        "\n",
        "**Architectural considerations:**\n",
        "- **Block size**: Batch size should divide evenly into block size for efficiency\n",
        "- **GPU memory**: Batch must fit in GPU memory during inference\n",
        "- **Object store**: Preprocessed batches must fit in object store\n",
        "- **Throughput**: Larger batches = better GPU utilization (up to a point)\n",
        "\n",
        "<div class=\"alert alert-info\">\n",
        "    <b>Note:</b> Individual configurations will vary between applications, these examples configs are for showcasing example logic that may be applied\n",
        "</div>\n",
        "\n",
        "**Example Decision framework:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal batch size: 10\n"
          ]
        }
      ],
      "source": [
        "# Calculate optimal batch size\n",
        "\n",
        "# Factor 1: GPU memory constraint\n",
        "gpu_memory_gb = 16  # Your GPU memory\n",
        "model_size_gb = 5\n",
        "batch_overhead_mb = 50  # Per sample\n",
        "max_batch_from_gpu = int((gpu_memory_gb - model_size_gb) * 1024 / batch_overhead_mb)\n",
        "\n",
        "# Factor 2: Block size alignment\n",
        "block_size_mb = 128\n",
        "samples_per_block = block_size_mb / 3  # 3MB per image\n",
        "ideal_batch_for_blocks = int(samples_per_block / 4)  # 4 batches per block\n",
        "\n",
        "# Factor 3: Throughput testing\n",
        "# Test different sizes: 16, 32, 64, 128\n",
        "# Choose largest that doesn't cause memory issues\n",
        "\n",
        "# Final choice: Minimum of all constraints\n",
        "optimal_batch_size = min(max_batch_from_gpu, ideal_batch_for_blocks, 128)\n",
        "print(f\"Optimal batch size: {optimal_batch_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Choosing Concurrency\n",
        "\n",
        "**Architectural considerations:**\n",
        "- **GPU count**: One actor per GPU maximum\n",
        "- **Memory per actor**: Model size + batch size determines how many actors fit\n",
        "- **Object store capacity**: More actors = more in-flight blocks\n",
        "- **CPU availability**: Preprocessing may need CPUs\n",
        "\n",
        "**Example Decision framework:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal concurrency: 6\n",
            "  GPU limit: 8\n",
            "  Memory limit: 6\n",
            "  Object store limit: 75\n"
          ]
        }
      ],
      "source": [
        "# Calculate optimal concurrency\n",
        "\n",
        "# Factor 1: GPU constraint\n",
        "num_gpus = 8  # Available GPUs\n",
        "max_concurrency_gpu = num_gpus  # One model per GPU\n",
        "\n",
        "# Factor 2: Memory constraint\n",
        "node_heap_memory_gb = 64 * 0.7  # 70% of 64GB node\n",
        "model_size_gb = 5\n",
        "batch_memory_gb = 2\n",
        "memory_per_actor = model_size_gb + batch_memory_gb\n",
        "max_concurrency_memory = int(node_heap_memory_gb / memory_per_actor)\n",
        "\n",
        "# Factor 3: Object store constraint\n",
        "object_store_gb = 64 * 0.3  # 30% of 64GB node\n",
        "block_size_gb = 0.128\n",
        "blocks_in_flight_per_actor = 2  # Preprocessing + inference\n",
        "required_object_store = max_concurrency_gpu * blocks_in_flight_per_actor * block_size_gb\n",
        "max_concurrency_object_store = int(object_store_gb / (blocks_in_flight_per_actor * block_size_gb))\n",
        "\n",
        "# Final choice: Minimum of all constraints\n",
        "optimal_concurrency = min(\n",
        "    max_concurrency_gpu,\n",
        "    max_concurrency_memory,\n",
        "    max_concurrency_object_store\n",
        ")\n",
        "\n",
        "print(f\"Optimal concurrency: {optimal_concurrency}\")\n",
        "print(f\"  GPU limit: {max_concurrency_gpu}\")\n",
        "print(f\"  Memory limit: {max_concurrency_memory}\")\n",
        "print(f\"  Object store limit: {max_concurrency_object_store}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Actor vs Task Decision\n",
        "\n",
        "**Architectural insight:** Actors are stateful, tasks are stateless.\n",
        "\n",
        "| Aspect | Tasks | Actors | Best For Inference |\n",
        "|--------|-------|--------|-------------------|\n",
        "| **Startup** | Launch per invocation | Launch once, reuse | - Actors (amortize model loading) |\n",
        "| **State** | Stateless | Stateful | - Actors (keep model in memory) |\n",
        "| **Resource overhead** | Low | Higher | Depends on model size |\n",
        "| **Scheduling overhead** | Higher (many tasks) | Lower (few actors) | - Actors (fewer scheduling decisions) |\n",
        "| **Memory** | Released after task | Held by actor | Tasks if memory-constrained |\n",
        "\n",
        "**For batch inference:** Almost always use actors because:\n",
        "- Model loading is expensive (2-5 seconds)\n",
        "- Models are large (500MB - 10GB)\n",
        "- Reducing load cost across 1000s of batches is critical\n",
        "\n",
        "#### 4. Understanding Performance Bottlenecks\n",
        "\n",
        "**Use Ray Dashboard to identify architectural bottlenecks:**\n",
        "\n",
        "**Symptom 1: Low GPU utilization**\n",
        "- **Possible cause**: Object store full (loading backpressured)\n",
        "- **Solution**: Increase object store limit or reduce block size\n",
        "- **How to verify**: Check \"Ray Data Metrics (Object Store Memory)\"\n",
        "\n",
        "**Symptom 2: Spilling to disk**\n",
        "- **Possible cause**: Too many concurrent actors generating blocks\n",
        "- **Solution**: Reduce concurrency or increase batch_size\n",
        "- **How to verify**: Check \"Spilled\" metric in object store\n",
        "\n",
        "**Symptom 3: High task overhead**\n",
        "- **Possible cause**: Blocks too small, too many tasks\n",
        "- **Solution**: Increase target_max_block_size\n",
        "- **How to verify**: Check task count vs throughput\n",
        "\n",
        "**Symptom 4: Actors idle**\n",
        "- **Possible cause**: Upstream loading too slow\n",
        "- **Solution**: Increase num_cpus for read operation\n",
        "- **How to verify**: Check \"Ray Data Metrics (Inputs)\" throughput\n",
        "\n",
        "---\n",
        "\n",
        "## Key Takeaways: Architecture and Optimization\n",
        "\n",
        "### Critical Architecture Concepts\n",
        "\n",
        "**1. Streaming Execution:**\n",
        "- Enables processing datasets larger than cluster memory\n",
        "- Provides pipeline parallelism for maximum throughput\n",
        "- Makes batch inference scalable from 1K to 1B samples\n",
        "\n",
        "**2. Blocks:**\n",
        "- 128MB default size balances parallelism and overhead\n",
        "- More blocks = more parallelism (up to a point)\n",
        "- Block size affects GPU batch size and task count\n",
        "\n",
        "**3. Memory Model:**\n",
        "- Object store (30%) holds blocks and transfers\n",
        "- Heap memory (70%) runs tasks and loads models\n",
        "- Both limits constrain concurrency and batch size\n",
        "\n",
        "**4. Operator Fusion:**\n",
        "- Combines operations to reduce overhead\n",
        "- Keeps intermediate data in task memory\n",
        "- Improves throughput and reduces latency\n",
        "\n",
        "**5. Backpressure:**\n",
        "- Automatically balances pipeline stages\n",
        "- Prevents memory overflow\n",
        "- Maximizes throughput within resource constraints\n",
        "\n",
        "### Optimization Decision Framework\n",
        "\n",
        "Use this sample framework informed by Ray Data architecture (just an example, actual configuration will change for different types of applications):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimized configuration:\n",
            "  Concurrency: 6 (limited by 6)\n",
            "  Batch size: 1877\n",
            "  Block size: 22524MB\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Start with architectural constraints\n",
        "gpu_count = 8\n",
        "gpu_memory_gb = 16\n",
        "node_memory_gb = 64\n",
        "model_size_gb = 5\n",
        "\n",
        "# Step 2: Calculate concurrency from memory\n",
        "heap_memory = node_memory_gb * 0.7\n",
        "actors_fit = int(heap_memory / (model_size_gb + 2))  # +2GB for batches\n",
        "concurrency = min(gpu_count, actors_fit)\n",
        "\n",
        "# Step 3: Calculate batch size from GPU memory\n",
        "available_gpu_mem = gpu_memory_gb - model_size_gb\n",
        "sample_size_mb = 3  # Per image\n",
        "batch_size = int(available_gpu_mem * 1024 / sample_size_mb / 2)  # /2 for safety\n",
        "\n",
        "# Step 4: Configure block size for efficiency\n",
        "ctx = ray.data.DataContext.get_current()\n",
        "\n",
        "# Make blocks contain ~4 batches worth of data\n",
        "ctx.target_max_block_size = batch_size * sample_size_mb * 1024**2 * 4\n",
        "\n",
        "# Step 5: Run with optimal settings\n",
        "# results = images.map_batches(\n",
        "#     InferenceModel,\n",
        "#     batch_size=batch_size,\n",
        "#     num_gpus=1,\n",
        "#     concurrency=concurrency\n",
        "# )\n",
        "\n",
        "print(f\"Optimized configuration:\")\n",
        "print(f\"  Concurrency: {concurrency} (limited by {min(gpu_count, actors_fit)})\")\n",
        "print(f\"  Batch size: {batch_size}\")\n",
        "print(f\"  Block size: {ctx.target_max_block_size / 1024**2:.0f}MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Architecture-Aware Performance Tips\n",
        "\n",
        "**Tip 1: Align batch_size with block_size**\n",
        "\n",
        "Good configuration:\n",
        "- Block size: 128MB, batch_size: 32 (4 batches per block - clean division)\n",
        "\n",
        "Suboptimal configuration:\n",
        "- Block size: 128MB, batch_size: 50 (2.56 batches per block - awkward division)\n",
        "\n",
        "**Tip 2: Monitor object store, not just GPUs**\n",
        "\n",
        "- GPU utilization high but throughput low?\n",
        "- Check object store - might be spilling to disk\n",
        "- Ray Dashboard → Metrics → Object Store Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Tip 3: Use fusion-friendly patterns**\n",
        "\n",
        "```python\n",
        "# Fusion-friendly: Same compute, compatible configs\n",
        "images.map_batches(prep, batch_size=32, num_cpus=1).map_batches(model, batch_size=32, num_gpus=1)\n",
        "\n",
        "# Fusion-incompatible: Different batch sizes  \n",
        "images.map_batches(prep, batch_size=64).map_batches(model, batch_size=32)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Tip 4: Respect memory limits**\n",
        "```python\n",
        "# Set limits based on architecture\n",
        "ctx.execution_options.resource_limits.object_store_memory = node_memory * 0.3 * 0.5\n",
        "\n",
        "# This leaves 50% object store for other workloads\n",
        "# Prevents OOM when running multiple jobs or have memory heavy UDFs\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Practical Architecture Examples\n",
        "\n",
        "### Example 1: Small Model, High Throughput\n",
        "\n",
        "**Scenario:** ResNet-50 (100MB model), process 1M images\n",
        "\n",
        "**Architectural analysis:**\n",
        "- **Model size**: Small (100MB) → Many actors fit in memory\n",
        "- **GPU memory**: 16GB → Large batches possible (128 images)\n",
        "- **Throughput goal**: Maximize images/second\n",
        "\n",
        "**Optimal configuration:**\n",
        "\n",
        "```python\n",
        "results = images.map_batches(\n",
        "    ResNet50Model,\n",
        "    batch_size=128,    # Large batches for throughput\n",
        "    num_gpus=1,\n",
        "    concurrency=8      # Use all 8 GPUs\n",
        ")\n",
        "```\n",
        "\n",
        "Why this works:\n",
        "- Heap memory: 64GB * 0.7 = 45GB\n",
        "- Per actor: 0.1GB model + 1GB batch = 1.1GB\n",
        "- Can fit: 45GB / 1.1GB = 40 actors\n",
        "- Limited by: 8 GPUs → concurrency=8\n",
        "- Object store: Minimal pressure (small batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2: Large Model, Memory-Constrained\n",
        "\n",
        "**Scenario:** LLaMA-70B (140GB model with quantization = 35GB), process 100K documents\n",
        "\n",
        "**Architectural analysis:**\n",
        "- **Model size**: Huge (35GB) → Very few actors fit\n",
        "- **GPU memory**: 80GB A100 → Moderate batches (16 documents)\n",
        "- **Memory goal**: Don't OOM\n",
        "\n",
        "**Optimal configuration:**\n",
        "```python\n",
        "results = documents.map_batches(\n",
        "    LLaMA70BModel,\n",
        "    batch_size=16,     # Conservative for large model\n",
        "    num_gpus=1,\n",
        "    concurrency=2      # Only 2 models fit in cluster memory\n",
        ")\n",
        "```\n",
        "\n",
        "Why this works:\n",
        "- Heap memory: 256GB * 0.7 = 179GB (multi-node)\n",
        "- Per actor: 35GB model + 5GB batch = 40GB\n",
        "- Can fit: 179GB / 40GB = 4 actors theoretical\n",
        "- Use: 2 for safety margin (avoid OOM)\n",
        "- Object store: Backpressure prevents overflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 3: Balanced Pipeline\n",
        "\n",
        "**Scenario:** BERT (500MB), preprocessing heavy (embedding generation)\n",
        "\n",
        "**Architectural analysis:**\n",
        "- **Preprocessing**: CPU-intensive (tokenization, embedding lookup)\n",
        "- **Inference**: GPU-intensive (transformer forward pass)\n",
        "- **Goal**: Balance both stages\n",
        "\n",
        "**Example Optimal configuration:**\n",
        "\n",
        "```python\n",
        "results = (\n",
        "    documents\n",
        "    .map_batches(\n",
        "        heavy_preprocessing,\n",
        "        batch_size=64,      # CPU batches can be larger\n",
        "        num_cpus=2,         # Allocate CPUs for preprocessing\n",
        "        concurrency=16       # Many CPU workers\n",
        "    )\n",
        "    .map_batches(\n",
        "        BERTInference,\n",
        "        batch_size=32,      # GPU batch size\n",
        "        num_gpus=1,\n",
        "        concurrency=4        # 4 GPUs\n",
        "    )\n",
        ")\n",
        "```\n",
        "\n",
        "Why this works:\n",
        "- Preprocessing: 16 workers × 2 CPUs = 32 CPUs used\n",
        "- Inference: 4 workers × 1 GPU = 4 GPUs used\n",
        "- Backpressure: Automatically balances if mismatch\n",
        "- Fusion: Disabled (different compute), but that's okay - different resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary: Architecture Drives Optimization\n",
        "\n",
        "**Key Architectural Principles for Batch Inference:**\n",
        "\n",
        "1. **Streaming execution** enables unlimited dataset sizes with constant memory\n",
        "2. **Blocks** are the unit of parallelism - more blocks = more parallel tasks\n",
        "3. **Object store** holds blocks and transfers - capacity limits in-flight data\n",
        "4. **Heap memory** holds models and executions - limits concurrent actors\n",
        "5. **Operator fusion** reduces overhead - keep configs compatible\n",
        "6. **Backpressure** prevents overflow - trust Ray Data's automatic balancing\n",
        "\n",
        "**Optimization Strategy:**\n",
        "1. Start with memory constraints (heap and object store)\n",
        "2. Calculate maximum concurrency from memory limits\n",
        "3. Choose batch size for GPU utilization\n",
        "4. Configure block size for parallelism\n",
        "5. Monitor Ray Dashboard for bottlenecks\n",
        "6. Adjust based on observed behavior\n",
        "\n",
        "---\n",
        "\n",
        "**[← Back to Part 1](01-inference-fundamentals.ipynb)** | **[Continue to Part 3](03-advanced-optimization.ipynb)** | **[Return to Overview](README.md)**\n",
        "\n",
        "---\n",
        "\n",
        "*This architectural deep-dive completes the batch inference optimization series. You now understand not just how to optimize, but why specific optimizations work based on Ray Data's design.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
