{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 3: Advanced Optimization\n",
        "\n",
        "**Time to complete**: 20 min | **Difficulty**: Intermediate | **Prerequisites**: Complete Part 1 and 2\n",
        "\n",
        "---\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "- Performance optimization decision framework\n",
        "- Systematic optimization process with visual guides\n",
        "- Observability tools for monitoring and debugging\n",
        "- Production deployment best practices\n",
        "\n",
        "---\n",
        "\n",
        "## Optimization Framework\n",
        "\n",
        "Ray Data performance tuning follows a clear hierarchy. Most issues can be resolved with simple parameter adjustments—always start with the simplest solutions first.\n",
        "\n",
        "<div style=\"background-color: #e3f2fd; padding: 15px; border-left: 4px solid #2196F3; margin: 20px 0;\">\n",
        "<strong>Core Principle</strong><br>\n",
        "Start with the simplest optimization first. Most performance issues can be solved with <code>num_cpus</code> adjustments or block sizing.\n",
        "</div>\n",
        "\n",
        "### Three-Level Optimization Hierarchy\n",
        "\n",
        "<table style=\"width:100%; border-collapse: collapse;\">\n",
        "<tr style=\"background-color: #4CAF50; color: white;\">\n",
        "<th style=\"padding: 12px; text-align: left;\">Level</th>\n",
        "<th style=\"padding: 12px; text-align: left;\">Complexity</th>\n",
        "<th style=\"padding: 12px; text-align: left;\">When to Use</th>\n",
        "</tr>\n",
        "<tr style=\"background-color: #e8f5e9;\">\n",
        "<td style=\"padding: 10px;\"><strong>1. num_cpus &amp; batch_size</strong></td>\n",
        "<td style=\"padding: 10px;\">Simple</td>\n",
        "<td style=\"padding: 10px;\">Low CPU utilization, memory/GPU bottlenecks, imbalanced pipeline stages</td>\n",
        "</tr>\n",
        "<tr style=\"background-color: #fff3e0;\">\n",
        "<td style=\"padding: 10px;\"><strong>2. Block sizing</strong></td>\n",
        "<td style=\"padding: 10px;\">Medium</td>\n",
        "<td style=\"padding: 10px;\">Fine-tuning throughput/memory, advanced performance tuning</td>\n",
        "</tr>\n",
        "<tr style=\"background-color: #ffebee;\">\n",
        "<td style=\"padding: 10px;\"><strong>3. DataContext configs</strong></td>\n",
        "<td style=\"padding: 10px;\">Complex</td>\n",
        "<td style=\"padding: 10px;\">Specialized, expert-level requirements</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "Each level increases in complexity and has diminishing returns. Level 1 optimizations are simple, safe, and highly effective. Level 2 requires understanding memory constraints. Level 3 should only be used when Levels 1 and 2 fail to resolve your issue.\n",
        "\n",
        "---\n",
        "\n",
        "## Quick Decision Guide\n",
        "\n",
        "### Master Decision Tree\n",
        "\n",
        "\n",
        "```text\n",
        "                    ┌─────────────────────────────┐\n",
        "                    │  What's your main symptom?  │\n",
        "                    └────────────┬────────────────┘\n",
        "                                 │\n",
        "         ┌───────────────────────┼───────────────────────┐\n",
        "         │                       │                       │\n",
        "    ┌────▼────┐            ┌─────▼─────┐          ┌──────▼──────┐\n",
        "    │ Too Slow│            │ Crashing  │          │  Imbalanced │\n",
        "    └────┬────┘            └─────┬─────┘          └──────┬──────┘\n",
        "         │                       │                       │\n",
        "         │                       │                       │\n",
        "    Check CPU & GPU         Check Logs              Compare Stage\n",
        "    Utilization            & Memory                   Progress Bars\n",
        "         │                       │                       │\n",
        "    ┌────┴────┐            ┌─────┴─────┐          ┌──────┴──────┐\n",
        "    │         │            │           │          │             │\n",
        "Low CPU  Low GPU       Workers     GPU OOM     One Stage     Pipeline\n",
        "(<50%)   (<50%)         Killed                   Slow         Stalls\n",
        "  │         │              │           │          │             │\n",
        "  │         │              │           │          │             │\n",
        "  ▼         ▼              ▼           ▼          ▼             ▼\n",
        "\n",
        "\n",
        "LOW CPU UTILIZATION (<50%)\n",
        "─────────────────────────────\n",
        "1. Check which stage is slow:\n",
        "   \n",
        "   I/O Operations (read/write)\n",
        "   └─→ num_cpus = 0.025-0.1\n",
        "       Reason: Hide network/disk latency with high concurrency\n",
        "   \n",
        "   Simple Transforms (filter, map)\n",
        "   └─→ num_cpus = 0.1-0.25\n",
        "       Reason: Fast operations benefit from high parallelism\n",
        "   \n",
        "   Complex CPU Work (preprocessing)\n",
        "   └─→ num_cpus = 0.25-0.5\n",
        "       Reason: Balance parallelism with task overhead\n",
        "   \n",
        "   Heavy Compute (CPU inference)\n",
        "   └─→ num_cpus = 2 * num_cpus\n",
        "       Reason: Minimize scheduling overhead\n",
        "\n",
        "\n",
        "LOW GPU UTILIZATION (<50%)\n",
        "─────────────────────────────\n",
        "1. Check if CPUs are busy:\n",
        "   \n",
        "   YES: Data preprocessing is bottleneck\n",
        "   └─→ Decrease num_cpus on CPU stages (0.5 → 0.25)\n",
        "   └─→ Increase preprocessing concurrency\n",
        "   └─→ Consider GPU preprocessing if available\n",
        "   \n",
        "   NO: Batch size too small\n",
        "   └─→ Increase batch_size (32 → 64 → 128)\n",
        "   └─→ Check GPU memory allows larger batches\n",
        "   \n",
        "   Spiky GPU usage:\n",
        "   └─→ Increase batch_size for smoother utilization\n",
        "   └─→ Check upstream tasks for backpressure issues\n",
        "\n",
        "\n",
        "WORKERS KILLED (OOM)\n",
        "─────────────────────────────\n",
        "1. Check error message:\n",
        "   \n",
        "   \"Killed\" or \"Out of memory\"\n",
        "   └─→ Increase num_cpus to reduce parallelism\n",
        "       (0.5 → 1.0 → 2.0)\n",
        "   └─→ Reduce concurrency parameter\n",
        "   └─→ Decrease batch_size if applicable\n",
        "   \n",
        "   \"Ray object store full\"\n",
        "   └─→ Increase num_cpus across the memory heavy stages\n",
        "   └─→ Increase number of blocks (override_num_blocks) so the blocks are smaller\n",
        "   └─→ Decrease target_max_block_size\n",
        "   \n",
        "   Still failing?\n",
        "   └─→ Reduce target_max_block_size (128MB → 64MB)\n",
        "   └─→ Enable eager_free in DataContext\n",
        "   └─→ Increase object store memory fraction\n",
        "\n",
        "\n",
        "GPU OUT OF MEMORY\n",
        "─────────────────────────────\n",
        "1. First: Reduce batch_size\n",
        "   └─→ 128 → 64 → 32 → 16 → 8\n",
        "   \n",
        "2. Still OOM? \n",
        "   └─→ Increase GPU allocation (if using fractional)\n",
        "   \n",
        "3. Still OOM? Advanced options:\n",
        "   └─→ Enable mixed precision (fp16/bf16)\n",
        "\n",
        "\n",
        "ONE STAGE SLOW (IMBALANCED)\n",
        "─────────────────────────────\n",
        "1. Identify the slow stage:\n",
        "   └─→ Check operator progress bars (watch out for backpressure)\n",
        "   \n",
        "2. Adjust num_cpus for that stage:\n",
        "   \n",
        "   Stage has empty output queue:\n",
        "   └─→ Decrease num_cpus (increase parallelism)\n",
        "   \n",
        "   Stage has large input queue:\n",
        "   └─→ Increase num_cpus (reduce parallelism)\n",
        "       OR decrease upstream num_cpus\n",
        "   \n",
        "3. Check for data skew:\n",
        "   └─→ Some tasks much slower than others?\n",
        "   └─→ Check the dataset.stats() to see if the block sizing looks right\n",
        "\n",
        "\n",
        "PIPELINE STALLS (NO PROGRESS)\n",
        "─────────────────────────────\n",
        "1. Check progress bars:\n",
        "   \n",
        "   All stages stuck:\n",
        "   └─→ Remove .count(), .show(), .schema() calls\n",
        "   └─→ These materialize the entire dataset\n",
        "   \n",
        "   One stage stuck:\n",
        "   └─→ Check for errors in that stage\n",
        "   └─→ Enable verbose logging\n",
        "   └─→ Check actor stack traces\n",
        "   └─→ Check the dataset block sizing in case the blocks are too large\n",
        "   └─→ Watch out for when preserve_order=True, use only when logically appropriate\n",
        "   \n",
        "   Intermittent stalls:\n",
        "   └─→ Network issues or rate limiting\n",
        "   └─→ Check retry configuration\n",
        "   └─→ Increase io_timeout\n",
        "\n",
        "   Scheduling issues:\n",
        "   └─→ A stage being incorrectly configured and blocking other stages\n",
        "```\n",
        "\n",
        "<div style=\"background-color: #fff9c4; padding: 15px; border-left: 4px solid #FFC107; margin: 20px 0;\">\n",
        "<strong>The num_cpus Paradox</strong><br>\n",
        "<strong>Lower num_cpus = MORE parallelism!</strong>\n",
        "<ul>\n",
        "<li><code>num_cpus=4.0</code> → Only 4 tasks on 16-CPU machine</li>\n",
        "<li><code>num_cpus=0.5</code> → 32 tasks on 16-CPU machine</li>\n",
        "</ul>\n",
        "Use LOW values (0.025-0.1) for I/O operations, HIGH values (2.0-4.0) for CPU-intensive work.\n",
        "</div>\n",
        "\n",
        "The `num_cpus` parameter tells Ray Data how many CPUs to *reserve* for each task. When you set `num_cpus=4.0`, Ray reserves 4 CPUs for each task, so only a few tasks run simultaneously. When you set `num_cpus=0.5`, Ray reserves half a CPU per task, allowing many more tasks to run in parallel—beneficial for I/O-bound operations where tasks spend time waiting for data.\n",
        "\n",
        "It doesn't actually isolate the hardware, it merely schedules the task to run on that node, so watch out for things like\n",
        "fractional GPU or CPU usage that might overwhelm the node.\n",
        "\n",
        "---\n",
        "\n",
        "## Resource Allocation Decision Matrix\n",
        "\n",
        "### By Operation Type\n",
        "\n",
        "<table style=\"width:100%; border-collapse: collapse;\">\n",
        "<tr style=\"background-color: #3f51b5; color: white;\">\n",
        "<th style=\"padding: 12px;\">Operation</th>\n",
        "<th style=\"padding: 12px;\">num_cpus</th>\n",
        "<th style=\"padding: 12px;\">batch_size</th>\n",
        "<th style=\"padding: 12px;\">Why?</th>\n",
        "</tr>\n",
        "<tr style=\"background-color: #f5f5f5;\">\n",
        "<td style=\"padding: 10px;\">Data loading</td>\n",
        "<td style=\"padding: 10px;\"><code>0.025-0.1</code></td>\n",
        "<td style=\"padding: 10px;\">N/A</td>\n",
        "<td style=\"padding: 10px;\">I/O bound, hide latency with high concurrency</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 10px;\">Filtering</td>\n",
        "<td style=\"padding: 10px;\"><code>0.1-0.25</code></td>\n",
        "<td style=\"padding: 10px;\">N/A</td>\n",
        "<td style=\"padding: 10px;\">Fast operation, high parallelism beneficial</td>\n",
        "</tr>\n",
        "<tr style=\"background-color: #f5f5f5;\">\n",
        "<td style=\"padding: 10px;\">CPU preprocessing</td>\n",
        "<td style=\"padding: 10px;\"><code>0.25-0.5</code></td>\n",
        "<td style=\"padding: 10px;\">100-1000</td>\n",
        "<td style=\"padding: 10px;\">Balance parallelism with task overhead</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 10px;\">CPU inference</td>\n",
        "<td style=\"padding: 10px;\"><code>2.0-4.0</code></td>\n",
        "<td style=\"padding: 10px;\">16-32</td>\n",
        "<td style=\"padding: 10px;\">Heavy compute, minimize task overhead</td>\n",
        "</tr>\n",
        "<tr style=\"background-color: #f5f5f5;\">\n",
        "<td style=\"padding: 10px;\">GPU inference</td>\n",
        "<td style=\"padding: 10px;\"><code>1.0</code> (CPU)<br><code>1.0</code> (GPU)</td>\n",
        "<td style=\"padding: 10px;\">32-128</td>\n",
        "<td style=\"padding: 10px;\">Match GPU memory, maximize utilization</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "### By Hardware Configuration\n",
        "\n",
        "<table style=\"width:100%; border-collapse: collapse;\">\n",
        "<tr style=\"background-color: #9c27b0; color: white;\">\n",
        "<th style=\"padding: 12px;\">Scenario</th>\n",
        "<th style=\"padding: 12px;\">Configuration</th>\n",
        "<th style=\"padding: 12px;\">Rationale</th>\n",
        "</tr>\n",
        "<tr style=\"background-color: #f3e5f5;\">\n",
        "<td style=\"padding: 10px;\"><strong>GPU Cluster</strong><br>(T4/A10G/A100)</td>\n",
        "<td style=\"padding: 10px;\">\n",
        "• concurrency = # GPUs<br>\n",
        "• batch_size = 32-128<br>\n",
        "• num_cpus = 1.0 per GPU\n",
        "</td>\n",
        "<td style=\"padding: 10px;\">One actor per GPU, maximize GPU utilization</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 10px;\"><strong>CPU Cluster</strong><br>(No GPUs)</td>\n",
        "<td style=\"padding: 10px;\">\n",
        "• concurrency = CPUs / 4<br>\n",
        "• batch_size = 16-32<br>\n",
        "• num_cpus = 4.0 per actor\n",
        "</td>\n",
        "<td style=\"padding: 10px;\">Balance actor count with CPU resources</td>\n",
        "</tr>\n",
        "<tr style=\"background-color: #f3e5f5;\">\n",
        "<td style=\"padding: 10px;\"><strong>Mixed Cluster</strong><br>(CPU + GPU nodes)</td>\n",
        "<td style=\"padding: 10px;\">\n",
        "• Separate CPU/GPU stages<br>\n",
        "• CPU: num_cpus=0.25<br>\n",
        "• GPU: as above\n",
        "</td>\n",
        "<td style=\"padding: 10px;\">Prevent CPU tasks from running on GPU nodes</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "---\n",
        "\n",
        "## Systematic Optimization Process\n",
        "\n",
        "### Six-Step Workflow\n",
        "\n",
        "```text\n",
        "┌──────────────┐\n",
        "│  1. Monitor  │  Enable progress bars, check dashboards\n",
        "└──────┬───────┘\n",
        "       │\n",
        "       ▼\n",
        "┌──────────────┐\n",
        "│  2. Baseline │  Measure current performance\n",
        "└──────┬───────┘\n",
        "       │\n",
        "       ▼\n",
        "┌──────────────┐\n",
        "│  3. Identify │  Find the bottleneck stage\n",
        "└──────┬───────┘\n",
        "       │\n",
        "       ▼\n",
        "┌──────────────┐\n",
        "│  4. One Fix  │  Apply single optimization\n",
        "└──────┬───────┘\n",
        "       │\n",
        "       ▼\n",
        "┌──────────────┐\n",
        "│  5. Measure  │  Calculate improvement\n",
        "└──────┬───────┘\n",
        "       │\n",
        "       ▼\n",
        "┌──────────────┐\n",
        "│  6. Repeat   │  Continue with next bottleneck\n",
        "└──────────────┘\n",
        "```\n",
        "\n",
        "**Key practices:**\n",
        "\n",
        "1. **Monitor**: Enable progress bars and open Ray Dashboard before optimizing\n",
        "2. **Baseline**: Write down how long your pipeline takes\n",
        "3. **Identify**: Use progress bars to see which stage is slowest\n",
        "4. **One Fix**: Apply a single optimization—never change multiple parameters at once\n",
        "5. **Measure**: Calculate improvement percentage\n",
        "6. **Repeat**: Move to the next bottleneck or try a different optimization\n",
        "\n",
        "---\n",
        "\n",
        "## Observability Tools\n",
        "\n",
        "### Progress Bars\n",
        "\n",
        "Ray Data provides two types of progress bars for monitoring:\n",
        "\n",
        "**Main Progress Bar**: Shows overall operation progress including total rows, execution time, and high-level resource usage.\n",
        "\n",
        "**Operator Progress Bars**: Display individual stage progress with detailed metrics per operator, showing which stage is the bottleneck.\n",
        "\n",
        "**When to use:**\n",
        "- Development/debugging: Enable both for real-time feedback\n",
        "- Production: Disable for performance but keep metrics collection\n",
        "- Notebooks: Enable for debugging, disable for cleaner saved output\n",
        "\n",
        "**Key metrics to watch:**\n",
        "- Rows processed (if stalled, pipeline may be hung)\n",
        "- Resource usage (low CPU <50% needs more parallelism)\n",
        "- Stage timing (longest stage is your bottleneck)\n",
        "- Block counts (large queues indicate backpressure)\n",
        "\n",
        "### Ray Dashboard\n",
        "\n",
        "Access at `http://localhost:8265` (OSS local dashboard) or through Anyscale's Dashboards.\n",
        "\n",
        "**Critical tabs for optimization:**\n",
        "\n",
        "**Cluster Tab**: Node-level resource utilization (CPU, GPU, memory, disk). Look for idle CPUs (need more parallelism), underutilized GPUs (data loading bottleneck), or memory near capacity (risk of spilling/OOM).\n",
        "\n",
        "**Jobs Tab**: Resource usage over time for active and completed jobs. Use to compare performance before and after optimization.\n",
        "\n",
        "**Metrics Tab**: Time-series graphs of system and application metrics for trend analysis.\n",
        "\n",
        "<table style=\"width:100%; border-collapse: collapse;\">\n",
        "<tr style=\"background-color: #1976d2; color: white;\">\n",
        "<th style=\"padding: 12px;\">Metric</th>\n",
        "<th style=\"padding: 12px;\">What to Look For</th>\n",
        "<th style=\"padding: 12px;\">Optimization Action</th>\n",
        "</tr>\n",
        "<tr style=\"background-color: #e3f2fd;\">\n",
        "<td style=\"padding: 10px;\"><strong>CPU Utilization</strong></td>\n",
        "<td style=\"padding: 10px;\">% of CPU cores actively computing</td>\n",
        "<td style=\"padding: 10px;\">\n",
        "Low (<50%): Decrease num_cpus<br>\n",
        "High (>90%): Well-optimized\n",
        "</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 10px;\"><strong>GPU Utilization</strong></td>\n",
        "<td style=\"padding: 10px;\">% of GPU compute active</td>\n",
        "<td style=\"padding: 10px;\">\n",
        "Low (<50%): Data loading bottleneck<br>\n",
        "Spiky: Increase batch size\n",
        "</td>\n",
        "</tr>\n",
        "<tr style=\"background-color: #e3f2fd;\">\n",
        "<td style=\"padding: 10px;\"><strong>Memory Usage</strong></td>\n",
        "<td style=\"padding: 10px;\">RAM and object store consumption</td>\n",
        "<td style=\"padding: 10px;\">\n",
        "Near capacity: Reduce parallelism<br>\n",
        "Spilling: Increase num_cpus\n",
        "</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 10px;\"><strong>Object Store</strong></td>\n",
        "<td style=\"padding: 10px;\">Memory for storing data blocks</td>\n",
        "<td style=\"padding: 10px;\">\n",
        "Rapidly filling: Downstream too slow<br>\n",
        "Empty: Upstream bottleneck\n",
        "</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "### Ray Data Dashboard\n",
        "\n",
        "Specialized view focusing on Ray Data pipeline metrics with detailed operator-level insights.\n",
        "\n",
        "<div style=\"background-color: #e3f2fd; padding: 15px; border-left: 4px solid #2196F3; margin: 20px 0;\">\n",
        "<strong>Anyscale Enhanced Dashboard</strong><br>\n",
        "If you're using Anyscale (Ray 2.44+), you have access to an enhanced dashboard with tree visualization for complex pipelines, integrated log views, and persistence across sessions. See the <a href=\"https://docs.anyscale.com/monitoring/workload-debugging/data-dashboard\">Anyscale Data Dashboard documentation</a> for details and screenshots.\n",
        "</div>\n",
        "\n",
        "**Access:**\n",
        "- **Open-source Ray**: Navigate to Ray Dashboard → Data or Metrics tabs\n",
        "- **Anyscale**: Ray Workloads tab → Data tab\n",
        "\n",
        "**Key sections:**\n",
        "\n",
        "**Overview**: Total throughput (rows/second), execution time, aggregate resource usage. Shows CPU/GPU usage per operator, queued rows, and task counts.\n",
        "\n",
        "**Inputs/Outputs**: Tracks data flow between operators. Rising input queues indicate backpressure from downstream; growing output queues indicate downstream is too slow.\n",
        "\n",
        "**Tasks**: Task execution metrics including running, completed, and average duration. Useful for understanding parallelism settings.\n",
        "\n",
        "**Object Store Memory**: Memory usage for Ray Data blocks. Shows which operators consume the most memory.\n",
        "\n",
        "**Iteration** (for training): Tracks how fast workers consume data from iterators.\n",
        "\n",
        "**Optimization workflow:**\n",
        "\n",
        "1. **Identify bottleneck**: Look for operator with lowest throughput\n",
        "2. **Check resources**: Is CPU low? (decrease num_cpus) Is memory high? (increase num_cpus)\n",
        "3. **Examine queues**: Large input queues = operator can't keep up; empty = upstream too slow\n",
        "4. **Monitor blocked time**: For training, linear increase = need more data parallelism\n",
        "\n",
        "**Anyscale-specific features:**\n",
        "\n",
        "**Operator drill-down**: Click operators to view estimated remaining runtime, peak memory, task statistics, and resource utilization over time.\n",
        "\n",
        "**Tree visualization**: For pipelines with `union`, `zip`, or `join`, see parent-child relationships and merge points in tree structure.\n",
        "\n",
        "**Dashboard persistence**: Unlike open-source, Anyscale preserves dashboards after job termination. Use for post-mortem analysis, comparing runs, and sharing with team members via session dropdown.\n",
        "\n",
        "### Actor Stack Traces\n",
        "\n",
        "View stack traces of running actors to diagnose hangs or slow operations:\n",
        "\n",
        "1. Navigate to Actors tab in Ray Dashboard\n",
        "2. Find the actor (search by name or filter by state)\n",
        "3. Click actor → Stack Trace tab\n",
        "\n",
        "The stack trace shows the exact file, line number, and function where the actor is executing—immediately revealing if it's doing productive work or stuck waiting.\n",
        "\n",
        "---\n",
        "\n",
        "## Batch Size Selection\n",
        "\n",
        "```text\n",
        "Start with estimated batch size\n",
        "       │\n",
        "       ▼\n",
        "┌─────────────────┐\n",
        "│ Run and measure │\n",
        "│  memory usage   │\n",
        "└────────┬────────┘\n",
        "         │\n",
        "    ┌────┴────┐\n",
        "    │         │\n",
        "Memory <50%  Memory >85%\n",
        "    │         │\n",
        "    ├→ 2x     ├→ 0.5x\n",
        "    │  larger │  smaller\n",
        "    │         │\n",
        "    └─────────┴───→ Repeat until optimal\n",
        "```\n",
        "\n",
        "**Measure memory usage:**\n",
        "- GPU: `torch.cuda.max_memory_allocated()` or `nvidia-smi`\n",
        "- CPU: Ray Dashboard memory graphs\n",
        "\n",
        "Target 60-80% memory usage for optimal performance without OOM risk.\n",
        "\n",
        "\n",
        "**Hint: Estimating Per-Block Memory Using a Local Test**\n",
        "\n",
        "Before tuning Ray Data batch size/concurrency, measure typical memory use for a single block or input file *outside* Ray. This gives a baseline for safe memory allocation per worker.\n",
        "\n",
        "**How-To:**\n",
        "\n",
        "1. Take a representative file or data block from your dataset.\n",
        "2. Write a function to load/process it in a plain Python script.  \n",
        "   To measure peak memory usage, either:\n",
        "   - Use `psutil` in your script, or\n",
        "   - Observe memory consumption in a system monitor (e.g., `htop`/`top`) or Ray Dashboard (see dashboard Grafana metrics).\n",
        "\n",
        "Example using `psutil`:\n",
        "\n",
        "```python\n",
        "import psutil\n",
        "import os\n",
        "import time\n",
        "\n",
        "def test_block(path):\n",
        "    data = load_file(path)   # Replace with your real loader\n",
        "    result = run_inference(data)  # Replace with model call\n",
        "    return result\n",
        "\n",
        "process = psutil.Process(os.getpid())\n",
        "mem_before = process.memory_info().rss / 1024**2  # in MB\n",
        "result = test_block(\"/path/to/sample\")\n",
        "mem_after = process.memory_info().rss / 1024**2   # in MB\n",
        "print(f\"Memory used during test: {(mem_after - mem_before):.1f} MB\")\n",
        "```\n",
        "\n",
        "Alternatively, for longer-running jobs or more granular tracking, poll `process.memory_info().rss` periodically and record the maximum value.\n",
        "\n",
        "Or, simply run your script and monitor the process's memory in `htop` or Ray's Dashboard (see the memory graphs and worker detail panels).\n",
        "\n",
        "3. Compare this peak with your per-worker RAM (e.g., from <table> above). Leave headroom (aim for ~80% of available).\n",
        "\n",
        "**Rule of Thumb:**  \n",
        "`Max batch_size = int(0.8 * worker_RAM_MB / block_memory_MB)`\n",
        "\n",
        "**Why this matters:** The ratio of block/file memory usage to available worker RAM is critical. If you exceed memory, Ray workers will be killed or jobs will stall. If you underuse, throughput will lag.\n",
        "\n",
        "**Tip:** For dynamic data, test on several blocks and use the largest measurement.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Troubleshooting Guide\n",
        "\n",
        "### Symptom → Solution Matrix\n",
        "\n",
        "<table style=\"width:100%; border-collapse: collapse;\">\n",
        "<tr style=\"background-color: #d32f2f; color: white;\">\n",
        "<th style=\"padding: 12px; width: 25%;\">Symptom</th>\n",
        "<th style=\"padding: 12px; width: 25%;\">Root Cause</th>\n",
        "<th style=\"padding: 12px; width: 25%;\">Solution</th>\n",
        "<th style=\"padding: 12px; width: 25%;\">Verification</th>\n",
        "</tr>\n",
        "<tr style=\"background-color: #ffebee;\">\n",
        "<td style=\"padding: 10px;\">Low CPU (<50%)</td>\n",
        "<td style=\"padding: 10px;\">Not enough parallel tasks or batch_size too small</td>\n",
        "<td style=\"padding: 10px;\"><strong>Decrease</strong> num_cpus<br>(1.0 → 0.5 → 0.25)</td>\n",
        "<td style=\"padding: 10px;\">CPU utilization >80%</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 10px;\">Workers killed</td>\n",
        "<td style=\"padding: 10px;\">Out of memory</td>\n",
        "<td style=\"padding: 10px;\"><strong>Increase</strong> num_cpus<br>(0.5 → 1.0 → 2.0) or decrease batch_size</td>\n",
        "<td style=\"padding: 10px;\">No more crashes</td>\n",
        "</tr>\n",
        "<tr style=\"background-color: #ffebee;\">\n",
        "<td style=\"padding: 10px;\">GPU OOM</td>\n",
        "<td style=\"padding: 10px;\">Batch too large</td>\n",
        "<td style=\"padding: 10px;\"><strong>Reduce</strong> batch_size<br>(64 → 32 → 16)</td>\n",
        "<td style=\"padding: 10px;\">No CUDA errors</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 10px;\">Pipeline stalls</td>\n",
        "<td style=\"padding: 10px;\">Materialization in pipeline, too large blocks, or an issue with scheduling</td>\n",
        "<td style=\"padding: 10px;\">Remove .count(), .schema() or other materialization steps, examine block sizing, or take a look at the user submitted allocation scheduling</td>\n",
        "<td style=\"padding: 10px;\">Continuous progress</td>\n",
        "</tr>\n",
        "<tr style=\"background-color: #ffebee;\">\n",
        "<td style=\"padding: 10px;\">One slow stage</td>\n",
        "<td style=\"padding: 10px;\">Imbalanced parallelism or improper scheduling</td>\n",
        "<td style=\"padding: 10px;\">Adjust that stage's num_cpus and/or concurrency</td>\n",
        "<td style=\"padding: 10px;\">Balanced progress bars</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 10px;\">Uneven progress</td>\n",
        "<td style=\"padding: 10px;\">Data skew</td>\n",
        "<td style=\"padding: 10px;\">Check data distribution</td>\n",
        "<td style=\"padding: 10px;\">Even task durations</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "### Memory Issue Decision Tree\n",
        "\n",
        "```text\n",
        "Memory Error?\n",
        "    │\n",
        "    ├─ Workers killed\n",
        "    │      │\n",
        "    │      ├─ First: num_cpus↑ (1.0 → 2.0)\n",
        "    │      ├─ Still OOM: batch_size↓\n",
        "    │      └─ Still OOM: Block size↓ (128MB → 64MB)\n",
        "    │\n",
        "    ├─ \"Ray object store full\"\n",
        "    │      │\n",
        "    │      ├─ First: num_cpus↑ all stages\n",
        "    │      ├─ Still full: Override object store fraction\n",
        "    │      └─ Still full: Reduce pipeline width\n",
        "    │\n",
        "    └─ \"CUDA out of memory\"\n",
        "           │\n",
        "           ├─ First: batch_size↓ (64 → 32 → 16 → 8)\n",
        "           ├─ Still OOM: Separate CPU/GPU stages\n",
        "           └─ Still OOM: Enable mixed precision\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Common Patterns\n",
        "\n",
        "### Pattern Comparison\n",
        "\n",
        "<table style=\"width:100%; border-collapse: collapse;\">\n",
        "<tr style=\"background-color: #1976d2; color: white;\">\n",
        "<th style=\"padding: 12px;\">Pattern</th>\n",
        "<th style=\"padding: 12px;\">Use Case</th>\n",
        "<th style=\"padding: 12px;\">Key Parameters</th>\n",
        "</tr>\n",
        "<tr style=\"background-color: #e3f2fd;\">\n",
        "<td style=\"padding: 10px;\"><strong>Optimized Pipeline</strong></td>\n",
        "<td style=\"padding: 10px;\">General ETL</td>\n",
        "<td style=\"padding: 10px;\">\n",
        "• Read: num_cpus=0.025<br>\n",
        "• Transform: num_cpus=0.5<br>\n",
        "• Write: num_cpus=0.1\n",
        "</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 10px;\"><strong>GPU Inference</strong></td>\n",
        "<td style=\"padding: 10px;\">Deep learning models</td>\n",
        "<td style=\"padding: 10px;\">\n",
        "• num_gpus=1.0<br>\n",
        "• batch_size=64<br>\n",
        "• concurrency=# of GPUs\n",
        "</td>\n",
        "</tr>\n",
        "<tr style=\"background-color: #e3f2fd;\">\n",
        "<td style=\"padding: 10px;\"><strong>CPU Inference</strong></td>\n",
        "<td style=\"padding: 10px;\">CPU-only clusters</td>\n",
        "<td style=\"padding: 10px;\">\n",
        "• num_cpus=4.0<br>\n",
        "• batch_size=16<br>\n",
        "• concurrency=CPUs/4\n",
        "</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "### Code Examples\n",
        "\n",
        "```python\n",
        "# ETL Pattern\n",
        "ds = (ray.data.read_parquet(path, columns=cols, num_cpus=0.025)\n",
        "      .filter(condition, num_cpus=0.1)\n",
        "      .map_batches(transform, num_cpus=0.5)\n",
        "      .write_parquet(output, num_cpus=0.1))\n",
        "\n",
        "# GPU Inference Pattern\n",
        "class GPUModel:\n",
        "    def __init__(self): self.model = load_model().cuda()\n",
        "    def __call__(self, batch): return self.model(batch)\n",
        "\n",
        "ds = ds.map_batches(GPUModel, num_gpus=1.0, batch_size=64, concurrency=2)\n",
        "\n",
        "# CPU Inference Pattern\n",
        "class CPUModel:\n",
        "    def __init__(self): self.model = load_model()\n",
        "    def __call__(self, batch): return self.model(batch)\n",
        "\n",
        "ds = ds.map_batches(CPUModel, num_cpus=4.0, batch_size=16, concurrency=8)\n",
        "```\n",
        "\n",
        "Use class-based actors for model inference—`__init__` loads the model once per worker, and `__call__` processes each batch.\n",
        "\n",
        "---\n",
        "\n",
        "## Production Checklist\n",
        "\n",
        "<table style=\"width:100%; border-collapse: collapse;\">\n",
        "<tr style=\"background-color: #388e3c; color: white;\">\n",
        "<th style=\"padding: 12px; width: 70%;\">Item</th>\n",
        "<th style=\"padding: 12px; width: 30%;\">Status</th>\n",
        "</tr>\n",
        "<tr style=\"background-color: #e8f5e9;\">\n",
        "<td style=\"padding: 10px;\">Column pruning enabled (columns= parameter)</td>\n",
        "<td style=\"padding: 10px;\">☐ Complete</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 10px;\">Early filtering applied (after read)</td>\n",
        "<td style=\"padding: 10px;\">☐ Complete</td>\n",
        "</tr>\n",
        "<tr style=\"background-color: #e8f5e9;\">\n",
        "<td style=\"padding: 10px;\">Class-based actors for stateful ops</td>\n",
        "<td style=\"padding: 10px;\">☐ Complete</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 10px;\">num_cpus set for each stage</td>\n",
        "<td style=\"padding: 10px;\">☐ Complete</td>\n",
        "</tr>\n",
        "<tr style=\"background-color: #e8f5e9;\">\n",
        "<td style=\"padding: 10px;\">batch_size tested with production data</td>\n",
        "<td style=\"padding: 10px;\">☐ Complete</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 10px;\">Concurrency matches resources</td>\n",
        "<td style=\"padding: 10px;\">☐ Complete</td>\n",
        "</tr>\n",
        "<tr style=\"background-color: #e8f5e9;\">\n",
        "<td style=\"padding: 10px;\">Error handling configured</td>\n",
        "<td style=\"padding: 10px;\">☐ Complete</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 10px;\">Monitoring configured</td>\n",
        "<td style=\"padding: 10px;\">☐ Complete</td>\n",
        "</tr>\n",
        "<tr style=\"background-color: #e8f5e9;\">\n",
        "<td style=\"padding: 10px;\">Full data testing completed</td>\n",
        "<td style=\"padding: 10px;\">☐ Complete</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "**Key items:**\n",
        "\n",
        "- **Column pruning**: Read only needed columns to reduce I/O by 60-95%\n",
        "- **Early filtering**: Filter immediately after reading to reduce downstream data volume\n",
        "- **Class-based actors**: Use classes with `__init__` and `__call__` for stateful operations\n",
        "- **Explicit num_cpus**: Set for every stage based on operation type\n",
        "- **Test with production data**: Memory patterns change with data size\n",
        "- **Match concurrency to resources**: GPU: concurrency = # GPUs; CPU: concurrency = total CPUs / CPUs per actor\n",
        "- **Configure error handling**: Set `max_errored_blocks` for large datasets with quality issues\n",
        "- **Enable monitoring**: Disable progress bars but enable metrics collection for production\n",
        "\n",
        "---\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "<div style=\"background-color: #c8e6c9; padding: 20px; border-radius: 8px; margin: 20px 0;\">\n",
        "<h3 style=\"margin-top: 0;\">Remember These Ten Hints</h3>\n",
        "<ol>\n",
        "<li><strong>Start simple:</strong> Try num_cpus before anything else</li>\n",
        "<li><strong>Monitor first:</strong> Enable progress bars to see bottlenecks, look at the Ray Data Dashboard, and the Grafana metrics</li>\n",
        "<li><strong>Check stats:</strong> Check `ds.stats()` on a subset of the dataset to make sure that the block sizing is working as expected.</li>\n",
        "<li><strong>Try fusing operators:</strong> Try to fuse operators together, but don't overdo it by assigning improper resources to stages</li>\n",
        "<li><strong>One change at a time:</strong> Measure each optimization's impact</li>\n",
        "<li><strong>Column pruning:</strong> Specify only needed columns in read operation via `ray.dataset.filter` or the read function</li>\n",
        "<li><strong>Class vs function:</strong> Functions have a lower overhead, but use classes for stateful operations</li>\n",
        "<li><strong>Batch size matters:</strong> Start high and reduce if you hit OOM</li>\n",
        "<li><strong>CPU vs I/O:</strong> Lower num_cpus = MORE parallelism for I/O, don't throttle the pipeline because of I/O limitations</li>\n",
        "<li><strong>Data locality:</strong> Ray will automatically try for best data locality, but also use the proper instance types and block sizing for large blocks</li>\n",
        "</ol>\n",
        "</div>\n",
        "\n",
        "Optimization is iterative. As data grows or requirements change, revisit your configuration. Monitor performance continuously and measure the impact of changes—intuition about performance is often wrong.\n",
        "\n",
        "---\n",
        "\n",
        "**[Return to Overview](README.md)** \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
