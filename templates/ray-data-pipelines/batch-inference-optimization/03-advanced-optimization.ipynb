{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2: Advanced Optimization\n",
        "\n",
        "**Time to complete**: 20 min | **Difficulty**: Intermediate | **Prerequisites**: Complete Part 1\n",
        "\n",
        "---\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "- Performance optimization decision framework\n",
        "- Systematic optimization process with visual guides\n",
        "- Observability tools for monitoring and debugging\n",
        "- Production deployment best practices\n",
        "\n",
        "---\n",
        "\n",
        "## Optimization Framework\n",
        "\n",
        "Ray Data performance tuning follows a clear hierarchy. Most issues can be resolved with simple parameter adjustments—always start with the simplest solutions first.\n",
        "\n",
        "<div style=\"background-color: #e3f2fd; padding: 15px; border-left: 4px solid #2196F3; margin: 20px 0;\">\n",
        "<strong>Core Principle</strong><br>\n",
        "Start with the simplest optimization first. Most performance issues can be solved with <code>num_cpus</code> adjustments alone.\n",
        "</div>\n",
        "\n",
        "### Three-Level Optimization Hierarchy\n",
        "\n",
        "<table style=\"width:100%; border-collapse: collapse;\">\n",
        "<tr style=\"background-color: #4CAF50; color: white;\">\n",
        "<th style=\"padding: 12px; text-align: left;\">Level</th>\n",
        "<th style=\"padding: 12px; text-align: left;\">Complexity</th>\n",
        "<th style=\"padding: 12px; text-align: left;\">When to Use</th>\n",
        "<th style=\"padding: 12px; text-align: left;\">Success Rate</th>\n",
        "</tr>\n",
        "<tr style=\"background-color: #e8f5e9;\">\n",
        "<td style=\"padding: 10px;\"><strong>1. num_cpus</strong></td>\n",
        "<td style=\"padding: 10px;\">Simple</td>\n",
        "<td style=\"padding: 10px;\">Low CPU utilization, imbalanced stages</td>\n",
        "<td style=\"padding: 10px;\">80% of issues</td>\n",
        "</tr>\n",
        "<tr style=\"background-color: #fff3e0;\">\n",
        "<td style=\"padding: 10px;\"><strong>2. batch_size</strong></td>\n",
        "<td style=\"padding: 10px;\">Medium</td>\n",
        "<td style=\"padding: 10px;\">Memory issues, GPU OOM</td>\n",
        "<td style=\"padding: 10px;\">15% of issues</td>\n",
        "</tr>\n",
        "<tr style=\"background-color: #ffebee;\">\n",
        "<td style=\"padding: 10px;\"><strong>3. DataContext configs</strong></td>\n",
        "<td style=\"padding: 10px;\">Complex</td>\n",
        "<td style=\"padding: 10px;\">Advanced requirements only</td>\n",
        "<td style=\"padding: 10px;\">5% of issues</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "Each level increases in complexity and has diminishing returns. Level 1 optimizations are simple, safe, and highly effective. Level 2 requires understanding memory constraints. Level 3 should only be used when Levels 1 and 2 fail to resolve your issue.\n",
        "\n",
        "---\n",
        "\n",
        "## Quick Decision Guide\n",
        "\n",
        "### Master Decision Tree\n",
        "\n",
        "\n",
        "```\n",
        "                    ┌─────────────────────────────┐\n",
        "                    │  What's your main symptom?  │\n",
        "                    └────────────┬────────────────┘\n",
        "                                 │\n",
        "         ┌───────────────────────┼───────────────────────┐\n",
        "         │                       │                       │\n",
        "    ┌────▼────┐            ┌─────▼─────┐          ┌──────▼──────┐\n",
        "    │ Too Slow│            │ Crashing  │          │  Imbalanced │\n",
        "    └────┬────┘            └─────┬─────┘          └──────┬──────┘\n",
        "         │                       │                       │\n",
        "         │                       │                       │\n",
        "    Check CPU & GPU         Check Logs              Compare Stage\n",
        "    Utilization            & Memory                   Progress Bars\n",
        "         │                       │                       │\n",
        "    ┌────┴────┐            ┌─────┴─────┐          ┌──────┴──────┐\n",
        "    │         │            │           │          │             │\n",
        "Low CPU  Low GPU       Workers     GPU OOM     One Stage     Pipeline\n",
        "(<50%)   (<50%)         Killed                   Slow         Stalls\n",
        "  │         │              │           │          │             │\n",
        "  │         │              │           │          │             │\n",
        "  ▼         ▼              ▼           ▼          ▼             ▼\n",
        "\n",
        "\n",
        "LOW CPU UTILIZATION (<50%)\n",
        "─────────────────────────────\n",
        "1. Check which stage is slow:\n",
        "   \n",
        "   I/O Operations (read/write)\n",
        "   └─→ num_cpus = 0.025-0.1\n",
        "       Reason: Hide network/disk latency with high concurrency\n",
        "   \n",
        "   Simple Transforms (filter, map)\n",
        "   └─→ num_cpus = 0.1-0.25\n",
        "       Reason: Fast operations benefit from high parallelism\n",
        "   \n",
        "   Complex CPU Work (preprocessing)\n",
        "   └─→ num_cpus = 0.25-0.5\n",
        "       Reason: Balance parallelism with task overhead\n",
        "   \n",
        "   Heavy Compute (CPU inference)\n",
        "   └─→ num_cpus = 2 * num_cpus\n",
        "       Reason: Minimize scheduling overhead\n",
        "\n",
        "\n",
        "LOW GPU UTILIZATION (<50%)\n",
        "─────────────────────────────\n",
        "1. Check if CPUs are busy:\n",
        "   \n",
        "   YES: Data preprocessing is bottleneck\n",
        "   └─→ Decrease num_cpus on CPU stages (0.5 → 0.25)\n",
        "   └─→ Increase preprocessing concurrency\n",
        "   └─→ Consider GPU preprocessing if available\n",
        "   \n",
        "   NO: Batch size too small\n",
        "   └─→ Increase batch_size (32 → 64 → 128)\n",
        "   └─→ Check GPU memory allows larger batches\n",
        "   \n",
        "   Spiky GPU usage:\n",
        "   └─→ Increase batch_size for smoother utilization\n",
        "   └─→ Increase prefetch_batches if available\n",
        "\n",
        "\n",
        "WORKERS KILLED (OOM)\n",
        "─────────────────────────────\n",
        "1. Check error message:\n",
        "   \n",
        "   \"Killed\" or \"Out of memory\"\n",
        "   └─→ Increase num_cpus to reduce parallelism\n",
        "       (0.5 → 1.0 → 2.0)\n",
        "   └─→ Reduce concurrency parameter\n",
        "   └─→ Decrease batch_size if applicable\n",
        "   \n",
        "   \"Ray object store full\"\n",
        "   └─→ Increase num_cpus across ALL stages\n",
        "   └─→ Reduce number of blocks (override_num_blocks)\n",
        "   └─→ Decrease target_max_block_size\n",
        "   \n",
        "   Still failing?\n",
        "   └─→ Reduce target_max_block_size (128MB → 64MB)\n",
        "   └─→ Enable eager_free in DataContext\n",
        "   └─→ Increase object store memory fraction\n",
        "\n",
        "\n",
        "GPU OUT OF MEMORY\n",
        "─────────────────────────────\n",
        "1. First: Reduce batch_size\n",
        "   └─→ 128 → 64 → 32 → 16 → 8\n",
        "   \n",
        "2. Still OOM? Separate CPU/GPU:\n",
        "   └─→ Preprocess on CPU nodes\n",
        "   └─→ Inference on GPU nodes\n",
        "   └─→ Use accelerator_type parameter\n",
        "   \n",
        "3. Still OOM? Advanced options:\n",
        "   └─→ Enable mixed precision (fp16/bf16)\n",
        "   └─→ Use gradient checkpointing\n",
        "   └─→ Enable model parallelism\n",
        "\n",
        "\n",
        "ONE STAGE SLOW (IMBALANCED)\n",
        "─────────────────────────────\n",
        "1. Identify the slow stage:\n",
        "   └─→ Check operator progress bars (watch out for backpressure)\n",
        "   \n",
        "2. Adjust num_cpus for that stage:\n",
        "   \n",
        "   Stage has empty output queue:\n",
        "   └─→ Decrease num_cpus (increase parallelism)\n",
        "   \n",
        "   Stage has large input queue:\n",
        "   └─→ Increase num_cpus (reduce parallelism)\n",
        "       OR decrease upstream num_cpus\n",
        "   \n",
        "3. Check for data skew:\n",
        "   └─→ Some tasks much slower than others?\n",
        "   └─→ Use repartition() for better distribution\n",
        "   └─→ Check the dataset.stats() to see if the block sizing looks right\n",
        "\n",
        "\n",
        "PIPELINE STALLS (NO PROGRESS)\n",
        "─────────────────────────────\n",
        "1. Check progress bars:\n",
        "   \n",
        "   All stages stuck:\n",
        "   └─→ Remove .count(), .show(), .schema() calls\n",
        "   └─→ These materialize the entire dataset\n",
        "   \n",
        "   One stage stuck:\n",
        "   └─→ Check for errors in that stage\n",
        "   └─→ Enable verbose logging\n",
        "   └─→ Check actor stack traces\n",
        "   \n",
        "   Intermittent stalls:\n",
        "   └─→ Network issues or rate limiting\n",
        "   └─→ Check retry configuration\n",
        "   └─→ Increase io_timeout\n",
        "\n",
        "   Scheduling issues:\n",
        "   └─→ A stage being incorrectly configured and blocking other stages\n",
        "```\n",
        "\n",
        "<div style=\"background-color: #fff9c4; padding: 15px; border-left: 4px solid #FFC107; margin: 20px 0;\">\n",
        "<strong>The num_cpus Paradox</strong><br>\n",
        "<strong>Lower num_cpus = MORE parallelism!</strong>\n",
        "<ul>\n",
        "<li><code>num_cpus=4.0</code> → Only 4 tasks on 16-CPU machine</li>\n",
        "<li><code>num_cpus=0.5</code> → 32 tasks on 16-CPU machine</li>\n",
        "</ul>\n",
        "Use LOW values (0.025-0.1) for I/O operations, HIGH values (2.0-4.0) for CPU-intensive work.\n",
        "</div>\n",
        "\n",
        "The `num_cpus` parameter tells Ray Data how many CPUs to *reserve* for each task. When you set `num_cpus=4.0`, Ray reserves 4 CPUs for each task, so only a few tasks run simultaneously. When you set `num_cpus=0.5`, Ray reserves half a CPU per task, allowing many more tasks to run in parallel—beneficial for I/O-bound operations where tasks spend time waiting for data.\n",
        "\n",
        "It doesn't actually isolate the hardware, it merely schedules the task to run on that node, so watch out for things like\n",
        "fractional GPU or CPU usage that might overwhelm the node.\n",
        "\n",
        "---\n",
        "\n",
        "## Resource Allocation Decision Matrix\n",
        "\n",
        "### By Operation Type\n",
        "\n",
        "<table style=\"width:100%; border-collapse: collapse;\">\n",
        "<tr style=\"background-color: #3f51b5; color: white;\">\n",
        "<th style=\"padding: 12px;\">Operation</th>\n",
        "<th style=\"padding: 12px;\">num_cpus</th>\n",
        "<th style=\"padding: 12px;\">batch_size</th>\n",
        "<th style=\"padding: 12px;\">Why?</th>\n",
        "</tr>\n",
        "<tr style=\"background-color: #f5f5f5;\">\n",
        "<td style=\"padding: 10px;\">Data loading</td>\n",
        "<td style=\"padding: 10px;\"><code>0.025-0.1</code></td>\n",
        "<td style=\"padding: 10px;\">N/A</td>\n",
        "<td style=\"padding: 10px;\">I/O bound, hide latency with high concurrency</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 10px;\">Filtering</td>\n",
        "<td style=\"padding: 10px;\"><code>0.1-0.25</code></td>\n",
        "<td style=\"padding: 10px;\">N/A</td>\n",
        "<td style=\"padding: 10px;\">Fast operation, high parallelism beneficial</td>\n",
        "</tr>\n",
        "<tr style=\"background-color: #f5f5f5;\">\n",
        "<td style=\"padding: 10px;\">CPU preprocessing</td>\n",
        "<td style=\"padding: 10px;\"><code>0.25-0.5</code></td>\n",
        "<td style=\"padding: 10px;\">100-1000</td>\n",
        "<td style=\"padding: 10px;\">Balance parallelism with task overhead</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 10px;\">CPU inference</td>\n",
        "<td style=\"padding: 10px;\"><code>2.0-4.0</code></td>\n",
        "<td style=\"padding: 10px;\">16-32</td>\n",
        "<td style=\"padding: 10px;\">Heavy compute, minimize task overhead</td>\n",
        "</tr>\n",
        "<tr style=\"background-color: #f5f5f5;\">\n",
        "<td style=\"padding: 10px;\">GPU inference</td>\n",
        "<td style=\"padding: 10px;\"><code>1.0</code> (CPU)<br><code>1.0</code> (GPU)</td>\n",
        "<td style=\"padding: 10px;\">32-128</td>\n",
        "<td style=\"padding: 10px;\">Match GPU memory, maximize utilization</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "### By Hardware Configuration\n",
        "\n",
        "<table style=\"width:100%; border-collapse: collapse;\">\n",
        "<tr style=\"background-color: #9c27b0; color: white;\">\n",
        "<th style=\"padding: 12px;\">Scenario</th>\n",
        "<th style=\"padding: 12px;\">Configuration</th>\n",
        "<th style=\"padding: 12px;\">Rationale</th>\n",
        "</tr>\n",
        "<tr style=\"background-color: #f3e5f5;\">\n",
        "<td style=\"padding: 10px;\"><strong>GPU Cluster</strong><br>(T4/A10G/A100)</td>\n",
        "<td style=\"padding: 10px;\">\n",
        "• concurrency = # GPUs<br>\n",
        "• batch_size = 32-128<br>\n",
        "• num_cpus = 1.0 per GPU\n",
        "</td>\n",
        "<td style=\"padding: 10px;\">One actor per GPU, maximize GPU utilization</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 10px;\"><strong>CPU Cluster</strong><br>(No GPUs)</td>\n",
        "<td style=\"padding: 10px;\">\n",
        "• concurrency = CPUs / 4<br>\n",
        "• batch_size = 16-32<br>\n",
        "• num_cpus = 4.0 per actor\n",
        "</td>\n",
        "<td style=\"padding: 10px;\">Balance actor count with CPU resources</td>\n",
        "</tr>\n",
        "<tr style=\"background-color: #f3e5f5;\">\n",
        "<td style=\"padding: 10px;\"><strong>Mixed Cluster</strong><br>(CPU + GPU nodes)</td>\n",
        "<td style=\"padding: 10px;\">\n",
        "• Separate CPU/GPU stages<br>\n",
        "• CPU: num_cpus=0.25<br>\n",
        "• GPU: as above\n",
        "</td>\n",
        "<td style=\"padding: 10px;\">Prevent CPU tasks from running on GPU nodes</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "---\n",
        "\n",
        "## Systematic Optimization Process\n",
        "\n",
        "### Six-Step Workflow\n",
        "\n",
        "```\n",
        "┌──────────────┐\n",
        "│  1. Monitor  │  Enable progress bars, check dashboards\n",
        "└──────┬───────┘\n",
        "       │\n",
        "       ▼\n",
        "┌──────────────┐\n",
        "│  2. Baseline │  Measure current performance\n",
        "└──────┬───────┘\n",
        "       │\n",
        "       ▼\n",
        "┌──────────────┐\n",
        "│  3. Identify │  Find the bottleneck stage\n",
        "└──────┬───────┘\n",
        "       │\n",
        "       ▼\n",
        "┌──────────────┐\n",
        "│  4. One Fix  │  Apply single optimization\n",
        "└──────┬───────┘\n",
        "       │\n",
        "       ▼\n",
        "┌──────────────┐\n",
        "│  5. Measure  │  Calculate improvement\n",
        "└──────┬───────┘\n",
        "       │\n",
        "       ▼\n",
        "┌──────────────┐\n",
        "│  6. Repeat   │  Continue with next bottleneck\n",
        "└──────────────┘\n",
        "```\n",
        "\n",
        "**Key practices:**\n",
        "\n",
        "1. **Monitor**: Enable progress bars and open Ray Dashboard before optimizing\n",
        "2. **Baseline**: Write down how long your pipeline takes\n",
        "3. **Identify**: Use progress bars to see which stage is slowest\n",
        "4. **One Fix**: Apply a single optimization—never change multiple parameters at once\n",
        "5. **Measure**: Calculate improvement percentage\n",
        "6. **Repeat**: Move to the next bottleneck or try a different optimization\n",
        "\n",
        "### Measurement Template\n",
        "\n",
        "```python\n",
        "# Enable monitoring\n",
        "ctx = ray.data.DataContext.get_current()\n",
        "ctx.enable_progress_bars = True\n",
        "ctx.enable_operator_progress_bars = True\n",
        "\n",
        "# Measure baseline\n",
        "import time\n",
        "start = time.time()\n",
        "ds.write_parquet(\"output.parquet\")\n",
        "baseline = time.time() - start\n",
        "print(f\"Baseline: {baseline:.2f}s\")\n",
        "\n",
        "# After optimization\n",
        "new_time = time.time() - start\n",
        "improvement = (baseline - new_time) / baseline * 100\n",
        "print(f\"Improvement: {improvement:.1f}%\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Observability Tools\n",
        "\n",
        "### Progress Bars\n",
        "\n",
        "Ray Data provides two types of progress bars for monitoring:\n",
        "\n",
        "**Main Progress Bar**: Shows overall operation progress including total rows, execution time, and high-level resource usage.\n",
        "\n",
        "**Operator Progress Bars**: Display individual stage progress with detailed metrics per operator, showing which stage is the bottleneck.\n",
        "\n",
        "**Configuration:**\n",
        "\n",
        "```python\n",
        "ctx = ray.data.DataContext.get_current()\n",
        "ctx.enable_progress_bars = True  # Overall progress\n",
        "ctx.enable_operator_progress_bars = True  # Per-stage details\n",
        "ctx.enable_progress_bar_name_truncation = False  # See full operator names\n",
        "```\n",
        "\n",
        "**When to use:**\n",
        "- Development/debugging: Enable both for real-time feedback\n",
        "- Production: Disable for performance but keep metrics collection\n",
        "- Notebooks: Enable for debugging, disable for cleaner saved output\n",
        "\n",
        "**Key metrics to watch:**\n",
        "- Rows processed (if stalled, pipeline may be hung)\n",
        "- Resource usage (low CPU <50% needs more parallelism)\n",
        "- Stage timing (longest stage is your bottleneck)\n",
        "- Block counts (large queues indicate backpressure)\n",
        "\n",
        "### Ray Dashboard\n",
        "\n",
        "Access at `http://localhost:8265` (local) or through your cluster management system.\n",
        "\n",
        "**Critical tabs for optimization:**\n",
        "\n",
        "**Cluster Tab**: Node-level resource utilization (CPU, GPU, memory, disk). Look for idle CPUs (need more parallelism), underutilized GPUs (data loading bottleneck), or memory near capacity (risk of spilling/OOM).\n",
        "\n",
        "**Jobs Tab**: Resource usage over time for active and completed jobs. Use to compare performance before and after optimization.\n",
        "\n",
        "**Metrics Tab**: Time-series graphs of system and application metrics for trend analysis.\n",
        "\n",
        "<table style=\"width:100%; border-collapse: collapse;\">\n",
        "<tr style=\"background-color: #1976d2; color: white;\">\n",
        "<th style=\"padding: 12px;\">Metric</th>\n",
        "<th style=\"padding: 12px;\">What to Look For</th>\n",
        "<th style=\"padding: 12px;\">Optimization Action</th>\n",
        "</tr>\n",
        "<tr style=\"background-color: #e3f2fd;\">\n",
        "<td style=\"padding: 10px;\"><strong>CPU Utilization</strong></td>\n",
        "<td style=\"padding: 10px;\">% of CPU cores actively computing</td>\n",
        "<td style=\"padding: 10px;\">\n",
        "Low (<50%): Decrease num_cpus<br>\n",
        "High (>90%): Well-optimized\n",
        "</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 10px;\"><strong>GPU Utilization</strong></td>\n",
        "<td style=\"padding: 10px;\">% of GPU compute active</td>\n",
        "<td style=\"padding: 10px;\">\n",
        "Low (<50%): Data loading bottleneck<br>\n",
        "Spiky: Increase batch size\n",
        "</td>\n",
        "</tr>\n",
        "<tr style=\"background-color: #e3f2fd;\">\n",
        "<td style=\"padding: 10px;\"><strong>Memory Usage</strong></td>\n",
        "<td style=\"padding: 10px;\">RAM and object store consumption</td>\n",
        "<td style=\"padding: 10px;\">\n",
        "Near capacity: Reduce parallelism<br>\n",
        "Spilling: Increase num_cpus\n",
        "</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 10px;\"><strong>Object Store</strong></td>\n",
        "<td style=\"padding: 10px;\">Memory for storing data blocks</td>\n",
        "<td style=\"padding: 10px;\">\n",
        "Rapidly filling: Downstream too slow<br>\n",
        "Empty: Upstream bottleneck\n",
        "</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "### Ray Data Dashboard\n",
        "\n",
        "Specialized view focusing on Ray Data pipeline metrics with detailed operator-level insights.\n",
        "\n",
        "<div style=\"background-color: #e3f2fd; padding: 15px; border-left: 4px solid #2196F3; margin: 20px 0;\">\n",
        "<strong>Anyscale Enhanced Dashboard</strong><br>\n",
        "If you're using Anyscale (Ray 2.44+), you have access to an enhanced dashboard with tree visualization for complex pipelines, integrated log views, and persistence across sessions. See the <a href=\"https://docs.anyscale.com/monitoring/workload-debugging/data-dashboard\">Anyscale Data Dashboard documentation</a> for details and screenshots.\n",
        "</div>\n",
        "\n",
        "**Access:**\n",
        "- **Open-source Ray**: Navigate to Ray Dashboard → Data or Metrics tabs\n",
        "- **Anyscale**: Ray Workloads tab → Data tab\n",
        "\n",
        "**Key sections:**\n",
        "\n",
        "**Overview**: Total throughput (rows/second), execution time, aggregate resource usage. Shows CPU/GPU usage per operator, queued rows, and task counts.\n",
        "\n",
        "**Inputs/Outputs**: Tracks data flow between operators. Rising input queues indicate backpressure from downstream; growing output queues indicate downstream is too slow.\n",
        "\n",
        "**Tasks**: Task execution metrics including running, completed, and average duration. Useful for understanding parallelism settings.\n",
        "\n",
        "**Object Store Memory**: Memory usage for Ray Data blocks. Shows which operators consume the most memory.\n",
        "\n",
        "**Iteration** (for training): Tracks how fast workers consume data from iterators. Increasing \"Iteration Blocked Time\" means data preprocessing can't keep up with training.\n",
        "\n",
        "**Optimization workflow:**\n",
        "\n",
        "1. **Identify bottleneck**: Look for operator with lowest throughput\n",
        "2. **Check resources**: Is CPU low? (decrease num_cpus) Is memory high? (increase num_cpus)\n",
        "3. **Examine queues**: Large input queues = operator can't keep up; empty = upstream too slow\n",
        "4. **Monitor blocked time**: For training, linear increase = need more data parallelism\n",
        "\n",
        "**Anyscale-specific features:**\n",
        "\n",
        "**Operator drill-down**: Click operators to view estimated remaining runtime, peak memory, task statistics, and resource utilization over time.\n",
        "\n",
        "**Tree visualization**: For pipelines with `union`, `zip`, or `join`, see parent-child relationships and merge points in tree structure.\n",
        "\n",
        "**Integrated logs**: View logs specific to each dataset from `/tmp/ray/{SESSION_NAME}/logs/ray-data/`. Includes automatic backpressure warnings, health checks, and OOM events. Add custom logs:\n",
        "\n",
        "```python\n",
        "import logging\n",
        "logger = logging.getLogger(\"ray.data\")\n",
        "\n",
        "def my_map_function(batch):\n",
        "    if is_interesting(batch):\n",
        "        logger.info(f\"Processing batch with {len(batch)} rows\")\n",
        "    return process(batch)\n",
        "```\n",
        "\n",
        "**Dashboard persistence**: Unlike open-source, Anyscale preserves dashboards after job termination. Use for post-mortem analysis, comparing runs, and sharing with team members via session dropdown.\n",
        "\n",
        "### Actor Stack Traces\n",
        "\n",
        "View stack traces of running actors to diagnose hangs or slow operations:\n",
        "\n",
        "1. Navigate to Actors tab in Ray Dashboard\n",
        "2. Find the actor (search by name or filter by state)\n",
        "3. Click actor → Stack Trace tab\n",
        "\n",
        "The stack trace shows the exact file, line number, and function where the actor is executing—immediately revealing if it's doing productive work or stuck waiting.\n",
        "\n",
        "---\n",
        "\n",
        "## Batch Size Selection\n",
        "\n",
        "### GPU Inference Batch Sizes\n",
        "\n",
        "<table style=\"width:100%; border-collapse: collapse;\">\n",
        "<tr style=\"background-color: #00897b; color: white;\">\n",
        "<th style=\"padding: 12px;\">GPU Memory</th>\n",
        "<th style=\"padding: 12px;\">Small Models<br>(ResNet50)</th>\n",
        "<th style=\"padding: 12px;\">Medium Models<br>(ResNet152)</th>\n",
        "<th style=\"padding: 12px;\">Large Models<br>(ViT-L)</th>\n",
        "</tr>\n",
        "<tr style=\"background-color: #e0f2f1;\">\n",
        "<td style=\"padding: 10px;\"><strong>16 GB</strong> (T4)</td>\n",
        "<td style=\"padding: 10px;\">128-256</td>\n",
        "<td style=\"padding: 10px;\">64-128</td>\n",
        "<td style=\"padding: 10px;\">32-64</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 10px;\"><strong>24 GB</strong> (A10G)</td>\n",
        "<td style=\"padding: 10px;\">256-512</td>\n",
        "<td style=\"padding: 10px;\">128-256</td>\n",
        "<td style=\"padding: 10px;\">64-128</td>\n",
        "</tr>\n",
        "<tr style=\"background-color: #e0f2f1;\">\n",
        "<td style=\"padding: 10px;\"><strong>40 GB</strong> (A100)</td>\n",
        "<td style=\"padding: 10px;\">512+</td>\n",
        "<td style=\"padding: 10px;\">256-512</td>\n",
        "<td style=\"padding: 10px;\">128-256</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "<div style=\"background-color: #ffe0b2; padding: 15px; border-left: 4px solid #FF9800; margin: 20px 0;\">\n",
        "<strong>Image Resolution Matters</strong><br>\n",
        "Values assume 224×224 resolution. For higher resolution:\n",
        "<ul>\n",
        "<li>448×448 → Divide by 4</li>\n",
        "<li>512×512 → Divide by 5</li>\n",
        "<li>1024×1024 → Divide by 16</li>\n",
        "</ul>\n",
        "</div>\n",
        "\n",
        "Start with the higher end of the range and reduce if you encounter out-of-memory errors.\n",
        "\n",
        "### CPU Inference Batch Sizes\n",
        "\n",
        "<table style=\"width:100%; border-collapse: collapse;\">\n",
        "<tr style=\"background-color: #5d4037; color: white;\">\n",
        "<th style=\"padding: 12px;\">Available RAM</th>\n",
        "<th style=\"padding: 12px;\">Recommended Batch Size</th>\n",
        "</tr>\n",
        "<tr style=\"background-color: #efebe9;\">\n",
        "<td style=\"padding: 10px;\">4-8 GB</td>\n",
        "<td style=\"padding: 10px;\">4-16</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 10px;\">8-16 GB</td>\n",
        "<td style=\"padding: 10px;\">16-32</td>\n",
        "</tr>\n",
        "<tr style=\"background-color: #efebe9;\">\n",
        "<td style=\"padding: 10px;\">16-32 GB</td>\n",
        "<td style=\"padding: 10px;\">32-64</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 10px;\">32+ GB</td>\n",
        "<td style=\"padding: 10px;\">64-128</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "RAM values refer to RAM per worker task. With high concurrency, each worker gets a fraction of total RAM.\n",
        "\n",
        "### Batch Size Tuning\n",
        "\n",
        "```\n",
        "Start with recommended batch size\n",
        "       │\n",
        "       ▼\n",
        "┌─────────────────┐\n",
        "│ Run and measure │\n",
        "│  memory usage   │\n",
        "└────────┬────────┘\n",
        "         │\n",
        "    ┌────┴────┐\n",
        "    │         │\n",
        "Memory <50%  Memory >85%\n",
        "    │         │\n",
        "    ├→ 2x    ├→ 0.5x\n",
        "    │  larger│  smaller\n",
        "    │         │\n",
        "    └─────────┴───→ Repeat until optimal\n",
        "```\n",
        "\n",
        "**Measure memory usage:**\n",
        "- GPU: `torch.cuda.max_memory_allocated()` or `nvidia-smi`\n",
        "- CPU: Ray Dashboard memory graphs\n",
        "\n",
        "Target 60-80% memory usage for optimal performance without OOM risk.\n",
        "\n",
        "---\n",
        "\n",
        "## Troubleshooting Guide\n",
        "\n",
        "### Symptom → Solution Matrix\n",
        "\n",
        "<table style=\"width:100%; border-collapse: collapse;\">\n",
        "<tr style=\"background-color: #d32f2f; color: white;\">\n",
        "<th style=\"padding: 12px; width: 25%;\">Symptom</th>\n",
        "<th style=\"padding: 12px; width: 25%;\">Root Cause</th>\n",
        "<th style=\"padding: 12px; width: 25%;\">Solution</th>\n",
        "<th style=\"padding: 12px; width: 25%;\">Verification</th>\n",
        "</tr>\n",
        "<tr style=\"background-color: #ffebee;\">\n",
        "<td style=\"padding: 10px;\">Low CPU (<50%)</td>\n",
        "<td style=\"padding: 10px;\">Not enough parallel tasks</td>\n",
        "<td style=\"padding: 10px;\"><strong>Decrease</strong> num_cpus<br>(1.0 → 0.5 → 0.25)</td>\n",
        "<td style=\"padding: 10px;\">CPU utilization >80%</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 10px;\">Workers killed</td>\n",
        "<td style=\"padding: 10px;\">Out of memory</td>\n",
        "<td style=\"padding: 10px;\"><strong>Increase</strong> num_cpus<br>(0.5 → 1.0 → 2.0)</td>\n",
        "<td style=\"padding: 10px;\">No more crashes</td>\n",
        "</tr>\n",
        "<tr style=\"background-color: #ffebee;\">\n",
        "<td style=\"padding: 10px;\">GPU OOM</td>\n",
        "<td style=\"padding: 10px;\">Batch too large</td>\n",
        "<td style=\"padding: 10px;\"><strong>Reduce</strong> batch_size<br>(64 → 32 → 16)</td>\n",
        "<td style=\"padding: 10px;\">No CUDA errors</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 10px;\">Pipeline stalls</td>\n",
        "<td style=\"padding: 10px;\">Materialization in pipeline</td>\n",
        "<td style=\"padding: 10px;\">Remove .count(), .schema()</td>\n",
        "<td style=\"padding: 10px;\">Continuous progress</td>\n",
        "</tr>\n",
        "<tr style=\"background-color: #ffebee;\">\n",
        "<td style=\"padding: 10px;\">One slow stage</td>\n",
        "<td style=\"padding: 10px;\">Imbalanced parallelism</td>\n",
        "<td style=\"padding: 10px;\">Adjust that stage's num_cpus</td>\n",
        "<td style=\"padding: 10px;\">Balanced progress bars</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 10px;\">Uneven progress</td>\n",
        "<td style=\"padding: 10px;\">Data skew</td>\n",
        "<td style=\"padding: 10px;\">Check data distribution</td>\n",
        "<td style=\"padding: 10px;\">Even task durations</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "### Memory Issue Decision Tree\n",
        "\n",
        "```\n",
        "Memory Error?\n",
        "    │\n",
        "    ├─ Workers killed\n",
        "    │      │\n",
        "    │      ├─ First: num_cpus↑ (1.0 → 2.0)\n",
        "    │      ├─ Still OOM: batch_size↓\n",
        "    │      └─ Still OOM: Block size↓ (128MB → 64MB)\n",
        "    │\n",
        "    ├─ \"Ray object store full\"\n",
        "    │      │\n",
        "    │      ├─ First: num_cpus↑ all stages\n",
        "    │      ├─ Still full: Override object store fraction\n",
        "    │      └─ Still full: Reduce pipeline width\n",
        "    │\n",
        "    └─ \"CUDA out of memory\"\n",
        "           │\n",
        "           ├─ First: batch_size↓ (64 → 32 → 16 → 8)\n",
        "           ├─ Still OOM: Separate CPU/GPU stages\n",
        "           └─ Still OOM: Enable mixed precision\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Common Patterns\n",
        "\n",
        "### Pattern Comparison\n",
        "\n",
        "<table style=\"width:100%; border-collapse: collapse;\">\n",
        "<tr style=\"background-color: #1976d2; color: white;\">\n",
        "<th style=\"padding: 12px;\">Pattern</th>\n",
        "<th style=\"padding: 12px;\">Use Case</th>\n",
        "<th style=\"padding: 12px;\">Key Parameters</th>\n",
        "</tr>\n",
        "<tr style=\"background-color: #e3f2fd;\">\n",
        "<td style=\"padding: 10px;\"><strong>Optimized Pipeline</strong></td>\n",
        "<td style=\"padding: 10px;\">General ETL</td>\n",
        "<td style=\"padding: 10px;\">\n",
        "• Read: num_cpus=0.025<br>\n",
        "• Transform: num_cpus=0.5<br>\n",
        "• Write: num_cpus=0.1\n",
        "</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 10px;\"><strong>GPU Inference</strong></td>\n",
        "<td style=\"padding: 10px;\">Deep learning models</td>\n",
        "<td style=\"padding: 10px;\">\n",
        "• num_gpus=1.0<br>\n",
        "• batch_size=64<br>\n",
        "• concurrency=# of GPUs\n",
        "</td>\n",
        "</tr>\n",
        "<tr style=\"background-color: #e3f2fd;\">\n",
        "<td style=\"padding: 10px;\"><strong>CPU Inference</strong></td>\n",
        "<td style=\"padding: 10px;\">CPU-only clusters</td>\n",
        "<td style=\"padding: 10px;\">\n",
        "• num_cpus=4.0<br>\n",
        "• batch_size=16<br>\n",
        "• concurrency=CPUs/4\n",
        "</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "### Code Examples\n",
        "\n",
        "```python\n",
        "# ETL Pattern\n",
        "ds = (ray.data.read_parquet(path, columns=cols, num_cpus=0.025)\n",
        "      .filter(condition, num_cpus=0.1)\n",
        "      .map_batches(transform, num_cpus=0.5)\n",
        "      .write_parquet(output, num_cpus=0.1))\n",
        "\n",
        "# GPU Inference Pattern\n",
        "class GPUModel:\n",
        "    def __init__(self): self.model = load_model().cuda()\n",
        "    def __call__(self, batch): return self.model(batch)\n",
        "\n",
        "ds = ds.map_batches(GPUModel, num_gpus=1.0, batch_size=64, concurrency=2)\n",
        "\n",
        "# CPU Inference Pattern\n",
        "class CPUModel:\n",
        "    def __init__(self): self.model = load_model()\n",
        "    def __call__(self, batch): return self.model(batch)\n",
        "\n",
        "ds = ds.map_batches(CPUModel, num_cpus=4.0, batch_size=16, concurrency=8)\n",
        "```\n",
        "\n",
        "Use class-based actors for model inference—`__init__` loads the model once per worker, and `__call__` processes each batch.\n",
        "\n",
        "---\n",
        "\n",
        "## Production Checklist\n",
        "\n",
        "<table style=\"width:100%; border-collapse: collapse;\">\n",
        "<tr style=\"background-color: #388e3c; color: white;\">\n",
        "<th style=\"padding: 12px; width: 70%;\">Item</th>\n",
        "<th style=\"padding: 12px; width: 30%;\">Status</th>\n",
        "</tr>\n",
        "<tr style=\"background-color: #e8f5e9;\">\n",
        "<td style=\"padding: 10px;\">Column pruning enabled (columns= parameter)</td>\n",
        "<td style=\"padding: 10px;\">☐ Complete</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 10px;\">Early filtering applied (after read)</td>\n",
        "<td style=\"padding: 10px;\">☐ Complete</td>\n",
        "</tr>\n",
        "<tr style=\"background-color: #e8f5e9;\">\n",
        "<td style=\"padding: 10px;\">Class-based actors for stateful ops</td>\n",
        "<td style=\"padding: 10px;\">☐ Complete</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 10px;\">num_cpus set for each stage</td>\n",
        "<td style=\"padding: 10px;\">☐ Complete</td>\n",
        "</tr>\n",
        "<tr style=\"background-color: #e8f5e9;\">\n",
        "<td style=\"padding: 10px;\">batch_size tested with production data</td>\n",
        "<td style=\"padding: 10px;\">☐ Complete</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 10px;\">Concurrency matches resources</td>\n",
        "<td style=\"padding: 10px;\">☐ Complete</td>\n",
        "</tr>\n",
        "<tr style=\"background-color: #e8f5e9;\">\n",
        "<td style=\"padding: 10px;\">Error handling configured</td>\n",
        "<td style=\"padding: 10px;\">☐ Complete</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 10px;\">Monitoring configured</td>\n",
        "<td style=\"padding: 10px;\">☐ Complete</td>\n",
        "</tr>\n",
        "<tr style=\"background-color: #e8f5e9;\">\n",
        "<td style=\"padding: 10px;\">Full data testing completed</td>\n",
        "<td style=\"padding: 10px;\">☐ Complete</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "**Key items:**\n",
        "\n",
        "- **Column pruning**: Read only needed columns to reduce I/O by 60-95%\n",
        "- **Early filtering**: Filter immediately after reading to reduce downstream data volume\n",
        "- **Class-based actors**: Use classes with `__init__` and `__call__` for stateful operations\n",
        "- **Explicit num_cpus**: Set for every stage based on operation type\n",
        "- **Test with production data**: Memory patterns change with data size\n",
        "- **Match concurrency to resources**: GPU: concurrency = # GPUs; CPU: concurrency = total CPUs / CPUs per actor\n",
        "- **Configure error handling**: Set `max_errored_blocks` for large datasets with quality issues\n",
        "- **Enable monitoring**: Disable progress bars but enable metrics collection for production\n",
        "\n",
        "---\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "<div style=\"background-color: #c8e6c9; padding: 20px; border-radius: 8px; margin: 20px 0;\">\n",
        "<h3 style=\"margin-top: 0;\">Remember These Seven Rules</h3>\n",
        "<ol>\n",
        "<li><strong>Start simple:</strong> Try num_cpus before anything else</li>\n",
        "<li><strong>Monitor first:</strong> Enable progress bars to see bottlenecks</li>\n",
        "<li><strong>One change at a time:</strong> Measure each optimization's impact</li>\n",
        "<li><strong>Column pruning:</strong> Specify only needed columns in read operations</li>\n",
        "<li><strong>Class-based pattern:</strong> Load models once per worker using __init__</li>\n",
        "<li><strong>Batch size matters:</strong> Start high and reduce if you hit OOM</li>\n",
        "<li><strong>CPU paradox:</strong> Lower num_cpus = MORE parallelism for I/O</li>\n",
        "</ol>\n",
        "</div>\n",
        "\n",
        "Optimization is iterative. As data grows or requirements change, revisit your configuration. Monitor performance continuously and measure the impact of changes—intuition about performance is often wrong.\n",
        "\n",
        "---\n",
        "\n",
        "**[← Back to Part 1](01-inference-fundamentals.md)** | **[Continue to Part 3 →](03-ray-data-architecture.md)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "You've learned advanced optimization techniques for batch inference. Continue to Part 3 to understand the Ray Data architecture that makes these optimizations possible.\n",
        "\n",
        "In Part 3, you'll learn:\n",
        "- How streaming execution enables unlimited dataset processing\n",
        "- How blocks and memory management affect your optimization choices\n",
        "- How operator fusion and backpressure work under the hood\n",
        "- How to calculate optimal parameters from architectural constraints\n",
        "\n",
        "---\n",
        "\n",
        "**[← Back to Part 1](01-inference-fundamentals.md)** | **[Return to Overview](README.md)** | **[Continue to Part 3 →](03-ray-data-architecture.md)**\n",
        "\n",
        "Or **[return to the overview](README.md)** to see all available parts.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
