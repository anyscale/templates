{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 3: Ray Data Architecture and Optimization\n",
        "\n",
        "**Time to complete**: 25 min | **Difficulty**: Advanced | **Prerequisites**: Complete Part 1 and Part 2\n",
        "\n",
        "**[← Back to Part 2](02-advanced-optimization.md)** | **[Return to Overview](README.md)**\n",
        "\n",
        "---\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "Understanding Ray Data's architecture is critical for making informed optimization decisions. In this part, you'll learn:\n",
        "\n",
        "1. How Ray Data's streaming execution model enables efficient batch inference\n",
        "2. How blocks and the object store affect memory management\n",
        "3. How operators and planning impact performance\n",
        "4. How resource management affects optimization strategies\n",
        "5. Why these architectural choices matter for your inference workloads\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Complete [Part 1: Inference Fundamentals](01-inference-fundamentals.md) and [Part 2: Advanced Optimization](02-advanced-optimization.md) before starting this part.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Streaming Execution Model](#streaming-execution-model)\n",
        "2. [Datasets and Blocks](#datasets-and-blocks)\n",
        "3. [Ray Memory Model](#ray-memory-model)\n",
        "4. [Operators and Planning](#operators-and-planning)\n",
        "5. [Resource Management](#resource-management-and-backpressure)\n",
        "6. [Optimization Implications](#optimization-implications)\n",
        "\n",
        "---\n",
        "\n",
        "## Streaming Execution Model\n",
        "\n",
        "### Why Streaming Execution Matters for Batch Inference\n",
        "\n",
        "Traditional batch processing loads entire datasets into memory before processing. For batch inference with 1M+ images or documents, this approach fails:\n",
        "\n",
        "**Traditional Batch Processing Problems:**\n",
        "- **Memory explosion**: Loading 1M images × 3MB each = 3TB memory required\n",
        "- **No pipeline parallelism**: Model loading, preprocessing, inference all sequential\n",
        "- **Long time to first result**: Wait for all data to load before any inference\n",
        "- **OOM errors**: Cluster runs out of memory frequently\n",
        "\n",
        "**Ray Data Streaming Execution Solution:**\n",
        "- **Constant memory**: Process 128MB blocks at a time, not full dataset\n",
        "- **Pipeline parallelism**: Load, preprocess, and infer simultaneously\n",
        "- **Fast time to first result**: Start inferring as soon as first block loads\n",
        "- **Automatic backpressure**: Prevents memory overflow dynamically\n",
        "\n",
        "| Characteristic | Traditional Batch | Ray Data Streaming | Advantage |\n",
        "|----------------|-------------------|-------------------|-----------|\n",
        "| **Memory Usage** | O(dataset size) | O(1) constant | 1000x less memory |\n",
        "| **Time to First Result** | Wait for full load | Immediate | 100x faster start |\n",
        "| **Pipeline Parallelism** | Sequential stages | All stages parallel | 3-5x throughput |\n",
        "| **Failure Recovery** | Restart from beginning | Resume from checkpoint | Minutes vs hours |\n",
        "\n",
        ":::note Why Streaming Matters\n",
        "For 1M images × 3MB each = 3TB dataset:\n",
        "- **Traditional**: Requires 3TB RAM cluster (expensive!)\n",
        "- **Ray Data**: Requires only 128MB × workers (affordable!)\n",
        "\n",
        "This architectural difference enables inference on datasets larger than cluster memory.\n",
        ":::\n",
        "\n",
        "### Visualizing the Difference\n",
        "\n",
        "**Traditional Batch Processing:**\n",
        "\n",
        "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/cko-2025-q1/batch-processing.png\" width=\"800\" alt=\"Traditional Batch Processing\">\n",
        "\n",
        "**Problems with traditional approach:**\n",
        "- x High memory - requires loading entire dataset\n",
        "- x No parallelism - stages run sequentially  \n",
        "- x Long latency - wait for complete load before processing\n",
        "- x Wasted resources - GPUs idle during load/write stages\n",
        "\n",
        "**Ray Data Streaming Execution:**\n",
        "\n",
        "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/cko-2025-q1/pipelining.png\" width=\"800\" alt=\"Ray Data Streaming Execution\">\n",
        "\n",
        "**Benefits of streaming execution:**\n",
        "- - Low memory - constant 128MB blocks regardless of dataset size\n",
        "- - Pipeline parallelism - all stages active simultaneously\n",
        "- - Fast first result - inference starts immediately\n",
        "- - Maximum throughput - all resources utilized continuously\n",
        "\n",
        "### How This Affects Your Optimization Decisions\n",
        "\n",
        "Understanding streaming execution helps you make better optimization choices:\n",
        "\n",
        "**1. Batch Size Selection:**\n",
        "- **Don't make batch_size too large**: Risk memory overflow\n",
        "- **Don't make batch_size too small**: Waste GPU capacity\n",
        "- **Sweet spot**: Match GPU memory and throughput needs\n",
        "\n",
        "**2. Concurrency Configuration:**\n",
        "- **Too many actors**: Backpressure kicks in, actors idle waiting for resources\n",
        "- **Too few actors**: Underutilized cluster, low throughput\n",
        "- **Optimal**: Match available GPUs and memory constraints\n",
        "\n",
        "**3. Model Loading Strategy:**\n",
        "- **Why actors work**: Model loads once, reused across many blocks\n",
        "- **Why tasks fail**: Model reloads for every block, massive overhead\n",
        "- **Architecture enables**: Stateful processing without memory bloat\n",
        "\n",
        "### Practical Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ray\n",
        "\n",
        "# Example: How streaming execution enables large-scale inference\n",
        "images = ray.data.read_images(\"s3://images/\", num_cpus=0.05)  # 1M images, 3TB total\n",
        "\n",
        "# This works even if cluster only has 64GB memory!\n",
        "# Why? Streaming execution processes 128MB blocks at a time\n",
        "results = images.map_batches(\n",
        "    InferenceModel,  # Loads once per actor\n",
        "    batch_size=32,   # Small batches prevent memory overflow\n",
        "    num_gpus=1,      # One model per GPU\n",
        "    concurrency=4    # 4 parallel actors\n",
        ")\n",
        "\n",
        "# As you iterate results, Ray Data:\n",
        "# 1. Loads blocks from S3 (streaming)\n",
        "# 2. Preprocesses in parallel (pipeline parallelism)\n",
        "# 3. Runs inference on GPUs (distributed)\n",
        "# 4. Writes results (continuous output)\n",
        "# All while maintaining constant memory footprint!\n",
        "\n",
        "for batch in results.iter_batches(batch_size=1000):\n",
        "    # First results available immediately\n",
        "    # Don't need to wait for all 1M images\n",
        "    process_results(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Datasets and Blocks\n",
        "\n",
        "### What Are Blocks?\n",
        "\n",
        "A **block** is the fundamental unit of data in Ray Data. Understanding blocks is essential for optimization.\n",
        "\n",
        "**Block characteristics:**\n",
        "- **Size**: Typically 128MB (configurable via `target_max_block_size`)\n",
        "- **Format**: Stored as PyArrow tables or pandas DataFrames\n",
        "- **Location**: Ray object store (shared memory)\n",
        "- **Processing unit**: One block = one task typically\n",
        "\n",
        "### Why Block Size Matters for Batch Inference\n",
        "\n",
        "Block size directly impacts inference performance:\n",
        "\n",
        "**Block Size Too Small (e.g., 1MB):**\n",
        "- x Too many tasks created (scheduling overhead)\n",
        "- x Poor GPU utilization (small batches)\n",
        "- x High network overhead (many small transfers)\n",
        "- x Scheduler bottleneck (managing thousands of tasks)\n",
        "\n",
        "**Block Size Too Large (e.g., 1GB):**\n",
        "- x Memory pressure (blocks don't fit in object store)\n",
        "- x Poor parallelism (few blocks = few parallel tasks)\n",
        "- x Spilling to disk (performance degradation)\n",
        "- x Uneven load balancing (some workers idle)\n",
        "\n",
        "**Optimal Block Size (128MB default):**\n",
        "- - Good parallelism (many blocks for distribution)\n",
        "- - Low overhead (reasonable number of tasks)\n",
        "- - Fits in memory (object store can hold multiple blocks)\n",
        "- - Efficient transfer (network overhead manageable)\n",
        "\n",
        "### Configuring Block Size for Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ray\n",
        "\n",
        "# Configure block size for your inference workload\n",
        "ctx = ray.data.DataContext.get_current()\n",
        "\n",
        "# Default: 128MB blocks\n",
        "print(f\"Default max block size: {ctx.target_max_block_size / 1024**2:.0f}MB\")\n",
        "\n",
        "# For image inference with large images:\n",
        "# Smaller blocks = more parallelism\n",
        "ctx.target_max_block_size = 64 * 1024**2  # 64MB blocks\n",
        "\n",
        "# For text inference with small documents:\n",
        "# Larger blocks = less overhead\n",
        "ctx.target_max_block_size = 256 * 1024**2  # 256MB blocks\n",
        "\n",
        "# Load images with configured block size\n",
        "images = ray.data.read_images(\"s3://images/\", num_cpus=0.05)\n",
        "\n",
        "print(f\"Dataset blocks: {images.num_blocks()}\")\n",
        "print(f\"Estimated blocks: {images.size_bytes() / ctx.target_max_block_size:.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How Blocks Flow Through Inference Pipeline\n",
        "\n",
        "Ray Data's block-based architecture enables parallelism at every stage:\n",
        "\n",
        "<img src=\"https://docs.ray.io/en/latest/_images/dataset-arch.svg\" width=\"700\" alt=\"Ray Data Block Architecture\">\n",
        "\n",
        "**Key insights:**\n",
        "- Each block contains a disjoint subset of rows\n",
        "- Blocks are processed in parallel across the cluster\n",
        "- Distributed object store enables efficient block transfer\n",
        "- Tasks operate on individual blocks for maximum parallelism\n",
        "\n",
        "---\n",
        "\n",
        "## Ray Memory Model\n",
        "\n",
        "### Object Store and Heap Memory\n",
        "\n",
        "Ray manages two types of memory that affect batch inference:\n",
        "\n",
        "<img src=\"https://docs.ray.io/en/latest/_images/memory.svg\" width=\"600\" alt=\"Ray Memory Model\">\n",
        "\n",
        "**1. Object Store Memory (30% of node memory by default):**\n",
        "- **Purpose**: Shared memory for passing data between tasks\n",
        "- **Contents**: Blocks (PyArrow tables), task outputs, intermediate results\n",
        "- **Optimization impact**: Determines how many blocks can be in-flight\n",
        "- **When full**: Triggers spilling to disk (major performance hit)\n",
        "\n",
        "**2. Heap Memory (70% of node memory by default):**\n",
        "- **Purpose**: Task execution, model loading, preprocessing\n",
        "- **Contents**: Loaded models, batch data being processed, Python objects\n",
        "- **Optimization impact**: Determines how many models fit in memory\n",
        "- **When full**: Python out-of-memory errors, task failures\n",
        "\n",
        "### How This Affects Batch Inference Optimization\n",
        "\n",
        "```\n",
        "Node Memory: 64GB\n",
        "├── Object Store (30% = 19GB)\n",
        "│   ├── Block 1 (128MB)\n",
        "│   ├── Block 2 (128MB)\n",
        "│   ├── ...\n",
        "│   └── Block N (up to ~148 blocks fit)\n",
        "│\n",
        "└── Heap Memory (70% = 45GB)\n",
        "    ├── Model weights (5GB per model)\n",
        "    ├── Batch preprocessing (2GB per actor)\n",
        "    ├── Python overhead (1GB)\n",
        "    └── Available for actors: ~37GB\n",
        "        → Can fit ~7 model actors at 5GB each\n",
        "```\n",
        "\n",
        "**Optimization implications:**\n",
        "\n",
        "**Object Store Pressure:**\n",
        "- **Symptom**: \"Object store full\" warnings in logs\n",
        "- **Cause**: Too many blocks generated too fast\n",
        "- **Solution**: Reduce `concurrency` or increase `batch_size`\n",
        "\n",
        "**Heap Memory Pressure:**\n",
        "- **Symptom**: Out-of-memory errors, task failures\n",
        "- **Cause**: Too many models loaded or batch_size too large\n",
        "- **Solution**: Reduce `concurrency` or `batch_size`\n",
        "\n",
        "### Practical Memory Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ray\n",
        "\n",
        "# Configure Ray Data to respect memory limits\n",
        "ctx = ray.data.DataContext.get_current()\n",
        "\n",
        "# Set object store memory limit for inference\n",
        "# Reserve 50% of object store (default) for Ray Data\n",
        "ctx.execution_options.resource_limits.object_store_memory = 10e9  # 10GB limit\n",
        "\n",
        "# This prevents Ray Data from overwhelming the object store\n",
        "# Automatically triggers backpressure when limit reached\n",
        "\n",
        "# Example: Conservative memory settings for large models\n",
        "images = ray.data.read_images(\"s3://images/\", num_cpus=0.05)\n",
        "\n",
        "results = images.map_batches(\n",
        "    LargeModelInference,  # 10GB model\n",
        "    batch_size=16,        # Small batches (GPU memory constraint)\n",
        "    num_gpus=1,\n",
        "    concurrency=2         # Only 2 models (heap memory constraint)\n",
        ")\n",
        "\n",
        "# Ray Data will:\n",
        "# - Backpressure image loading when object store fills\n",
        "# - Limit concurrent tasks to respect memory limits\n",
        "# - Automatically balance throughput vs memory usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zero-Copy Optimization\n",
        "\n",
        "Ray Data uses **zero-copy deserialization** for efficiency:\n",
        "\n",
        "**What it means:**\n",
        "- Blocks stored in object store are PyArrow tables\n",
        "- Accessing a block doesn't copy data - just creates a pointer\n",
        "- Multiple tasks can read same block without duplication\n",
        "\n",
        "**Why it matters for inference:**\n",
        "- **Memory efficiency**: 10 actors can share same preprocessed block\n",
        "- **Performance**: No serialization overhead between stages\n",
        "- **Scalability**: Enables high-throughput pipelines\n",
        "\n",
        "**Practical impact:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Without zero-copy (hypothetical):\n",
        "# Block in object store: 128MB\n",
        "# Actor 1 reads block: +128MB copy → 256MB total\n",
        "# Actor 2 reads block: +128MB copy → 384MB total\n",
        "# Result: 3x memory usage!\n",
        "\n",
        "# With zero-copy (Ray Data actual):\n",
        "# Block in object store: 128MB\n",
        "# Actor 1 reads block: 0MB copy (pointer) → 128MB total\n",
        "# Actor 2 reads block: 0MB copy (pointer) → 128MB total\n",
        "# Result: Constant memory!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Visual representation of object store usage:**\n",
        "\n",
        "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-data-deep-dive/producer-consumer-object-store-v2.png\" width=\"700\" alt=\"Object Store Data Flow\">\n",
        "\n",
        "---\n",
        "\n",
        "## Operators and Planning\n",
        "\n",
        "### Logical vs Physical Plans\n",
        "\n",
        "Ray Data transforms your code into an optimized execution plan:\n",
        "\n",
        "**Your Code:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = (\n",
        "    ray.data.read_images(\"s3://images/\")\n",
        "    .map_batches(preprocess_images)\n",
        "    .map_batches(InferenceModel, num_gpus=1)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Logical Plan (What to do):**\n",
        "```\n",
        "ReadFiles → MapBatches[preprocess] → MapBatches[inference]\n",
        "```\n",
        "\n",
        "**Physical Plan (How to do it):**\n",
        "```\n",
        "TaskPoolMapOperator[ReadFiles→preprocess→inference]\n",
        "```\n",
        "\n",
        "**Note the fusion:** Ray Data combined all three operations into a single operator!\n",
        "\n",
        "### Operator Fusion and Its Impact on Inference\n",
        "\n",
        "**Operator fusion** combines multiple operations into single tasks to reduce overhead.\n",
        "\n",
        "**Benefits for batch inference:**\n",
        "- **Reduced data movement**: Preprocessed images go straight to model (no object store roundtrip)\n",
        "- **Lower task overhead**: One task instead of three per block\n",
        "- **Better GPU utilization**: Continuous processing without gaps\n",
        "- **Memory efficiency**: Intermediate results stay in task memory\n",
        "\n",
        "**Example showing fusion:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ray\n",
        "from ray.data._internal.logical.optimizers import get_execution_plan\n",
        "\n",
        "# Create inference pipeline\n",
        "ds = (\n",
        "    ray.data.read_images(\"s3://images/\")\n",
        "    .map_batches(preprocess, batch_size=32)\n",
        "    .map_batches(InferenceModel, batch_size=32, num_gpus=1)\n",
        ")\n",
        "\n",
        "# Inspect the execution plan\n",
        "physical_plan = get_execution_plan(ds._plan._logical_plan)\n",
        "print(physical_plan.dag)\n",
        "\n",
        "# Output shows: TaskPoolMapOperator[ReadFiles->MapBatches->MapBatches]\n",
        "# All three operations fused into single operator!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**When fusion happens:**\n",
        "- Same compute configuration (both use tasks or both use actors)\n",
        "- Compatible batch sizes\n",
        "- Compatible resource requirements\n",
        "- No shuffle/repartition operations between them\n",
        "\n",
        "**When fusion doesn't happen:**\n",
        "- Different compute strategies (task vs actor)\n",
        "- Different resource requirements (CPU vs GPU)\n",
        "- Shuffle operations (groupby, sort, repartition)\n",
        "\n",
        "**Optimization strategy:**\n",
        "Keep preprocessing and inference configs compatible to enable fusion:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Good: Fusion enabled\n",
        "images.map_batches(preprocess, batch_size=32).map_batches(InferenceModel, batch_size=32, num_gpus=1)\n",
        "\n",
        "# Suboptimal: Fusion disabled (different batch sizes)\n",
        "images.map_batches(preprocess, batch_size=64).map_batches(InferenceModel, batch_size=32, num_gpus=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Resource Management and Backpressure\n",
        "\n",
        "### Dynamic Resource Allocation\n",
        "\n",
        "Ray Data automatically manages resources across operators to maximize throughput. Understanding this helps you set optimal parameters.\n",
        "\n",
        "**The Challenge:**\n",
        "- **Too aggressive loading**: Object store fills up, spilling to disk\n",
        "- **Too conservative loading**: GPUs idle waiting for data\n",
        "- **Unbalanced pipeline**: Some stages bottleneck while others wait\n",
        "\n",
        "**Ray Data's Solution:**\n",
        "Dynamically allocates resources based on operator throughput and backpressure policies.\n",
        "\n",
        "### Backpressure Mechanisms\n",
        "\n",
        "**1. Submission-Based Backpressure:**\n",
        "Prevents operators from submitting new tasks when resource budgets exceeded.\n",
        "\n",
        "**Example scenario:**\n",
        "```\n",
        "GPU inference slower than data loading\n",
        "↓\n",
        "Object store filling with preprocessed images\n",
        "↓\n",
        "Ray Data backpressures data loading\n",
        "↓\n",
        "Loading slows down to match inference throughput\n",
        "↓\n",
        "Balanced pipeline - no memory overflow\n",
        "```\n",
        "\n",
        "**2. Output-Based Backpressure:**\n",
        "Limits how many task outputs move to operator queues based on memory availability.\n",
        "\n",
        "**Practical impact on inference:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scenario: Fast preprocessing, slow inference\n",
        "\n",
        "images = ray.data.read_images(\"s3://images/\", num_cpus=0.05)\n",
        "\n",
        "results = images.map_batches(\n",
        "    fast_preprocess,     # Processes 1000 images/sec\n",
        "    batch_size=64,\n",
        "    concurrency=16        # Many parallel preprocessors\n",
        ").map_batches(\n",
        "    SlowInferenceModel,  # Processes 100 images/sec\n",
        "    batch_size=16,\n",
        "    num_gpus=1,\n",
        "    concurrency=2         # Only 2 GPUs available\n",
        ")\n",
        "\n",
        "# What Ray Data does automatically:\n",
        "# 1. Preprocessing generates blocks faster than inference consumes\n",
        "# 2. Object store starts filling with preprocessed blocks\n",
        "# 3. Backpressure kicks in - preprocessing tasks not scheduled\n",
        "# 4. Pipeline balances - preprocessing matches inference rate\n",
        "# 5. Memory stays constant - no overflow despite throughput mismatch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resource Budgets and Limits\n",
        "\n",
        "Ray Data allocates resources using reservation-based budgeting:\n",
        "\n",
        "**Default behavior:**\n",
        "- **50% reserved for outputs**: Ensures downstream operators have resources\n",
        "- **50% shared across operators**: Enables flexible allocation\n",
        "- **Dynamic adjustment**: Budgets change based on operator throughput\n",
        "\n",
        "**Configure limits for inference workloads:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ray\n",
        "\n",
        "ctx = ray.data.DataContext.get_current()\n",
        "\n",
        "# Set overall object store memory limit\n",
        "# Useful when running inference alongside other workloads\n",
        "ctx.execution_options.resource_limits.object_store_memory = 50e9  # 50GB\n",
        "\n",
        "# Set CPU limits\n",
        "# Useful to reserve CPUs for other processes\n",
        "ctx.execution_options.resource_limits.cpu = 32  # Use only 32 CPUs\n",
        "\n",
        "# Set GPU limits\n",
        "# Useful when sharing cluster with training jobs\n",
        "ctx.execution_options.resource_limits.gpu = 4  # Use only 4 GPUs\n",
        "\n",
        "# Exclude specific resources\n",
        "# Useful to reserve resources for head node or other services\n",
        "from ray.data import ExecutionResources\n",
        "ctx.execution_options.exclude_resources = ExecutionResources(cpu=4, gpu=1)\n",
        "\n",
        "print(\"Resource limits configured for batch inference\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Monitoring Resource Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enable Ray Data resource manager debug logging\n",
        "import os\n",
        "os.environ['RAY_DATA_DEBUG_RESOURCE_MANAGER'] = '1'\n",
        "\n",
        "# Run your inference pipeline\n",
        "results = images.map_batches(InferenceModel, num_gpus=1, concurrency=4)\n",
        "\n",
        "# You'll see output like:\n",
        "# [ResourceManager] Operator budgets:\n",
        "#   ReadImages: object_store_memory=5.0GB, cpu=8.0\n",
        "#   MapBatches: object_store_memory=5.0GB, cpu=0.0, gpu=4.0\n",
        "#   MapBatches: object_store_memory=10.0GB, cpu=0.0, gpu=0.0\n",
        "#\n",
        "# This shows:\n",
        "# - How resources are allocated across operators\n",
        "# - Where bottlenecks might occur\n",
        "# - If backpressure is active"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Optimization Implications\n",
        "\n",
        "### How Architecture Informs Optimization Decisions\n",
        "\n",
        "Understanding Ray Data's architecture helps you make better optimization choices:\n",
        "\n",
        "#### 1. Choosing Batch Size\n",
        "\n",
        "**Architectural considerations:**\n",
        "- **Block size**: Batch size should divide evenly into block size for efficiency\n",
        "- **GPU memory**: Batch must fit in GPU memory during inference\n",
        "- **Object store**: Preprocessed batches must fit in object store\n",
        "- **Throughput**: Larger batches = better GPU utilization (up to a point)\n",
        "\n",
        "**Decision framework:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate optimal batch size\n",
        "\n",
        "# Factor 1: GPU memory constraint\n",
        "gpu_memory_gb = 16  # Your GPU memory\n",
        "model_size_gb = 5\n",
        "batch_overhead_mb = 50  # Per sample\n",
        "max_batch_from_gpu = int((gpu_memory_gb - model_size_gb) * 1024 / batch_overhead_mb)\n",
        "\n",
        "# Factor 2: Block size alignment\n",
        "block_size_mb = 128\n",
        "samples_per_block = block_size_mb / 3  # 3MB per image\n",
        "ideal_batch_for_blocks = int(samples_per_block / 4)  # 4 batches per block\n",
        "\n",
        "# Factor 3: Throughput testing\n",
        "# Test different sizes: 16, 32, 64, 128\n",
        "# Choose largest that doesn't cause memory issues\n",
        "\n",
        "# Final choice: Minimum of all constraints\n",
        "optimal_batch_size = min(max_batch_from_gpu, ideal_batch_for_blocks, 128)\n",
        "print(f\"Optimal batch size: {optimal_batch_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Choosing Concurrency\n",
        "\n",
        "**Architectural considerations:**\n",
        "- **GPU count**: One actor per GPU maximum\n",
        "- **Memory per actor**: Model size + batch size determines how many actors fit\n",
        "- **Object store capacity**: More actors = more in-flight blocks\n",
        "- **CPU availability**: Preprocessing may need CPUs\n",
        "\n",
        "**Decision framework:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate optimal concurrency\n",
        "\n",
        "# Factor 1: GPU constraint\n",
        "num_gpus = 8  # Available GPUs\n",
        "max_concurrency_gpu = num_gpus  # One model per GPU\n",
        "\n",
        "# Factor 2: Memory constraint\n",
        "node_heap_memory_gb = 64 * 0.7  # 70% of 64GB node\n",
        "model_size_gb = 5\n",
        "batch_memory_gb = 2\n",
        "memory_per_actor = model_size_gb + batch_memory_gb\n",
        "max_concurrency_memory = int(node_heap_memory_gb / memory_per_actor)\n",
        "\n",
        "# Factor 3: Object store constraint\n",
        "object_store_gb = 64 * 0.3  # 30% of 64GB node\n",
        "block_size_gb = 0.128\n",
        "blocks_in_flight_per_actor = 2  # Preprocessing + inference\n",
        "required_object_store = concurrency * blocks_in_flight_per_actor * block_size_gb\n",
        "max_concurrency_object_store = int(object_store_gb / (blocks_in_flight_per_actor * block_size_gb))\n",
        "\n",
        "# Final choice: Minimum of all constraints\n",
        "optimal_concurrency = min(\n",
        "    max_concurrency_gpu,\n",
        "    max_concurrency_memory,\n",
        "    max_concurrency_object_store\n",
        ")\n",
        "\n",
        "print(f\"Optimal concurrency: {optimal_concurrency}\")\n",
        "print(f\"  GPU limit: {max_concurrency_gpu}\")\n",
        "print(f\"  Memory limit: {max_concurrency_memory}\")\n",
        "print(f\"  Object store limit: {max_concurrency_object_store}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Actor vs Task Decision\n",
        "\n",
        "**Architectural insight:** Actors are stateful, tasks are stateless.\n",
        "\n",
        "| Aspect | Tasks | Actors | Best For Inference |\n",
        "|--------|-------|--------|-------------------|\n",
        "| **Startup** | Launch per invocation | Launch once, reuse | - Actors (amortize model loading) |\n",
        "| **State** | Stateless | Stateful | - Actors (keep model in memory) |\n",
        "| **Resource overhead** | Low | Higher | Depends on model size |\n",
        "| **Scheduling overhead** | Higher (many tasks) | Lower (few actors) | - Actors (fewer scheduling decisions) |\n",
        "| **Memory** | Released after task | Held by actor | Tasks if memory-constrained |\n",
        "\n",
        "**For batch inference:** Almost always use actors because:\n",
        "- Model loading is expensive (2-5 seconds)\n",
        "- Models are large (500MB - 10GB)\n",
        "- Amortizing load cost across 1000s of batches is critical\n",
        "\n",
        "#### 4. Understanding Performance Bottlenecks\n",
        "\n",
        "**Use Ray Dashboard to identify architectural bottlenecks:**\n",
        "\n",
        "**Symptom 1: Low GPU utilization**\n",
        "- **Possible cause**: Object store full (loading backpressured)\n",
        "- **Solution**: Increase object store limit or reduce block size\n",
        "- **How to verify**: Check \"Ray Data Metrics (Object Store Memory)\"\n",
        "\n",
        "**Symptom 2: Spilling to disk**\n",
        "- **Possible cause**: Too many concurrent actors generating blocks\n",
        "- **Solution**: Reduce concurrency or increase batch_size\n",
        "- **How to verify**: Check \"Spilled\" metric in object store\n",
        "\n",
        "**Symptom 3: High task overhead**\n",
        "- **Possible cause**: Blocks too small, too many tasks\n",
        "- **Solution**: Increase target_max_block_size\n",
        "- **How to verify**: Check task count vs throughput\n",
        "\n",
        "**Symptom 4: Actors idle**\n",
        "- **Possible cause**: Upstream loading too slow\n",
        "- **Solution**: Increase num_cpus for read operation\n",
        "- **How to verify**: Check \"Ray Data Metrics (Inputs)\" throughput\n",
        "\n",
        "---\n",
        "\n",
        "## Key Takeaways: Architecture and Optimization\n",
        "\n",
        "### Critical Architecture Concepts\n",
        "\n",
        "**1. Streaming Execution:**\n",
        "- Enables processing datasets larger than cluster memory\n",
        "- Provides pipeline parallelism for maximum throughput\n",
        "- Makes batch inference scalable from 1K to 1B samples\n",
        "\n",
        "**2. Blocks:**\n",
        "- 128MB default size balances parallelism and overhead\n",
        "- More blocks = more parallelism (up to a point)\n",
        "- Block size affects GPU batch size and task count\n",
        "\n",
        "**3. Memory Model:**\n",
        "- Object store (30%) holds blocks and transfers\n",
        "- Heap memory (70%) runs tasks and loads models\n",
        "- Both limits constrain concurrency and batch size\n",
        "\n",
        "**4. Operator Fusion:**\n",
        "- Combines operations to reduce overhead\n",
        "- Keeps intermediate data in task memory\n",
        "- Improves throughput and reduces latency\n",
        "\n",
        "**5. Backpressure:**\n",
        "- Automatically balances pipeline stages\n",
        "- Prevents memory overflow\n",
        "- Maximizes throughput within resource constraints\n",
        "\n",
        "### Optimization Decision Framework\n",
        "\n",
        "Use this sample framework informed by Ray Data architecture (just an example, actual configuration will change for different types of applications):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Start with architectural constraints\n",
        "gpu_count = 8\n",
        "gpu_memory_gb = 16\n",
        "node_memory_gb = 64\n",
        "model_size_gb = 5\n",
        "\n",
        "# Step 2: Calculate concurrency from memory\n",
        "heap_memory = node_memory_gb * 0.7\n",
        "actors_fit = int(heap_memory / (model_size_gb + 2))  # +2GB for batches\n",
        "concurrency = min(gpu_count, actors_fit)\n",
        "\n",
        "# Step 3: Calculate batch size from GPU memory\n",
        "available_gpu_mem = gpu_memory_gb - model_size_gb\n",
        "sample_size_mb = 3  # Per image\n",
        "batch_size = int(available_gpu_mem * 1024 / sample_size_mb / 2)  # /2 for safety\n",
        "\n",
        "# Step 4: Configure block size for efficiency\n",
        "ctx = ray.data.DataContext.get_current()\n",
        "\n",
        "# Make blocks contain ~4 batches worth of data\n",
        "ctx.target_max_block_size = batch_size * sample_size_mb * 1024**2 * 4\n",
        "\n",
        "# Step 5: Run with optimal settings\n",
        "results = images.map_batches(\n",
        "    InferenceModel,\n",
        "    batch_size=batch_size,\n",
        "    num_gpus=1,\n",
        "    concurrency=concurrency\n",
        ")\n",
        "\n",
        "print(f\"Optimized configuration:\")\n",
        "print(f\"  Concurrency: {concurrency} (limited by {min(gpu_count, actors_fit)})\")\n",
        "print(f\"  Batch size: {batch_size}\")\n",
        "print(f\"  Block size: {ctx.target_max_block_size / 1024**2:.0f}MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Architecture-Aware Performance Tips\n",
        "\n",
        "**Tip 1: Align batch_size with block_size**\n",
        "\n",
        "Good configuration:\n",
        "- Block size: 128MB, batch_size: 32 (4 batches per block - clean division)\n",
        "\n",
        "Suboptimal configuration:\n",
        "- Block size: 128MB, batch_size: 50 (2.56 batches per block - awkward division)\n",
        "\n",
        "**Tip 2: Monitor object store, not just GPUs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU utilization high but throughput low?\n",
        "# Check object store - might be spilling to disk\n",
        "# Ray Dashboard → Metrics → Object Store Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Tip 3: Use fusion-friendly patterns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fusion-friendly: Same compute, compatible configs\n",
        "images.map_batches(prep, batch_size=32, num_cpus=1).map_batches(model, batch_size=32, num_gpus=1)\n",
        "\n",
        "# Fusion-incompatible: Different batch sizes  \n",
        "images.map_batches(prep, batch_size=64).map_batches(model, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Tip 4: Respect memory limits**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set limits based on architecture\n",
        "ctx.execution_options.resource_limits.object_store_memory = node_memory * 0.3 * 0.5\n",
        "\n",
        "# This leaves 50% object store for other workloads\n",
        "# Prevents OOM when running multiple jobs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Practical Architecture Examples\n",
        "\n",
        "### Example 1: Small Model, High Throughput\n",
        "\n",
        "**Scenario:** ResNet-50 (100MB model), process 1M images\n",
        "\n",
        "**Architectural analysis:**\n",
        "- **Model size**: Small (100MB) → Many actors fit in memory\n",
        "- **GPU memory**: 16GB → Large batches possible (128 images)\n",
        "- **Throughput goal**: Maximize images/second\n",
        "\n",
        "**Optimal configuration:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = images.map_batches(\n",
        "    ResNet50Model,\n",
        "    batch_size=128,    # Large batches for throughput\n",
        "    num_gpus=1,\n",
        "    concurrency=8      # Use all 8 GPUs\n",
        ")\n",
        "\n",
        "# Why this works:\n",
        "# - Heap memory: 64GB * 0.7 = 45GB\n",
        "# - Per actor: 0.1GB model + 1GB batch = 1.1GB\n",
        "# - Can fit: 45GB / 1.1GB = 40 actors\n",
        "# - Limited by: 8 GPUs → concurrency=8\n",
        "# - Object store: Minimal pressure (small batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2: Large Model, Memory-Constrained\n",
        "\n",
        "**Scenario:** LLaMA-70B (140GB model with quantization = 35GB), process 100K documents\n",
        "\n",
        "**Architectural analysis:**\n",
        "- **Model size**: Huge (35GB) → Very few actors fit\n",
        "- **GPU memory**: 80GB A100 → Moderate batches (16 documents)\n",
        "- **Memory goal**: Don't OOM\n",
        "\n",
        "**Optimal configuration:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = documents.map_batches(\n",
        "    LLaMA70BModel,\n",
        "    batch_size=16,     # Conservative for large model\n",
        "    num_gpus=1,\n",
        "    concurrency=2      # Only 2 models fit in cluster memory\n",
        ")\n",
        "\n",
        "# Why this works:\n",
        "# - Heap memory: 256GB * 0.7 = 179GB (multi-node)\n",
        "# - Per actor: 35GB model + 5GB batch = 40GB\n",
        "# - Can fit: 179GB / 40GB = 4 actors theoretical\n",
        "# - Use: 2 for safety margin (avoid OOM)\n",
        "# - Object store: Backpressure prevents overflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 3: Balanced Pipeline\n",
        "\n",
        "**Scenario:** BERT (500MB), preprocessing heavy (embedding generation)\n",
        "\n",
        "**Architectural analysis:**\n",
        "- **Preprocessing**: CPU-intensive (tokenization, embedding lookup)\n",
        "- **Inference**: GPU-intensive (transformer forward pass)\n",
        "- **Goal**: Balance both stages\n",
        "\n",
        "**Optimal configuration:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = (\n",
        "    documents\n",
        "    .map_batches(\n",
        "        heavy_preprocessing,\n",
        "        batch_size=64,      # CPU batches can be larger\n",
        "        num_cpus=2,         # Allocate CPUs for preprocessing\n",
        "        concurrency=16       # Many CPU workers\n",
        "    )\n",
        "    .map_batches(\n",
        "        BERTInference,\n",
        "        batch_size=32,      # GPU batch size\n",
        "        num_gpus=1,\n",
        "        concurrency=4        # 4 GPUs\n",
        "    )\n",
        ")\n",
        "\n",
        "# Why this works:\n",
        "# - Preprocessing: 16 workers × 2 CPUs = 32 CPUs used\n",
        "# - Inference: 4 workers × 1 GPU = 4 GPUs used\n",
        "# - Backpressure: Automatically balances if mismatch\n",
        "# - Fusion: Disabled (different compute), but that's okay - different resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up Ray resources\n",
        "if ray.is_initialized():\n",
        "    ray.shutdown()\n",
        "    print(\"Ray cluster shutdown complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary: Architecture Drives Optimization\n",
        "\n",
        "**Key Architectural Principles for Batch Inference:**\n",
        "\n",
        "1. **Streaming execution** enables unlimited dataset sizes with constant memory\n",
        "2. **Blocks** are the unit of parallelism - more blocks = more parallel tasks\n",
        "3. **Object store** holds blocks and transfers - capacity limits in-flight data\n",
        "4. **Heap memory** holds models and executions - limits concurrent actors\n",
        "5. **Operator fusion** reduces overhead - keep configs compatible\n",
        "6. **Backpressure** prevents overflow - trust Ray Data's automatic balancing\n",
        "\n",
        "**Optimization Strategy:**\n",
        "1. Start with memory constraints (heap and object store)\n",
        "2. Calculate maximum concurrency from memory limits\n",
        "3. Choose batch size for GPU utilization\n",
        "4. Configure block size for parallelism\n",
        "5. Monitor Ray Dashboard for bottlenecks\n",
        "6. Adjust based on observed behavior\n",
        "\n",
        "---\n",
        "\n",
        "**[← Back to Part 2](02-advanced-optimization.md)** | **[Return to Overview](README.md)**\n",
        "\n",
        "---\n",
        "\n",
        "*This architectural deep-dive completes the batch inference optimization series. You now understand not just how to optimize, but why specific optimizations work based on Ray Data's design.*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
