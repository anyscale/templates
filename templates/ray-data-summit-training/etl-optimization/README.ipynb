{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ETL Processing and Optimization With Ray Data\n",
        "\n",
        "**Time to complete**: 40 min | **Difficulty**: Intermediate | **Prerequisites**: ETL concepts, basic SQL knowledge, data processing experience\n",
        "\n",
        "## What you'll build\n",
        "\n",
        "Build comprehensive ETL pipelines using Ray Data's distributed processing capabilities, from foundational concepts with TPC-H benchmark to production-scale optimization techniques for enterprise data processing.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [ETL Fundamentals with TPC-H](#step-1-etl-fundamentals-with-tpc-h) (10 min)\n",
        "2. [Data Transformations and Processing](#step-2-data-transformations-and-processing) (12 min)\n",
        "3. [Performance Optimization Techniques](#step-3-performance-optimization-techniques) (10 min)\n",
        "4. [Large-Scale ETL Patterns](#step-4-large-scale-etl-patterns) (8 min)\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "**Why ETL optimization matters**: The difference between fast and slow data pipelines directly impacts business agility and operational costs. Understanding optimization techniques enables data teams to deliver insights faster while reducing infrastructure costs.\n",
        "\n",
        "**Ray Data's ETL capabilities**: Native operations for distributed processing that automatically optimize memory, CPU, and I/O utilization. You'll learn how Ray Data's architecture enables efficient processing of large datasets.\n",
        "\n",
        "**TPC-H benchmark patterns**: Learn ETL fundamentals using the TPC-H benchmark that simulates complex business environments with customers, orders, suppliers, and products.\n",
        "\n",
        "**Production optimization strategies**: Memory management, parallel processing, and resource configuration patterns for production ETL workloads that scale from gigabytes to petabytes.\n",
        "\n",
        "**Enterprise ETL patterns**: Techniques used by data engineering teams to process large datasets efficiently while maintaining data quality and performance.\n",
        "\n",
        "## Prerequisites Checklist\n",
        "\n",
        "Before starting, ensure you have:\n",
        "- Understanding of ETL (Extract, Transform, Load) concepts\n",
        "- Basic SQL knowledge for data transformations\n",
        "- Python experience with data processing\n",
        "- Familiarity with distributed computing concepts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick start (3 minutes)\n",
        "\n",
        "This section demonstrates ETL processing concepts using Ray Data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:21:59,528\tINFO worker.py:1771 -- Connecting to existing Ray cluster at address: 10.0.71.116:6379...\n",
            "2025-10-10 20:21:59,540\tINFO worker.py:1942 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-77uweunq3awbhqefvry4lwcqq5.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
            "2025-10-10 20:21:59,542\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_bfd427b63b81c2f4449778ffaca41253837f9946.zip' (0.16MiB) to Ray cluster...\n",
            "2025-10-10 20:21:59,543\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_bfd427b63b81c2f4449778ffaca41253837f9946.zip'.\n",
            "/home/ray/anaconda3/lib/python3.12/site-packages/ray/data/_internal/datasource/parquet_datasource.py:750: FutureWarning: The default `file_extensions` for `read_parquet` will change from `None` to ['parquet'] after Ray 2.43, and your dataset contains files that don't match the new `file_extensions`. To maintain backwards compatibility, set `file_extensions=None` explicitly.\n",
            "  warnings.warn(\n",
            "2025-10-10 20:21:59,787\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_308_0\n",
            "2025-10-10 20:21:59,808\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_308_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:21:59,809\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_308_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)] -> LimitOperator[limit=1]\n",
            "2025-10-10 20:21:59,816\tWARNING resource_manager.py:134 -- ⚠️  Ray's object store is configured to use only 27.9% of available memory (98.3GiB out of 352.0GiB total). For optimal Ray Data performance, we recommend setting the object store to at least 50% of available memory. You can do this by setting the 'object_store_memory' parameter when calling ray.init() or by setting the RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION environment variable.\n",
            "2025-10-10 20:22:05,051\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_308_0 execution finished in 5.24 seconds\n",
            "2025-10-10 20:22:05,058\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_310_0\n",
            "2025-10-10 20:22:05,067\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_310_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:22:05,068\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_310_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)->Project] -> AggregateNumRows[AggregateNumRows]\n",
            "2025-10-10 20:22:10,287\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_310_0 execution finished in 5.22 seconds\n",
            "2025-10-10 20:22:10,295\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_311_0\n",
            "2025-10-10 20:22:10,302\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_311_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:22:10,303\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_311_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)] -> LimitOperator[limit=1] -> TaskPoolMapOperator[Project]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded ETL sample dataset: 150000 records\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:22:16,948\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_311_0 execution finished in 6.64 seconds\n",
            "2025-10-10 20:22:16,953\tINFO dataset.py:3248 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n",
            "2025-10-10 20:22:16,955\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_312_0\n",
            "2025-10-10 20:22:16,964\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_312_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:22:16,965\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_312_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)] -> LimitOperator[limit=3] -> TaskPoolMapOperator[Project]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Schema: Column        Type\n",
            "------        ----\n",
            "c_custkey     int64\n",
            "c_name        string\n",
            "c_address     string\n",
            "c_nationkey   int64\n",
            "c_phone       string\n",
            "c_acctbal     double\n",
            "c_mktsegment  string\n",
            "c_comment     string\n",
            "\n",
            "Sample records:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:22:17,536\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_312_0 execution finished in 0.57 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  1. Customer 1: Customer#000000001 from BUILDING\n",
            "  2. Customer 2: Customer#000000002 from AUTOMOBILE\n",
            "  3. Customer 3: Customer#000000003 from AUTOMOBILE\n"
          ]
        }
      ],
      "source": [
        "from typing import Dict, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import ray\n",
        "from ray.data.expressions import col, lit\n",
        "\n",
        "from typing import Dict, Any, List\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import ray\n",
        "from ray.data.aggregate import Count, Mean, Sum, Max\n",
        "from ray.data.expressions import col, lit\n",
        "\n",
        "\n",
        "# Configure Ray Data for optimal performance monitoring\n",
        "ctx = ray.data.DataContext.get_current()\n",
        "ctx.enable_progress_bars = False\n",
        "ctx.enable_operator_progress_bars = False\n",
        "\n",
        "# Initialize Ray\n",
        "ray.init(ignore_reinit_error=True)\n",
        "\n",
        "# Load sample dataset for ETL demonstration\n",
        "sample_data = ray.data.read_parquet(\n",
        "    \"s3://ray-benchmark-data/tpch/parquet/sf1/customer\",\n",
        ")\n",
        "\n",
        "sample_data = sample_data.drop_columns([\"column8\"])\n",
        "sample_data = sample_data.rename_columns([\n",
        "    \"c_custkey\",\n",
        "    \"c_name\",\n",
        "    \"c_address\",\n",
        "    \"c_nationkey\",\n",
        "    \"c_phone\",\n",
        "    \"c_acctbal\",\n",
        "    \"c_mktsegment\",\n",
        "    \"c_comment\",\n",
        "    ])\n",
        "\n",
        "print(f\"Loaded ETL sample dataset: {sample_data.count()} records\")\n",
        "print(f\"Schema: {sample_data.schema()}\")\n",
        "print(\"\\nSample records:\")\n",
        "for i, record in enumerate(sample_data.take(3)):\n",
        "    print(f\"  {i+1}. Customer {record['c_custkey']}: {record['c_name']} from {record['c_mktsegment']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "**Challenge**: Traditional ETL tools struggle with modern data volumes and complexity. Processing large datasets can take significant time, creating bottlenecks in data-driven organizations.\n",
        "\n",
        "**Solution**: Ray Data's distributed architecture and optimized operations enable efficient processing of large datasets through parallel computation and native operations.\n",
        "\n",
        "**Impact**: Data engineering teams process terabytes of data daily using Ray Data's ETL capabilities. Companies transform raw data into analytics-ready datasets efficiently while maintaining data quality and performance.\n",
        "\n",
        "### ETL pipeline architecture\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────────────────────────────────┐\n",
        "│                    Ray Data ETL Pipeline                        │\n",
        "├─────────────────────────────────────────────────────────────────┤\n",
        "│                                                                 │\n",
        "│  Extract              Transform              Load               │\n",
        "│  ────────            ──────────            ──────              │\n",
        "│                                                                 │\n",
        "│  read_parquet()  →   map_batches()    →   write_parquet()     │\n",
        "│  (TPC-H Data)        (Business Logic)     (Data Warehouse)     │\n",
        "│                                                                 │\n",
        "│  ↓ Column Pruning    ↓ Filter/Join       ↓ Partitioning       │\n",
        "│  ↓ Parallel I/O      ↓ Aggregations      ↓ Compression        │\n",
        "│  ↓ High Concurrency  ↓ Enrichment        ↓ Schema Optimization│\n",
        "│                                                                 │\n",
        "└─────────────────────────────────────────────────────────────────┘\n",
        "\n",
        "Data Flow:\n",
        "  TPC-H Customer (150K) ─┐\n",
        "  TPC-H Orders (1.5M)   ─┼→ Join → Enrich → Aggregate → Warehouse\n",
        "  TPC-H LineItems (6M)  ─┘      ↓         ↓            ↓\n",
        "                            Filter    Transform    Partition\n",
        "```\n",
        "\n",
        "### ETL performance comparison\n",
        "\n",
        "| Approach | Data Loading | Transformations | Joins | Output | Use Case |\n",
        "|-----------|--------------|------------------|--------|----------|-----------|\n",
        "| **Traditional** | Sequential | Single-threaded | Memory-limited | Slow writes | Small datasets |\n",
        "| **Ray Data** | Parallel I/O | Distributed | Scalable | Optimized writes | Production scale |\n",
        "\n",
        "**Key advantages**:\n",
        "- **Parallel processing**: Distribute transformations across cluster nodes\n",
        "- **Memory efficiency**: Stream processing without materializing full datasets\n",
        "- **Native operations**: Optimized filter, join, and aggregate functions\n",
        "- **Scalability**: Handle datasets from gigabytes to petabytes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: ETL Fundamentals with TPC-H\n",
        "\n",
        "### Understanding TPC-H benchmark\n",
        "\n",
        "**What is TPC-H?**\n",
        "\n",
        "The TPC-H benchmark is used for testing database and data processing performance. It simulates a business environment with data relationships that represent business scenarios.\n",
        "\n",
        "**TPC-H Business Context**: The benchmark models a wholesale supplier managing customer orders, inventory, and supplier relationships - representing business data systems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TPC-H schema overview\n",
        "\n",
        "The TPC-H benchmark provides realistic business data for learning ETL patterns. Understanding the schema helps you apply these techniques to your own data.\n",
        "\n",
        "| Table | Description | Typical Size (SF10) | Primary Use |\n",
        "|-----------|------------------|--------------------------|------------------|\n",
        "| **CUSTOMER** | Customer master data | 1.5M rows | Dimensional analysis |\n",
        "| **ORDERS** | Order transactions | 15M rows | Fact table, time series |\n",
        "| **LINEITEM** | Order line items | 60M rows | Largest fact table |\n",
        "| **PART** | Product catalog | 2M rows | Product dimensions |\n",
        "| **SUPPLIER** | Supplier information | 100K rows | Supplier analytics |\n",
        "| **PARTSUPP** | Part-supplier links | 8M rows | Supply chain |\n",
        "| **NATION** | Geographic data | 25 rows | Geographic grouping |\n",
        "| **REGION** | Regional groups | 5 rows | High-level geography |\n",
        "\n",
        "**Schema relationships**:\n",
        "\n",
        "```\n",
        "CUSTOMER ──one-to-many──→ ORDERS ──one-to-many──→ LINEITEM\n",
        "                                                      ↓\n",
        "NATION ──one-to-many──→ SUPPLIER                   PART\n",
        "   ↓                        ↓                         ↓\n",
        "REGION                  PARTSUPP ←────many-to-one────┘\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TPC-H Schema (8 Tables):\n",
            "  CUSTOMER: Customer master data with demographics and market segments\n",
            "  ORDERS: Order header information with dates, priorities, and status\n",
            "  LINEITEM: Detailed line items for each order (largest table)\n",
            "  PART: Parts catalog with specifications and retail prices\n",
            "  SUPPLIER: Supplier information including contact details\n",
            "  PARTSUPP: Part-supplier relationships with costs\n",
            "  NATION: Nation reference data with geographic regions\n",
            "  REGION: Regional groupings for geographic analysis\n"
          ]
        }
      ],
      "source": [
        "# TPC-H Schema Overview for ETL Processing\n",
        "tpch_tables = {\n",
        "    \"customer\": \"Customer master data with demographics and market segments\",\n",
        "    \"orders\": \"Order header information with dates, priorities, and status\",\n",
        "    \"lineitem\": \"Detailed line items for each order (largest table)\",\n",
        "    \"part\": \"Parts catalog with specifications and retail prices\", \n",
        "    \"supplier\": \"Supplier information including contact details\",\n",
        "    \"partsupp\": \"Part-supplier relationships with costs\",\n",
        "    \"nation\": \"Nation reference data with geographic regions\",\n",
        "    \"region\": \"Regional groupings for geographic analysis\"\n",
        "}\n",
        "\n",
        "print(\"TPC-H Schema (8 Tables):\")\n",
        "for table, description in tpch_tables.items():\n",
        "    print(f\"  {table.upper()}: {description}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading TPC-H data with Ray Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:22:17,785\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_315_0\n",
            "2025-10-10 20:22:17,795\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_315_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:22:17,796\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_315_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)] -> LimitOperator[limit=1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading TPC-H benchmark data for distributed processing...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:22:18,587\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_315_0 execution finished in 0.79 seconds\n",
            "2025-10-10 20:22:18,604\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_319_0\n",
            "2025-10-10 20:22:18,611\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_319_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:22:18,612\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_319_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> LimitOperator[limit=1] -> TaskPoolMapOperator[Project]\n",
            "2025-10-10 20:22:20,823\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_319_0 execution finished in 2.21 seconds\n",
            "2025-10-10 20:22:20,837\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_323_0\n",
            "2025-10-10 20:22:20,847\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_323_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:22:20,848\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_323_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> LimitOperator[limit=1] -> TaskPoolMapOperator[Project]\n",
            "2025-10-10 20:22:23,870\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_323_0 execution finished in 3.02 seconds\n",
            "2025-10-10 20:22:23,877\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_325_0\n",
            "2025-10-10 20:22:23,885\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_325_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:22:23,886\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_325_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)->Project] -> AggregateNumRows[AggregateNumRows]\n",
            "2025-10-10 20:22:24,730\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_325_0 execution finished in 0.84 seconds\n",
            "2025-10-10 20:22:24,738\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_326_0\n",
            "2025-10-10 20:22:24,744\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_326_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:22:24,744\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_326_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[MapBatches(count_rows)]\n",
            "2025-10-10 20:22:25,051\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_326_0 execution finished in 0.31 seconds\n",
            "2025-10-10 20:22:25,057\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_327_0\n",
            "2025-10-10 20:22:25,063\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_327_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:22:25,064\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_327_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[MapBatches(count_rows)]\n",
            "2025-10-10 20:22:25,469\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_327_0 execution finished in 0.41 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TPC-H data loaded successfully in 6.10 seconds\n",
            "   Customers: 1,500,000\n",
            "   Orders: 15,000,000\n",
            "   Line items: 59,986,052\n",
            "   Total records: 76,486,052\n"
          ]
        }
      ],
      "source": [
        "# TPC-H benchmark data location\n",
        "TPCH_S3_PATH = \"s3://ray-benchmark-data/tpch/parquet/sf10\"\n",
        "\n",
        "print(\"Loading TPC-H benchmark data for distributed processing...\")\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    # Read TPC-H Customer Master Data\n",
        "    customers_ds = ray.data.read_parquet(\n",
        "        f\"{TPCH_S3_PATH}/customer\",\n",
        "        ray_remote_args={\"num_cpus\":0.25}\n",
        "    )\n",
        "    customers_ds = customers_ds.drop_columns([\"column8\"])\n",
        "    customers_ds = customers_ds.rename_columns([\n",
        "        \"c_custkey\",\n",
        "        \"c_name\",\n",
        "        \"c_address\",\n",
        "        \"c_nationkey\",\n",
        "        \"c_phone\",\n",
        "        \"c_acctbal\",\n",
        "        \"c_mktsegment\",\n",
        "        \"c_comment\",\n",
        "        ])\n",
        "    \n",
        "    # Read TPC-H Orders Data\n",
        "    orders_ds = ray.data.read_parquet(\n",
        "        f\"{TPCH_S3_PATH}/orders\", \n",
        "        ray_remote_args={\"num_cpus\":0.25}\n",
        "    )\n",
        "    orders_ds = (orders_ds\n",
        "        .select_columns([f\"column{i}\" for i in range(9)])\n",
        "        .rename_columns([\n",
        "            \"o_orderkey\",\n",
        "            \"o_custkey\",\n",
        "            \"o_orderstatus\",\n",
        "            \"o_totalprice\",\n",
        "            \"o_orderdate\",\n",
        "            \"o_orderpriority\",\n",
        "            \"o_clerk\",\n",
        "            \"o_shippriority\",\n",
        "            \"o_comment\",\n",
        "        ])\n",
        "    )\n",
        "    \n",
        "    # Read TPC-H Line Items (largest table)\n",
        "    lineitems_ds = ray.data.read_parquet(\n",
        "        f\"{TPCH_S3_PATH}/lineitem\",\n",
        "        ray_remote_args={\"num_cpus\":0.25}\n",
        "    )\n",
        "    lineitem_cols = [f\"column{str(i).zfill(2)}\" for i in range(16)]\n",
        "    lineitems_ds = (lineitems_ds\n",
        "        .select_columns(lineitem_cols)\n",
        "        .rename_columns([\n",
        "            \"l_orderkey\",\n",
        "            \"l_partkey\",\n",
        "            \"l_suppkey\",\n",
        "            \"l_linenumber\",\n",
        "            \"l_quantity\",\n",
        "            \"l_extendedprice\",\n",
        "            \"l_discount\",\n",
        "            \"l_tax\",\n",
        "            \"l_returnflag\",\n",
        "            \"l_linestatus\",\n",
        "            \"l_shipdate\",\n",
        "            \"l_commitdate\",\n",
        "            \"l_receiptdate\",\n",
        "            \"l_shipinstruct\",\n",
        "            \"l_shipmode\",\n",
        "            \"l_comment\",\n",
        "        ])\n",
        "    )\n",
        "    \n",
        "    load_time = time.time() - start_time\n",
        "    \n",
        "    # Count records in parallel\n",
        "    customer_count = customers_ds.count()\n",
        "    orders_count = orders_ds.count()\n",
        "    lineitems_count = lineitems_ds.count()\n",
        "    \n",
        "    print(f\"TPC-H data loaded successfully in {load_time:.2f} seconds\")\n",
        "    print(f\"   Customers: {customer_count:,}\")\n",
        "    print(f\"   Orders: {orders_count:,}\")\n",
        "    print(f\"   Line items: {lineitems_count:,}\")\n",
        "    print(f\"   Total records: {customer_count + orders_count + lineitems_count:,}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Failed to load TPC-H data: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Basic ETL transformations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:22:25,623\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_329_0\n",
            "2025-10-10 20:22:25,634\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_329_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:22:25,634\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_329_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)->Project] -> TaskPoolMapOperator[MapBatches(segment_customers)] -> AggregateNumRows[AggregateNumRows]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying customer segmentation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:22:26,877\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_329_0 execution finished in 1.24 seconds\n",
            "2025-10-10 20:22:26,887\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_331_0\n",
            "2025-10-10 20:22:26,896\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_331_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:22:26,897\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_331_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)->Project] -> TaskPoolMapOperator[MapBatches(segment_customers)] -> TaskPoolMapOperator[Filter(<expression>)] -> AggregateNumRows[AggregateNumRows]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Customer segmentation completed: 1,500,000 customers segmented\n",
            "Filtering high-value customers...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:22:28,473\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_331_0 execution finished in 1.58 seconds\n",
            "2025-10-10 20:22:28,480\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_332_0\n",
            "2025-10-10 20:22:28,489\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_332_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:22:28,490\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_332_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)->Project] -> TaskPoolMapOperator[MapBatches(segment_customers)] -> AggregateNumRows[AggregateNumRows]\n",
            "2025-10-10 20:22:29,739\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_332_0 execution finished in 1.25 seconds\n",
            "2025-10-10 20:22:29,749\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_335_0\n",
            "2025-10-10 20:22:29,759\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_335_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:22:29,760\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_335_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)->Project] -> TaskPoolMapOperator[MapBatches(segment_customers)] -> AllToAllOperator[Aggregate] -> LimitOperator[limit=10]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "High-value customers: 1,227,529 (81.8% of total)\n",
            "Customer Statistics by Market Segment:\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:22:31,286\tWARNING streaming_executor_state.py:793 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: c_mktsegment: string\n",
            "count(): int64\n",
            "mean(c_acctbal): double\n",
            "sum(c_acctbal): double\n",
            "max(c_acctbal): double, new schema: None. This may lead to unexpected behavior.\n",
            "\u001b[36m(reduce pid=75571, ip=10.0.93.34)\u001b[0m Failed to hash the schemas (for deduplication): unhashable type: 'dict'\n",
            "2025-10-10 20:22:31,314\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_335_0 execution finished in 1.55 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c_mktsegment  count()  mean(c_acctbal)  sum(c_acctbal)  max(c_acctbal)\n",
            "  AUTOMOBILE   300036      4496.230542    1.349031e+09         9999.96\n",
            "    BUILDING   300276      4505.869852    1.353005e+09         9999.99\n",
            "   FURNITURE   299496      4500.162798    1.347781e+09         9999.98\n",
            "   HOUSEHOLD   299751      4499.862741    1.348838e+09         9999.99\n",
            "   MACHINERY   300441      4492.427445    1.349709e+09         9999.96\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ETL Transform: Customer segmentation using Ray Data native operations\n",
        "def segment_customers(batch: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Apply business rules for customer segmentation.\n",
        "    \n",
        "    This demonstrates common ETL pattern of adding derived business attributes\n",
        "    based on rules and thresholds.\n",
        "    \n",
        "    Args:\n",
        "        batch: Pandas DataFrame with customer records\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with added customer_segment column\n",
        "    \"\"\"\n",
        "    # Business logic for customer segmentation based on account balance\n",
        "    batch['customer_segment'] = 'standard'\n",
        "    batch.loc[batch['c_acctbal'] > 5000, 'customer_segment'] = 'premium'\n",
        "    batch.loc[batch['c_acctbal'] > 10000, 'customer_segment'] = 'enterprise'\n",
        "    \n",
        "    return batch\n",
        "\n",
        "# Apply customer segmentation transformation\n",
        "print(\"Applying customer segmentation...\")\n",
        "\n",
        "try:\n",
        "    segmented_customers = customers_ds.map_batches(\n",
        "        segment_customers,\n",
        "        num_cpus=0.5,  # Medium complexity transformation\n",
        "        batch_format=\"pandas\"\n",
        "    )\n",
        "    \n",
        "    segment_count = segmented_customers.count()\n",
        "    print(f\"Customer segmentation completed: {segment_count:,} customers segmented\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Segmentation failed: {e}\")\n",
        "    raise\n",
        "\n",
        "# ETL Filter: High-value customers using expressions API\n",
        "print(\"Filtering high-value customers...\")\n",
        "\n",
        "try:\n",
        "    high_value_customers = segmented_customers.filter(\n",
        "        expr=\"c_acctbal > 1000\",\n",
        "        num_cpus=0.1\n",
        "    )\n",
        "    \n",
        "    high_value_count = high_value_customers.count()\n",
        "    total_count = segmented_customers.count()\n",
        "    percentage = (high_value_count / total_count) * 100 if total_count > 0 else 0\n",
        "    \n",
        "    print(f\"High-value customers: {high_value_count:,} ({percentage:.1f}% of total)\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error during filtering: {e}\")\n",
        "    raise\n",
        "\n",
        "# ETL Aggregation: Customer statistics by market segment\n",
        "customer_stats = segmented_customers.groupby(\"c_mktsegment\").aggregate(\n",
        "    Count(),\n",
        "    Mean(\"c_acctbal\"),\n",
        "    Sum(\"c_acctbal\"),\n",
        "    Max(\"c_acctbal\")\n",
        ")\n",
        "\n",
        "print(\"Customer Statistics by Market Segment:\")\n",
        "print(\"=\" * 70)\n",
        "# Display customer statistics\n",
        "stats_df = customer_stats.limit(10).to_pandas()\n",
        "print(stats_df.to_string(index=False))\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Data Transformations and Processing\n",
        "\n",
        "This section demonstrates how Ray Data handles common ETL transformation patterns including data enrichment, filtering, and complex business logic. You'll learn to build production-grade transformations that scale efficiently.\n",
        "\n",
        "### Why transformations are critical\n",
        "\n",
        "Data transformations convert raw data into business-valuable information. Common transformation patterns include:\n",
        "\n",
        "- **Enrichment**: Adding calculated fields and derived metrics\n",
        "- **Filtering**: Removing irrelevant or invalid records  \n",
        "- **Joins**: Combining data from multiple sources\n",
        "- **Aggregations**: Computing summary statistics and rollups\n",
        "- **Type conversions**: Ensuring correct data types for analytics\n",
        "\n",
        "### Transformation performance comparison\n",
        "\n",
        "| Transformation Type | Traditional Approach | Ray Data Approach | Scalability |\n",
        "|-------------------|---------------------|-------------------|--------------|\n",
        "| **Column calculations** | Row-by-row processing | Vectorized batches | Linear scaling |\n",
        "| **Date parsing** | Sequential parsing | Parallel batch parsing | High throughput |\n",
        "| **Categorization** | Conditional logic loops | Pandas vectorization | Efficient |\n",
        "| **Business rules** | Single-threaded | Distributed map_batches | Scales to cluster |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Complex data transformations\n",
        "\n",
        "\n",
        "<div style=\"margin:1em 0; padding:12px 16px; border-left:4px solid #2e7d32; background:#f1f8e9; border-radius:4px;\">\n",
        "\n",
        "  **GPU Acceleration for Pandas ETL Operations**: For complex pandas transformations in your ETL pipeline, you can use **NVIDIA RAPIDS cuDF** to accelerate DataFrame operations on GPUs. \n",
        "  \n",
        "  Replace `import pandas as pd` with `import cudf as pd` in your `map_batches` functions to use GPU acceleration for operations like datetime parsing, groupby, joins, and aggregations.\n",
        "\n",
        "**When to use cuDF**:\n",
        "- Complex datetime operations (parsing, extracting components)\n",
        "- Large aggregations and groupby operations\n",
        "- String operations on millions of rows\n",
        "- Join operations on large datasets\n",
        "- Statistical calculations across many columns\n",
        "\n",
        "**Performance benefit**: GPU-accelerated pandas operations can be 10-50x faster for large batches (1000+ rows) with complex transformations.\n",
        "\n",
        "**Requirements**: Add `cudf` to your dependencies and ensure GPU-enabled cluster nodes.\n",
        "\n",
        "**Before**\n",
        "\n",
        "```python\n",
        "def my_fnc(batch):\n",
        "    # Process batch with pandas operations here\n",
        "    res = ...\n",
        "    return res\n",
        "\n",
        "ds = ds.map_batches(my_fnc, format=\"pandas\")\n",
        "```\n",
        "\n",
        "**After**\n",
        "\n",
        "```python\n",
        "def my_fnc(batch):\n",
        "    batch = cudf.from_pandas(batch)\n",
        "    res = ...\n",
        "    return res\n",
        "\n",
        "ds = ds.map_batches(my_fnc, format=\"pandas\", num_gpus=1)\n",
        "```\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(reduce pid=77073, ip=10.0.109.213)\u001b[0m Failed to hash the schemas (for deduplication): unhashable type: 'dict'\n",
            "2025-10-10 20:22:31,441\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_337_0\n",
            "2025-10-10 20:22:31,451\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_337_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:22:31,452\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_337_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(enrich_orders_with_metrics)] -> AggregateNumRows[AggregateNumRows]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Enriching orders with business metrics...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:22:36,198\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_337_0 execution finished in 4.75 seconds\n",
            "2025-10-10 20:22:36,205\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_338_0\n",
            "2025-10-10 20:22:36,216\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_338_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:22:36,216\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_338_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(enrich_orders_with_metrics)] -> LimitOperator[limit=1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Order enrichment completed: 15,000,000 orders processed\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:22:38,806\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_338_0 execution finished in 2.59 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample enriched order:\n",
            "   Order ID: 4423681\n",
            "   Year: 1996, Quarter: 4\n",
            "   Revenue Tier: Medium\n",
            "   Is Large Order: False\n",
            "   Is Urgent: True\n"
          ]
        }
      ],
      "source": [
        "# ETL Transform: Order enrichment with business metrics\n",
        "def enrich_orders_with_metrics(batch):\n",
        "    \"\"\"Enrich orders with calculated business metrics.\n",
        "    \n",
        "    For GPU acceleration, replace 'import pandas as pd' with 'import cudf as pd'\n",
        "    to speed up complex DataFrame operations like datetime parsing and categorization.\n",
        "    \"\"\"\n",
        "    import pandas as pd  # or 'import cudf as pd' for GPU acceleration\n",
        "    df = pd.DataFrame(batch)\n",
        "    \n",
        "    # Parse order date and create time dimensions\n",
        "    # This datetime parsing is GPU-accelerated with cuDF\n",
        "    df['o_orderdate'] = pd.to_datetime(df['o_orderdate'])\n",
        "    df['order_year'] = df['o_orderdate'].dt.year\n",
        "    df['order_quarter'] = df['o_orderdate'].dt.quarter\n",
        "    df['order_month'] = df['o_orderdate'].dt.month\n",
        "    \n",
        "    # Business classifications\n",
        "    # These conditional operations are GPU-accelerated with cuDF\n",
        "    df['is_large_order'] = df['o_totalprice'] > 200000\n",
        "    df['is_urgent'] = df['o_orderpriority'].isin(['1-URGENT', '2-HIGH'])\n",
        "    df['revenue_tier'] = pd.cut(\n",
        "        df['o_totalprice'],\n",
        "        bins=[0, 50000, 150000, 300000, float('inf')],\n",
        "        labels=['Small', 'Medium', 'Large', 'Enterprise']\n",
        "    ).astype(str)  # Convert categorical to string for Ray Data compatibility\n",
        "    \n",
        "    return df\n",
        "    \n",
        "# Apply order enrichment\n",
        "print(\"\\nEnriching orders with business metrics...\")\n",
        "\n",
        "try:\n",
        "    enriched_orders = orders_ds.map_batches(\n",
        "        enrich_orders_with_metrics,\n",
        "        num_cpus=0.5,  # Medium complexity transformation\n",
        "        batch_format=\"pandas\"\n",
        "    )\n",
        "    \n",
        "    enriched_count = enriched_orders.count()\n",
        "    print(f\"Order enrichment completed: {enriched_count:,} orders processed\")\n",
        "    \n",
        "    # Show sample enriched record\n",
        "    sample = enriched_orders.take(1)[0]\n",
        "    print(f\"\\nSample enriched order:\")\n",
        "    print(f\"   Order ID: {sample.get('o_orderkey')}\")\n",
        "    print(f\"   Year: {sample.get('order_year')}, Quarter: {sample.get('order_quarter')}\")\n",
        "    print(f\"   Revenue Tier: {sample.get('revenue_tier')}\")\n",
        "    print(f\"   Is Large Order: {sample.get('is_large_order')}\")\n",
        "    print(f\"   Is Urgent: {sample.get('is_urgent')}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error during enrichment: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Advanced filtering and selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:22:38,933\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_342_0\n",
            "2025-10-10 20:22:38,943\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_342_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:22:38,944\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_342_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(enrich_orders_with_metrics)] -> TaskPoolMapOperator[Filter(<expression>)] -> AggregateNumRows[AggregateNumRows]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying advanced filtering techniques...\n",
            "Advanced filtering results:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:22:44,212\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_342_0 execution finished in 5.27 seconds\n",
            "2025-10-10 20:22:44,221\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_343_0\n",
            "2025-10-10 20:22:44,229\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_343_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:22:44,230\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_343_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(enrich_orders_with_metrics)] -> TaskPoolMapOperator[Filter(<expression>)] -> AggregateNumRows[AggregateNumRows]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Recent high-value orders: 2,176,683\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:22:49,489\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_343_0 execution finished in 5.26 seconds\n",
            "2025-10-10 20:22:49,497\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_344_0\n",
            "2025-10-10 20:22:49,505\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_344_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:22:49,506\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_344_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(enrich_orders_with_metrics)] -> TaskPoolMapOperator[Filter(<expression>)] -> AggregateNumRows[AggregateNumRows]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Enterprise orders: 854,969\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:22:54,715\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_344_0 execution finished in 5.21 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Complex filtered orders: 1,479,415\n"
          ]
        }
      ],
      "source": [
        "# Advanced filtering using Ray Data expressions API\n",
        "print(\"Applying advanced filtering techniques...\")\n",
        "\n",
        "recent_high_value_orders = enriched_orders.filter(\n",
        "expr=\"order_year >= 1995 and o_totalprice > 100000 and is_urgent\",\n",
        "num_cpus=0.1\n",
        ")\n",
        "\n",
        "enterprise_orders = enriched_orders.filter(\n",
        "expr=\"revenue_tier == 'Enterprise'\",\n",
        "num_cpus=0.1\n",
        ")\n",
        "\n",
        "complex_filtered_orders = enriched_orders.filter(\n",
        "expr=\"order_quarter == 4 and o_orderstatus == 'F' and o_totalprice > 50000\",\n",
        "num_cpus=0.1\n",
        ")\n",
        "\n",
        "print(\"Advanced filtering results:\")\n",
        "print(f\"  Recent high-value orders: {recent_high_value_orders.count():,}\")\n",
        "print(f\"  Enterprise orders: {enterprise_orders.count():,}\")\n",
        "print(f\"  Complex filtered orders: {complex_filtered_orders.count():,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:22:54,830\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_345_0\n",
            "2025-10-10 20:22:54,839\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_345_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:22:54,840\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_345_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(enrich_orders_with_metrics)] -> TaskPoolMapOperator[Filter(<expression>)] -> LimitOperator[limit=5]\n",
            "2025-10-10 20:22:57,743\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_345_0 execution finished in 2.90 seconds\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>o_orderkey</th>\n",
              "      <th>o_custkey</th>\n",
              "      <th>o_orderstatus</th>\n",
              "      <th>o_totalprice</th>\n",
              "      <th>o_orderdate</th>\n",
              "      <th>o_orderpriority</th>\n",
              "      <th>o_clerk</th>\n",
              "      <th>o_shippriority</th>\n",
              "      <th>o_comment</th>\n",
              "      <th>order_year</th>\n",
              "      <th>order_quarter</th>\n",
              "      <th>order_month</th>\n",
              "      <th>is_large_order</th>\n",
              "      <th>is_urgent</th>\n",
              "      <th>revenue_tier</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4423712</td>\n",
              "      <td>1115005</td>\n",
              "      <td>P</td>\n",
              "      <td>309802.49</td>\n",
              "      <td>1995-04-16</td>\n",
              "      <td>1-URGENT</td>\n",
              "      <td>Clerk#000009508</td>\n",
              "      <td>0</td>\n",
              "      <td>ounts. furiously bold accou</td>\n",
              "      <td>1995</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>Enterprise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4423719</td>\n",
              "      <td>773638</td>\n",
              "      <td>O</td>\n",
              "      <td>140131.93</td>\n",
              "      <td>1997-02-22</td>\n",
              "      <td>2-HIGH</td>\n",
              "      <td>Clerk#000000979</td>\n",
              "      <td>0</td>\n",
              "      <td>kages nag along the pending ideas. even, expre...</td>\n",
              "      <td>1997</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>Medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4423745</td>\n",
              "      <td>1477240</td>\n",
              "      <td>O</td>\n",
              "      <td>227181.93</td>\n",
              "      <td>1995-09-30</td>\n",
              "      <td>2-HIGH</td>\n",
              "      <td>Clerk#000001033</td>\n",
              "      <td>0</td>\n",
              "      <td>ly whithout the final deposits;</td>\n",
              "      <td>1995</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>Large</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4423748</td>\n",
              "      <td>528055</td>\n",
              "      <td>O</td>\n",
              "      <td>141626.74</td>\n",
              "      <td>1996-06-11</td>\n",
              "      <td>1-URGENT</td>\n",
              "      <td>Clerk#000000618</td>\n",
              "      <td>0</td>\n",
              "      <td>ly regular sentiments integrate unusual reques...</td>\n",
              "      <td>1996</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>Medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4423840</td>\n",
              "      <td>325541</td>\n",
              "      <td>O</td>\n",
              "      <td>154640.69</td>\n",
              "      <td>1996-04-28</td>\n",
              "      <td>2-HIGH</td>\n",
              "      <td>Clerk#000003311</td>\n",
              "      <td>0</td>\n",
              "      <td>de of the closely final pi</td>\n",
              "      <td>1996</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>Large</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   o_orderkey  o_custkey o_orderstatus  o_totalprice o_orderdate  \\\n",
              "0     4423712    1115005             P     309802.49  1995-04-16   \n",
              "1     4423719     773638             O     140131.93  1997-02-22   \n",
              "2     4423745    1477240             O     227181.93  1995-09-30   \n",
              "3     4423748     528055             O     141626.74  1996-06-11   \n",
              "4     4423840     325541             O     154640.69  1996-04-28   \n",
              "\n",
              "  o_orderpriority          o_clerk  o_shippriority  \\\n",
              "0        1-URGENT  Clerk#000009508               0   \n",
              "1          2-HIGH  Clerk#000000979               0   \n",
              "2          2-HIGH  Clerk#000001033               0   \n",
              "3        1-URGENT  Clerk#000000618               0   \n",
              "4          2-HIGH  Clerk#000003311               0   \n",
              "\n",
              "                                           o_comment  order_year  \\\n",
              "0                        ounts. furiously bold accou        1995   \n",
              "1  kages nag along the pending ideas. even, expre...        1997   \n",
              "2                    ly whithout the final deposits;        1995   \n",
              "3  ly regular sentiments integrate unusual reques...        1996   \n",
              "4                         de of the closely final pi        1996   \n",
              "\n",
              "   order_quarter  order_month  is_large_order  is_urgent revenue_tier  \n",
              "0              2            4            True       True   Enterprise  \n",
              "1              1            2           False       True       Medium  \n",
              "2              3            9            True       True        Large  \n",
              "3              2            6           False       True       Medium  \n",
              "4              2            4           False       True        Large  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "recent_high_value_orders.limit(5).to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:22:57,873\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_346_0\n",
            "2025-10-10 20:22:57,886\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_346_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:22:57,887\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_346_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(enrich_orders_with_metrics)] -> TaskPoolMapOperator[Filter(<expression>)] -> LimitOperator[limit=5]\n",
            "2025-10-10 20:23:00,786\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_346_0 execution finished in 2.90 seconds\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>o_orderkey</th>\n",
              "      <th>o_custkey</th>\n",
              "      <th>o_orderstatus</th>\n",
              "      <th>o_totalprice</th>\n",
              "      <th>o_orderdate</th>\n",
              "      <th>o_orderpriority</th>\n",
              "      <th>o_clerk</th>\n",
              "      <th>o_shippriority</th>\n",
              "      <th>o_comment</th>\n",
              "      <th>order_year</th>\n",
              "      <th>order_quarter</th>\n",
              "      <th>order_month</th>\n",
              "      <th>is_large_order</th>\n",
              "      <th>is_urgent</th>\n",
              "      <th>revenue_tier</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>58423751</td>\n",
              "      <td>649027</td>\n",
              "      <td>O</td>\n",
              "      <td>328280.64</td>\n",
              "      <td>1998-04-22</td>\n",
              "      <td>2-HIGH</td>\n",
              "      <td>Clerk#000008681</td>\n",
              "      <td>0</td>\n",
              "      <td>lly regular foxes. final, bold requests are da...</td>\n",
              "      <td>1998</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>Enterprise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>58423810</td>\n",
              "      <td>297619</td>\n",
              "      <td>F</td>\n",
              "      <td>330490.15</td>\n",
              "      <td>1992-02-02</td>\n",
              "      <td>4-NOT SPECIFIED</td>\n",
              "      <td>Clerk#000006006</td>\n",
              "      <td>0</td>\n",
              "      <td>pending, unusual deposits haggle? carefully r...</td>\n",
              "      <td>1992</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>Enterprise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>58423938</td>\n",
              "      <td>968440</td>\n",
              "      <td>F</td>\n",
              "      <td>309202.74</td>\n",
              "      <td>1992-05-10</td>\n",
              "      <td>3-MEDIUM</td>\n",
              "      <td>Clerk#000006188</td>\n",
              "      <td>0</td>\n",
              "      <td>ly special accounts haggle? fluf</td>\n",
              "      <td>1992</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>Enterprise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>58424166</td>\n",
              "      <td>1269979</td>\n",
              "      <td>O</td>\n",
              "      <td>318800.60</td>\n",
              "      <td>1996-01-04</td>\n",
              "      <td>5-LOW</td>\n",
              "      <td>Clerk#000004535</td>\n",
              "      <td>0</td>\n",
              "      <td>. blithely final platelets wake blithely!</td>\n",
              "      <td>1996</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>Enterprise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>58424423</td>\n",
              "      <td>652471</td>\n",
              "      <td>O</td>\n",
              "      <td>312001.92</td>\n",
              "      <td>1997-04-22</td>\n",
              "      <td>5-LOW</td>\n",
              "      <td>Clerk#000003204</td>\n",
              "      <td>0</td>\n",
              "      <td>ideas thrash never along the furiousl</td>\n",
              "      <td>1997</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>Enterprise</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   o_orderkey  o_custkey o_orderstatus  o_totalprice o_orderdate  \\\n",
              "0    58423751     649027             O     328280.64  1998-04-22   \n",
              "1    58423810     297619             F     330490.15  1992-02-02   \n",
              "2    58423938     968440             F     309202.74  1992-05-10   \n",
              "3    58424166    1269979             O     318800.60  1996-01-04   \n",
              "4    58424423     652471             O     312001.92  1997-04-22   \n",
              "\n",
              "   o_orderpriority          o_clerk  o_shippriority  \\\n",
              "0           2-HIGH  Clerk#000008681               0   \n",
              "1  4-NOT SPECIFIED  Clerk#000006006               0   \n",
              "2         3-MEDIUM  Clerk#000006188               0   \n",
              "3            5-LOW  Clerk#000004535               0   \n",
              "4            5-LOW  Clerk#000003204               0   \n",
              "\n",
              "                                           o_comment  order_year  \\\n",
              "0  lly regular foxes. final, bold requests are da...        1998   \n",
              "1   pending, unusual deposits haggle? carefully r...        1992   \n",
              "2                   ly special accounts haggle? fluf        1992   \n",
              "3         . blithely final platelets wake blithely!         1996   \n",
              "4              ideas thrash never along the furiousl        1997   \n",
              "\n",
              "   order_quarter  order_month  is_large_order  is_urgent revenue_tier  \n",
              "0              2            4            True       True   Enterprise  \n",
              "1              1            2            True      False   Enterprise  \n",
              "2              2            5            True      False   Enterprise  \n",
              "3              1            1            True      False   Enterprise  \n",
              "4              2            4            True      False   Enterprise  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "enterprise_orders.limit(5).to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:23:00,970\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_347_0\n",
            "2025-10-10 20:23:00,979\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_347_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:23:00,980\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_347_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(enrich_orders_with_metrics)] -> TaskPoolMapOperator[Filter(<expression>)] -> LimitOperator[limit=5]\n",
            "2025-10-10 20:23:03,887\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_347_0 execution finished in 2.91 seconds\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>o_orderkey</th>\n",
              "      <th>o_custkey</th>\n",
              "      <th>o_orderstatus</th>\n",
              "      <th>o_totalprice</th>\n",
              "      <th>o_orderdate</th>\n",
              "      <th>o_orderpriority</th>\n",
              "      <th>o_clerk</th>\n",
              "      <th>o_shippriority</th>\n",
              "      <th>o_comment</th>\n",
              "      <th>order_year</th>\n",
              "      <th>order_quarter</th>\n",
              "      <th>order_month</th>\n",
              "      <th>is_large_order</th>\n",
              "      <th>is_urgent</th>\n",
              "      <th>revenue_tier</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4423715</td>\n",
              "      <td>1391221</td>\n",
              "      <td>F</td>\n",
              "      <td>240515.93</td>\n",
              "      <td>1992-12-24</td>\n",
              "      <td>3-MEDIUM</td>\n",
              "      <td>Clerk#000002392</td>\n",
              "      <td>0</td>\n",
              "      <td>s; carefully bold packages solve slyly. specia...</td>\n",
              "      <td>1992</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>Large</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4423716</td>\n",
              "      <td>236656</td>\n",
              "      <td>F</td>\n",
              "      <td>182232.10</td>\n",
              "      <td>1992-11-02</td>\n",
              "      <td>5-LOW</td>\n",
              "      <td>Clerk#000006940</td>\n",
              "      <td>0</td>\n",
              "      <td>regular pinto beans. regula</td>\n",
              "      <td>1992</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>Large</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4423842</td>\n",
              "      <td>624805</td>\n",
              "      <td>F</td>\n",
              "      <td>212789.42</td>\n",
              "      <td>1994-12-26</td>\n",
              "      <td>5-LOW</td>\n",
              "      <td>Clerk#000009544</td>\n",
              "      <td>0</td>\n",
              "      <td>breach furiously. carefully regular patterns a...</td>\n",
              "      <td>1994</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>Large</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4423873</td>\n",
              "      <td>493880</td>\n",
              "      <td>F</td>\n",
              "      <td>184870.28</td>\n",
              "      <td>1992-10-28</td>\n",
              "      <td>1-URGENT</td>\n",
              "      <td>Clerk#000001600</td>\n",
              "      <td>0</td>\n",
              "      <td>sly unusual accounts play furiously across the...</td>\n",
              "      <td>1992</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>Large</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4423878</td>\n",
              "      <td>174965</td>\n",
              "      <td>F</td>\n",
              "      <td>64043.56</td>\n",
              "      <td>1994-11-18</td>\n",
              "      <td>1-URGENT</td>\n",
              "      <td>Clerk#000008651</td>\n",
              "      <td>0</td>\n",
              "      <td>ackages. fluffily ironic r</td>\n",
              "      <td>1994</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>Medium</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   o_orderkey  o_custkey o_orderstatus  o_totalprice o_orderdate  \\\n",
              "0     4423715    1391221             F     240515.93  1992-12-24   \n",
              "1     4423716     236656             F     182232.10  1992-11-02   \n",
              "2     4423842     624805             F     212789.42  1994-12-26   \n",
              "3     4423873     493880             F     184870.28  1992-10-28   \n",
              "4     4423878     174965             F      64043.56  1994-11-18   \n",
              "\n",
              "  o_orderpriority          o_clerk  o_shippriority  \\\n",
              "0        3-MEDIUM  Clerk#000002392               0   \n",
              "1           5-LOW  Clerk#000006940               0   \n",
              "2           5-LOW  Clerk#000009544               0   \n",
              "3        1-URGENT  Clerk#000001600               0   \n",
              "4        1-URGENT  Clerk#000008651               0   \n",
              "\n",
              "                                           o_comment  order_year  \\\n",
              "0  s; carefully bold packages solve slyly. specia...        1992   \n",
              "1                        regular pinto beans. regula        1992   \n",
              "2  breach furiously. carefully regular patterns a...        1994   \n",
              "3  sly unusual accounts play furiously across the...        1992   \n",
              "4                         ackages. fluffily ironic r        1994   \n",
              "\n",
              "   order_quarter  order_month  is_large_order  is_urgent revenue_tier  \n",
              "0              4           12            True      False        Large  \n",
              "1              4           11           False      False        Large  \n",
              "2              4           12            True      False        Large  \n",
              "3              4           10           False       True        Large  \n",
              "4              4           11           False       True       Medium  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "complex_filtered_orders.limit(5).to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data joins and relationships\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:23:04,007\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_349_0\n",
            "2025-10-10 20:23:04,031\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_349_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:23:04,032\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_349_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)->Project], InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(enrich_orders_with_metrics)] -> JoinOperatorWithPolars[Join(num_partitions=100)] -> AggregateNumRows[AggregateNumRows]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Performing distributed joins for customer-order analysis...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(HashShuffleAggregator pid=78838, ip=10.0.116.84)\u001b[0m Failed to hash the schemas (for deduplication): unhashable type: 'dict'\n",
            "2025-10-10 20:23:15,154\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_349_0 execution finished in 11.12 seconds\n",
            "2025-10-10 20:23:15,209\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_350_0\n",
            "2025-10-10 20:23:15,223\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_350_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:23:15,224\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_350_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)->Project] -> AggregateNumRows[AggregateNumRows]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Customer-order join completed: 15,000,000 records\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:23:16,205\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_350_0 execution finished in 0.98 seconds\n",
            "2025-10-10 20:23:16,214\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_351_0\n",
            "2025-10-10 20:23:16,223\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_351_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:23:16,224\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_351_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(enrich_orders_with_metrics)] -> AggregateNumRows[AggregateNumRows]\n",
            "2025-10-10 20:23:21,066\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_351_0 execution finished in 4.84 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Input: 1,500,000 customers, 15,000,000 orders\n",
            "   Join ratio: 100.0% of orders matched\n"
          ]
        }
      ],
      "source": [
        "# ETL Join: Customer-Order analysis using Ray Data joins\n",
        "print(\"\\nPerforming distributed joins for customer-order analysis...\")\n",
        "\n",
        "try:\n",
        "    # Join customers with their orders for comprehensive analysis\n",
        "    # Ray Data optimizes join execution across distributed nodes\n",
        "    customer_order_analysis = customers_ds.join(\n",
        "        enriched_orders,\n",
        "        on=(\"c_custkey\",),\n",
        "        right_on=(\"o_custkey\",),\n",
        "        join_type=\"inner\",\n",
        "        num_partitions=100\n",
        "    )\n",
        "    \n",
        "    join_count = customer_order_analysis.count()\n",
        "    print(f\"Customer-order join completed: {join_count:,} records\")\n",
        "    \n",
        "    # Calculate join statistics\n",
        "    customer_count = customers_ds.count()\n",
        "    orders_count = enriched_orders.count()\n",
        "    join_ratio = (join_count / orders_count) * 100 if orders_count > 0 else 0\n",
        "    \n",
        "    print(f\"   Input: {customer_count:,} customers, {orders_count:,} orders\")\n",
        "    print(f\"   Join ratio: {join_ratio:.1f}% of orders matched\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error during join: {e}\")\n",
        "    raise\n",
        "\n",
        "# Aggregate customer order metrics\n",
        "customer_order_metrics = customer_order_analysis.groupby(\"c_mktsegment\").aggregate(\n",
        "    Count(),\n",
        "    Mean(\"o_totalprice\"),\n",
        "    Sum(\"o_totalprice\"),\n",
        "    Count(\"o_orderkey\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:23:21,275\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_354_0\n",
            "2025-10-10 20:23:21,295\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_354_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:23:21,296\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_354_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)->Project], InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(enrich_orders_with_metrics)] -> JoinOperatorWithPolars[Join(num_partitions=100)] -> AllToAllOperator[Aggregate] -> LimitOperator[limit=5]\n",
            "\u001b[36m(HashShuffleAggregator pid=82352, ip=10.0.99.160)\u001b[0m Failed to hash the schemas (for deduplication): unhashable type: 'dict'\n",
            "2025-10-10 20:23:36,024\tWARNING streaming_executor_state.py:793 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: c_mktsegment: string\n",
            "count(): int64\n",
            "mean(o_totalprice): double\n",
            "sum(o_totalprice): double\n",
            "count(o_orderkey): int64, new schema: None. This may lead to unexpected behavior.\n",
            "2025-10-10 20:23:36,179\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_354_0 execution finished in 14.88 seconds\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c_mktsegment</th>\n",
              "      <th>count()</th>\n",
              "      <th>mean(o_totalprice)</th>\n",
              "      <th>sum(o_totalprice)</th>\n",
              "      <th>count(o_orderkey)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AUTOMOBILE</td>\n",
              "      <td>3000540</td>\n",
              "      <td>151096.214697</td>\n",
              "      <td>4.533702e+11</td>\n",
              "      <td>3000540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BUILDING</td>\n",
              "      <td>3004382</td>\n",
              "      <td>151053.909702</td>\n",
              "      <td>4.538236e+11</td>\n",
              "      <td>3004382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>FURNITURE</td>\n",
              "      <td>3001268</td>\n",
              "      <td>151022.834817</td>\n",
              "      <td>4.532600e+11</td>\n",
              "      <td>3001268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HOUSEHOLD</td>\n",
              "      <td>2990828</td>\n",
              "      <td>151207.419625</td>\n",
              "      <td>4.522354e+11</td>\n",
              "      <td>2990828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>MACHINERY</td>\n",
              "      <td>3002982</td>\n",
              "      <td>151052.827336</td>\n",
              "      <td>4.536089e+11</td>\n",
              "      <td>3002982</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  c_mktsegment  count()  mean(o_totalprice)  sum(o_totalprice)  \\\n",
              "0   AUTOMOBILE  3000540       151096.214697       4.533702e+11   \n",
              "1     BUILDING  3004382       151053.909702       4.538236e+11   \n",
              "2    FURNITURE  3001268       151022.834817       4.532600e+11   \n",
              "3    HOUSEHOLD  2990828       151207.419625       4.522354e+11   \n",
              "4    MACHINERY  3002982       151052.827336       4.536089e+11   \n",
              "\n",
              "   count(o_orderkey)  \n",
              "0            3000540  \n",
              "1            3004382  \n",
              "2            3001268  \n",
              "3            2990828  \n",
              "4            3002982  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "customer_order_metrics.limit(5).to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Performance Optimization Techniques\n",
        "\n",
        "This section covers advanced optimization techniques for production ETL workloads.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuring Ray Data for ETL optimization...\n",
            "Ray Data configured for optimal ETL performance\n"
          ]
        }
      ],
      "source": [
        "# Configure Ray Data for optimal ETL performance\n",
        "print(\"Configuring Ray Data for ETL optimization...\")\n",
        "\n",
        "# Memory optimization for large datasets\n",
        "ctx.target_max_block_size = 128 * 1024 * 1024  # 128 MB blocks\n",
        "ctx.eager_free = True  # Aggressive memory cleanup\n",
        "\n",
        "# Enable performance monitoring\n",
        "ctx.enable_auto_log_stats = True\n",
        "ctx.memory_usage_poll_interval_s = 5.0\n",
        "\n",
        "print(\"Ray Data configured for optimal ETL performance\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Batch size and concurrency optimization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:23:36,440\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_356_0\n",
            "2025-10-10 20:23:36,450\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_356_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:23:36,451\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_356_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(enrich_orders_with_metrics)] -> TaskPoolMapOperator[MapBatches(memory_intensive_etl)] -> AggregateNumRows[AggregateNumRows]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing ETL batch size optimization...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:23:48,070\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_356_0 execution finished in 11.62 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memory-optimized processing: 15,000,000 records\n"
          ]
        }
      ],
      "source": [
        "import uuid\n",
        "from datetime import datetime\n",
        "\n",
        "# Demonstrate different batch size strategies for ETL operations\n",
        "print(\"Testing ETL batch size optimization...\")\n",
        "\n",
        "# Small batch processing for memory-constrained operations\n",
        "def memory_intensive_etl(batch):\n",
        "    \"\"\"Memory-intensive ETL transformation.\"\"\"\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    df = pd.DataFrame(batch)\n",
        "    \n",
        "    # Simulate memory-intensive operations\n",
        "    df['complex_metric'] = df['o_totalprice'] * np.log(df['o_totalprice'] + 1)\n",
        "    df['percentile_rank'] = df['o_totalprice'].rank(pct=True)\n",
        "    \n",
        "    return df \n",
        "\n",
        "# Apply with optimized batch size for memory management\n",
        "memory_optimized_orders = enriched_orders.map_batches(\n",
        "    memory_intensive_etl,\n",
        "    num_cpus=1.0,  # Fewer concurrent tasks for memory management\n",
        "    batch_size=500,  # Smaller batches for memory efficiency\n",
        "    batch_format=\"pandas\"\n",
        ")\n",
        "\n",
        "print(f\"Memory-optimized processing: {memory_optimized_orders.count():,} records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:23:48,185\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_357_0\n",
            "2025-10-10 20:23:48,196\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_357_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:23:48,197\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_357_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(enrich_orders_with_metrics)] -> TaskPoolMapOperator[MapBatches(memory_intensive_etl)] -> LimitOperator[limit=5]\n",
            "2025-10-10 20:23:56,723\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_357_0 execution finished in 8.53 seconds\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>o_orderkey</th>\n",
              "      <th>o_custkey</th>\n",
              "      <th>o_orderstatus</th>\n",
              "      <th>o_totalprice</th>\n",
              "      <th>o_orderdate</th>\n",
              "      <th>o_orderpriority</th>\n",
              "      <th>o_clerk</th>\n",
              "      <th>o_shippriority</th>\n",
              "      <th>o_comment</th>\n",
              "      <th>order_year</th>\n",
              "      <th>order_quarter</th>\n",
              "      <th>order_month</th>\n",
              "      <th>is_large_order</th>\n",
              "      <th>is_urgent</th>\n",
              "      <th>revenue_tier</th>\n",
              "      <th>complex_metric</th>\n",
              "      <th>percentile_rank</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28423681</td>\n",
              "      <td>1067063</td>\n",
              "      <td>F</td>\n",
              "      <td>163555.68</td>\n",
              "      <td>1992-12-14</td>\n",
              "      <td>2-HIGH</td>\n",
              "      <td>Clerk#000001449</td>\n",
              "      <td>0</td>\n",
              "      <td>es since the quickly final requests haggle</td>\n",
              "      <td>1992</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>Large</td>\n",
              "      <td>1.963472e+06</td>\n",
              "      <td>0.570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>28423682</td>\n",
              "      <td>122650</td>\n",
              "      <td>F</td>\n",
              "      <td>208484.81</td>\n",
              "      <td>1994-06-11</td>\n",
              "      <td>4-NOT SPECIFIED</td>\n",
              "      <td>Clerk#000002316</td>\n",
              "      <td>0</td>\n",
              "      <td>al instructions. even deposits detect carefull...</td>\n",
              "      <td>1994</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>Large</td>\n",
              "      <td>2.553444e+06</td>\n",
              "      <td>0.712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>28423683</td>\n",
              "      <td>372205</td>\n",
              "      <td>O</td>\n",
              "      <td>113902.82</td>\n",
              "      <td>1998-07-19</td>\n",
              "      <td>3-MEDIUM</td>\n",
              "      <td>Clerk#000001667</td>\n",
              "      <td>0</td>\n",
              "      <td>g furiously even de</td>\n",
              "      <td>1998</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>Medium</td>\n",
              "      <td>1.326183e+06</td>\n",
              "      <td>0.390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>28423684</td>\n",
              "      <td>638413</td>\n",
              "      <td>O</td>\n",
              "      <td>121829.88</td>\n",
              "      <td>1996-06-17</td>\n",
              "      <td>3-MEDIUM</td>\n",
              "      <td>Clerk#000006284</td>\n",
              "      <td>0</td>\n",
              "      <td>ve the blithely ironic deposi</td>\n",
              "      <td>1996</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>Medium</td>\n",
              "      <td>1.426675e+06</td>\n",
              "      <td>0.412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28423685</td>\n",
              "      <td>305218</td>\n",
              "      <td>O</td>\n",
              "      <td>148385.59</td>\n",
              "      <td>1996-11-09</td>\n",
              "      <td>4-NOT SPECIFIED</td>\n",
              "      <td>Clerk#000008442</td>\n",
              "      <td>0</td>\n",
              "      <td>ns serve slyly against the b</td>\n",
              "      <td>1996</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>Medium</td>\n",
              "      <td>1.766913e+06</td>\n",
              "      <td>0.518</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   o_orderkey  o_custkey o_orderstatus  o_totalprice o_orderdate  \\\n",
              "0    28423681    1067063             F     163555.68  1992-12-14   \n",
              "1    28423682     122650             F     208484.81  1994-06-11   \n",
              "2    28423683     372205             O     113902.82  1998-07-19   \n",
              "3    28423684     638413             O     121829.88  1996-06-17   \n",
              "4    28423685     305218             O     148385.59  1996-11-09   \n",
              "\n",
              "   o_orderpriority          o_clerk  o_shippriority  \\\n",
              "0           2-HIGH  Clerk#000001449               0   \n",
              "1  4-NOT SPECIFIED  Clerk#000002316               0   \n",
              "2         3-MEDIUM  Clerk#000001667               0   \n",
              "3         3-MEDIUM  Clerk#000006284               0   \n",
              "4  4-NOT SPECIFIED  Clerk#000008442               0   \n",
              "\n",
              "                                           o_comment  order_year  \\\n",
              "0        es since the quickly final requests haggle         1992   \n",
              "1  al instructions. even deposits detect carefull...        1994   \n",
              "2                                g furiously even de        1998   \n",
              "3                      ve the blithely ironic deposi        1996   \n",
              "4                       ns serve slyly against the b        1996   \n",
              "\n",
              "   order_quarter  order_month  is_large_order  is_urgent revenue_tier  \\\n",
              "0              4           12           False       True        Large   \n",
              "1              2            6            True      False        Large   \n",
              "2              3            7           False      False       Medium   \n",
              "3              2            6           False      False       Medium   \n",
              "4              4           11           False      False       Medium   \n",
              "\n",
              "   complex_metric  percentile_rank  \n",
              "0    1.963472e+06            0.570  \n",
              "1    2.553444e+06            0.712  \n",
              "2    1.326183e+06            0.390  \n",
              "3    1.426675e+06            0.412  \n",
              "4    1.766913e+06            0.518  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory_optimized_orders.limit(5).to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:23:56,873\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_359_0\n",
            "2025-10-10 20:23:56,882\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_359_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:23:56,883\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_359_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(enrich_orders_with_metrics)] -> TaskPoolMapOperator[MapBatches(io_intensive_etl)] -> AggregateNumRows[AggregateNumRows]\n",
            "2025-10-10 20:24:29,266\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_359_0 execution finished in 32.38 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I/O-optimized processing: 15,000,000 records\n",
            "Batch files written to: /mnt/cluster_storage/temp_etl_batches/\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "output_dir = \"/mnt/cluster_storage/temp_etl_batches\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "def io_intensive_etl(batch):\n",
        "    \"\"\"I/O-intensive ETL transformation with actual disk writes.\"\"\"\n",
        "    import pandas as pd\n",
        "    from datetime import datetime\n",
        "    import uuid\n",
        "    \n",
        "    df = pd.DataFrame(batch)\n",
        "    \n",
        "    # Add processing metadata\n",
        "    df['processing_timestamp'] = datetime.now().isoformat()\n",
        "    batch_id = str(uuid.uuid4())[:8]\n",
        "    df['batch_id'] = batch_id\n",
        "    \n",
        "    # Actual I/O operation: write batch to disk\n",
        "    output_path = f\"{output_dir}/batch_{batch_id}.parquet\"\n",
        "    df.to_parquet(output_path, index=False)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Apply with optimized batch size for I/O efficiency\n",
        "io_optimized_orders = enriched_orders.map_batches(\n",
        "    io_intensive_etl,\n",
        "    num_cpus=0.25,  # Higher concurrency for I/O operations\n",
        "    batch_size=2000,  # Larger batches for I/O efficiency\n",
        "    batch_format=\"pandas\"\n",
        ")\n",
        "\n",
        "print(f\"I/O-optimized processing: {io_optimized_orders.count():,} records\")\n",
        "print(f\"Batch files written to: /mnt/cluster_storage/temp_etl_batches/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:24:29,908\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_360_0\n",
            "2025-10-10 20:24:29,918\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_360_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:24:29,918\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_360_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(enrich_orders_with_metrics)] -> TaskPoolMapOperator[MapBatches(io_intensive_etl)] -> LimitOperator[limit=5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(autoscaler +2m36s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:24:47,969\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_360_0 execution finished in 18.05 seconds\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>o_orderkey</th>\n",
              "      <th>o_custkey</th>\n",
              "      <th>o_orderstatus</th>\n",
              "      <th>o_totalprice</th>\n",
              "      <th>o_orderdate</th>\n",
              "      <th>o_orderpriority</th>\n",
              "      <th>o_clerk</th>\n",
              "      <th>o_shippriority</th>\n",
              "      <th>o_comment</th>\n",
              "      <th>order_year</th>\n",
              "      <th>order_quarter</th>\n",
              "      <th>order_month</th>\n",
              "      <th>is_large_order</th>\n",
              "      <th>is_urgent</th>\n",
              "      <th>revenue_tier</th>\n",
              "      <th>processing_timestamp</th>\n",
              "      <th>batch_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>40423681</td>\n",
              "      <td>1202450</td>\n",
              "      <td>O</td>\n",
              "      <td>24797.87</td>\n",
              "      <td>1996-10-07</td>\n",
              "      <td>2-HIGH</td>\n",
              "      <td>Clerk#000002862</td>\n",
              "      <td>0</td>\n",
              "      <td>ly fluffy orbits. unusual, unusual ideas thras...</td>\n",
              "      <td>1996</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>Small</td>\n",
              "      <td>2025-10-10T20:24:32.461085</td>\n",
              "      <td>0bf79647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>40423682</td>\n",
              "      <td>65267</td>\n",
              "      <td>O</td>\n",
              "      <td>290473.34</td>\n",
              "      <td>1998-01-31</td>\n",
              "      <td>1-URGENT</td>\n",
              "      <td>Clerk#000007249</td>\n",
              "      <td>0</td>\n",
              "      <td>ide of the platelets; slyly silent requests af...</td>\n",
              "      <td>1998</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>Large</td>\n",
              "      <td>2025-10-10T20:24:32.461085</td>\n",
              "      <td>0bf79647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>40423683</td>\n",
              "      <td>441604</td>\n",
              "      <td>P</td>\n",
              "      <td>226618.10</td>\n",
              "      <td>1995-04-17</td>\n",
              "      <td>4-NOT SPECIFIED</td>\n",
              "      <td>Clerk#000009686</td>\n",
              "      <td>0</td>\n",
              "      <td>cuses about the furiously even account</td>\n",
              "      <td>1995</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>Large</td>\n",
              "      <td>2025-10-10T20:24:32.461085</td>\n",
              "      <td>0bf79647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>40423684</td>\n",
              "      <td>16730</td>\n",
              "      <td>F</td>\n",
              "      <td>245172.80</td>\n",
              "      <td>1993-06-24</td>\n",
              "      <td>3-MEDIUM</td>\n",
              "      <td>Clerk#000009880</td>\n",
              "      <td>0</td>\n",
              "      <td>heaves. even requests sleep b</td>\n",
              "      <td>1993</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>Large</td>\n",
              "      <td>2025-10-10T20:24:32.461085</td>\n",
              "      <td>0bf79647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>40423685</td>\n",
              "      <td>677485</td>\n",
              "      <td>O</td>\n",
              "      <td>239125.96</td>\n",
              "      <td>1996-08-04</td>\n",
              "      <td>3-MEDIUM</td>\n",
              "      <td>Clerk#000009403</td>\n",
              "      <td>0</td>\n",
              "      <td>elieve finally above the regular, final reques...</td>\n",
              "      <td>1996</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>Large</td>\n",
              "      <td>2025-10-10T20:24:32.461085</td>\n",
              "      <td>0bf79647</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   o_orderkey  o_custkey o_orderstatus  o_totalprice o_orderdate  \\\n",
              "0    40423681    1202450             O      24797.87  1996-10-07   \n",
              "1    40423682      65267             O     290473.34  1998-01-31   \n",
              "2    40423683     441604             P     226618.10  1995-04-17   \n",
              "3    40423684      16730             F     245172.80  1993-06-24   \n",
              "4    40423685     677485             O     239125.96  1996-08-04   \n",
              "\n",
              "   o_orderpriority          o_clerk  o_shippriority  \\\n",
              "0           2-HIGH  Clerk#000002862               0   \n",
              "1         1-URGENT  Clerk#000007249               0   \n",
              "2  4-NOT SPECIFIED  Clerk#000009686               0   \n",
              "3         3-MEDIUM  Clerk#000009880               0   \n",
              "4         3-MEDIUM  Clerk#000009403               0   \n",
              "\n",
              "                                           o_comment  order_year  \\\n",
              "0  ly fluffy orbits. unusual, unusual ideas thras...        1996   \n",
              "1  ide of the platelets; slyly silent requests af...        1998   \n",
              "2             cuses about the furiously even account        1995   \n",
              "3                      heaves. even requests sleep b        1993   \n",
              "4  elieve finally above the regular, final reques...        1996   \n",
              "\n",
              "   order_quarter  order_month  is_large_order  is_urgent revenue_tier  \\\n",
              "0              4           10           False       True        Small   \n",
              "1              1            1            True       True        Large   \n",
              "2              2            4            True      False        Large   \n",
              "3              2            6            True      False        Large   \n",
              "4              3            8            True      False        Large   \n",
              "\n",
              "         processing_timestamp  batch_id  \n",
              "0  2025-10-10T20:24:32.461085  0bf79647  \n",
              "1  2025-10-10T20:24:32.461085  0bf79647  \n",
              "2  2025-10-10T20:24:32.461085  0bf79647  \n",
              "3  2025-10-10T20:24:32.461085  0bf79647  \n",
              "4  2025-10-10T20:24:32.461085  0bf79647  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "io_optimized_orders.limit(5).to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Column selection and schema optimization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:24:48,115\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_363_0\n",
            "2025-10-10 20:24:48,124\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_363_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:24:48,125\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_363_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)] -> LimitOperator[limit=1] -> TaskPoolMapOperator[Project]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying column selection optimization...\n",
            "Column optimization:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:24:49,111\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_363_0 execution finished in 0.99 seconds\n",
            "2025-10-10 20:24:49,117\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_364_0\n",
            "2025-10-10 20:24:49,126\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_364_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:24:49,127\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_364_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(enrich_orders_with_metrics)] -> LimitOperator[limit=1] -> TaskPoolMapOperator[Project]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Customer columns: 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:24:51,828\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_364_0 execution finished in 2.70 seconds\n",
            "2025-10-10 20:24:51,836\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_366_0\n",
            "2025-10-10 20:24:51,856\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_366_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:24:51,857\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_366_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)->Project], InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(enrich_orders_with_metrics)] -> TaskPoolMapOperator[Project] -> JoinOperatorWithPolars[Join(num_partitions=100)] -> AggregateNumRows[AggregateNumRows]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Order columns: 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(HashShuffleAggregator pid=91607, ip=10.0.127.67)\u001b[0m Failed to hash the schemas (for deduplication): unhashable type: 'dict'\n",
            "2025-10-10 20:25:01,325\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_366_0 execution finished in 9.47 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimized join completed: 15,000,000 records\n"
          ]
        }
      ],
      "source": [
        "# ETL Optimization: Column pruning for performance\n",
        "print(\"Applying column selection optimization...\")\n",
        "\n",
        "# Select only essential columns for downstream processing\n",
        "essential_customer_columns = customers_ds.select_columns([\n",
        "    \"c_custkey\", \"c_name\", \"c_mktsegment\", \"c_acctbal\", \"c_nationkey\"\n",
        "])\n",
        "\n",
        "essential_order_columns = enriched_orders.select_columns([\n",
        "    \"o_orderkey\", \"o_custkey\", \"o_totalprice\", \"o_orderdate\", \n",
        "    \"order_year\", \"revenue_tier\", \"is_large_order\"\n",
        "])\n",
        "\n",
        "print(f\"Column optimization:\")\n",
        "print(f\"  Customer columns: {len(essential_customer_columns.schema().names)}\")\n",
        "print(f\"  Order columns: {len(essential_order_columns.schema().names)}\")\n",
        "\n",
        "# Optimized join with selected columns\n",
        "optimized_join = essential_customer_columns.join(\n",
        "    essential_order_columns,\n",
        "    on=(\"c_custkey\",),\n",
        "    right_on=(\"o_custkey\",),\n",
        "    num_partitions=100,\n",
        "    join_type=\"inner\",\n",
        ")\n",
        "\n",
        "print(f\"Optimized join completed: {optimized_join.count():,} records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:25:01,468\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_367_0\n",
            "2025-10-10 20:25:01,487\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_367_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:25:01,488\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_367_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)->Project], InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(enrich_orders_with_metrics)] -> TaskPoolMapOperator[Project] -> JoinOperatorWithPolars[Join(num_partitions=100)] -> LimitOperator[limit=5]\n",
            "\u001b[36m(HashShuffleAggregator pid=89233, ip=10.0.83.124)\u001b[0m Failed to hash the schemas (for deduplication): unhashable type: 'dict'\n",
            "2025-10-10 20:25:10,851\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_367_0 execution finished in 9.36 seconds\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c_custkey</th>\n",
              "      <th>c_name</th>\n",
              "      <th>c_mktsegment</th>\n",
              "      <th>c_acctbal</th>\n",
              "      <th>c_nationkey</th>\n",
              "      <th>o_orderkey</th>\n",
              "      <th>o_totalprice</th>\n",
              "      <th>o_orderdate</th>\n",
              "      <th>order_year</th>\n",
              "      <th>revenue_tier</th>\n",
              "      <th>is_large_order</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>841561</td>\n",
              "      <td>Customer#000841561</td>\n",
              "      <td>MACHINERY</td>\n",
              "      <td>1961.63</td>\n",
              "      <td>16</td>\n",
              "      <td>10423846</td>\n",
              "      <td>168237.54</td>\n",
              "      <td>1994-07-20</td>\n",
              "      <td>1994</td>\n",
              "      <td>Large</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>701276</td>\n",
              "      <td>Customer#000701276</td>\n",
              "      <td>BUILDING</td>\n",
              "      <td>5727.18</td>\n",
              "      <td>13</td>\n",
              "      <td>10423936</td>\n",
              "      <td>228113.77</td>\n",
              "      <td>1995-05-19</td>\n",
              "      <td>1995</td>\n",
              "      <td>Large</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>266596</td>\n",
              "      <td>Customer#000266596</td>\n",
              "      <td>HOUSEHOLD</td>\n",
              "      <td>7294.46</td>\n",
              "      <td>1</td>\n",
              "      <td>10424358</td>\n",
              "      <td>193268.14</td>\n",
              "      <td>1992-05-07</td>\n",
              "      <td>1992</td>\n",
              "      <td>Large</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>562309</td>\n",
              "      <td>Customer#000562309</td>\n",
              "      <td>AUTOMOBILE</td>\n",
              "      <td>7688.35</td>\n",
              "      <td>18</td>\n",
              "      <td>10424513</td>\n",
              "      <td>248754.64</td>\n",
              "      <td>1994-10-05</td>\n",
              "      <td>1994</td>\n",
              "      <td>Large</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>887143</td>\n",
              "      <td>Customer#000887143</td>\n",
              "      <td>MACHINERY</td>\n",
              "      <td>7866.95</td>\n",
              "      <td>5</td>\n",
              "      <td>10424577</td>\n",
              "      <td>64566.07</td>\n",
              "      <td>1993-11-20</td>\n",
              "      <td>1993</td>\n",
              "      <td>Medium</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   c_custkey              c_name c_mktsegment  c_acctbal  c_nationkey  \\\n",
              "0     841561  Customer#000841561    MACHINERY    1961.63           16   \n",
              "1     701276  Customer#000701276     BUILDING    5727.18           13   \n",
              "2     266596  Customer#000266596    HOUSEHOLD    7294.46            1   \n",
              "3     562309  Customer#000562309   AUTOMOBILE    7688.35           18   \n",
              "4     887143  Customer#000887143    MACHINERY    7866.95            5   \n",
              "\n",
              "   o_orderkey  o_totalprice o_orderdate  order_year revenue_tier  \\\n",
              "0    10423846     168237.54  1994-07-20        1994        Large   \n",
              "1    10423936     228113.77  1995-05-19        1995        Large   \n",
              "2    10424358     193268.14  1992-05-07        1992        Large   \n",
              "3    10424513     248754.64  1994-10-05        1994        Large   \n",
              "4    10424577      64566.07  1993-11-20        1993       Medium   \n",
              "\n",
              "   is_large_order  \n",
              "0           False  \n",
              "1            True  \n",
              "2           False  \n",
              "3            True  \n",
              "4           False  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimized_join.limit(5).to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Large-Scale ETL Patterns\n",
        "\n",
        "Production ETL systems must handle billions of records efficiently. This section demonstrates Ray Data patterns for large-scale data processing including distributed aggregations, multi-dimensional analysis, and data warehouse integration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:25:11,092\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_370_0\n",
            "2025-10-10 20:25:11,112\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_370_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:25:11,113\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_370_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)->Project], InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(enrich_orders_with_metrics)] -> TaskPoolMapOperator[Project] -> JoinOperatorWithPolars[Join(num_partitions=100)] -> AllToAllOperator[Aggregate] -> LimitOperator[limit=5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing large-scale distributed aggregations...\n",
            "Comprehensive Business Metrics:\n",
            "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff628638057c2f0b7254ad1dc90a000000 Worker ID: ab015acd9569881de1592e4b363f7dfec9124d774b02ca1717232f4b Node ID: 45eb4f9cbe98aac441206a556e4505c3153634633056153efe93760e Worker IP address: 10.0.83.247 Worker port: 10525 Worker PID: 92026 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly by a signal. SystemExit is raised (sys.exit is called). Exit code: 1. The process receives a SIGTERM.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "{\"asctime\":\"2025-10-10 20:25:23,000\",\"levelname\":\"E\",\"message\":\":info_message: Attempting to recover 1 lost objects by resubmitting their tasks or setting a new primary location from existing copies. To disable object reconstruction, set @ray.remote(max_retries=0).\",\"filename\":\"core_worker.cc\",\"lineno\":445}\n",
            "{\"asctime\":\"2025-10-10 20:25:23,800\",\"levelname\":\"E\",\"message\":\":info_message: Attempting to recover 1 lost objects by resubmitting their tasks or setting a new primary location from existing copies. To disable object reconstruction, set @ray.remote(max_retries=0).\",\"filename\":\"core_worker.cc\",\"lineno\":445}\n",
            "{\"asctime\":\"2025-10-10 20:25:25,601\",\"levelname\":\"E\",\"message\":\":info_message: Attempting to recover 1 lost objects by resubmitting their tasks or setting a new primary location from existing copies. To disable object reconstruction, set @ray.remote(max_retries=0).\",\"filename\":\"core_worker.cc\",\"lineno\":445}\n",
            "{\"asctime\":\"2025-10-10 20:25:26,001\",\"levelname\":\"E\",\"message\":\":info_message: Attempting to recover 1 lost objects by resubmitting their tasks or setting a new primary location from existing copies. To disable object reconstruction, set @ray.remote(max_retries=0).\",\"filename\":\"core_worker.cc\",\"lineno\":445}\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Large-scale aggregations using Ray Data \n",
        "print(\"Performing large-scale distributed aggregations...\")\n",
        "\n",
        "# Multi-dimensional aggregations for business intelligence\n",
        "comprehensive_metrics = optimized_join.groupby([\"c_mktsegment\", \"order_year\", \"revenue_tier\"]).aggregate(\n",
        "    Count(),\n",
        "    Sum(\"o_totalprice\"),\n",
        "    Mean(\"o_totalprice\"),\n",
        "    Max(\"o_totalprice\"),\n",
        "    Mean(\"c_acctbal\")\n",
        ")\n",
        "\n",
        "print(\"Comprehensive Business Metrics:\")\n",
        "print(comprehensive_metrics.limit(5).to_pandas())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 18:56:27,516\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_207_0\n",
            "2025-10-10 18:56:27,534\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_207_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 18:56:27,535\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_207_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)->Project], InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(enrich_orders_with_metrics)] -> TaskPoolMapOperator[Project] -> JoinOperatorWithPolars[Join(num_partitions=100)] -> AllToAllOperator[Aggregate] -> LimitOperator[limit=5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Yearly Trends Analysis:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(HashShuffleAggregator pid=43668, ip=10.0.127.67)\u001b[0m Failed to hash the schemas (for deduplication): unhashable type: 'dict'\n",
            "2025-10-10 18:56:41,444\tWARNING streaming_executor_state.py:793 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: order_year: int64\n",
            "count(): int64\n",
            "sum(o_totalprice): double\n",
            "mean(o_totalprice): double, new schema: None. This may lead to unexpected behavior.\n",
            "2025-10-10 18:56:41,596\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_207_0 execution finished in 14.06 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   order_year  count()  sum(o_totalprice)  mean(o_totalprice)\n",
            "0        1992  2281205       3.444725e+11       151004.621657\n",
            "1        1993  2276638       3.440619e+11       151127.204812\n",
            "2        1994  2275919       3.440890e+11       151186.860472\n",
            "3        1995  2275575       3.437713e+11       151070.064800\n",
            "4        1996  2281938       3.447880e+11       151094.386277\n"
          ]
        }
      ],
      "source": [
        "# Time-series aggregations for trend analysis\n",
        "yearly_trends = optimized_join.groupby(\"order_year\").aggregate(\n",
        "    Count(),\n",
        "    Sum(\"o_totalprice\"),\n",
        "    Mean(\"o_totalprice\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 20:18:00,460\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_305_0\n",
            "2025-10-10 20:18:00,479\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_305_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 20:18:00,480\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_305_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)->Project], InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(enrich_orders_with_metrics)] -> TaskPoolMapOperator[Project] -> JoinOperatorWithPolars[Join(num_partitions=100)] -> AllToAllOperator[Aggregate] -> LimitOperator[limit=5]\n",
            "\u001b[36m(HashShuffleAggregator pid=75410, ip=10.0.83.124)\u001b[0m Failed to hash the schemas (for deduplication): unhashable type: 'dict'\n",
            "2025-10-10 20:18:16,923\tWARNING streaming_executor_state.py:793 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: order_year: int64\n",
            "count(): int64\n",
            "sum(o_totalprice): double\n",
            "mean(o_totalprice): double, new schema: None. This may lead to unexpected behavior.\n",
            "2025-10-10 20:18:17,078\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_305_0 execution finished in 16.60 seconds\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>order_year</th>\n",
              "      <th>count()</th>\n",
              "      <th>sum(o_totalprice)</th>\n",
              "      <th>mean(o_totalprice)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1992</td>\n",
              "      <td>2281205</td>\n",
              "      <td>3.444725e+11</td>\n",
              "      <td>151004.621657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1993</td>\n",
              "      <td>2276638</td>\n",
              "      <td>3.440619e+11</td>\n",
              "      <td>151127.204812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1994</td>\n",
              "      <td>2275919</td>\n",
              "      <td>3.440890e+11</td>\n",
              "      <td>151186.860472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1995</td>\n",
              "      <td>2275575</td>\n",
              "      <td>3.437713e+11</td>\n",
              "      <td>151070.064800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1996</td>\n",
              "      <td>2281938</td>\n",
              "      <td>3.447880e+11</td>\n",
              "      <td>151094.386277</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   order_year  count()  sum(o_totalprice)  mean(o_totalprice)\n",
              "0        1992  2281205       3.444725e+11       151004.621657\n",
              "1        1993  2276638       3.440619e+11       151127.204812\n",
              "2        1994  2275919       3.440890e+11       151186.860472\n",
              "3        1995  2275575       3.437713e+11       151070.064800\n",
              "4        1996  2281938       3.447880e+11       151094.386277"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "yearly_trends.limit(5).to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 18:56:41,750\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_210_0\n",
            "2025-10-10 18:56:41,766\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_210_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 18:56:41,767\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_210_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)->Project], InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(enrich_orders_with_metrics)] -> TaskPoolMapOperator[Project] -> JoinOperatorWithPolars[Join(num_partitions=100)] -> AllToAllOperator[Aggregate] -> LimitOperator[limit=5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Customer Segment Performance:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(HashShuffleAggregator pid=45378, ip=10.0.127.67)\u001b[0m Failed to hash the schemas (for deduplication): unhashable type: 'dict'\n",
            "2025-10-10 18:56:55,131\tWARNING streaming_executor_state.py:793 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: c_mktsegment: string\n",
            "revenue_tier: string\n",
            "count(): int64\n",
            "sum(o_totalprice): double\n",
            "mean(c_acctbal): double, new schema: None. This may lead to unexpected behavior.\n",
            "2025-10-10 18:56:55,322\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_210_0 execution finished in 13.55 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  c_mktsegment revenue_tier  count()  sum(o_totalprice)  mean(c_acctbal)\n",
            "0   AUTOMOBILE   Enterprise   171395       5.749540e+10      4496.630832\n",
            "1   AUTOMOBILE        Large  1264302       2.703461e+11      4497.482494\n",
            "2   AUTOMOBILE       Medium  1141136       1.136020e+11      4496.741620\n",
            "3   AUTOMOBILE        Small   423707       1.192671e+10      4501.300247\n",
            "4     BUILDING   Enterprise   170942       5.733719e+10      4514.232680\n"
          ]
        }
      ],
      "source": [
        "# Customer segment performance analysis\n",
        "segment_performance = optimized_join.groupby([\"c_mktsegment\", \"revenue_tier\"]).aggregate(\n",
        "    Count(),\n",
        "    Sum(\"o_totalprice\"),\n",
        "    Mean(\"c_acctbal\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "segment_performance.limit(5).to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ETL output and data warehouse integration\n",
        "\n",
        "Ray Data provides native write functions for various data warehouses and file formats, enabling you to export processed datasets directly to your target storage systems. You can write to Snowflake using `write_snowflake()`, which handles authentication and schema management automatically. \n",
        "\n",
        "\n",
        "For other data warehouses, Ray Data supports writing to BigQuery with `write_bigquery()`, SQL databases with `write_sql()`, and modern table formats like Delta Lake (`write_delta()` and `write_unity_catalog()`, *coming soon*) and Apache Iceberg (`write_iceberg()`). Additionally, you can write to file-based formats such as Parquet using `write_parquet(),` which offers efficient columnar storage with compression options. \n",
        "\n",
        "\n",
        "These native write functions integrate seamlessly with Ray Data's distributed processing, allowing you to scale data export operations across your cluster while maintaining data consistency and optimizing write performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 18:56:55,476\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_212_0\n",
            "2025-10-10 18:56:55,486\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_212_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 18:56:55,489\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_212_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)->Project] -> TaskPoolMapOperator[MapBatches(segment_customers)] -> TaskPoolMapOperator[Write]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing ETL results to data warehouse...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 18:56:57,295\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_212_0 execution finished in 1.81 seconds\n",
            "2025-10-10 18:56:57,346\tINFO dataset.py:4871 -- Data sink Parquet finished. 1500000 rows and 665.0MB data written.\n",
            "2025-10-10 18:56:57,352\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_215_0\n",
            "2025-10-10 18:56:57,363\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_215_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 18:56:57,364\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_215_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(enrich_orders_with_metrics)] -> TaskPoolMapOperator[Write]\n",
            "2025-10-10 18:57:04,817\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_215_0 execution finished in 7.45 seconds\n",
            "2025-10-10 18:57:04,850\tINFO dataset.py:4871 -- Data sink Parquet finished. 15000000 rows and 5.3GB data written.\n",
            "2025-10-10 18:57:04,858\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_220_0\n",
            "2025-10-10 18:57:04,874\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_220_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 18:57:04,875\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_220_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)->Project], InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(enrich_orders_with_metrics)] -> TaskPoolMapOperator[Project] -> JoinOperatorWithPolars[Join(num_partitions=100)] -> AllToAllOperator[Aggregate] -> TaskPoolMapOperator[Write]\n",
            "\u001b[36m(HashShuffleAggregator pid=44991, ip=10.0.99.160)\u001b[0m Failed to hash the schemas (for deduplication): unhashable type: 'dict'\n",
            "2025-10-10 18:57:32,014\tWARNING streaming_executor_state.py:793 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: c_mktsegment: string\n",
            "revenue_tier: string\n",
            "order_year: int64\n",
            "count(): int64\n",
            "sum(o_totalprice): double\n",
            "mean(o_totalprice): double\n",
            "mean(c_acctbal): double, new schema: None. This may lead to unexpected behavior.\n",
            "2025-10-10 18:57:32,336\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_220_0 execution finished in 27.46 seconds\n",
            "2025-10-10 18:57:32,425\tINFO dataset.py:4871 -- Data sink Parquet finished. 140 rows and 8.7KB data written.\n"
          ]
        }
      ],
      "source": [
        "# Write ETL results to data warehouse formats\n",
        "print(\"Writing ETL results to data warehouse...\")\n",
        "\n",
        "# Replace with S3 or other cloud storage in a real production use case\n",
        "BASE_DIRECTORY = \"/mnt/cluster_storage/\"\n",
        "\n",
        "# Write customer analytics with partitioning\n",
        "enriched_customers = segmented_customers\n",
        "enriched_customers.write_parquet(\n",
        "    f\"{BASE_DIRECTORY}/etl_warehouse/customers/\",\n",
        "    partition_cols=[\"customer_segment\"],\n",
        "    compression=\"snappy\",\n",
        "    ray_remote_args={\"num_cpus\": 0.1}\n",
        ")\n",
        "\n",
        "# Write order analytics with time-based partitioning\n",
        "enriched_orders.write_parquet(\n",
        "    f\"{BASE_DIRECTORY}/etl_warehouse/orders/\",\n",
        "    partition_cols=[\"order_year\"],\n",
        "    compression=\"snappy\",\n",
        "    ray_remote_args={\"num_cpus\": 0.1}\n",
        ")\n",
        "\n",
        "# Write aggregated analytics for BI tools\n",
        "final_analytics = optimized_join.groupby([\"c_mktsegment\", \"revenue_tier\", \"order_year\"]).aggregate(\n",
        "    Count(),\n",
        "    Sum(\"o_totalprice\"),\n",
        "    Mean(\"o_totalprice\"),\n",
        "    Mean(\"c_acctbal\")\n",
        ")\n",
        "\n",
        "final_analytics.write_parquet(\n",
        "    f\"{BASE_DIRECTORY}/etl_warehouse/analytics/\",\n",
        "    partition_cols=[\"order_year\"],\n",
        "    compression=\"snappy\",\n",
        "    ray_remote_args={\"num_cpus\": 0.1}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Output Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 18:57:32,543\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_225_0\n",
            "2025-10-10 18:57:32,546\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_225_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 18:57:32,547\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_225_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[MapBatches(count_rows)]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validating ETL output...\n",
            "ETL Pipeline Verification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 18:57:32,751\tINFO streaming_executor.py:264 -- Operator 2 MapBatches(count_rows): 1 tasks executed, 1 blocks produced in 0.14s\n",
            "* Remote wall time: 138.37ms min, 138.37ms max, 138.37ms mean, 138.37ms total\n",
            "* Remote cpu time: 72.1ms min, 72.1ms max, 72.1ms mean, 72.1ms total\n",
            "* UDF time: 136.67ms min, 136.67ms max, 136.67ms mean, 136.67ms total\n",
            "* Peak heap memory usage (MiB): 848.18 min, 848.18 max, 848 mean\n",
            "* Output num rows per block: 5 min, 5 max, 5 mean, 5 total\n",
            "* Output size bytes per block: 40 min, 40 max, 40 mean, 40 total\n",
            "* Output rows per task: 5 min, 5 max, 5 mean, 1 tasks used\n",
            "* Tasks per node: 1 min, 1 max, 1 mean; 1 nodes used\n",
            "* Operator throughput:\n",
            "\t* Ray Data throughput: 36.13621943996574 rows/s\n",
            "\t* Estimated single node throughput: 36.13621943996574 rows/s\n",
            "\n",
            "Dataset throughput:\n",
            "\t* Ray Data throughput: 5.706518275526538 rows/s\n",
            "\t* Estimated single node throughput: 31.402439216023936 rows/s\n",
            "\n",
            "2025-10-10 18:57:32,752\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_225_0 execution finished in 0.20 seconds\n",
            "2025-10-10 18:57:32,759\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_226_0\n",
            "2025-10-10 18:57:32,763\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_226_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 18:57:32,763\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_226_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[MapBatches(count_rows)]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Customer records: 6,000,000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 18:57:34,786\tINFO streaming_executor.py:264 -- Operator 2 MapBatches(count_rows): 2 tasks executed, 2 blocks produced in 1.73s\n",
            "* Remote wall time: 226.21ms min, 1.73s max, 978.67ms mean, 1.96s total\n",
            "* Remote cpu time: 135.91ms min, 1.1s max, 618.5ms mean, 1.24s total\n",
            "* UDF time: 224.13ms min, 1.72s max, 970.72ms mean, 1.94s total\n",
            "* Peak heap memory usage (MiB): 538.77 min, 1204.13 max, 871 mean\n",
            "* Output num rows per block: 8 min, 63 max, 35 mean, 71 total\n",
            "* Output size bytes per block: 64 min, 504 max, 284 mean, 568 total\n",
            "* Output rows per task: 8 min, 63 max, 35 mean, 2 tasks used\n",
            "* Tasks per node: 1 min, 1 max, 1 mean; 2 nodes used\n",
            "* Operator throughput:\n",
            "\t* Ray Data throughput: 41.013836292613426 rows/s\n",
            "\t* Estimated single node throughput: 36.27380445871822 rows/s\n",
            "\n",
            "Dataset throughput:\n",
            "\t* Ray Data throughput: 41.013836292613426 rows/s\n",
            "\t* Estimated single node throughput: 32.23611590996947 rows/s\n",
            "\n",
            "2025-10-10 18:57:34,787\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_226_0 execution finished in 2.02 seconds\n",
            "2025-10-10 18:57:34,795\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_227_0\n",
            "2025-10-10 18:57:34,799\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_227_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 18:57:34,799\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_227_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[MapBatches(count_rows)]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Order records: 60,000,000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 18:57:35,316\tINFO streaming_executor.py:264 -- Operator 2 MapBatches(count_rows): 1 tasks executed, 1 blocks produced in 0.39s\n",
            "* Remote wall time: 391.92ms min, 391.92ms max, 391.92ms mean, 391.92ms total\n",
            "* Remote cpu time: 240.02ms min, 240.02ms max, 240.02ms mean, 240.02ms total\n",
            "* UDF time: 386.1ms min, 386.1ms max, 386.1ms mean, 386.1ms total\n",
            "* Peak heap memory usage (MiB): 1116.09 min, 1116.09 max, 1116 mean\n",
            "* Output num rows per block: 27 min, 27 max, 27 mean, 27 total\n",
            "* Output size bytes per block: 216 min, 216 max, 216 mean, 216 total\n",
            "* Output rows per task: 27 min, 27 max, 27 mean, 1 tasks used\n",
            "* Tasks per node: 1 min, 1 max, 1 mean; 1 nodes used\n",
            "* Operator throughput:\n",
            "\t* Ray Data throughput: 68.89138940244673 rows/s\n",
            "\t* Estimated single node throughput: 68.89138940244673 rows/s\n",
            "\n",
            "Dataset throughput:\n",
            "\t* Ray Data throughput: 42.26306318732737 rows/s\n",
            "\t* Estimated single node throughput: 57.932190036850805 rows/s\n",
            "\n",
            "2025-10-10 18:57:35,317\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_227_0 execution finished in 0.52 seconds\n",
            "2025-10-10 18:57:35,324\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_228_0\n",
            "2025-10-10 18:57:35,330\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_228_0. Full logs are in /tmp/ray/session_2025-10-10_16-23-49_015346_2333/logs/ray-data\n",
            "2025-10-10 18:57:35,330\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_228_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> LimitOperator[limit=25]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Analytics records: 420\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 18:57:37,106\tINFO streaming_executor.py:264 -- Operator 3 limit=25: 1 tasks executed, 1 blocks produced in 0.09s\n",
            "* Remote wall time: 89.4ms min, 89.4ms max, 89.4ms mean, 89.4ms total\n",
            "* Remote cpu time: 186.83ms min, 186.83ms max, 186.83ms mean, 186.83ms total\n",
            "* UDF time: 0us min, 0us max, 0.0us mean, 0us total\n",
            "* Peak heap memory usage (MiB): 856.52 min, 856.52 max, 856 mean\n",
            "* Output num rows per block: 25 min, 25 max, 25 mean, 25 total\n",
            "* Output size bytes per block: 1594 min, 1594 max, 1594 mean, 1594 total\n",
            "* Output rows per task: 25 min, 25 max, 25 mean, 1 tasks used\n",
            "* Tasks per node: 1 min, 1 max, 1 mean; 1 nodes used\n",
            "* Operator throughput:\n",
            "\t* Ray Data throughput: 279.62736677939245 rows/s\n",
            "\t* Estimated single node throughput: 279.62736677939245 rows/s\n",
            "\n",
            "Dataset throughput:\n",
            "\t* Ray Data throughput: 13.58305853827714 rows/s\n",
            "\t* Estimated single node throughput: 46.42515826415853 rows/s\n",
            "\n",
            "2025-10-10 18:57:37,107\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_228_0 execution finished in 1.78 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample ETL Analytics Results:\n",
            "  1. Segment: AUTOMOBILE, Tier: Enterprise, Year: 1992, Orders: 25919, Revenue: $8,691,666,789\n",
            "  2. Segment: AUTOMOBILE, Tier: Large, Year: 1992, Orders: 191421, Revenue: $40,936,308,218\n",
            "  3. Segment: AUTOMOBILE, Tier: Medium, Year: 1992, Orders: 173366, Revenue: $17,240,817,851\n",
            "  4. Segment: AUTOMOBILE, Tier: Small, Year: 1992, Orders: 64755, Revenue: $1,819,743,455\n",
            "  5. Segment: BUILDING, Tier: Enterprise, Year: 1992, Orders: 25978, Revenue: $8,716,921,785\n",
            "  6. Segment: BUILDING, Tier: Large, Year: 1992, Orders: 192720, Revenue: $41,234,190,203\n",
            "  7. Segment: BUILDING, Tier: Medium, Year: 1992, Orders: 174342, Revenue: $17,355,772,628\n",
            "  8. Segment: BUILDING, Tier: Small, Year: 1992, Orders: 64775, Revenue: $1,828,037,736\n",
            "  9. Segment: FURNITURE, Tier: Enterprise, Year: 1992, Orders: 25807, Revenue: $8,650,677,599\n",
            "  10. Segment: FURNITURE, Tier: Large, Year: 1992, Orders: 192641, Revenue: $41,209,906,769\n",
            "  11. Segment: FURNITURE, Tier: Medium, Year: 1992, Orders: 173672, Revenue: $17,294,243,789\n",
            "  12. Segment: FURNITURE, Tier: Small, Year: 1992, Orders: 64385, Revenue: $1,806,817,337\n",
            "  13. Segment: HOUSEHOLD, Tier: Enterprise, Year: 1992, Orders: 26105, Revenue: $8,753,166,132\n",
            "  14. Segment: HOUSEHOLD, Tier: Large, Year: 1992, Orders: 191579, Revenue: $40,959,940,953\n",
            "  15. Segment: HOUSEHOLD, Tier: Medium, Year: 1992, Orders: 173111, Revenue: $17,211,742,048\n",
            "  16. Segment: HOUSEHOLD, Tier: Small, Year: 1992, Orders: 63916, Revenue: $1,800,192,999\n",
            "  17. Segment: MACHINERY, Tier: Enterprise, Year: 1992, Orders: 26091, Revenue: $8,751,905,563\n",
            "  18. Segment: MACHINERY, Tier: Large, Year: 1992, Orders: 192131, Revenue: $41,070,411,989\n",
            "  19. Segment: MACHINERY, Tier: Medium, Year: 1992, Orders: 173966, Revenue: $17,325,248,289\n",
            "  20. Segment: MACHINERY, Tier: Small, Year: 1992, Orders: 64525, Revenue: $1,814,785,814\n",
            "  21. Segment: AUTOMOBILE, Tier: Enterprise, Year: 1992, Orders: 25919, Revenue: $8,691,666,789\n",
            "  22. Segment: AUTOMOBILE, Tier: Large, Year: 1992, Orders: 191421, Revenue: $40,936,308,218\n",
            "  23. Segment: AUTOMOBILE, Tier: Medium, Year: 1992, Orders: 173366, Revenue: $17,240,817,851\n",
            "  24. Segment: AUTOMOBILE, Tier: Small, Year: 1992, Orders: 64755, Revenue: $1,819,743,455\n",
            "  25. Segment: BUILDING, Tier: Enterprise, Year: 1992, Orders: 25978, Revenue: $8,716,921,785\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m(raylet)\u001b[0m WARNING: 32 PYTHON worker processes have been started on node: 649a75ab4f03d0f79d9397c9d249bc0977d766b0eb9085dfc81526c2 with address: 10.0.127.67. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).\n"
          ]
        }
      ],
      "source": [
        "# Validate ETL pipeline performance\n",
        "print(\"Validating ETL output...\")\n",
        "\n",
        "BASE_DIRECTORY = \"/mnt/cluster_storage/\"\n",
        "\n",
        "# Read back and verify outputs\n",
        "customer_verification = ray.data.read_parquet(\n",
        "    f\"{BASE_DIRECTORY}/etl_warehouse/customers/\",\n",
        "    ray_remote_args={\"num_cpus\":0.025}\n",
        ")\n",
        "\n",
        "order_verification = ray.data.read_parquet(\n",
        "    f\"{BASE_DIRECTORY}/etl_warehouse/orders/\",\n",
        "    ray_remote_args={\"num_cpus\":0.025}\n",
        ")\n",
        "\n",
        "analytics_verification = ray.data.read_parquet(\n",
        "    f\"{BASE_DIRECTORY}/etl_warehouse/analytics/\",\n",
        "    ray_remote_args={\"num_cpus\":0.025}\n",
        ")\n",
        "\n",
        "print(f\"ETL Pipeline Verification:\")\n",
        "print(f\"  Customer records: {customer_verification.count():,}\")\n",
        "print(f\"  Order records: {order_verification.count():,}\")\n",
        "print(f\"  Analytics records: {analytics_verification.count():,}\")\n",
        "\n",
        "# Display sample results\n",
        "sample_analytics = analytics_verification.take(25)\n",
        "print(\"\\nSample ETL Analytics Results:\")\n",
        "for i, record in enumerate(sample_analytics):\n",
        "    print(f\"  {i+1}. Segment: {record['c_mktsegment']}, Tier: {record['revenue_tier']}, \"\n",
        "          f\"Year: {record['order_year']}, Orders: {record['count()']}, Revenue: ${record['sum(o_totalprice)']:,.0f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
