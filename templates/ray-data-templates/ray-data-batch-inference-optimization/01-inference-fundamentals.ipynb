{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1: Inference Fundamentals\n",
        "\n",
        "**Time to complete**: 20 min | **Difficulty**: Beginner | **Prerequisites**: Basic Python, ML model concepts\n",
        "\n",
        "**[← Back to Overview](README.md)** | **[Continue to Part 2 →](02-advanced-optimization.md)**\n",
        "\n",
        "---\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "In this part, you'll understand the fundamentals of batch inference optimization by comparing inefficient and efficient approaches:\n",
        "- How to set up Ray Data for accelerated inference (CPU or GPU)\n",
        "- Why naive inference patterns create performance bottlenecks\n",
        "- How Ray Data's actor-based pattern solves these problems\n",
        "- How to implement optimized inference with proper resource allocation for both CPU and GPU\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Introduction and Setup](#introduction-and-setup)\n",
        "2. [The Wrong Way: Inefficient Batch Inference](#the-wrong-way-inefficient-batch-inference)\n",
        "3. [Why the Naive Approach Fails](#why-the-naive-approach-fails)\n",
        "4. [The Right Way: Optimized with Ray Data](#the-right-way-optimized-with-ray-data)\n",
        "\n",
        "---\n",
        "\n",
        "## Introduction and Setup\n",
        "\n",
        "Batch inference is the process of running ML model predictions on large batches of data. While this sounds straightforward, naive implementations create severe performance bottlenecks that prevent production deployment. This part shows you the difference between inefficient and optimized approaches using real-world examples.\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "By comparing inefficient and optimized implementations, you'll understand:\n",
        "- **Why** repeated model loading destroys performance\n",
        "- **How** Ray Data's actor pattern solves the problem\n",
        "- **When** to apply specific optimization techniques\n",
        "- **What** parameters to tune for your workload\n",
        "\n",
        "### Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ray/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "module 'numpy' has no attribute 'ndarray'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/home/ray/default_cld_g54aiirwj1s8t9ktgzikqur41k/templates/templates/ray-data-templates/ray-data-batch-inference-optimization/01-inference-fundamentals.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-session-g3cs331q2c6ppx12j9ir5htn6l.i.anyscaleuserdata.com/home/ray/default_cld_g54aiirwj1s8t9ktgzikqur41k/templates/templates/ray-data-templates/ray-data-batch-inference-optimization/01-inference-fundamentals.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mray\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://vscode-session-g3cs331q2c6ppx12j9ir5htn6l.i.anyscaleuserdata.com/home/ray/default_cld_g54aiirwj1s8t9ktgzikqur41k/templates/templates/ray-data-templates/ray-data-batch-inference-optimization/01-inference-fundamentals.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-session-g3cs331q2c6ppx12j9ir5htn6l.i.anyscaleuserdata.com/home/ray/default_cld_g54aiirwj1s8t9ktgzikqur41k/templates/templates/ray-data-templates/ray-data-batch-inference-optimization/01-inference-fundamentals.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-session-g3cs331q2c6ppx12j9ir5htn6l.i.anyscaleuserdata.com/home/ray/default_cld_g54aiirwj1s8t9ktgzikqur41k/templates/templates/ray-data-templates/ray-data-batch-inference-optimization/01-inference-fundamentals.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/__init__.py:2150\u001b[0m\n\u001b[1;32m   2143\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_compile\u001b[39;00m \u001b[39mimport\u001b[39;00m _disable_dynamo  \u001b[39m# usort: skip\u001b[39;00m\n\u001b[1;32m   2145\u001b[0m \u001b[39m################################################################################\u001b[39;00m\n\u001b[1;32m   2146\u001b[0m \u001b[39m# Import interface functions defined in Python\u001b[39;00m\n\u001b[1;32m   2147\u001b[0m \u001b[39m################################################################################\u001b[39;00m\n\u001b[1;32m   2148\u001b[0m \n\u001b[1;32m   2149\u001b[0m \u001b[39m# needs to be after the above ATen bindings so we can overwrite from Python side\u001b[39;00m\n\u001b[0;32m-> 2150\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m _VF \u001b[39mas\u001b[39;00m _VF, functional \u001b[39mas\u001b[39;00m functional  \u001b[39m# usort: skip\u001b[39;00m\n\u001b[1;32m   2151\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[1;32m   2153\u001b[0m \u001b[39m################################################################################\u001b[39;00m\n\u001b[1;32m   2154\u001b[0m \u001b[39m# Remove unnecessary members\u001b[39;00m\n\u001b[1;32m   2155\u001b[0m \u001b[39m################################################################################\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/functional.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, Optional, TYPE_CHECKING, Union\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m _VF, Tensor\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_C\u001b[39;00m \u001b[39mimport\u001b[39;00m _add_docstr\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# mypy: allow-untyped-defs\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparameter\u001b[39;00m \u001b[39mimport\u001b[39;00m (  \u001b[39m# usort: skip\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     Buffer \u001b[39mas\u001b[39;00m Buffer,\n\u001b[1;32m      4\u001b[0m     Parameter \u001b[39mas\u001b[39;00m Parameter,\n\u001b[1;32m      5\u001b[0m     UninitializedBuffer \u001b[39mas\u001b[39;00m UninitializedBuffer,\n\u001b[1;32m      6\u001b[0m     UninitializedParameter \u001b[39mas\u001b[39;00m UninitializedParameter,\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     attention \u001b[39mas\u001b[39;00m attention,\n\u001b[1;32m     11\u001b[0m     functional \u001b[39mas\u001b[39;00m functional,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     utils \u001b[39mas\u001b[39;00m utils,\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparallel\u001b[39;00m \u001b[39mimport\u001b[39;00m DataParallel \u001b[39mas\u001b[39;00m DataParallel\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodule\u001b[39;00m \u001b[39mimport\u001b[39;00m Module  \u001b[39m# usort: skip\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlinear\u001b[39;00m \u001b[39mimport\u001b[39;00m Bilinear, Identity, LazyLinear, Linear  \u001b[39m# usort: skip\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mactivation\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     CELU,\n\u001b[1;32m      5\u001b[0m     ELU,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     Threshold,\n\u001b[1;32m     33\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_prims_common\u001b[39;00m \u001b[39mimport\u001b[39;00m DeviceLikeType\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparameter\u001b[39;00m \u001b[39mimport\u001b[39;00m Buffer, Parameter\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_python_dispatch\u001b[39;00m \u001b[39mimport\u001b[39;00m is_traceable_wrapper_subclass\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhooks\u001b[39;00m \u001b[39mimport\u001b[39;00m BackwardHook, RemovableHandle\n\u001b[1;32m     21\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m     22\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mregister_module_forward_pre_hook\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mregister_module_forward_hook\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mModule\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     31\u001b[0m ]\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/__init__.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mweakref\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     backcompat \u001b[39mas\u001b[39;00m backcompat,\n\u001b[1;32m     10\u001b[0m     collect_env \u001b[39mas\u001b[39;00m collect_env,\n\u001b[1;32m     11\u001b[0m     data \u001b[39mas\u001b[39;00m data,\n\u001b[1;32m     12\u001b[0m     deterministic \u001b[39mas\u001b[39;00m deterministic,\n\u001b[1;32m     13\u001b[0m     hooks \u001b[39mas\u001b[39;00m hooks,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend_registration\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     generate_methods_for_privateuse1_backend,\n\u001b[1;32m     17\u001b[0m     rename_privateuse1_backend,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcpp_backtrace\u001b[39;00m \u001b[39mimport\u001b[39;00m get_cpp_backtrace\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataloader\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     _DatasetKind,\n\u001b[1;32m      3\u001b[0m     DataLoader,\n\u001b[1;32m      4\u001b[0m     default_collate,\n\u001b[1;32m      5\u001b[0m     default_convert,\n\u001b[1;32m      6\u001b[0m     get_worker_info,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_decorator\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     argument_validation,\n\u001b[1;32m     10\u001b[0m     functional_datapipe,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     runtime_validation_disabled,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipe\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     DataChunk,\n\u001b[1;32m     18\u001b[0m     DFIterDataPipe,\n\u001b[1;32m     19\u001b[0m     IterDataPipe,\n\u001b[1;32m     20\u001b[0m     MapDataPipe,\n\u001b[1;32m     21\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mdist\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgraph_settings\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m ExceptionWrapper\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m _utils\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/graph_settings.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping_extensions\u001b[39;00m \u001b[39mimport\u001b[39;00m deprecated\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39miter\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msharding\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     _ShardingIterDataPipe,\n\u001b[1;32m     10\u001b[0m     SHARDING_PRIORITIES,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgraph\u001b[39;00m \u001b[39mimport\u001b[39;00m DataPipe, DataPipeGraph, traverse_dps\n\u001b[1;32m     15\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m     16\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mapply_random_seed\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mapply_sharding\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mget_all_graph_pipes\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m ]\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/datapipes/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m \u001b[39mimport\u001b[39;00m dataframe \u001b[39mas\u001b[39;00m dataframe, \u001b[39miter\u001b[39m \u001b[39mas\u001b[39;00m \u001b[39miter\u001b[39m, \u001b[39mmap\u001b[39m \u001b[39mas\u001b[39;00m \u001b[39mmap\u001b[39m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/datapipes/iter/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39miter\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallable\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     CollatorIterDataPipe \u001b[39mas\u001b[39;00m Collator,\n\u001b[1;32m      3\u001b[0m     MapperIterDataPipe \u001b[39mas\u001b[39;00m Mapper,\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39miter\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcombinatorics\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     SamplerIterDataPipe \u001b[39mas\u001b[39;00m Sampler,\n\u001b[1;32m      7\u001b[0m     ShufflerIterDataPipe \u001b[39mas\u001b[39;00m Shuffler,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39miter\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcombining\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     ConcaterIterDataPipe \u001b[39mas\u001b[39;00m Concater,\n\u001b[1;32m     11\u001b[0m     DemultiplexerIterDataPipe \u001b[39mas\u001b[39;00m Demultiplexer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     ZipperIterDataPipe \u001b[39mas\u001b[39;00m Zipper,\n\u001b[1;32m     15\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/datapipes/iter/callable.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mabc\u001b[39;00m \u001b[39mimport\u001b[39;00m Iterator, Sized\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, Callable, Optional, TypeVar, Union\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_utils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcollate\u001b[39;00m \u001b[39mimport\u001b[39;00m default_collate\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_decorator\u001b[39;00m \u001b[39mimport\u001b[39;00m functional_datapipe\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataframe\u001b[39;00m \u001b[39mimport\u001b[39;00m dataframe_wrapper \u001b[39mas\u001b[39;00m df_wrapper\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/__init__.py:54\u001b[0m\n\u001b[1;32m     48\u001b[0m     python_exit_status \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     51\u001b[0m atexit\u001b[39m.\u001b[39mregister(_set_python_exit_flag)\n\u001b[0;32m---> 54\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m collate, fetch, pin_memory, signal_handling, worker\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:327\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[39m# For both ndarray and memmap (subclass of ndarray)\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m default_collate_fn_map[np\u001b[39m.\u001b[39;49mndarray] \u001b[39m=\u001b[39m collate_numpy_array_fn\n\u001b[1;32m    328\u001b[0m \u001b[39m# See scalars hierarchy: https://numpy.org/doc/stable/reference/arrays.scalars.html\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[39m# Skip string scalars\u001b[39;00m\n\u001b[1;32m    330\u001b[0m default_collate_fn_map[(np\u001b[39m.\u001b[39mbool_, np\u001b[39m.\u001b[39mnumber, np\u001b[39m.\u001b[39mobject_)] \u001b[39m=\u001b[39m collate_numpy_scalar_fn\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'ndarray'"
          ]
        }
      ],
      "source": [
        "import ray\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import time\n",
        "\n",
        "# Initialize Ray for distributed processing\n",
        "ray.init(ignore_reinit_error=True)\n",
        "\n",
        "print(\"Ray cluster initialized for batch inference optimization\")\n",
        "print(f\"Available resources: {ray.cluster_resources()}\")\n",
        "\n",
        "# Configure Ray Data for optimal performance monitoring\n",
        "try:\n",
        "    ctx = ray.data.DataContext.get_current()\n",
        "    ctx.enable_progress_bars = True\n",
        "    ctx.enable_operator_progress_bars = True\n",
        "    print(\"Ray Data progress bars enabled\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: Could not configure Ray Data context (progress bars disabled): {e}\")\n",
        "    print(\"This doesn't affect functionality - continuing with notebook...\")\n",
        "\n",
        "# Detect hardware availability\n",
        "HAS_GPU = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if HAS_GPU else \"cpu\")\n",
        "\n",
        "print(f\"\\nUsing device: {device}\")\n",
        "if HAS_GPU:\n",
        "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
        "    print(\"GPU detected - examples will use GPU acceleration\")\n",
        "else:\n",
        "    print(\"No GPU detected - examples will run on CPU\")\n",
        "    print(\"   (All patterns work identically on CPU, just use num_cpus instead of num_gpus)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::note CPU and GPU Compatibility\n",
        "**This template works on both CPU-only and GPU clusters!**\n",
        "\n",
        "- **GPU clusters**: Code automatically detects GPUs and uses `num_gpus=1` for acceleration\n",
        "- **CPU clusters**: Code falls back to CPU and uses `num_cpus=2` for parallelism\n",
        "\n",
        "All optimization concepts (actor-based loading, batching, concurrency) apply equally to both environments.\n",
        ":::\n",
        "\n",
        "### Load Demo Dataset\n",
        "\n",
        "For this demonstration, you'll use the Imagenette dataset, which provides a realistic subset of ImageNet with 10 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Imagenette dataset from S3...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(TemporaryActor pid=96977)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::datasets_stats_actor:_StatsActor.__init__()\u001b[39m (pid=96977, ip=10.0.60.61, actor_id=460e91963bc9181821a215a606000000, repr=<ray.data._internal.stats.FunctionActorManager._create_fake_actor_class.<locals>.TemporaryActor object at 0x747c092459a0>)\n",
            "\u001b[36m(TemporaryActor pid=96977)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(TemporaryActor pid=96977)\u001b[0m RuntimeError: The actor with name _StatsActor failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
            "\u001b[36m(TemporaryActor pid=96977)\u001b[0m \n",
            "\u001b[36m(TemporaryActor pid=96977)\u001b[0m \u001b[36mray::datasets_stats_actor:_StatsActor.__init__()\u001b[39m (pid=96977, ip=10.0.60.61, actor_id=460e91963bc9181821a215a606000000, repr=<ray.data._internal.stats.FunctionActorManager._create_fake_actor_class.<locals>.TemporaryActor object at 0x747c092459a0>)\n",
            "\u001b[36m(TemporaryActor pid=96977)\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(TemporaryActor pid=96977)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/data/__init__.py\", line 3, in <module>\n",
            "\u001b[36m(TemporaryActor pid=96977)\u001b[0m     import pandas  # noqa\n",
            "\u001b[36m(TemporaryActor pid=96977)\u001b[0m     ^^^^^^^^^^^^^\n",
            "\u001b[36m(TemporaryActor pid=96977)\u001b[0m   File \"/tmp/ray/session_2025-10-08_16-09-42_447124_2335/runtime_resources/pip/8dbcd6388552d44db1cd46fd41790ddd4f4599c8/virtualenv/lib/python3.12/site-packages/pandas/__init__.py\", line 46, in <module>\n",
            "\u001b[36m(TemporaryActor pid=96977)\u001b[0m     from pandas.core.api import (\n",
            "\u001b[36m(TemporaryActor pid=96977)\u001b[0m   File \"/tmp/ray/session_2025-10-08_16-09-42_447124_2335/runtime_resources/pip/8dbcd6388552d44db1cd46fd41790ddd4f4599c8/virtualenv/lib/python3.12/site-packages/pandas/core/api.py\", line 1, in <module>\n",
            "\u001b[36m(TemporaryActor pid=96977)\u001b[0m     from pandas._libs import (\n",
            "\u001b[36m(TemporaryActor pid=96977)\u001b[0m   File \"/tmp/ray/session_2025-10-08_16-09-42_447124_2335/runtime_resources/pip/8dbcd6388552d44db1cd46fd41790ddd4f4599c8/virtualenv/lib/python3.12/site-packages/pandas/_libs/__init__.py\", line 18, in <module>\n",
            "\u001b[36m(TemporaryActor pid=96977)\u001b[0m     from pandas._libs.interval import Interval\n",
            "\u001b[36m(TemporaryActor pid=96977)\u001b[0m   File \"interval.pyx\", line 1, in init pandas._libs.interval\n",
            "\u001b[36m(TemporaryActor pid=96977)\u001b[0m ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Loaded ImageNet dataset for batch inference demo\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(TemporaryActor pid=97053)\u001b[0m \n",
            "\u001b[36m(TemporaryActor pid=97133)\u001b[0m \n",
            "2025-10-08 18:06:37,427\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_c0faddca62974092a319717e6034840c_0\n",
            "2025-10-08 18:06:37,429\tERROR exceptions.py:73 -- Exception occurred in Ray Data or Ray Core internal code. If you continue to see this error, please open an issue on the Ray project GitHub page with the full stack trace below: https://github.com/ray-project/ray/issues/new/choose\n",
            "2025-10-08 18:06:37,430\tERROR exceptions.py:81 -- Full stack trace:\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/data/exceptions.py\", line 49, in handle_trace\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/data/_internal/plan.py\", line 439, in execute_to_iterator\n",
            "    bundle_iter = execute_to_legacy_bundle_iterator(executor, self)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/data/_internal/execution/legacy_compat.py\", line 43, in execute_to_legacy_bundle_iterator\n",
            "    dag, stats = _get_execution_dag(\n",
            "                 ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/data/_internal/execution/legacy_compat.py\", line 145, in _get_execution_dag\n",
            "    dag = get_execution_plan(plan._logical_plan).dag\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/data/_internal/logical/optimizers.py\", line 80, in get_execution_plan\n",
            "    optimized_logical_plan = LogicalOptimizer().optimize(logical_plan)\n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/data/_internal/logical/interfaces/optimizer.py\", line 41, in optimize\n",
            "    plan = rule.apply(plan)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/anyscale/data/_internal/logical/rules/pushdown_count_files.py\", line 52, in apply\n",
            "    assert isinstance(list_files, ListFiles), list_files\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: Limit[limit=1000]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✗ ERROR: Failed to load dataset\n",
            "   Error details: AssertionError: Limit[limit=1000]\n",
            "\n",
            "Possible solutions:\n",
            "   1. Check if numpy/pandas versions are compatible:\n",
            "      pip install --upgrade numpy pandas\n",
            "   2. Or reinstall with matching versions:\n",
            "      pip install 'numpy<2.0' 'pandas>=2.0'\n",
            "   3. Check S3 access and ray.data availability\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "Limit[limit=1000]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSystemException\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;31mSystemException\u001b[0m: ",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/home/ray/default_cld_g54aiirwj1s8t9ktgzikqur41k/templates/templates/ray-data-templates/ray-data-batch-inference-optimization/01-inference-fundamentals.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-session-g3cs331q2c6ppx12j9ir5htn6l.i.anyscaleuserdata.com/home/ray/default_cld_g54aiirwj1s8t9ktgzikqur41k/templates/templates/ray-data-templates/ray-data-batch-inference-optimization/01-inference-fundamentals.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m dataset \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mread_images(\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-session-g3cs331q2c6ppx12j9ir5htn6l.i.anyscaleuserdata.com/home/ray/default_cld_g54aiirwj1s8t9ktgzikqur41k/templates/templates/ray-data-templates/ray-data-batch-inference-optimization/01-inference-fundamentals.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39ms3://ray-benchmark-data/imagenette2/train/\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-session-g3cs331q2c6ppx12j9ir5htn6l.i.anyscaleuserdata.com/home/ray/default_cld_g54aiirwj1s8t9ktgzikqur41k/templates/templates/ray-data-templates/ray-data-batch-inference-optimization/01-inference-fundamentals.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m,  \u001b[39m# Ensure consistent RGB color format\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-session-g3cs331q2c6ppx12j9ir5htn6l.i.anyscaleuserdata.com/home/ray/default_cld_g54aiirwj1s8t9ktgzikqur41k/templates/templates/ray-data-templates/ray-data-batch-inference-optimization/01-inference-fundamentals.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m )\u001b[39m.\u001b[39mlimit(\u001b[39m1000\u001b[39m)  \u001b[39m# Use 1K images for focused performance comparison\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode-session-g3cs331q2c6ppx12j9ir5htn6l.i.anyscaleuserdata.com/home/ray/default_cld_g54aiirwj1s8t9ktgzikqur41k/templates/templates/ray-data-templates/ray-data-batch-inference-optimization/01-inference-fundamentals.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m✓ Loaded ImageNet dataset for batch inference demo\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://vscode-session-g3cs331q2c6ppx12j9ir5htn6l.i.anyscaleuserdata.com/home/ray/default_cld_g54aiirwj1s8t9ktgzikqur41k/templates/templates/ray-data-templates/ray-data-batch-inference-optimization/01-inference-fundamentals.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m   Dataset size: \u001b[39m\u001b[39m{\u001b[39;00mdataset\u001b[39m.\u001b[39;49mcount()\u001b[39m}\u001b[39;00m\u001b[39m images\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-g3cs331q2c6ppx12j9ir5htn6l.i.anyscaleuserdata.com/home/ray/default_cld_g54aiirwj1s8t9ktgzikqur41k/templates/templates/ray-data-templates/ray-data-batch-inference-optimization/01-inference-fundamentals.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mSample dataset:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-session-g3cs331q2c6ppx12j9ir5htn6l.i.anyscaleuserdata.com/home/ray/default_cld_g54aiirwj1s8t9ktgzikqur41k/templates/templates/ray-data-templates/ray-data-batch-inference-optimization/01-inference-fundamentals.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m sample_batch \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mtake_batch(\u001b[39m3\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/ray/data/dataset.py:3373\u001b[0m, in \u001b[0;36mDataset.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3370\u001b[0m count_ds \u001b[39m=\u001b[39m Dataset(plan, logical_plan)\n\u001b[1;32m   3372\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 3373\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m count_ds\u001b[39m.\u001b[39miter_batches(batch_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   3374\u001b[0m     \u001b[39massert\u001b[39;00m Count\u001b[39m.\u001b[39mCOLUMN_NAME \u001b[39min\u001b[39;00m batch, (\n\u001b[1;32m   3375\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mOutputs from the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mCount\u001b[39m\u001b[39m'\u001b[39m\u001b[39m logical operator should contain a column \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3376\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnamed \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mCount\u001b[39m.\u001b[39mCOLUMN_NAME\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3377\u001b[0m     )\n\u001b[1;32m   3378\u001b[0m     count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch[Count\u001b[39m.\u001b[39mCOLUMN_NAME]\u001b[39m.\u001b[39msum()\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/ray/data/iterator.py:185\u001b[0m, in \u001b[0;36mDataIterator._iter_batches.<locals>._create_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    176\u001b[0m time_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m    177\u001b[0m \u001b[39m# Iterate through the dataset from the start each time\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[39m# _iterator_gen is called.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[39m# This allows multiple iterations of the dataset without\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[39m# needing to explicitly call `iter_batches()` multiple times.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m (\n\u001b[1;32m    182\u001b[0m     ref_bundles_iterator,\n\u001b[1;32m    183\u001b[0m     stats,\n\u001b[1;32m    184\u001b[0m     blocks_owned_by_consumer,\n\u001b[0;32m--> 185\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_to_ref_bundle_iterator()\n\u001b[1;32m    187\u001b[0m dataset_tag \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_dataset_tag()\n\u001b[1;32m    189\u001b[0m batch_iterator \u001b[39m=\u001b[39m BatchIterator(\n\u001b[1;32m    190\u001b[0m     ref_bundles_iterator,\n\u001b[1;32m    191\u001b[0m     stats\u001b[39m=\u001b[39mstats,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m     prefetch_batches\u001b[39m=\u001b[39mprefetch_batches,\n\u001b[1;32m    202\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/ray/data/_internal/iterator/iterator_impl.py:27\u001b[0m, in \u001b[0;36mDataIteratorImpl._to_ref_bundle_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_ref_bundle_iterator\u001b[39m(\n\u001b[1;32m     25\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     26\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Iterator[RefBundle], Optional[DatasetStats], \u001b[39mbool\u001b[39m]:\n\u001b[0;32m---> 27\u001b[0m     ref_bundles_iterator, stats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_base_dataset\u001b[39m.\u001b[39;49m_execute_to_iterator()\n\u001b[1;32m     28\u001b[0m     \u001b[39mreturn\u001b[39;00m ref_bundles_iterator, stats, \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/ray/data/dataset.py:6355\u001b[0m, in \u001b[0;36mDataset._execute_to_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   6354\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_execute_to_iterator\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Iterator[RefBundle], DatasetStats]:\n\u001b[0;32m-> 6355\u001b[0m     bundle_iter, stats, executor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plan\u001b[39m.\u001b[39;49mexecute_to_iterator()\n\u001b[1;32m   6356\u001b[0m     \u001b[39m# Capture current executor to be able to clean it up properly, once\u001b[39;00m\n\u001b[1;32m   6357\u001b[0m     \u001b[39m# dataset is garbage-collected\u001b[39;00m\n\u001b[1;32m   6358\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_executor \u001b[39m=\u001b[39m executor\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/ray/data/exceptions.py:89\u001b[0m, in \u001b[0;36momit_traceback_stdout.<locals>.handle_trace\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     88\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 89\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(\u001b[39mNone\u001b[39;00m) \u001b[39mfrom\u001b[39;00m \u001b[39mSystemException\u001b[39;00m()\n",
            "\u001b[0;31mAssertionError\u001b[0m: Limit[limit=1000]"
          ]
        }
      ],
      "source": [
        "# Load real ImageNet dataset for batch inference demonstration\n",
        "try:\n",
        "    print(\"Loading Imagenette dataset from S3...\")\n",
        "    dataset = ray.data.read_images(\n",
        "        \"s3://ray-benchmark-data/imagenette2/train/\",\n",
        "        mode=\"RGB\",  # Ensure consistent RGB color format\n",
        "    ).limit(1000)  # Use 1K images for focused performance comparison\n",
        "    \n",
        "    print(\"✓ Loaded ImageNet dataset for batch inference demo\")\n",
        "    print(f\"   Dataset size: {dataset.count()} images\")\n",
        "    print(\"\\nSample dataset:\")\n",
        "    sample_batch = dataset.take_batch(3)\n",
        "    print(f\"   Batch contains {len(sample_batch['image'])} images\")\n",
        "    print(f\"   Image shape: {sample_batch['image'][0].shape}\")\n",
        "    print(f\"   Image dtype: {sample_batch['image'][0].dtype}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n✗ ERROR: Failed to load dataset\")\n",
        "    print(f\"   Error details: {type(e).__name__}: {str(e)[:200]}\")\n",
        "    print(\"\\nPossible solutions:\")\n",
        "    print(\"   1. Check if numpy/pandas versions are compatible:\")\n",
        "    print(\"      pip install --upgrade numpy pandas\")\n",
        "    print(\"   2. Or reinstall with matching versions:\")\n",
        "    print(\"      pip install 'numpy<2.0' 'pandas>=2.0'\")\n",
        "    print(\"   3. Check S3 access and ray.data availability\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## The Wrong Way: Inefficient Batch Inference\n",
        "\n",
        "This section demonstrates a common anti-pattern in ML inference systems. Understanding why this approach fails is essential before learning the optimized solution.\n",
        "\n",
        "When models are loaded repeatedly for each batch, the initialization overhead dominates processing time. This pattern is unfortunately common in production systems where developers haven't considered the cost of model loading operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Dict, Any, List\n",
        "\n",
        "def inefficient_inference(batch: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"INEFFICIENT: Loads model for every single batch.\n",
        "    \n",
        "    Anti-pattern demonstration - DO NOT use this approach in production!\n",
        "    This function intentionally shows bad practices to highlight optimization opportunities.\n",
        "    \n",
        "    Args:\n",
        "        batch: Dictionary containing 'image' key with list of PIL Images\n",
        "        \n",
        "    Returns:\n",
        "        List of prediction dictionaries with 'prediction' and 'confidence' keys\n",
        "    \"\"\"\n",
        "    import time\n",
        "    from transformers import pipeline\n",
        "    \n",
        "    # BAD: Model loading happens inside function - repeats for every batch!\n",
        "    print(\"Loading model... (this happens for every batch!)\")\n",
        "    start_load = time.time()\n",
        "    classifier = pipeline(\"image-classification\", model=\"microsoft/resnet-50\")\n",
        "    load_time = time.time() - start_load\n",
        "    print(f\"Model loading took: {load_time:.2f} seconds\")\n",
        "    \n",
        "    # BAD: Processing images one by one instead of batched inference\n",
        "    results = []\n",
        "    for image in batch[\"image\"]:\n",
        "        prediction = classifier(image)\n",
        "        results.append({\n",
        "            \"prediction\": prediction[0][\"label\"],\n",
        "            \"confidence\": prediction[0][\"score\"]\n",
        "        })\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"Testing inefficient approach...\")\n",
        "print(\"TIP: Watch Ray Dashboard to see the performance problems\")\n",
        "\n",
        "# Run inefficient batch inference with small batches\n",
        "inefficient_results = dataset.limit(100).map_batches(\n",
        "    inefficient_inference,\n",
        "    batch_size=4,\n",
        "    concurrency=2\n",
        ").take(20)\n",
        "\n",
        "print(\"Inefficient approach completed\")\n",
        "print(\"   Problems: repeated model loading, poor batching, wasted resources\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected issues:**\n",
        "```\n",
        "Loading model... (this happens for every batch!)\n",
        "Model loading took: 3.45 seconds\n",
        "Loading model... (this happens for every batch!)\n",
        "Model loading took: 3.52 seconds\n",
        "[repeated 25 times...]\n",
        "```\n",
        "\n",
        ":::caution Performance Anti-Pattern\n",
        "- Model loads 25 times (one per batch)  \n",
        "- Each load takes 3+ seconds = 87.5 seconds wasted  \n",
        "- CPU/GPU mostly idle waiting for model loading  \n",
        "- Total throughput: ~1 image/second (unacceptable)\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "## Why the Naive Approach Fails\n",
        "\n",
        "Now that you've seen the inefficient implementation, you can understand exactly why it performs poorly.\n",
        "\n",
        "### Performance Bottlenecks Explained\n",
        "\n",
        "The inefficient approach suffers from three critical problems:\n",
        "\n",
        "#### Problem 1: Repeated Model Loading\n",
        "\n",
        "**What happens**: The model loads from scratch for every batch of 4 images.\n",
        "\n",
        "**Why it's expensive**:\n",
        "- Model weights file: 100-500 MB download per load\n",
        "- Neural network initialization: 2-5 seconds of setup\n",
        "- GPU memory allocation: Repeated allocation/deallocation cycles\n",
        "- Wasted overhead: Model loading time >> actual inference time\n",
        "\n",
        "**Impact**: If model loading takes 3 seconds and inference takes 0.1 seconds, you're spending 97% of time on overhead!\n",
        "\n",
        "#### Problem 2: Poor Batch Utilization\n",
        "\n",
        "**What happens**: Processing only 4 images at a time with individual processing.\n",
        "\n",
        "**Why it's inefficient**:\n",
        "- GPU underutilization: Modern GPUs can handle 32-128 images simultaneously\n",
        "- Memory waste: Using <10% of available GPU memory\n",
        "- No vectorization: Processing images one-by-one instead of batched tensors\n",
        "- Task overhead: Creating many small tasks instead of fewer large ones\n",
        "\n",
        "**Impact**: GPU sits idle 90% of the time waiting for data.\n",
        "\n",
        "#### Problem 3: Inefficient Resource Allocation\n",
        "\n",
        "**What happens**: Low concurrency with default settings.\n",
        "\n",
        "**Why it creates bottlenecks**:\n",
        "- Limited parallelism: Only 2 concurrent workers\n",
        "- Unbalanced pipeline: Preprocessing can't keep up with potential GPU throughput\n",
        "- Resource waste: CPU cores sit idle while waiting for model loading\n",
        "\n",
        "**Impact**: Cluster resources are underutilized, extending total processing time.\n",
        "\n",
        "### Performance Anti-pattern Summary\n",
        "\n",
        "| Anti-Pattern | Why It's Bad | Typical Impact |\n",
        "|--------------|-------------|----------------|\n",
        "| **Model loading per batch** | Initialization overhead >> inference time | 10-100x slower |\n",
        "| **Small batch sizes** | GPU memory underutilized | 5-10x slower |\n",
        "| **Sequential processing** | No vectorization benefits | 3-5x slower |\n",
        "| **Low concurrency** | Limited parallelism | 2-4x slower |\n",
        "\n",
        "**Combined effect**: These anti-patterns compound, making the inefficient approach significantly slower than optimized implementations.\n",
        "\n",
        "---\n",
        "\n",
        "## The Right Way: Optimized with Ray Data\n",
        "\n",
        "Now you can see how Ray Data solves these problems with actor-based inference, proper batching, and optimized resource allocation.\n",
        "\n",
        "Ray Data solves the model loading problem by letting you run stateful, class-based `map_batches` with an actor pool strategy. Each worker loads the model once and reuses it across many batches, eliminating repeated initialization overhead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Dict, Any, List\n",
        "\n",
        "# Efficient: Use Ray Data class-based map_batches with optimized actor configuration\n",
        "\n",
        "class InferenceWorker:\n",
        "    \"\"\"Stateful worker that loads the model once and reuses it.\n",
        "    \n",
        "    This is the CORRECT pattern for batch inference - model loads once in __init__\n",
        "    and is reused across all batches processed by this worker.\n",
        "    \n",
        "    Works on both CPU and GPU - automatically detects hardware.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize worker - called once per actor, not per batch!\"\"\"\n",
        "        from transformers import pipeline\n",
        "        import torch\n",
        "        \n",
        "        # Automatically use GPU if available, otherwise CPU\n",
        "        device = 0 if torch.cuda.is_available() else -1\n",
        "        self.classifier = pipeline(\n",
        "            \"image-classification\",\n",
        "            model=\"microsoft/resnet-50\",\n",
        "            device=device,\n",
        "        )\n",
        "        print(f\"Model loaded on: {'GPU' if device >= 0 else 'CPU'}\")\n",
        "\n",
        "    def __call__(self, batch: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Process a batch of images - called many times, reuses loaded model.\n",
        "        \n",
        "        Args:\n",
        "            batch: Dictionary containing 'image' key with list of PIL Images\n",
        "            \n",
        "        Returns:\n",
        "            List of prediction dictionaries\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        for image in batch[\"image\"]:\n",
        "            pred = self.classifier(image)\n",
        "            results.append({\n",
        "                \"prediction\": pred[0][\"label\"],\n",
        "                \"confidence\": pred[0][\"score\"],\n",
        "            })\n",
        "        return results\n",
        "\n",
        "print(\"Running optimized Ray Data inference with stateful workers...\")\n",
        "\n",
        "# Best practice: Use the new concurrency parameter for actor-based processing\n",
        "# Resource allocation adapts to available hardware\n",
        "try:\n",
        "    inference_results = dataset.limit(100).map_batches(\n",
        "        InferenceWorker,\n",
        "        concurrency=2,      # Number of parallel actors\n",
        "        num_gpus=1 if HAS_GPU else 0,  # Allocate GPU if available\n",
        "        num_cpus=2 if not HAS_GPU else 1,  # Use more CPU cores if no GPU\n",
        "        batch_size=16,      # Optimal batch size for resource utilization\n",
        "    ).take(20)\n",
        "    \n",
        "    print(\"Optimized approach completed successfully!\")\n",
        "    print(\"   Improvements: single model load per worker, better batching, efficient resource use\")\n",
        "    print(f\"   Processed {len(inference_results)} images\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Inference failed: {e}\")\n",
        "    print(\"   Check that transformers and torch are installed\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**What's Better:**\n",
        "- Model loads only once per worker via Ray Data `ActorPoolStrategy`\n",
        "- Larger batch sizes for better resource utilization\n",
        "- Proper resource allocation with `num_gpus=1` (GPU) or `num_cpus=2` (CPU)\n",
        "- Ray Data automatically manages distribution across workers\n",
        "- **Works identically on CPU and GPU clusters with zero code changes**\n",
        "\n",
        ":::tip Resource Allocation Patterns\n",
        "**GPU clusters**: Use `num_gpus=1` to allocate one GPU per actor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example pattern for GPU clusters:\n",
        "# .map_batches(InferenceWorker, num_gpus=1, concurrency=2)  # 2 GPUs used"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**CPU clusters**: Use `num_cpus=2` to allocate CPU cores per actor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example pattern for CPU clusters:\n",
        "# .map_batches(InferenceWorker, num_cpus=2, concurrency=4)  # 8 CPU cores used"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The patterns are identical - Ray Data abstracts away the hardware differences!\n",
        ":::\n",
        "\n",
        "### Performance Expectations: CPU vs GPU\n",
        "\n",
        ":::tip Performance Scaling\n",
        "**Both CPU and GPU deployments benefit from Ray Data optimizations!**\n",
        "\n",
        "**GPU clusters** (when available):\n",
        "- **Throughput**: 500-2000 images/second (model dependent)\n",
        "- **Best for**: Large models, high-volume inference\n",
        "- **Resource**: Use `num_gpus=1` per actor\n",
        "\n",
        "**CPU clusters** (always available):\n",
        "- **Throughput**: 50-200 images/second (model dependent)\n",
        "- **Best for**: Development, smaller models, cost-sensitive workloads\n",
        "- **Resource**: Use `num_cpus=2-4` per actor\n",
        "\n",
        "**Key insight**: The optimization patterns (actor-based loading, batching, concurrency) provide similar relative speedups on both CPU and GPU!\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Adaptive resource allocation based on available hardware\n",
        "def create_inference_config():\n",
        "    \"\"\"Generate optimal configuration for available hardware.\"\"\"\n",
        "    import torch\n",
        "    has_gpu = torch.cuda.is_available()\n",
        "    \n",
        "    if has_gpu:\n",
        "        return {\n",
        "            \"num_gpus\": 1,\n",
        "            \"concurrency\": torch.cuda.device_count(),\n",
        "            \"batch_size\": 32,  # Larger batches for GPU\n",
        "            \"expected_throughput\": \"500-2000 images/sec\"\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            \"num_cpus\": 2,\n",
        "            \"concurrency\": int(ray.available_resources().get(\"CPU\", 4) // 2),\n",
        "            \"batch_size\": 16,  # Smaller batches for CPU\n",
        "            \"expected_throughput\": \"50-200 images/sec\"\n",
        "        }\n",
        "\n",
        "config = create_inference_config()\n",
        "print(f\"Optimized configuration for your cluster: {config}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Key Takeaways from Part 1\n",
        "\n",
        "You've learned the fundamentals of batch inference optimization:\n",
        "- Identified common anti-patterns that destroy performance\n",
        "- Understood why repeated model loading is catastrophic  \n",
        "- Implemented class-based actors for stateful model loading\n",
        "- Used proper resource allocation with `num_gpus` and `concurrency`\n",
        "- Learned CPU and GPU compatibility patterns\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "Now that you understand the fundamentals, you're ready to learn systematic optimization techniques.\n",
        "\n",
        "**[← Back to Overview](README.md)** | **[Continue to Part 2: Advanced Optimization →](02-advanced-optimization.md)**\n",
        "\n",
        "In Part 2, you'll learn:\n",
        "- Systematic decision frameworks for choosing optimization techniques\n",
        "- Multi-model ensemble inference patterns\n",
        "- Performance monitoring and diagnostics\n",
        "- Production deployment best practices\n",
        "\n",
        "**Or skip ahead to Part 3** for a deep dive into Ray Data's architecture:\n",
        "\n",
        "**[Jump to Part 3: Ray Data Architecture →](03-ray-data-architecture.md)**\n",
        "\n",
        "In Part 3, you'll learn:\n",
        "- How streaming execution enables unlimited dataset processing\n",
        "- How blocks and memory management affect optimization\n",
        "- How operator fusion and backpressure work\n",
        "- How to calculate optimal parameters from architectural principles\n",
        "\n",
        "**[Return to overview](README.md)** to see all available parts.\n",
        "\n",
        "---\n",
        "\n",
        "## Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up Ray resources when done\n",
        "print(\"\\nCleaning up Ray resources...\")\n",
        "try:\n",
        "    if ray.is_initialized():\n",
        "        ray.shutdown()\n",
        "        print(\"Ray resources cleaned up successfully\")\n",
        "    else:\n",
        "        print(\"INFO: Ray was not initialized, no cleanup needed\")\n",
        "except Exception as e:\n",
        "    print(f\"WARNING: Error during cleanup: {e}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
