{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4810fa5-a00b-4708-9177-a71a7f9e9f5b",
   "metadata": {},
   "source": [
    "# Part 1: Inference Fundamentals\n",
    "\n",
    "**Time to complete**: 20 min \\| **Difficulty**: Beginner \\| **Prerequisites**: Basic Python, ML model concepts\n",
    "\n",
    "**[← Back to Overview](README.md)** \\| **[Continue to Part 2 →](02-advanced-optimization.md)**\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## What You’ll Learn\n",
    "\n",
    "In this part, you’ll understand the fundamentals of batch inference optimization by comparing inefficient and efficient approaches:\n",
    "- How to set up Ray Data for accelerated inference (CPU or GPU)\n",
    "- Why naive inference patterns create performance bottlenecks\n",
    "- How Ray Data’s actor-based pattern solves these problems\n",
    "- How to implement optimized inference with proper resource allocation for both CPU and GPU\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1.  [Introduction and Setup](#introduction-and-setup)\n",
    "2.  [The Wrong Way: Inefficient Batch Inference](#the-wrong-way-inefficient-batch-inference)\n",
    "3.  [Why the Naive Approach Fails](#why-the-naive-approach-fails)\n",
    "4.  [The Right Way: Optimized with Ray Data](#the-right-way-optimized-with-ray-data)\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Introduction and Setup\n",
    "\n",
    "Batch inference is the process of running ML model predictions on large batches of data. While this sounds straightforward, naive implementations create severe performance bottlenecks that prevent production deployment. This part shows you the difference between inefficient and optimized approaches using real-world examples.\n",
    "\n",
    "### What You’ll Learn\n",
    "\n",
    "By comparing inefficient and optimized implementations, you’ll understand:\n",
    "- **Why** repeated model loading destroys performance\n",
    "- **How** Ray Data’s actor pattern solves the problem\n",
    "- **When** to apply specific optimization techniques\n",
    "- **What** parameters to tune for your workload\n",
    "\n",
    "### Initial Setup\n",
    "\n",
    "``` python\n",
    "import ray\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "# Initialize Ray for distributed processing\n",
    "ray.init()\n",
    "\n",
    "# Configure Ray Data for optimal performance monitoring\n",
    "ctx = ray.data.DataContext.get_current()\n",
    "ctx.enable_progress_bars = True\n",
    "ctx.enable_operator_progress_bars = True\n",
    "\n",
    "print(\"Ray cluster initialized for batch inference optimization\")\n",
    "print(f\"Available resources: {ray.cluster_resources()}\")\n",
    "\n",
    "# Detect hardware availability\n",
    "HAS_GPU = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if HAS_GPU else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "if HAS_GPU:\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "    print(\"GPU detected - examples will use GPU acceleration\")\n",
    "else:\n",
    "    print(\"No GPU detected - examples will run on CPU\")\n",
    "    print(\"   (All patterns work identically on CPU, just use num_cpus instead of num_gpus)\")\n",
    "```\n",
    "\n",
    ":::note CPU and GPU Compatibility\n",
    "**This template works on both CPU-only and GPU clusters!**\n",
    "\n",
    "- **GPU clusters**: Code automatically detects GPUs and uses `num_gpus=1` for acceleration\n",
    "- **CPU clusters**: Code falls back to CPU and uses `num_cpus=2` for parallelism\n",
    "\n",
    "All optimization concepts (actor-based loading, batching, concurrency) apply equally to both environments.\n",
    ":::\n",
    "\n",
    "### Load Demo Dataset\n",
    "\n",
    "For this demonstration, you’ll use the Imagenette dataset, which provides a realistic subset of ImageNet with 10 classes.\n",
    "\n",
    "``` python\n",
    "# Load real ImageNet dataset for batch inference demonstration\n",
    "try:\n",
    "    dataset = ray.data.read_images(\n",
    "        \"s3://ray-benchmark-data/imagenette2/train/\",\n",
    "        mode=\"RGB\",  # Ensure consistent RGB color format\n",
    "        num_cpus=0.05\n",
    "    ).limit(1000)  # Use 1K images for focused performance comparison\n",
    "    \n",
    "    print(\"Loaded ImageNet dataset for batch inference demo\")\n",
    "    print(f\"   Dataset size: {dataset.count()} images\")\n",
    "    print(\"\\nSample dataset:\")\n",
    "    sample_batch = dataset.take_batch(3)\n",
    "    print(f\"   Batch contains {len(sample_batch['image'])} images\")\n",
    "    print(f\"   Image shape: {sample_batch['image'][0].shape}\")\n",
    "    print(f\"   Image dtype: {sample_batch['image'][0].dtype}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to load dataset: {e}\")\n",
    "    print(\"   Check S3 access and ray.data.read_images() availability\")\n",
    "    raise\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## The Wrong Way: Inefficient Batch Inference\n",
    "\n",
    "This section demonstrates a common anti-pattern in ML inference systems. Understanding why this approach fails is essential before learning the optimized solution.\n",
    "\n",
    "When models are loaded repeatedly for each batch, the initialization overhead dominates processing time. This pattern is unfortunately common in production systems where developers haven’t considered the cost of model loading operations.\n",
    "\n",
    "``` python\n",
    "def inefficient_inference(batch: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"INEFFICIENT: Loads model for every single batch.\n",
    "    \n",
    "    Anti-pattern demonstration - DO NOT use this approach in production!\n",
    "    This function intentionally shows bad practices to highlight optimization opportunities.\n",
    "    \n",
    "    Args:\n",
    "        batch: Dictionary containing 'image' key with list of PIL Images\n",
    "        \n",
    "    Returns:\n",
    "        List of prediction dictionaries with 'prediction' and 'confidence' keys\n",
    "    \"\"\"\n",
    "    import time\n",
    "    from transformers import pipeline\n",
    "    \n",
    "    # BAD: Model loading happens inside function - repeats for every batch!\n",
    "    print(\"Loading model... (this happens for every batch!)\")\n",
    "    start_load = time.time()\n",
    "    classifier = pipeline(\"image-classification\", model=\"microsoft/resnet-50\")\n",
    "    load_time = time.time() - start_load\n",
    "    print(f\"Model loading took: {load_time:.2f} seconds\")\n",
    "    \n",
    "    # BAD: Processing images one by one instead of batched inference\n",
    "    results = []\n",
    "    for image in batch[\"image\"]:\n",
    "        prediction = classifier(image)\n",
    "        results.append({\n",
    "            \"prediction\": prediction[0][\"label\"],\n",
    "            \"confidence\": prediction[0][\"score\"]\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Testing inefficient approach...\")\n",
    "print(\"TIP: Watch Ray Dashboard to see the performance problems\")\n",
    "\n",
    "# Run inefficient batch inference with small batches\n",
    "inefficient_results = dataset.limit(100).map_batches(\n",
    "    inefficient_inference,\n",
    "    batch_size=4,\n",
    "    concurrency=2\n",
    ").take(20)\n",
    "\n",
    "print(\"Inefficient approach completed\")\n",
    "print(\"   Problems: repeated model loading, poor batching, wasted resources\")\n",
    "```\n",
    "\n",
    "**Expected issues:**\n",
    "\n",
    "    Loading model... (this happens for every batch!)\n",
    "    Model loading took: 3.45 seconds\n",
    "    Loading model... (this happens for every batch!)\n",
    "    Model loading took: 3.52 seconds\n",
    "    [repeated 25 times...]\n",
    "\n",
    ":::caution Performance Anti-Pattern\n",
    "- Model loads 25 times (one per batch)  \n",
    "- Each load takes 3+ seconds = 87.5 seconds wasted  \n",
    "- CPU/GPU mostly idle waiting for model loading  \n",
    "- Total throughput: ~1 image/second (unacceptable)\n",
    ":::\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Why the Naive Approach Fails\n",
    "\n",
    "Now that you’ve seen the inefficient implementation, you can understand exactly why it performs poorly.\n",
    "\n",
    "### Performance Bottlenecks Explained\n",
    "\n",
    "The inefficient approach suffers from three critical problems:\n",
    "\n",
    "#### Problem 1: Repeated Model Loading\n",
    "\n",
    "**What happens**: The model loads from scratch for every batch of 4 images.\n",
    "\n",
    "**Why it’s expensive**:\n",
    "- Model weights file: 100-500 MB download per load\n",
    "- Neural network initialization: 2-5 seconds of setup\n",
    "- GPU memory allocation: Repeated allocation/deallocation cycles\n",
    "- Wasted overhead: Model loading time \\>\\> actual inference time\n",
    "\n",
    "**Impact**: If model loading takes 3 seconds and inference takes 0.1 seconds, you’re spending 97% of time on overhead!\n",
    "\n",
    "#### Problem 2: Poor Batch Utilization\n",
    "\n",
    "**What happens**: Processing only 4 images at a time with individual processing.\n",
    "\n",
    "**Why it’s inefficient**:\n",
    "- GPU underutilization: Modern GPUs can handle 32-128 images simultaneously\n",
    "- Memory waste: Using \\<10% of available GPU memory\n",
    "- No vectorization: Processing images one-by-one instead of batched tensors\n",
    "- Task overhead: Creating many small tasks instead of fewer large ones\n",
    "\n",
    "**Impact**: GPU sits idle 90% of the time waiting for data.\n",
    "\n",
    "#### Problem 3: Inefficient Resource Allocation\n",
    "\n",
    "**What happens**: Low concurrency with default settings.\n",
    "\n",
    "**Why it creates bottlenecks**:\n",
    "- Limited parallelism: Only 2 concurrent workers\n",
    "- Unbalanced pipeline: Preprocessing can’t keep up with potential GPU throughput\n",
    "- Resource waste: CPU cores sit idle while waiting for model loading\n",
    "\n",
    "**Impact**: Cluster resources are underutilized, extending total processing time.\n",
    "\n",
    "### Performance Anti-pattern Summary\n",
    "\n",
    "| Anti-Pattern | Why It’s Bad | Typical Impact |\n",
    "|------------------------|----------------------|---------------------------|\n",
    "| **Model loading per batch** | Initialization overhead \\>\\> inference time | 10-100x slower |\n",
    "| **Small batch sizes** | GPU memory underutilized | 5-10x slower |\n",
    "| **Sequential processing** | No vectorization benefits | 3-5x slower |\n",
    "| **Low concurrency** | Limited parallelism | 2-4x slower |\n",
    "\n",
    "**Combined effect**: These anti-patterns compound, making the inefficient approach significantly slower than optimized implementations.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## The Right Way: Optimized with Ray Data\n",
    "\n",
    "Now you can see how Ray Data solves these problems with actor-based inference, proper batching, and optimized resource allocation.\n",
    "\n",
    "Ray Data solves the model loading problem by letting you run stateful, class-based `map_batches` with an actor pool strategy. Each worker loads the model once and reuses it across many batches, eliminating repeated initialization overhead.\n",
    "\n",
    "``` python\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# Efficient: Use Ray Data class-based map_batches with optimized actor configuration\n",
    "\n",
    "class InferenceWorker:\n",
    "    \"\"\"Stateful worker that loads the model once and reuses it.\n",
    "    \n",
    "    This is the CORRECT pattern for batch inference - model loads once in __init__\n",
    "    and is reused across all batches processed by this worker.\n",
    "    \n",
    "    Works on both CPU and GPU - automatically detects hardware.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize worker - called once per actor, not per batch!\"\"\"\n",
    "        from transformers import pipeline\n",
    "        import torch\n",
    "        \n",
    "        # Automatically use GPU if available, otherwise CPU\n",
    "        device = 0 if torch.cuda.is_available() else -1\n",
    "        self.classifier = pipeline(\n",
    "            \"image-classification\",\n",
    "            model=\"microsoft/resnet-50\",\n",
    "            device=device,\n",
    "        )\n",
    "        print(f\"Model loaded on: {'GPU' if device >= 0 else 'CPU'}\")\n",
    "\n",
    "    def __call__(self, batch: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process a batch of images - called many times, reuses loaded model.\n",
    "        \n",
    "        Args:\n",
    "            batch: Dictionary containing 'image' key with list of PIL Images\n",
    "            \n",
    "        Returns:\n",
    "            List of prediction dictionaries\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for image in batch[\"image\"]:\n",
    "            pred = self.classifier(image)\n",
    "            results.append({\n",
    "                \"prediction\": pred[0][\"label\"],\n",
    "                \"confidence\": pred[0][\"score\"],\n",
    "            })\n",
    "        return results\n",
    "\n",
    "print(\"Running optimized Ray Data inference with stateful workers...\")\n",
    "\n",
    "# Best practice: Use the new concurrency parameter for actor-based processing\n",
    "# Resource allocation adapts to available hardware\n",
    "try:\n",
    "    inference_results = dataset.limit(100).map_batches(\n",
    "        InferenceWorker,\n",
    "        concurrency=2,      # Number of parallel actors\n",
    "        num_gpus=1 if HAS_GPU else 0,  # Allocate GPU if available\n",
    "        num_cpus=2 if not HAS_GPU else 1,  # Use more CPU cores if no GPU\n",
    "        batch_size=16,      # Optimal batch size for resource utilization\n",
    "    ).take(20)\n",
    "    \n",
    "    print(\"Optimized approach completed successfully!\")\n",
    "    print(\"   Improvements: single model load per worker, better batching, efficient resource use\")\n",
    "    print(f\"   Processed {len(inference_results)} images\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Inference failed: {e}\")\n",
    "    print(\"   Check that transformers and torch are installed\")\n",
    "    raise\n",
    "```\n",
    "\n",
    "**What’s Better:**\n",
    "- Model loads only once per worker via Ray Data `ActorPoolStrategy`\n",
    "- Larger batch sizes for better resource utilization\n",
    "- Proper resource allocation with `num_gpus=1` (GPU) or `num_cpus=2` (CPU)\n",
    "- Ray Data automatically manages distribution across workers\n",
    "- **Works identically on CPU and GPU clusters with zero code changes**\n",
    "\n",
    ":::tip Resource Allocation Patterns\n",
    "**GPU clusters**: Use `num_gpus=1` to allocate one GPU per actor\n",
    "\n",
    "``` python\n",
    ".map_batches(InferenceWorker, num_gpus=1, concurrency=2)  # 2 GPUs used\n",
    "```\n",
    "\n",
    "**CPU clusters**: Use `num_cpus=2` to allocate CPU cores per actor\n",
    "\n",
    "``` python\n",
    ".map_batches(InferenceWorker, num_cpus=2, concurrency=4)  # 8 CPU cores used\n",
    "```\n",
    "\n",
    "The patterns are identical - Ray Data abstracts away the hardware differences!\n",
    ":::\n",
    "\n",
    "### Performance Expectations: CPU vs GPU\n",
    "\n",
    ":::tip Performance Scaling\n",
    "**Both CPU and GPU deployments benefit from Ray Data optimizations!**\n",
    "\n",
    "**GPU clusters** (when available):\n",
    "- **Throughput**: 500-2000 images/second (model dependent)\n",
    "- **Best for**: Large models, high-volume inference\n",
    "- **Resource**: Use `num_gpus=1` per actor\n",
    "\n",
    "**CPU clusters** (always available):\n",
    "- **Throughput**: 50-200 images/second (model dependent)\n",
    "- **Best for**: Development, smaller models, cost-sensitive workloads\n",
    "- **Resource**: Use `num_cpus=2-4` per actor\n",
    "\n",
    "**Key insight**: The optimization patterns (actor-based loading, batching, concurrency) provide similar relative speedups on both CPU and GPU!\n",
    ":::\n",
    "\n",
    "``` python\n",
    "# Example: Adaptive resource allocation based on available hardware\n",
    "def create_inference_config():\n",
    "    \"\"\"Generate optimal configuration for available hardware.\"\"\"\n",
    "    import torch\n",
    "    has_gpu = torch.cuda.is_available()\n",
    "    \n",
    "    if has_gpu:\n",
    "        return {\n",
    "            \"num_gpus\": 1,\n",
    "            \"concurrency\": torch.cuda.device_count(),\n",
    "            \"batch_size\": 32,  # Larger batches for GPU\n",
    "            \"expected_throughput\": \"500-2000 images/sec\"\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"num_cpus\": 2,\n",
    "            \"concurrency\": ray.available_resources()[\"CPU\"] // 2,\n",
    "            \"batch_size\": 16,  # Smaller batches for CPU\n",
    "            \"expected_throughput\": \"50-200 images/sec\"\n",
    "        }\n",
    "\n",
    "config = create_inference_config()\n",
    "print(f\"Optimized configuration for your cluster: {config}\")\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Key Takeaways from Part 1\n",
    "\n",
    "You’ve learned the fundamentals of batch inference optimization:\n",
    "- Identified common anti-patterns that destroy performance\n",
    "- Understood why repeated model loading is catastrophic  \n",
    "- Implemented class-based actors for stateful model loading\n",
    "- Used proper resource allocation with `num_gpus` and `concurrency`\n",
    "- Learned CPU and GPU compatibility patterns\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you understand the fundamentals, you’re ready to learn systematic optimization techniques.\n",
    "\n",
    "**[← Back to Overview](README.md)** \\| **[Continue to Part 2: Advanced Optimization →](02-advanced-optimization.md)**\n",
    "\n",
    "In Part 2, you’ll learn:\n",
    "- Systematic decision frameworks for choosing optimization techniques\n",
    "- Multi-model ensemble inference patterns\n",
    "- Performance monitoring and diagnostics\n",
    "- Production deployment best practices\n",
    "\n",
    "**Or skip ahead to Part 3** for a deep dive into Ray Data’s architecture:\n",
    "\n",
    "**[Jump to Part 3: Ray Data Architecture →](03-ray-data-architecture.md)**\n",
    "\n",
    "In Part 3, you’ll learn:\n",
    "- How streaming execution enables unlimited dataset processing\n",
    "- How blocks and memory management affect optimization\n",
    "- How operator fusion and backpressure work\n",
    "- How to calculate optimal parameters from architectural principles\n",
    "\n",
    "**[Return to overview](README.md)** to see all available parts.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "``` python\n",
    "# Clean up Ray resources when done\n",
    "print(\"\\nCleaning up Ray resources...\")\n",
    "try:\n",
    "    if ray.is_initialized():\n",
    "        ray.shutdown()\n",
    "        print(\"Ray resources cleaned up successfully\")\n",
    "    else:\n",
    "        print(\"INFO: Ray was not initialized, no cleanup needed\")\n",
    "except Exception as e:\n",
    "    print(f\"WARNING: Error during cleanup: {e}\")\n",
    "```"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
