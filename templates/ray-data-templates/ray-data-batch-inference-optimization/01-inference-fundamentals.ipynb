{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1: Inference Fundamentals\n",
        "\n",
        "**Time to complete**: 20 min | **Difficulty**: Beginner | **Prerequisites**: Basic Python, ML model concepts\n",
        "\n",
        "**[← Back to Overview](README.md)** | **[Continue to Part 2 →](02-advanced-optimization.md)**\n",
        "\n",
        "---\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "In this part, you'll understand the fundamentals of batch inference optimization by comparing inefficient and efficient approaches:\n",
        "- How to set up Ray Data for accelerated inference (CPU or GPU)\n",
        "- Why naive inference patterns create performance bottlenecks\n",
        "- How Ray Data's actor-based pattern solves these problems\n",
        "- How to implement optimized inference with proper resource allocation for both CPU and GPU\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Introduction and Setup](#introduction-and-setup)\n",
        "2. [The Wrong Way: Inefficient Batch Inference](#the-wrong-way-inefficient-batch-inference)\n",
        "3. [Why the Naive Approach Fails](#why-the-naive-approach-fails)\n",
        "4. [The Right Way: Optimized with Ray Data](#the-right-way-optimized-with-ray-data)\n",
        "\n",
        "---\n",
        "\n",
        "## Introduction and Setup\n",
        "\n",
        "Batch inference is the process of running ML model predictions on large batches of data. While this sounds straightforward, naive implementations create severe performance bottlenecks that prevent production deployment. This part shows you the difference between inefficient and optimized approaches using real-world examples.\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "By comparing inefficient and optimized implementations, you'll understand:\n",
        "- **Why** repeated model loading destroys performance\n",
        "- **How** Ray Data's actor pattern solves the problem\n",
        "- **When** to apply specific optimization techniques\n",
        "- **What** parameters to tune for your workload\n",
        "\n",
        "### Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ray/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "2025-10-08 16:53:53,918\tINFO worker.py:1771 -- Connecting to existing Ray cluster at address: 10.0.60.61:6379...\n",
            "2025-10-08 16:53:53,930\tINFO worker.py:1942 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-g3cs331q2c6ppx12j9ir5htn6l.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
            "2025-10-08 16:53:53,936\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_65d2749b7e044d1a9bfd6a976e75b481109bb335.zip' (0.19MiB) to Ray cluster...\n",
            "2025-10-08 16:53:53,938\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_65d2749b7e044d1a9bfd6a976e75b481109bb335.zip'.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m ray\u001b[38;5;241m.\u001b[39minit()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Configure Ray Data for optimal performance monitoring\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m ctx \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mDataContext\u001b[38;5;241m.\u001b[39mget_current()\n\u001b[1;32m     12\u001b[0m ctx\u001b[38;5;241m.\u001b[39menable_progress_bars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     13\u001b[0m ctx\u001b[38;5;241m.\u001b[39menable_operator_progress_bars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/ray/__init__.py:297\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworkflow\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautoscaler\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap_external>:999\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/ray/data/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Short term workaround for https://github.com/ray-project/ray/issues/32435\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Dataset has a hard dependency on pandas, so it doesn't need to be delayed.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_private\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_pyarrow_version\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/__init__.py:46\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     ArrowDtype,\n\u001b[1;32m     49\u001b[0m     Int8Dtype,\n\u001b[1;32m     50\u001b[0m     Int16Dtype,\n\u001b[1;32m     51\u001b[0m     Int32Dtype,\n\u001b[1;32m     52\u001b[0m     Int64Dtype,\n\u001b[1;32m     53\u001b[0m     UInt8Dtype,\n\u001b[1;32m     54\u001b[0m     UInt16Dtype,\n\u001b[1;32m     55\u001b[0m     UInt32Dtype,\n\u001b[1;32m     56\u001b[0m     UInt64Dtype,\n\u001b[1;32m     57\u001b[0m     Float32Dtype,\n\u001b[1;32m     58\u001b[0m     Float64Dtype,\n\u001b[1;32m     59\u001b[0m     CategoricalDtype,\n\u001b[1;32m     60\u001b[0m     PeriodDtype,\n\u001b[1;32m     61\u001b[0m     IntervalDtype,\n\u001b[1;32m     62\u001b[0m     DatetimeTZDtype,\n\u001b[1;32m     63\u001b[0m     StringDtype,\n\u001b[1;32m     64\u001b[0m     BooleanDtype,\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     NA,\n\u001b[1;32m     67\u001b[0m     isna,\n\u001b[1;32m     68\u001b[0m     isnull,\n\u001b[1;32m     69\u001b[0m     notna,\n\u001b[1;32m     70\u001b[0m     notnull,\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     Index,\n\u001b[1;32m     73\u001b[0m     CategoricalIndex,\n\u001b[1;32m     74\u001b[0m     RangeIndex,\n\u001b[1;32m     75\u001b[0m     MultiIndex,\n\u001b[1;32m     76\u001b[0m     IntervalIndex,\n\u001b[1;32m     77\u001b[0m     TimedeltaIndex,\n\u001b[1;32m     78\u001b[0m     DatetimeIndex,\n\u001b[1;32m     79\u001b[0m     PeriodIndex,\n\u001b[1;32m     80\u001b[0m     IndexSlice,\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     NaT,\n\u001b[1;32m     83\u001b[0m     Period,\n\u001b[1;32m     84\u001b[0m     period_range,\n\u001b[1;32m     85\u001b[0m     Timedelta,\n\u001b[1;32m     86\u001b[0m     timedelta_range,\n\u001b[1;32m     87\u001b[0m     Timestamp,\n\u001b[1;32m     88\u001b[0m     date_range,\n\u001b[1;32m     89\u001b[0m     bdate_range,\n\u001b[1;32m     90\u001b[0m     Interval,\n\u001b[1;32m     91\u001b[0m     interval_range,\n\u001b[1;32m     92\u001b[0m     DateOffset,\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     to_numeric,\n\u001b[1;32m     95\u001b[0m     to_datetime,\n\u001b[1;32m     96\u001b[0m     to_timedelta,\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     Flags,\n\u001b[1;32m     99\u001b[0m     Grouper,\n\u001b[1;32m    100\u001b[0m     factorize,\n\u001b[1;32m    101\u001b[0m     unique,\n\u001b[1;32m    102\u001b[0m     value_counts,\n\u001b[1;32m    103\u001b[0m     NamedAgg,\n\u001b[1;32m    104\u001b[0m     array,\n\u001b[1;32m    105\u001b[0m     Categorical,\n\u001b[1;32m    106\u001b[0m     set_eng_float_format,\n\u001b[1;32m    107\u001b[0m     Series,\n\u001b[1;32m    108\u001b[0m     DataFrame,\n\u001b[1;32m    109\u001b[0m )\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/api.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     NaT,\n\u001b[1;32m      3\u001b[0m     Period,\n\u001b[1;32m      4\u001b[0m     Timedelta,\n\u001b[1;32m      5\u001b[0m     Timestamp,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     ArrowDtype,\n\u001b[1;32m     11\u001b[0m     CategoricalDtype,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     PeriodDtype,\n\u001b[1;32m     15\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/_libs/__init__.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401,E501 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     20\u001b[0m     NaT,\n\u001b[1;32m     21\u001b[0m     NaTType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     iNaT,\n\u001b[1;32m     27\u001b[0m )\n",
            "File \u001b[0;32minterval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "import ray\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import time\n",
        "\n",
        "# Initialize Ray for distributed processing\n",
        "ray.init()\n",
        "\n",
        "print(\"Ray cluster initialized for batch inference optimization\")\n",
        "print(f\"Available resources: {ray.cluster_resources()}\")\n",
        "\n",
        "# Configure Ray Data for optimal performance monitoring\n",
        "try:\n",
        "    ctx = ray.data.DataContext.get_current()\n",
        "    ctx.enable_progress_bars = True\n",
        "    ctx.enable_operator_progress_bars = True\n",
        "    print(\"Ray Data progress bars enabled\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: Could not configure Ray Data context (progress bars disabled): {e}\")\n",
        "    print(\"This doesn't affect functionality - continuing with notebook...\")\n",
        "\n",
        "# Detect hardware availability\n",
        "HAS_GPU = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if HAS_GPU else \"cpu\")\n",
        "\n",
        "print(f\"\\nUsing device: {device}\")\n",
        "if HAS_GPU:\n",
        "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
        "    print(\"GPU detected - examples will use GPU acceleration\")\n",
        "else:\n",
        "    print(\"No GPU detected - examples will run on CPU\")\n",
        "    print(\"   (All patterns work identically on CPU, just use num_cpus instead of num_gpus)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::note CPU and GPU Compatibility\n",
        "**This template works on both CPU-only and GPU clusters!**\n",
        "\n",
        "- **GPU clusters**: Code automatically detects GPUs and uses `num_gpus=1` for acceleration\n",
        "- **CPU clusters**: Code falls back to CPU and uses `num_cpus=2` for parallelism\n",
        "\n",
        "All optimization concepts (actor-based loading, batching, concurrency) apply equally to both environments.\n",
        ":::\n",
        "\n",
        "### Load Demo Dataset\n",
        "\n",
        "For this demonstration, you'll use the Imagenette dataset, which provides a realistic subset of ImageNet with 10 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load real ImageNet dataset for batch inference demonstration\n",
        "try:\n",
        "    print(\"Loading Imagenette dataset from S3...\")\n",
        "    dataset = ray.data.read_images(\n",
        "        \"s3://ray-benchmark-data/imagenette2/train/\",\n",
        "        mode=\"RGB\",  # Ensure consistent RGB color format\n",
        "        num_cpus=0.05\n",
        "    ).limit(1000)  # Use 1K images for focused performance comparison\n",
        "    \n",
        "    print(\"✓ Loaded ImageNet dataset for batch inference demo\")\n",
        "    print(f\"   Dataset size: {dataset.count()} images\")\n",
        "    print(\"\\nSample dataset:\")\n",
        "    sample_batch = dataset.take_batch(3)\n",
        "    print(f\"   Batch contains {len(sample_batch['image'])} images\")\n",
        "    print(f\"   Image shape: {sample_batch['image'][0].shape}\")\n",
        "    print(f\"   Image dtype: {sample_batch['image'][0].dtype}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n✗ ERROR: Failed to load dataset\")\n",
        "    print(f\"   Error details: {type(e).__name__}: {str(e)[:200]}\")\n",
        "    print(\"\\nPossible solutions:\")\n",
        "    print(\"   1. Check if numpy/pandas versions are compatible:\")\n",
        "    print(\"      pip install --upgrade numpy pandas\")\n",
        "    print(\"   2. Or reinstall with matching versions:\")\n",
        "    print(\"      pip install 'numpy<2.0' 'pandas>=2.0'\")\n",
        "    print(\"   3. Check S3 access and ray.data availability\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## The Wrong Way: Inefficient Batch Inference\n",
        "\n",
        "This section demonstrates a common anti-pattern in ML inference systems. Understanding why this approach fails is essential before learning the optimized solution.\n",
        "\n",
        "When models are loaded repeatedly for each batch, the initialization overhead dominates processing time. This pattern is unfortunately common in production systems where developers haven't considered the cost of model loading operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Dict, Any, List\n",
        "\n",
        "def inefficient_inference(batch: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"INEFFICIENT: Loads model for every single batch.\n",
        "    \n",
        "    Anti-pattern demonstration - DO NOT use this approach in production!\n",
        "    This function intentionally shows bad practices to highlight optimization opportunities.\n",
        "    \n",
        "    Args:\n",
        "        batch: Dictionary containing 'image' key with list of PIL Images\n",
        "        \n",
        "    Returns:\n",
        "        List of prediction dictionaries with 'prediction' and 'confidence' keys\n",
        "    \"\"\"\n",
        "    import time\n",
        "    from transformers import pipeline\n",
        "    \n",
        "    # BAD: Model loading happens inside function - repeats for every batch!\n",
        "    print(\"Loading model... (this happens for every batch!)\")\n",
        "    start_load = time.time()\n",
        "    classifier = pipeline(\"image-classification\", model=\"microsoft/resnet-50\")\n",
        "    load_time = time.time() - start_load\n",
        "    print(f\"Model loading took: {load_time:.2f} seconds\")\n",
        "    \n",
        "    # BAD: Processing images one by one instead of batched inference\n",
        "    results = []\n",
        "    for image in batch[\"image\"]:\n",
        "        prediction = classifier(image)\n",
        "        results.append({\n",
        "            \"prediction\": prediction[0][\"label\"],\n",
        "            \"confidence\": prediction[0][\"score\"]\n",
        "        })\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"Testing inefficient approach...\")\n",
        "print(\"TIP: Watch Ray Dashboard to see the performance problems\")\n",
        "\n",
        "# Run inefficient batch inference with small batches\n",
        "inefficient_results = dataset.limit(100).map_batches(\n",
        "    inefficient_inference,\n",
        "    batch_size=4,\n",
        "    concurrency=2\n",
        ").take(20)\n",
        "\n",
        "print(\"Inefficient approach completed\")\n",
        "print(\"   Problems: repeated model loading, poor batching, wasted resources\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected issues:**\n",
        "```\n",
        "Loading model... (this happens for every batch!)\n",
        "Model loading took: 3.45 seconds\n",
        "Loading model... (this happens for every batch!)\n",
        "Model loading took: 3.52 seconds\n",
        "[repeated 25 times...]\n",
        "```\n",
        "\n",
        ":::caution Performance Anti-Pattern\n",
        "- Model loads 25 times (one per batch)  \n",
        "- Each load takes 3+ seconds = 87.5 seconds wasted  \n",
        "- CPU/GPU mostly idle waiting for model loading  \n",
        "- Total throughput: ~1 image/second (unacceptable)\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "## Why the Naive Approach Fails\n",
        "\n",
        "Now that you've seen the inefficient implementation, you can understand exactly why it performs poorly.\n",
        "\n",
        "### Performance Bottlenecks Explained\n",
        "\n",
        "The inefficient approach suffers from three critical problems:\n",
        "\n",
        "#### Problem 1: Repeated Model Loading\n",
        "\n",
        "**What happens**: The model loads from scratch for every batch of 4 images.\n",
        "\n",
        "**Why it's expensive**:\n",
        "- Model weights file: 100-500 MB download per load\n",
        "- Neural network initialization: 2-5 seconds of setup\n",
        "- GPU memory allocation: Repeated allocation/deallocation cycles\n",
        "- Wasted overhead: Model loading time >> actual inference time\n",
        "\n",
        "**Impact**: If model loading takes 3 seconds and inference takes 0.1 seconds, you're spending 97% of time on overhead!\n",
        "\n",
        "#### Problem 2: Poor Batch Utilization\n",
        "\n",
        "**What happens**: Processing only 4 images at a time with individual processing.\n",
        "\n",
        "**Why it's inefficient**:\n",
        "- GPU underutilization: Modern GPUs can handle 32-128 images simultaneously\n",
        "- Memory waste: Using <10% of available GPU memory\n",
        "- No vectorization: Processing images one-by-one instead of batched tensors\n",
        "- Task overhead: Creating many small tasks instead of fewer large ones\n",
        "\n",
        "**Impact**: GPU sits idle 90% of the time waiting for data.\n",
        "\n",
        "#### Problem 3: Inefficient Resource Allocation\n",
        "\n",
        "**What happens**: Low concurrency with default settings.\n",
        "\n",
        "**Why it creates bottlenecks**:\n",
        "- Limited parallelism: Only 2 concurrent workers\n",
        "- Unbalanced pipeline: Preprocessing can't keep up with potential GPU throughput\n",
        "- Resource waste: CPU cores sit idle while waiting for model loading\n",
        "\n",
        "**Impact**: Cluster resources are underutilized, extending total processing time.\n",
        "\n",
        "### Performance Anti-pattern Summary\n",
        "\n",
        "| Anti-Pattern | Why It's Bad | Typical Impact |\n",
        "|--------------|-------------|----------------|\n",
        "| **Model loading per batch** | Initialization overhead >> inference time | 10-100x slower |\n",
        "| **Small batch sizes** | GPU memory underutilized | 5-10x slower |\n",
        "| **Sequential processing** | No vectorization benefits | 3-5x slower |\n",
        "| **Low concurrency** | Limited parallelism | 2-4x slower |\n",
        "\n",
        "**Combined effect**: These anti-patterns compound, making the inefficient approach significantly slower than optimized implementations.\n",
        "\n",
        "---\n",
        "\n",
        "## The Right Way: Optimized with Ray Data\n",
        "\n",
        "Now you can see how Ray Data solves these problems with actor-based inference, proper batching, and optimized resource allocation.\n",
        "\n",
        "Ray Data solves the model loading problem by letting you run stateful, class-based `map_batches` with an actor pool strategy. Each worker loads the model once and reuses it across many batches, eliminating repeated initialization overhead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Dict, Any, List\n",
        "\n",
        "# Efficient: Use Ray Data class-based map_batches with optimized actor configuration\n",
        "\n",
        "class InferenceWorker:\n",
        "    \"\"\"Stateful worker that loads the model once and reuses it.\n",
        "    \n",
        "    This is the CORRECT pattern for batch inference - model loads once in __init__\n",
        "    and is reused across all batches processed by this worker.\n",
        "    \n",
        "    Works on both CPU and GPU - automatically detects hardware.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize worker - called once per actor, not per batch!\"\"\"\n",
        "        from transformers import pipeline\n",
        "        import torch\n",
        "        \n",
        "        # Automatically use GPU if available, otherwise CPU\n",
        "        device = 0 if torch.cuda.is_available() else -1\n",
        "        self.classifier = pipeline(\n",
        "            \"image-classification\",\n",
        "            model=\"microsoft/resnet-50\",\n",
        "            device=device,\n",
        "        )\n",
        "        print(f\"Model loaded on: {'GPU' if device >= 0 else 'CPU'}\")\n",
        "\n",
        "    def __call__(self, batch: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Process a batch of images - called many times, reuses loaded model.\n",
        "        \n",
        "        Args:\n",
        "            batch: Dictionary containing 'image' key with list of PIL Images\n",
        "            \n",
        "        Returns:\n",
        "            List of prediction dictionaries\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        for image in batch[\"image\"]:\n",
        "            pred = self.classifier(image)\n",
        "            results.append({\n",
        "                \"prediction\": pred[0][\"label\"],\n",
        "                \"confidence\": pred[0][\"score\"],\n",
        "            })\n",
        "        return results\n",
        "\n",
        "print(\"Running optimized Ray Data inference with stateful workers...\")\n",
        "\n",
        "# Best practice: Use the new concurrency parameter for actor-based processing\n",
        "# Resource allocation adapts to available hardware\n",
        "try:\n",
        "    inference_results = dataset.limit(100).map_batches(\n",
        "        InferenceWorker,\n",
        "        concurrency=2,      # Number of parallel actors\n",
        "        num_gpus=1 if HAS_GPU else 0,  # Allocate GPU if available\n",
        "        num_cpus=2 if not HAS_GPU else 1,  # Use more CPU cores if no GPU\n",
        "        batch_size=16,      # Optimal batch size for resource utilization\n",
        "    ).take(20)\n",
        "    \n",
        "    print(\"Optimized approach completed successfully!\")\n",
        "    print(\"   Improvements: single model load per worker, better batching, efficient resource use\")\n",
        "    print(f\"   Processed {len(inference_results)} images\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Inference failed: {e}\")\n",
        "    print(\"   Check that transformers and torch are installed\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**What's Better:**\n",
        "- Model loads only once per worker via Ray Data `ActorPoolStrategy`\n",
        "- Larger batch sizes for better resource utilization\n",
        "- Proper resource allocation with `num_gpus=1` (GPU) or `num_cpus=2` (CPU)\n",
        "- Ray Data automatically manages distribution across workers\n",
        "- **Works identically on CPU and GPU clusters with zero code changes**\n",
        "\n",
        ":::tip Resource Allocation Patterns\n",
        "**GPU clusters**: Use `num_gpus=1` to allocate one GPU per actor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example pattern for GPU clusters:\n",
        "# .map_batches(InferenceWorker, num_gpus=1, concurrency=2)  # 2 GPUs used"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**CPU clusters**: Use `num_cpus=2` to allocate CPU cores per actor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example pattern for CPU clusters:\n",
        "# .map_batches(InferenceWorker, num_cpus=2, concurrency=4)  # 8 CPU cores used"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The patterns are identical - Ray Data abstracts away the hardware differences!\n",
        ":::\n",
        "\n",
        "### Performance Expectations: CPU vs GPU\n",
        "\n",
        ":::tip Performance Scaling\n",
        "**Both CPU and GPU deployments benefit from Ray Data optimizations!**\n",
        "\n",
        "**GPU clusters** (when available):\n",
        "- **Throughput**: 500-2000 images/second (model dependent)\n",
        "- **Best for**: Large models, high-volume inference\n",
        "- **Resource**: Use `num_gpus=1` per actor\n",
        "\n",
        "**CPU clusters** (always available):\n",
        "- **Throughput**: 50-200 images/second (model dependent)\n",
        "- **Best for**: Development, smaller models, cost-sensitive workloads\n",
        "- **Resource**: Use `num_cpus=2-4` per actor\n",
        "\n",
        "**Key insight**: The optimization patterns (actor-based loading, batching, concurrency) provide similar relative speedups on both CPU and GPU!\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Adaptive resource allocation based on available hardware\n",
        "def create_inference_config():\n",
        "    \"\"\"Generate optimal configuration for available hardware.\"\"\"\n",
        "    import torch\n",
        "    has_gpu = torch.cuda.is_available()\n",
        "    \n",
        "    if has_gpu:\n",
        "        return {\n",
        "            \"num_gpus\": 1,\n",
        "            \"concurrency\": torch.cuda.device_count(),\n",
        "            \"batch_size\": 32,  # Larger batches for GPU\n",
        "            \"expected_throughput\": \"500-2000 images/sec\"\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            \"num_cpus\": 2,\n",
        "            \"concurrency\": int(ray.available_resources().get(\"CPU\", 4) // 2),\n",
        "            \"batch_size\": 16,  # Smaller batches for CPU\n",
        "            \"expected_throughput\": \"50-200 images/sec\"\n",
        "        }\n",
        "\n",
        "config = create_inference_config()\n",
        "print(f\"Optimized configuration for your cluster: {config}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Key Takeaways from Part 1\n",
        "\n",
        "You've learned the fundamentals of batch inference optimization:\n",
        "- Identified common anti-patterns that destroy performance\n",
        "- Understood why repeated model loading is catastrophic  \n",
        "- Implemented class-based actors for stateful model loading\n",
        "- Used proper resource allocation with `num_gpus` and `concurrency`\n",
        "- Learned CPU and GPU compatibility patterns\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "Now that you understand the fundamentals, you're ready to learn systematic optimization techniques.\n",
        "\n",
        "**[← Back to Overview](README.md)** | **[Continue to Part 2: Advanced Optimization →](02-advanced-optimization.md)**\n",
        "\n",
        "In Part 2, you'll learn:\n",
        "- Systematic decision frameworks for choosing optimization techniques\n",
        "- Multi-model ensemble inference patterns\n",
        "- Performance monitoring and diagnostics\n",
        "- Production deployment best practices\n",
        "\n",
        "**Or skip ahead to Part 3** for a deep dive into Ray Data's architecture:\n",
        "\n",
        "**[Jump to Part 3: Ray Data Architecture →](03-ray-data-architecture.md)**\n",
        "\n",
        "In Part 3, you'll learn:\n",
        "- How streaming execution enables unlimited dataset processing\n",
        "- How blocks and memory management affect optimization\n",
        "- How operator fusion and backpressure work\n",
        "- How to calculate optimal parameters from architectural principles\n",
        "\n",
        "**[Return to overview](README.md)** to see all available parts.\n",
        "\n",
        "---\n",
        "\n",
        "## Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up Ray resources when done\n",
        "print(\"\\nCleaning up Ray resources...\")\n",
        "try:\n",
        "    if ray.is_initialized():\n",
        "        ray.shutdown()\n",
        "        print(\"Ray resources cleaned up successfully\")\n",
        "    else:\n",
        "        print(\"INFO: Ray was not initialized, no cleanup needed\")\n",
        "except Exception as e:\n",
        "    print(f\"WARNING: Error during cleanup: {e}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
