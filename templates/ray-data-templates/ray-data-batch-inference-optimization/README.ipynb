{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Batch Inference Optimization With Ray Data\n",
        "\n",
        "**Time to complete**: 65 min (across 3 parts) | **Difficulty**: Intermediate | **Prerequisites**: ML inference experience, Python knowledge\n",
        "\n",
        "Create an optimized ML batch inference pipeline that demonstrates the performance difference between naive and efficient approaches. Learn how Ray Data's actor-based patterns eliminate common bottlenecks in production ML inference.\n",
        "\n",
        "## Template Parts\n",
        "\n",
        "This template is split into three parts for comprehensive learning:\n",
        "\n",
        "| Part | Description | Time | File |\n",
        "|------|-------------|------|------|\n",
        "| **Part 1** | Inference Fundamentals | 20 min | [01-inference-fundamentals.md](01-inference-fundamentals.md) |\n",
        "| **Part 2** | Advanced Optimization | 20 min | [02-advanced-optimization.md](02-advanced-optimization.md) |\n",
        "| **Part 3** | Ray Data Architecture | 25 min | [03-ray-data-architecture.md](03-ray-data-architecture.md) |\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "### Part 1: Inference Fundamentals\n",
        "Learn the core concepts of batch inference optimization by comparing inefficient and efficient approaches:\n",
        "- **Setup**: Initialize Ray Data for accelerated inference (CPU or GPU)\n",
        "- **The Wrong Way**: Understand anti-patterns that cause performance bottlenecks\n",
        "- **Why It Fails**: Learn the technical reasons behind poor performance\n",
        "- **The Right Way**: Implement optimized actor-based inference (works on CPU and GPU)\n",
        "\n",
        "### Part 2: Advanced Optimization\n",
        "Master systematic optimization techniques for production deployment:\n",
        "- **Decision Framework**: Learn when to use each optimization parameter\n",
        "- **Advanced Techniques**: Multi-model ensembles, systematic parameter tuning\n",
        "- **Performance Monitoring**: Use Ray Dashboard for optimization decisions\n",
        "- **Production Deployment**: Best practices for enterprise-scale inference\n",
        "\n",
        "### Part 3: Ray Data Architecture (NEW)\n",
        "Understand how Ray Data's architecture enables optimization:\n",
        "- **Streaming Execution**: How Ray Data processes unlimited datasets with constant memory\n",
        "- **Blocks and Memory Model**: Understanding object store, heap memory, and zero-copy\n",
        "- **Operators and Fusion**: How Ray Data combines operations for efficiency\n",
        "- **Resource Management**: Automatic backpressure and dynamic allocation\n",
        "- **Architecture-Informed Decisions**: Calculate optimal batch_size and concurrency from first principles\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "**Why batch inference optimization matters**: Poor optimization wastes significant compute resources through repeated model loading and inefficient batching. Understanding these bottlenecks is crucial for production ML systems.\n",
        "\n",
        "**Ray Data's inference capabilities**: Stateful per-worker model loading and distributed processing eliminate performance bottlenecks that plague traditional ML pipelines.\n",
        "\n",
        "**Real-world optimization patterns**: Production ML systems at companies like Netflix, Tesla, and search engines process millions of inference requests using distributed techniques.\n",
        "\n",
        "## Overview\n",
        "\n",
        "**Challenge**: Batch inference bottlenecks waste compute resources and slow ML pipelines:\n",
        "- **Repeated model loading**: Loading 500MB+ models for every batch wastes 97% of time\n",
        "- **Poor resource utilization**: Small batches leave CPUs/GPUs idle 90% of the time\n",
        "- **Memory inefficiency**: Materializing full datasets causes OOM errors\n",
        "- **Sequential processing**: Single-threaded inference limits throughput\n",
        "\n",
        "**Solution**: Ray Data's actor-based inference eliminates common bottlenecks:\n",
        "\n",
        "| Inference Challenge | Naive Approach | Ray Data Solution | Performance Impact |\n",
        "|---------------------|---------------|-------------------|-------------------|\n",
        "| **Model Loading** | Load per batch (2-5 sec) | Load once per actor (one-time cost) | 10-100x throughput improvement |\n",
        "| **Batch Sizing** | Small batches (4-16 samples) | Optimized batches (32-128 samples) | 5-10x resource utilization |\n",
        "| **Resource Management** | Manual allocation | Automatic with `num_gpus` (GPU) or `num_cpus` (CPU) | Zero configuration |\n",
        "| **Concurrency** | Sequential or over-subscribed | Optimal actor pool with `concurrency` param | Maximum cluster efficiency |\n",
        "\n",
        ":::tip Ray Data for ML Inference\n",
        "Batch inference showcases Ray Data's strengths for ML workloads:\n",
        "- **Stateful actors**: Models load once in `__init__()`, reused across 1000s of batches\n",
        "- **Resource allocation**: `num_gpus=1` (GPU) or `num_cpus=2` (CPU) for proper resource management\n",
        "- **Batch optimization**: `batch_size` parameter controls memory vs throughput\n",
        "- **Concurrency tuning**: `concurrency` parameter matches cluster resources (GPUs or CPU cores)\n",
        "- **Built-in monitoring**: Ray Dashboard shows resource utilization and bottlenecks\n",
        "- **Universal patterns**: Same optimization patterns work on CPU-only and GPU clusters\n",
        ":::\n",
        "\n",
        "**Impact**: Companies use these patterns at massive scale:\n",
        "- **OpenAI**: Processes billions of ChatGPT requests using Ray Serve (built on Ray Data patterns)\n",
        "- **Tesla**: Analyzes millions of autonomous driving images using distributed inference\n",
        "- **Netflix**: Generates recommendations for 200M+ users using scalable ML pipelines\n",
        "- **Shopify**: Processes millions of product images for search and categorization\n",
        "- **Instacart**: Analyzes grocery images and generates recommendations at scale\n",
        "\n",
        "**Performance results** you can expect:\n",
        "- **10-100x throughput improvement** over naive implementations\n",
        "- **90%+ resource utilization** vs 10-20% with poor patterns\n",
        "- **Linear scaling** as you add more CPUs/GPUs to cluster\n",
        "- **Sub-second latency** for batch sizes of 1000+ images\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before starting, ensure you have:\n",
        "- [ ] Python 3.8+ with machine learning libraries installed\n",
        "- [ ] Ray Data installed (`pip install \"ray[data]>=2.7.0\"` for full features)\n",
        "- [ ] Transformers library (`pip install transformers torch`) for model examples\n",
        "- [ ] Basic understanding of ML model inference concepts\n",
        "- [ ] Familiarity with PyTorch or Transformers library (helpful but not required)\n",
        "- [ ] Access to compute resources (CPU-only clusters work fine, GPU optional)\n",
        "\n",
        "**Quick Installation:**\n",
        "```bash\n",
        "pip install \"ray[data]>=2.7.0\" transformers torch pillow\n",
        "```\n",
        "\n",
        "**Verify Installation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ray\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "print(f\"Ray version: {ray.__version__}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"GPU available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::tip CPU and GPU Compatibility\n",
        "**All examples work on both CPU-only and GPU clusters!**\n",
        "\n",
        "- **GPU clusters**: Examples use `num_gpus=1` for optimal GPU acceleration\n",
        "- **CPU clusters**: Simply set `num_gpus=0` or omit the parameter entirely\n",
        "\n",
        "The optimization patterns and architecture concepts apply equally to both CPU and GPU workloads.\n",
        ":::\n",
        "\n",
        "## Getting Started\n",
        "\n",
        "**Recommended learning path** (65 minutes total):\n",
        "\n",
        "1. **Start with Part 1** (20 min) - Understand fundamentals and common anti-patterns\n",
        "   - See the problem: inefficient inference code\n",
        "   - Learn the solution: actor-based patterns\n",
        "   - Get immediate results: 10-50x throughput improvement\n",
        "   \n",
        "2. **Continue to Part 2** (20 min) - Master advanced optimization techniques\n",
        "   - Systematic decision frameworks\n",
        "   - Multi-model ensemble patterns\n",
        "   - Performance monitoring and tuning\n",
        "   \n",
        "3. **Finish with Part 3** (25 min) - Deep dive into Ray Data architecture\n",
        "   - How streaming execution works\n",
        "   - Memory model and zero-copy optimizations\n",
        "   - Calculate optimal parameters from first principles\n",
        "\n",
        "Each part builds on the previous, so complete them in order for the best learning experience.\n",
        "\n",
        "**Time-saving tips**:\n",
        "- Each part includes a 3-5 minute \"Quick Start\" for immediate hands-on experience\n",
        "- Code examples are copy-paste ready for rapid experimentation\n",
        "- All examples work on both CPU-only and GPU clusters\n",
        "\n",
        "**Alternative learning paths:**\n",
        "\n",
        "**Quick path** (20 minutes - for immediate results):\n",
        "- Part 1 only - Learn the basics and avoid common mistakes\n",
        "- **Best for**: Quick wins, proof of concepts, immediate performance improvements\n",
        "- **You'll learn**: Actor-based patterns, basic resource allocation\n",
        "- **Result**: 10-50x throughput improvement in your inference code\n",
        "\n",
        "**Architecture-focused path** (45 minutes - for deep understanding):\n",
        "- Part 1 \u2192 Part 3 \u2192 Part 2\n",
        "- **Best for**: Understanding how Ray Data works under the hood\n",
        "- **You'll learn**: Streaming execution, memory model, operator fusion\n",
        "- **Result**: Calculate optimal parameters and debug performance issues confidently\n",
        "\n",
        "**Production path** (40 minutes - for deployment):\n",
        "- Part 1 \u2192 Part 2\n",
        "- **Best for**: Getting code production-ready quickly\n",
        "- **You'll learn**: Practical optimization, monitoring, troubleshooting\n",
        "- **Result**: Production-ready inference pipeline with monitoring\n",
        "\n",
        "**Troubleshooting path** (as needed):\n",
        "- When stuck: Check [Troubleshooting Guide](../TROUBLESHOOTING_GUIDE.md)\n",
        "- Performance issues: See [Performance Tuning Guide](../PERFORMANCE_TUNING_GUIDE.md)\n",
        "- CPU/GPU questions: Read [CPU and GPU Usage Guide](CPU_GPU_USAGE_GUIDE.md)\n",
        "\n",
        "---\n",
        "\n",
        "## Additional Resources\n",
        "\n",
        "- **[CPU and GPU Usage Guide](CPU_GPU_USAGE_GUIDE.md)** - Comprehensive guide for running templates on any hardware\n",
        "- **[Production Checklist](../PRODUCTION_CHECKLIST.md)** - Production deployment best practices\n",
        "- **[Performance Tuning Guide](../PERFORMANCE_TUNING_GUIDE.md)** - Systematic optimization strategies\n",
        "- **[Troubleshooting Guide](../TROUBLESHOOTING_GUIDE.md)** - Debug common issues\n",
        "\n",
        "---\n",
        "\n",
        "**Ready to begin?** \u2192 Start with [Part 1: Inference Fundamentals](01-inference-fundamentals.md)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}