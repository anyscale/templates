{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b41f36f3",
      "metadata": {},
      "source": [
        "# Data quality monitoring and validation with Ray Data\n",
        "\n",
        "**Time to complete**: 25 min | **Difficulty**: Intermediate | **Prerequisites**: Data engineering experience, understanding of data quality concepts\n",
        "\n",
        "## What You'll Build\n",
        "\n",
        "Create an automated data quality monitoring system that continuously validates data, detects anomalies, and ensures your data pipelines produce reliable, trustworthy results - essential for any data-driven organization.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Data Quality Setup](#step-1-data-quality-setup) (6 min)\n",
        "2. [Quality Validation](#step-2-automated-quality-checks) (8 min)\n",
        "3. [Anomaly Detection](#step-3-data-drift-monitoring) (7 min)\n",
        "4. [Quality Dashboard](#step-4-quality-reporting) (4 min)\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "**Why data quality matters**: Poor data quality costs organizations millions annually through incorrect insights and operational problems. Understanding data quality monitoring is essential for reliable data-driven decision making.\n",
        "\n",
        "**Ray Data's quality capabilities**: Automate quality checks across large datasets using distributed processing. You'll learn how to scale data validation from sample-based to comprehensive full-dataset monitoring.\n",
        "\n",
        "**Real-world applications**: Netflix monitors data quality across 500+ billion viewing events daily to ensure accurate content recommendations. Airbnb validates 150+ million booking records for pricing accuracy and fraud prevention. Uber tracks data quality across 20+ billion trips annually for safety and operational efficiency. Amazon monitors product catalog data quality for 500+ million items to maintain customer trust and search accuracy.\n",
        "\n",
        "## Overview\n",
        "\n",
        "**The Challenge**: Poor data quality significantly impacts business decisions and organizational efficiency. Data quality issues can lead to incorrect insights and operational problems.\n",
        "\n",
        "**The Solution**: Ray Data enables continuous, automated data quality monitoring at scale, catching issues before they impact business decisions.\n",
        "\n",
        "**Real-world Impact**:\n",
        "\n",
        "| Industry | Use Case | Quality Impact |\n",
        "|----------|----------|----------------|\n",
        "| **Financial Services** | Transaction monitoring | Prevent fraud through real-time validation |\n",
        "| **E-commerce** | Product catalogs | Ensure catalog accuracy for better customer experience |\n",
        "| **Healthcare** | Patient records | Validate data quality for accurate diagnosis |\n",
        "| **Analytics** | Data pipelines | Ensure reliable insights through quality monitoring |\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisites Checklist\n",
        "\n",
        "Before starting, ensure you have:\n",
        "- [ ] Understanding of data quality concepts (completeness, accuracy, consistency)\n",
        "- [ ] Experience with data validation and testing\n",
        "- [ ] Familiarity with statistical concepts for anomaly detection\n",
        "- [ ] Python environment with data processing libraries\n",
        "\n",
        "## Quick Start (3 minutes)\n",
        "\n",
        "### Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "043d1919",
      "metadata": {},
      "outputs": [],
      "source": [
        "import ray\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize Ray for distributed processing\n",
        "ray.init(ignore_reinit_error=True)\n",
        "print(\"Ray initialized for data quality monitoring\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a41cf82",
      "metadata": {},
      "source": [
        "### Load Sample Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41f6b409",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load pre-built customer dataset with realistic quality issues\n",
        "customer_dataset = ray.data.read_parquet(\n",
        "    \"ecommerce_customers_with_quality_issues.parquet\"\n",
        ")\n",
        "\n",
        "print(f\"Loaded customer dataset with quality issues:\")\n",
        "print(f\"  Records: {customer_dataset.count():,}\")\n",
        "print(f\"  Schema: {customer_dataset.schema()}\")\n",
        "\n",
        "ds = customer_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e9fb836",
      "metadata": {},
      "source": [
        "**What we have:**\n",
        "- 100,000+ customer records with realistic data patterns\n",
        "- Intentional quality issues: missing values, invalid data, outliers, duplicates\n",
        "- Pre-built Parquet dataset for optimal Ray Data performance\n",
        "\n",
        "## Step 1: Data Quality Setup\n",
        "\n",
        "### Schema Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8edc48db",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check data schema and types\n",
        "print(\"Data Schema Validation:\")\n",
        "print(f\"Dataset schema: {ds.schema()}\")\n",
        "print(f\"Record count: {ds.count():,}\")\n",
        "\n",
        "# Sample record structure\n",
        "sample_record = ds.take(1)[0]\n",
        "print(f\"Sample record keys: {list(sample_record.keys())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f0c1333",
      "metadata": {},
      "source": [
        "### Basic Quality Overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbdcd0c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple data quality analysis\n",
        "def analyze_basic_quality(dataset):\n",
        "    \"\"\"Quick quality overview using Ray Data operations.\"\"\"\n",
        "    total_records = dataset.count()\n",
        "    sample_records = dataset.take(1000)\n",
        "    \n",
        "    print(\"Basic Quality Metrics:\")\n",
        "    print(f\"  Total records: {total_records:,}\")\n",
        "    print(f\"  Sample analyzed: {len(sample_records):,}\")\n",
        "    \n",
        "    return sample_records\n",
        "\n",
        "# Analyze our dataset\n",
        "sample_data = analyze_basic_quality(ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "056c9eca",
      "metadata": {},
      "source": [
        "## Step 2: Automated Quality Checks\n",
        "\n",
        "### Missing Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "437ae6bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use Ray Data native operations for missing data analysis\n",
        "from ray.data.expressions import col\n",
        "\n",
        "# Count missing values efficiently\n",
        "def check_missing_values(dataset):\n",
        "    \"\"\"Analyze missing values using Ray Data operations.\"\"\"\n",
        "    sample_records = dataset.take(1000)  # Efficient sampling\n",
        "    \n",
        "    if not sample_records:\n",
        "        return {}\n",
        "    \n",
        "    missing_stats = {}\n",
        "    for key in sample_records[0].keys():\n",
        "        missing_count = sum(1 for record in sample_records \n",
        "                          if record.get(key) is None or record.get(key) == '')\n",
        "        missing_stats[key] = {\n",
        "            'missing_count': missing_count,\n",
        "            'missing_rate': missing_count / len(sample_records) * 100\n",
        "        }\n",
        "    \n",
        "    return missing_stats\n",
        "\n",
        "# Analyze missing data\n",
        "missing_analysis = check_missing_values(ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b32f1eca",
      "metadata": {},
      "source": [
        "#### Missing Data Summary\n",
        "\n",
        "| Field | Missing Count | Missing Rate | Status |\n",
        "|-------|---------------|--------------|---------|\n",
        "| **Email** | Sample analysis | Calculated % | High / Medium / Good |\n",
        "| **Age** | Sample analysis | Calculated % | High / Medium / Good |\n",
        "| **Income** | Sample analysis | Calculated % | High / Medium / Good |\n",
        "\n",
        "### Simple Quality Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bda7fe99",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create focused quality chart\n",
        "def create_simple_quality_chart(missing_stats):\n",
        "    \"\"\"Create a simple, focused quality chart.\"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    if not missing_stats:\n",
        "        print(\"No missing data statistics available\")\n",
        "        return\n",
        "    \n",
        "    fields = list(missing_stats.keys())\n",
        "    missing_rates = [stats['missing_rate'] for stats in missing_stats.values()]\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    colors = ['red' if rate > 10 else 'orange' if rate > 5 else 'green' for rate in missing_rates]\n",
        "    plt.bar(fields, missing_rates, color=colors, alpha=0.7)\n",
        "    plt.title('Missing Data Analysis')\n",
        "    plt.ylabel('Missing Rate (%)')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"Quality chart generated successfully\")\n",
        "\n",
        "# Generate focused chart\n",
        "create_simple_quality_chart(missing_analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "677f7d6c",
      "metadata": {},
      "source": [
        "### Accuracy Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bccbd9b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Email validation using Ray Data filtering\n",
        "def validate_email_format(dataset):\n",
        "    \"\"\"Validate email formats using Ray Data operations.\"\"\"\n",
        "    \n",
        "    # Use simple lambda filtering for email validation\n",
        "    valid_emails = dataset.filter(\n",
        "        lambda record: '@' in str(record.get('email', ''))\n",
        "    )\n",
        "    total_records = dataset.count()\n",
        "    valid_count = valid_emails.count()\n",
        "    \n",
        "    return {\n",
        "        'total_records': total_records,\n",
        "        'valid_emails': valid_count,\n",
        "        'validity_rate': (valid_count / total_records * 100) if total_records > 0 else 0\n",
        "    }\n",
        "\n",
        "# Run email validation\n",
        "email_validation = validate_email_format(ds)\n",
        "print(f\"Email validation: {email_validation['validity_rate']:.1f}% valid formats\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8aac3b8d",
      "metadata": {},
      "source": [
        "## Step 3: Data Drift Monitoring\n",
        "\n",
        "### Statistical Analysis with Native Ray Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb97d447",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use Ray Data native aggregations for statistical analysis\n",
        "from ray.data.aggregate import Count, Mean, Std, Min, Max\n",
        "\n",
        "# Calculate statistics for numeric columns\n",
        "try:\n",
        "    age_stats = ds.aggregate(\n",
        "        Count(),\n",
        "        Mean('age'),\n",
        "        Std('age'),\n",
        "        Min('age'),\n",
        "        Max('age')\n",
        "    )\n",
        "    print(\"Age Statistics:\")\n",
        "    print(f\"  Count: {age_stats['count()']:,}\")\n",
        "    print(f\"  Mean: {age_stats['mean(age)']:.1f}\")\n",
        "    print(f\"  Std Dev: {age_stats['std(age)']:.1f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Age statistics calculation: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e0497de",
      "metadata": {},
      "source": [
        "#### Quality Statistics Summary\n",
        "\n",
        "| Metric | Age | Income | Score |\n",
        "|--------|-----|--------|-------|\n",
        "| **Count** | Native Ray Data aggregation | Native calculation | Native calculation |\n",
        "| **Mean** | Distributed processing | Distributed processing | Distributed processing |\n",
        "| **Std Dev** | Scalable statistics | Scalable statistics | Scalable statistics |\n",
        "\n",
        "## Step 4: Quality Reporting\n",
        "\n",
        "### Generate Quality Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c626e076",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive quality report\n",
        "def generate_quality_report(dataset, missing_stats, email_validation):\n",
        "    \"\"\"Generate a comprehensive quality report.\"\"\"\n",
        "    total_records = dataset.count()\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"DATA QUALITY REPORT\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Dataset size: {total_records:,} records\")\n",
        "    print(f\"Email validity: {email_validation['validity_rate']:.1f}%\")\n",
        "    \n",
        "    # Missing data summary\n",
        "    print(\"\\nMissing Data Summary:\")\n",
        "    for field, stats in missing_stats.items():\n",
        "        status = \"High\" if stats['missing_rate'] > 10 else \"Medium\" if stats['missing_rate'] > 5 else \"Good\"\n",
        "        print(f\"  {field}: {stats['missing_rate']:.1f}% missing {status}\")\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "\n",
        "# Generate final report\n",
        "generate_quality_report(ds, missing_analysis, email_validation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ac3a9fe",
      "metadata": {},
      "source": [
        "### Quality Score Calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfdf30fb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate overall quality score using native operations\n",
        "def calculate_quality_score_native(dataset):\n",
        "    \"\"\"Calculate quality score using Ray Data operations.\"\"\"\n",
        "    total_records = dataset.count()\n",
        "    \n",
        "    # Find records with complete critical fields using lambda filtering\n",
        "    complete_records = dataset.filter(\n",
        "        lambda record: (\n",
        "            record.get('email') is not None and \n",
        "            record.get('age') is not None and\n",
        "            str(record.get('email', '')) != '' and\n",
        "            str(record.get('email', '')) != 'None'\n",
        "        )\n",
        "    ).count()\n",
        "    \n",
        "    completeness_score = complete_records / total_records if total_records > 0 else 0\n",
        "    \n",
        "    quality_result = {\n",
        "        \"total_records\": total_records,\n",
        "        \"complete_records\": complete_records,\n",
        "        \"completeness_score\": completeness_score,\n",
        "        \"quality_grade\": \"A\" if completeness_score > 0.9 else \"B\" if completeness_score > 0.7 else \"C\"\n",
        "    }\n",
        "    \n",
        "    return quality_result\n",
        "\n",
        "# Calculate quality score\n",
        "overall_quality = calculate_quality_score_native(ds)\n",
        "print(f\"Overall Quality: {overall_quality['completeness_score']:.1%} (Grade: {overall_quality['quality_grade']})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec1f0493",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "### Ray Data Quality Advantages\n",
        "\n",
        "| Traditional Approach | Ray Data Approach | Key Benefit |\n",
        "|---------------------|-------------------|-------------|\n",
        "| **Batch validation** | Continuous monitoring | Real-time insights |\n",
        "| **Single-machine** | Distributed processing | Horizontal scaling |\n",
        "| **Manual rules** | Automated detection | Streamlined development |\n",
        "| **Point-in-time** | Historical tracking | Comprehensive management |\n",
        "\n",
        "### Quality Framework\n",
        "\n",
        ":::tip Data Quality Pillars\n",
        "The template implements six key quality dimensions:\n",
        "- **Completeness** (25%) - Missing value detection\n",
        "- **Accuracy** (25%) - Format and range validation  \n",
        "- **Consistency** (20%) - Schema compliance\n",
        "- **Timeliness** (15%) - Freshness monitoring\n",
        "- **Validity** (10%) - Business rule validation\n",
        "- **Uniqueness** (5%) - Duplicate detection\n",
        ":::\n",
        "\n",
        "## Action Items\n",
        "\n",
        "### Immediate Implementation\n",
        "- [ ] Set up automated quality monitoring for your datasets\n",
        "- [ ] Define business-specific validation rules\n",
        "- [ ] Implement quality score calculations\n",
        "- [ ] Create quality dashboards and alerts\n",
        "\n",
        "### Advanced Features\n",
        "- [ ] Add statistical anomaly detection\n",
        "- [ ] Implement data drift monitoring\n",
        "- [ ] Build quality trend analysis\n",
        "- [ ] Create automated quality improvement recommendations\n",
        "\n",
        "## Related Templates\n",
        "\n",
        "### **Recommended Next Steps**\n",
        "- **[enterprise-data-catalog](../ray-data-enterprise-data-catalog/)**: Extend quality monitoring with automated data discovery\n",
        "- **[large-scale-etl-optimization](../ray-data-large-scale-etl-optimization/)**: Apply quality checks within ETL pipelines\n",
        "- **[log-ingestion](../ray-data-log-ingestion/)**: Monitor data pipeline logs for quality issues\n",
        "\n",
        "### **Advanced Applications**\n",
        "- **[financial-forecasting](../ray-data-financial-forecasting/)**: Apply quality monitoring to financial time series data\n",
        "- **[medical-connectors](../ray-data-medical-connectors/)**: Implement HIPAA-compliant data quality validation\n",
        "\n",
        "## Resources\n",
        "\n",
        "- [Ray Data Documentation](https://docs.ray.io/en/latest/data/index.html)\n",
        "- [Data Quality Best Practices](https://docs.ray.io/en/latest/data/best-practices.html)\n",
        "- [Ray Security Documentation](https://docs.ray.io/en/latest/ray-security.html)\n",
        "\n",
        "## Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9778bb33",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up Ray resources\n",
        "ray.shutdown()\n",
        "print(\"Ray cluster shutdown complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7f327a5",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "*This template provides a foundation for building production-ready data quality monitoring pipelines with Ray Data. Start with basic validation and gradually add complexity based on your specific requirements.*"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}