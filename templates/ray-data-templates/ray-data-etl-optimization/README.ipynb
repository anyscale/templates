{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da0775f2-a4da-4f9a-bbbe-4219959840b1",
   "metadata": {},
   "source": [
    "# ETL Processing and Optimization With Ray Data\n",
    "\n",
    "**Time to complete**: 40 min \\| **Difficulty**: Intermediate \\| **Prerequisites**: ETL concepts, basic SQL knowledge, data processing experience\n",
    "\n",
    "## What you’ll build\n",
    "\n",
    "Build comprehensive ETL pipelines using Ray Data’s distributed processing capabilities, from foundational concepts with TPC-H benchmark to production-scale optimization techniques for enterprise data processing.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1.  [ETL Fundamentals with TPC-H](#step-1-etl-fundamentals-with-tpc-h) (10 min)\n",
    "2.  [Data Transformations and Processing](#step-2-data-transformations-and-processing) (12 min)\n",
    "3.  [Performance Optimization Techniques](#step-3-performance-optimization-techniques) (10 min)\n",
    "4.  [Large-Scale ETL Patterns](#step-4-large-scale-etl-patterns) (8 min)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "**Why ETL optimization matters**: The difference between fast and slow data pipelines directly impacts business agility and operational costs. Understanding optimization techniques enables data teams to deliver insights faster while reducing infrastructure costs.\n",
    "\n",
    "**Ray Data’s ETL capabilities**: Native operations for distributed processing that automatically optimize memory, CPU, and I/O utilization. You’ll learn how Ray Data’s architecture enables efficient processing of large datasets.\n",
    "\n",
    "**TPC-H benchmark patterns**: Learn ETL fundamentals using the TPC-H benchmark that simulates complex business environments with customers, orders, suppliers, and products.\n",
    "\n",
    "**Production optimization strategies**: Memory management, parallel processing, and resource configuration patterns for production ETL workloads that scale from gigabytes to petabytes.\n",
    "\n",
    "**Enterprise ETL patterns**: Techniques used by data engineering teams to process large datasets efficiently while maintaining data quality and performance.\n",
    "\n",
    "## Prerequisites Checklist\n",
    "\n",
    "Before starting, ensure you have:\n",
    "- \\[ \\] Understanding of ETL (Extract, Transform, Load) concepts\n",
    "- \\[ \\] Basic SQL knowledge for data transformations\n",
    "- \\[ \\] Python experience with data processing\n",
    "- \\[ \\] Familiarity with distributed computing concepts\n",
    "\n",
    "## Quick start (3 minutes)\n",
    "\n",
    "This section demonstrates ETL processing concepts using Ray Data:\n",
    "\n",
    "``` python\n",
    "from typing import Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import ray\n",
    "from ray.data.expressions import col, lit\n",
    "\n",
    "# Initialize Ray for distributed ETL processing\n",
    "ray.init()\n",
    "\n",
    "# Configure Ray Data for optimal performance monitoring\n",
    "ctx = ray.data.DataContext.get_current()\n",
    "ctx.enable_progress_bars = True\n",
    "ctx.enable_operator_progress_bars = True\n",
    "\n",
    "# Load sample dataset for ETL demonstration\n",
    "sample_data = ray.data.read_parquet(\n",
    "    \"s3://ray-benchmark-data/tpch/parquet/sf1/customer\",\n",
    "    num_cpus=0.025  # High I/O concurrency\n",
    ")\n",
    "\n",
    "print(f\"Loaded ETL sample dataset: {sample_data.count()} records\")\n",
    "print(f\"Schema: {sample_data.schema()}\")\n",
    "print(\"\\nSample records:\")\n",
    "for i, record in enumerate(sample_data.take(3)):\n",
    "    print(f\"  {i+1}. Customer {record['c_custkey']}: {record['c_name']} from {record['c_mktsegment']}\")\n",
    "```\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Challenge**: Traditional ETL tools struggle with modern data volumes and complexity. Processing large datasets can take significant time, creating bottlenecks in data-driven organizations.\n",
    "\n",
    "**Solution**: Ray Data’s distributed architecture and optimized operations enable efficient processing of large datasets through parallel computation and native operations.\n",
    "\n",
    "**Impact**: Data engineering teams process terabytes of data daily using Ray Data’s ETL capabilities. Companies transform raw data into analytics-ready datasets efficiently while maintaining data quality and performance.\n",
    "\n",
    "### ETL pipeline architecture\n",
    "\n",
    "    ┌─────────────────────────────────────────────────────────────────┐\n",
    "    │                    Ray Data ETL Pipeline                        │\n",
    "    ├─────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                 │\n",
    "    │  Extract              Transform              Load               │\n",
    "    │  ────────            ──────────            ──────              │\n",
    "    │                                                                 │\n",
    "    │  read_parquet()  →   map_batches()    →   write_parquet()     │\n",
    "    │  (TPC-H Data)        (Business Logic)     (Data Warehouse)     │\n",
    "    │                                                                 │\n",
    "    │  ↓ Column Pruning    ↓ Filter/Join       ↓ Partitioning       │\n",
    "    │  ↓ Parallel I/O      ↓ Aggregations      ↓ Compression        │\n",
    "    │  ↓ High Concurrency  ↓ Enrichment        ↓ Schema Optimization│\n",
    "    │                                                                 │\n",
    "    └─────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "    Data Flow:\n",
    "      TPC-H Customer (150K) ─┐\n",
    "      TPC-H Orders (1.5M)   ─┼→ Join → Enrich → Aggregate → Warehouse\n",
    "      TPC-H LineItems (6M)  ─┘      ↓         ↓            ↓\n",
    "                                Filter    Transform    Partition\n",
    "\n",
    "### ETL performance comparison\n",
    "\n",
    "| Approach | Data Loading | Transformations | Joins | Output | Use Case |\n",
    "|-----------|--------------|------------------|--------|----------|-----------|\n",
    "| **Traditional** | Sequential | Single-threaded | Memory-limited | Slow writes | Small datasets |\n",
    "| **Ray Data** | Parallel I/O | Distributed | Scalable | Optimized writes | Production scale |\n",
    "\n",
    "**Key advantages**:\n",
    "- **Parallel processing**: Distribute transformations across cluster nodes\n",
    "- **Memory efficiency**: Stream processing without materializing full datasets\n",
    "- **Native operations**: Optimized filter, join, and aggregate functions\n",
    "- **Scalability**: Handle datasets from gigabytes to petabytes\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Step 1: ETL Fundamentals with TPC-H\n",
    "\n",
    "### Understanding TPC-H benchmark\n",
    "\n",
    "``` python\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import ray\n",
    "from ray.data.aggregate import Count, Mean, Sum, Max\n",
    "from ray.data.expressions import col, lit\n",
    "\n",
    "# Initialize Ray for ETL processing\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Configure Ray Data for optimal performance monitoring\n",
    "ctx = ray.data.DataContext.get_current()\n",
    "ctx.enable_progress_bars = True\n",
    "ctx.enable_operator_progress_bars = True\n",
    "\n",
    "print(f\"Ray Data initialized for ETL processing\")\n",
    "print(f\"   Ray version: {ray.__version__}\")\n",
    "print(f\"   Cluster resources: {ray.cluster_resources()}\")\n",
    "```\n",
    "\n",
    "**What is TPC-H?**\n",
    "\n",
    "The TPC-H benchmark is used for testing database and data processing performance. It simulates a business environment with data relationships that represent business scenarios.\n",
    "\n",
    "**TPC-H Business Context**: The benchmark models a wholesale supplier managing customer orders, inventory, and supplier relationships - representing business data systems.\n",
    "\n",
    "### TPC-H schema overview\n",
    "\n",
    "The TPC-H benchmark provides realistic business data for learning ETL patterns. Understanding the schema helps you apply these techniques to your own data.\n",
    "\n",
    "| Table | Description | Typical Size (SF10) | Primary Use |\n",
    "|-----------|------------------|--------------------------|------------------|\n",
    "| **CUSTOMER** | Customer master data | 1.5M rows | Dimensional analysis |\n",
    "| **ORDERS** | Order transactions | 15M rows | Fact table, time series |\n",
    "| **LINEITEM** | Order line items | 60M rows | Largest fact table |\n",
    "| **PART** | Product catalog | 2M rows | Product dimensions |\n",
    "| **SUPPLIER** | Supplier information | 100K rows | Supplier analytics |\n",
    "| **PARTSUPP** | Part-supplier links | 8M rows | Supply chain |\n",
    "| **NATION** | Geographic data | 25 rows | Geographic grouping |\n",
    "| **REGION** | Regional groups | 5 rows | High-level geography |\n",
    "\n",
    "**Schema relationships**:\n",
    "\n",
    "    CUSTOMER ──one-to-many──→ ORDERS ──one-to-many──→ LINEITEM\n",
    "                                                          ↓\n",
    "    NATION ──one-to-many──→ SUPPLIER                   PART\n",
    "       ↓                        ↓                         ↓\n",
    "    REGION                  PARTSUPP ←────many-to-one────┘\n",
    "\n",
    "``` python\n",
    "# TPC-H Schema Overview for ETL Processing\n",
    "tpch_tables = {\n",
    "    \"customer\": \"Customer master data with demographics and market segments\",\n",
    "    \"orders\": \"Order header information with dates, priorities, and status\",\n",
    "    \"lineitem\": \"Detailed line items for each order (largest table)\",\n",
    "    \"part\": \"Parts catalog with specifications and retail prices\", \n",
    "    \"supplier\": \"Supplier information including contact details\",\n",
    "    \"partsupp\": \"Part-supplier relationships with costs\",\n",
    "    \"nation\": \"Nation reference data with geographic regions\",\n",
    "    \"region\": \"Regional groupings for geographic analysis\"\n",
    "}\n",
    "\n",
    "print(\"TPC-H Schema (8 Tables):\")\n",
    "for table, description in tpch_tables.items():\n",
    "    print(f\"  {table.upper()}: {description}\")\n",
    "```\n",
    "\n",
    "### Loading TPC-H data with Ray Data\n",
    "\n",
    "``` python\n",
    "# TPC-H benchmark data location\n",
    "TPCH_S3_PATH = \"s3://ray-benchmark-data/tpch/parquet/sf10\"\n",
    "\n",
    "print(\"Loading TPC-H benchmark data for distributed processing...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Read TPC-H Customer Master Data\n",
    "    customers_ds = ray.data.read_parquet(\n",
    "        f\"{TPCH_S3_PATH}/customer\",\n",
    "        num_cpus=0.025  # High I/O concurrency for reading\n",
    "    )\n",
    "    \n",
    "    # Read TPC-H Orders Data\n",
    "    orders_ds = ray.data.read_parquet(\n",
    "        f\"{TPCH_S3_PATH}/orders\", \n",
    "        num_cpus=0.025\n",
    "    )\n",
    "    \n",
    "    # Read TPC-H Line Items (largest table)\n",
    "    lineitems_ds = ray.data.read_parquet(\n",
    "        f\"{TPCH_S3_PATH}/lineitem\",\n",
    "        num_cpus=0.025\n",
    "    )\n",
    "    \n",
    "    load_time = time.time() - start_time\n",
    "    \n",
    "    # Count records in parallel\n",
    "    customer_count = customers_ds.count()\n",
    "    orders_count = orders_ds.count()\n",
    "    lineitems_count = lineitems_ds.count()\n",
    "    \n",
    "    print(f\"TPC-H data loaded successfully in {load_time:.2f} seconds\")\n",
    "    print(f\"   Customers: {customer_count:,}\")\n",
    "    print(f\"   Orders: {orders_count:,}\")\n",
    "    print(f\"   Line items: {lineitems_count:,}\")\n",
    "    print(f\"   Total records: {customer_count + orders_count + lineitems_count:,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to load TPC-H data: {e}\")\n",
    "    print(\"   Check S3 access and data availability\")\n",
    "    raise\n",
    "```\n",
    "\n",
    "### Basic ETL transformations\n",
    "\n",
    "``` python\n",
    "# ETL Transform: Customer segmentation using Ray Data native operations\n",
    "def segment_customers(batch: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Apply business rules for customer segmentation.\n",
    "    \n",
    "    This demonstrates common ETL pattern of adding derived business attributes\n",
    "    based on rules and thresholds.\n",
    "    \n",
    "    Args:\n",
    "        batch: Pandas DataFrame with customer records\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with added customer_segment column\n",
    "    \"\"\"\n",
    "    # Business logic for customer segmentation based on account balance\n",
    "    batch['customer_segment'] = 'standard'\n",
    "    batch.loc[batch['c_acctbal'] > 5000, 'customer_segment'] = 'premium'\n",
    "    batch.loc[batch['c_acctbal'] > 10000, 'customer_segment'] = 'enterprise'\n",
    "    \n",
    "    return batch\n",
    "\n",
    "# Apply customer segmentation transformation\n",
    "print(\"\\n🔄 Applying customer segmentation...\")\n",
    "\n",
    "try:\n",
    "    segmented_customers = customers_ds.map_batches(\n",
    "        segment_customers,\n",
    "        num_cpus=0.5,  # Medium complexity transformation\n",
    "        batch_format=\"pandas\"\n",
    "    )\n",
    "    \n",
    "    segment_count = segmented_customers.count()\n",
    "    print(f\"Customer segmentation completed: {segment_count:,} customers segmented\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Segmentation failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# ETL Filter: High-value customers using expressions API\n",
    "print(\"\\nFiltering high-value customers...\")\n",
    "\n",
    "try:\n",
    "    high_value_customers = segmented_customers.filter(\n",
    "        col(\"c_acctbal\") > lit(1000),\n",
    "        num_cpus=0.1\n",
    "    )\n",
    "    \n",
    "    high_value_count = high_value_customers.count()\n",
    "    total_count = segmented_customers.count()\n",
    "    percentage = (high_value_count / total_count) * 100 if total_count > 0 else 0\n",
    "    \n",
    "    print(f\"✅ High-value customers: {high_value_count:,} ({percentage:.1f}% of total)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during filtering: {e}\")\n",
    "    raise\n",
    "\n",
    "# ETL Aggregation: Customer statistics by market segment\n",
    "customer_stats = segmented_customers.groupby(\"c_mktsegment\").aggregate(\n",
    "    Count(),\n",
    "    Mean(\"c_acctbal\"),\n",
    "    Sum(\"c_acctbal\"),\n",
    "    Max(\"c_acctbal\")\n",
    ")\n",
    "\n",
    "print(\"\\nCustomer Statistics by Market Segment:\")\n",
    "print(\"=\" * 70)\n",
    "# Display customer statistics\n",
    "stats_df = customer_stats.limit(10).to_pandas()\n",
    "print(stats_df.to_string(index=False))\n",
    "print(\"=\" * 70)\n",
    "```\n",
    "\n",
    "## Step 2: Data Transformations and Processing\n",
    "\n",
    "This section demonstrates how Ray Data handles common ETL transformation patterns including data enrichment, filtering, and complex business logic. You’ll learn to build production-grade transformations that scale efficiently.\n",
    "\n",
    "### Why transformations are critical\n",
    "\n",
    "Data transformations convert raw data into business-valuable information. Common transformation patterns include:\n",
    "\n",
    "- **Enrichment**: Adding calculated fields and derived metrics\n",
    "- **Filtering**: Removing irrelevant or invalid records  \n",
    "- **Joins**: Combining data from multiple sources\n",
    "- **Aggregations**: Computing summary statistics and rollups\n",
    "- **Type conversions**: Ensuring correct data types for analytics\n",
    "\n",
    "### Transformation performance comparison\n",
    "\n",
    "| Transformation Type | Traditional Approach | Ray Data Approach | Scalability |\n",
    "|-------------------|---------------------|-------------------|--------------|\n",
    "| **Column calculations** | Row-by-row processing | Vectorized batches | Linear scaling |\n",
    "| **Date parsing** | Sequential parsing | Parallel batch parsing | High throughput |\n",
    "| **Categorization** | Conditional logic loops | Pandas vectorization | Efficient |\n",
    "| **Business rules** | Single-threaded | Distributed map_batches | Scales to cluster |\n",
    "\n",
    "### Complex data transformations\n",
    "\n",
    ":::tip GPU Acceleration for Pandas ETL Operations\n",
    "For complex pandas transformations in your ETL pipeline, you can use **NVIDIA RAPIDS cuDF** to accelerate DataFrame operations on GPUs. Replace `import pandas as pd` with `import cudf as pd` in your `map_batches` functions to use GPU acceleration for operations like datetime parsing, groupby, joins, and aggregations.\n",
    "\n",
    "**When to use cuDF**:\n",
    "- Complex datetime operations (parsing, extracting components)\n",
    "- Large aggregations and groupby operations\n",
    "- String operations on millions of rows\n",
    "- Join operations on large datasets\n",
    "- Statistical calculations across many columns\n",
    "\n",
    "**Performance benefit**: GPU-accelerated pandas operations can be 10-50x faster for large batches (1000+ rows) with complex transformations.\n",
    "\n",
    "**Requirements**: Add `cudf` to your dependencies and ensure GPU-enabled cluster nodes.\n",
    ":::\n",
    "\n",
    "``` python\n",
    "# ETL Transform: Order enrichment with business metrics\n",
    "def enrich_orders_with_metrics(batch):\n",
    "    \"\"\"Enrich orders with calculated business metrics.\n",
    "    \n",
    "    For GPU acceleration, replace 'import pandas as pd' with 'import cudf as pd'\n",
    "    to speed up complex DataFrame operations like datetime parsing and categorization.\n",
    "    \"\"\"\n",
    "    import pandas as pd  # or 'import cudf as pd' for GPU acceleration\n",
    "    df = pd.DataFrame(batch)\n",
    "    \n",
    "    # Parse order date and create time dimensions\n",
    "    # This datetime parsing is GPU-accelerated with cuDF\n",
    "\n",
    "    # Data transformation    df['o_orderdate'] = pd.to_datetime(df['o_orderdate'])\n",
    "\n",
    "    # Data transformation    df['order_year'] = df['o_orderdate'].dt.year\n",
    "\n",
    "    # Data transformation    df['order_quarter'] = df['o_orderdate'].dt.quarter\n",
    "\n",
    "    # Data transformation    df['order_month'] = df['o_orderdate'].dt.month\n",
    "    \n",
    "    # Business classifications\n",
    "    # These conditional operations are GPU-accelerated with cuDF\n",
    "\n",
    "    # Data transformation    df['is_large_order'] = df['o_totalprice'] > 200000\n",
    "\n",
    "    # Data transformation    df['is_urgent'] = df['o_orderpriority'].isin(['1-URGENT', '2-HIGH'])\n",
    "\n",
    "    # Data transformation    df['revenue_tier'] = pd.cut(\n",
    "        df['o_totalprice'],\n",
    "        bins=[0, 50000, 150000, 300000, float('inf')],\n",
    "        labels=['Small', 'Medium', 'Large', 'Enterprise']\n",
    "    )\n",
    "    \n",
    "    return df.to_dict('records')\n",
    "\n",
    "# Apply order enrichment\n",
    "print(\"\\n🔄 Enriching orders with business metrics...\")\n",
    "\n",
    "try:\n",
    "    enriched_orders = orders_ds.map_batches(\n",
    "        enrich_orders_with_metrics,\n",
    "        num_cpus=0.5,  # Medium complexity transformation\n",
    "        batch_format=\"pandas\"\n",
    "    )\n",
    "    \n",
    "    enriched_count = enriched_orders.count()\n",
    "    print(f\"✅ Order enrichment completed: {enriched_count:,} orders processed\")\n",
    "    \n",
    "    # Show sample enriched record\n",
    "    sample = enriched_orders.take(1)[0]\n",
    "    print(f\"\\n📋 Sample enriched order:\")\n",
    "    print(f\"   Order ID: {sample.get('o_orderkey')}\")\n",
    "    print(f\"   Year: {sample.get('order_year')}, Quarter: {sample.get('order_quarter')}\")\n",
    "    print(f\"   Category: {sample.get('order_size_category')}\")\n",
    "    print(f\"   Revenue Tier: {sample.get('revenue_tier')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during enrichment: {e}\")\n",
    "    raise\n",
    "```\n",
    "\n",
    "### Advanced filtering and selection\n",
    "\n",
    "``` python\n",
    "# Advanced filtering using Ray Data expressions API\n",
    "print(\"Applying advanced filtering techniques...\")\n",
    "\n",
    "# Filter recent high-value orders\n",
    "recent_high_value_orders = enriched_orders.filter(\n",
    "    (col(\"order_year\") >= lit(1995)) & \n",
    "    (col(\"o_totalprice\") > lit(100000)) &\n",
    "    (col(\"is_urgent\") == lit(True)),\n",
    "    num_cpus=0.1\n",
    ")\n",
    "\n",
    "# Filter by revenue tier using expressions\n",
    "enterprise_orders = enriched_orders.filter(\n",
    "    col(\"revenue_tier\") == lit(\"Enterprise\"),\n",
    "    num_cpus=0.1\n",
    ")\n",
    "\n",
    "# Complex filtering with multiple conditions\n",
    "complex_filtered_orders = enriched_orders.filter(\n",
    "    (col(\"order_quarter\") == lit(4)) &\n",
    "    (col(\"o_orderstatus\") == lit(\"F\")) &\n",
    "    (col(\"o_totalprice\") > lit(50000)),\n",
    "    num_cpus=0.1\n",
    ")\n",
    "\n",
    "print(f\"Advanced filtering results:\")\n",
    "print(f\"  Recent high-value orders: {recent_high_value_orders.count():,}\")\n",
    "print(f\"  Enterprise orders: {enterprise_orders.count():,}\")\n",
    "print(f\"  Complex filtered orders: {complex_filtered_orders.count():,}\")\n",
    "```\n",
    "\n",
    "### Data joins and relationships\n",
    "\n",
    "``` python\n",
    "# ETL Join: Customer-Order analysis using Ray Data joins\n",
    "print(\"\\n🔗 Performing distributed joins for customer-order analysis...\")\n",
    "\n",
    "try:\n",
    "    # Join customers with their orders for comprehensive analysis\n",
    "    # Ray Data optimizes join execution across distributed nodes\n",
    "    customer_order_analysis = customers_ds.join(\n",
    "        enriched_orders,\n",
    "        left_key=\"c_custkey\",\n",
    "        right_key=\"o_custkey\",\n",
    "        join_type=\"inner\"\n",
    "    )\n",
    "    \n",
    "    join_count = customer_order_analysis.count()\n",
    "    print(f\"✅ Customer-order join completed: {join_count:,} records\")\n",
    "    \n",
    "    # Calculate join statistics\n",
    "    customer_count = customers_ds.count()\n",
    "    orders_count = enriched_orders.count()\n",
    "    join_ratio = (join_count / orders_count) * 100 if orders_count > 0 else 0\n",
    "    \n",
    "    print(f\"   Input: {customer_count:,} customers, {orders_count:,} orders\")\n",
    "    print(f\"   Join ratio: {join_ratio:.1f}% of orders matched\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during join: {e}\")\n",
    "    raise\n",
    "\n",
    "# Aggregate customer order metrics\n",
    "customer_order_metrics = customer_order_analysis.groupby(\"c_mktsegment\").aggregate(\n",
    "    Count(),\n",
    "    Mean(\"o_totalprice\"),\n",
    "    Sum(\"o_totalprice\"),\n",
    "    Count(\"o_orderkey\")\n",
    ")\n",
    "\n",
    "print(\"Customer Order Metrics by Market Segment:\")\n",
    "# Display customer order metrics\n",
    "print(\"Customer Order Metrics by Market Segment:\")\n",
    "print(customer_order_metrics.limit(10).to_pandas())\n",
    "```\n",
    "\n",
    "## Step 3: Performance Optimization Techniques\n",
    "\n",
    "This section covers advanced optimization techniques for production ETL workloads. You’ll learn how to tune memory usage, batch sizes, and resource allocation for optimal performance.\n",
    "\n",
    "### ETL optimization decision framework\n",
    "\n",
    "Understanding when and how to optimize is crucial for production ETL systems. Follow this systematic approach:\n",
    "\n",
    "    ETL Performance Issue\n",
    "    ├── Is data loading slow?\n",
    "    │   └── Solution: Increase I/O concurrency (num_cpus=0.025)\n",
    "    │       Column pruning, file format optimization\n",
    "    │\n",
    "    ├── Are transformations slow?\n",
    "    │   └── Solution: Balance num_cpus based on complexity\n",
    "    │       Light transforms: num_cpus=0.25-0.5\n",
    "    │       Heavy transforms: num_cpus=1.0-2.0\n",
    "    │\n",
    "    ├── Are joins slow?\n",
    "    │   └── Solution: Column selection before joins\n",
    "    │       Filter data early, use expressions API\n",
    "    │\n",
    "    ├── Is output slow?\n",
    "    │   └── Solution: Optimize write concurrency (num_cpus=0.1)\n",
    "    │       Compression, partitioning strategy\n",
    "    │\n",
    "    └── Memory issues?\n",
    "        └── Solution: Adjust batch_size and block_size\n",
    "            Monitor object store usage\n",
    "\n",
    "### Resource allocation guidelines for ETL\n",
    "\n",
    "| ETL Stage | Operation Type | num_cpus Recommendation | Batch Size | Reasoning |\n",
    "|-----------|---------------|-----------------------|------------|-----------|\n",
    "| **Extract** | I/O-bound | 0.025 | Default | Maximum parallel reads |\n",
    "| **Light Transform** | CPU-light | 0.25-0.5 | Default | Balanced parallelism |\n",
    "| **Heavy Transform** | CPU-intensive | 1.0-2.0 | 500-1000 | Reduce task overhead |\n",
    "| **Filter** | Simple logic | 0.1 | Default | Fast filtering |\n",
    "| **Join** | Shuffle operation | Default | Default | Let Ray optimize |\n",
    "| **Aggregate** | Reduction | Default | Default | Ray Data optimized |\n",
    "| **Load** | I/O-bound | 0.1 | Default | Balanced writes |\n",
    "\n",
    "### Memory and resource optimization\n",
    "\n",
    "``` python\n",
    "# Configure Ray Data for optimal ETL performance\n",
    "print(\"Configuring Ray Data for ETL optimization...\")\n",
    "\n",
    "# Memory optimization for large datasets\n",
    "ctx.target_max_block_size = 128 * 1024 * 1024  # 128 MB blocks\n",
    "ctx.eager_free = True  # Aggressive memory cleanup\n",
    "\n",
    "# Enable performance monitoring\n",
    "ctx.enable_auto_log_stats = True\n",
    "ctx.memory_usage_poll_interval_s = 5.0\n",
    "\n",
    "print(\"Ray Data configured for optimal ETL performance\")\n",
    "```\n",
    "\n",
    "### Batch size and concurrency optimization\n",
    "\n",
    "``` python\n",
    "# Demonstrate different batch size strategies for ETL operations\n",
    "print(\"Testing ETL batch size optimization...\")\n",
    "\n",
    "# Small batch processing for memory-constrained operations\n",
    "def memory_intensive_etl(batch):\n",
    "    \"\"\"Memory-intensive ETL transformation.\"\"\"\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(batch)\n",
    "    \n",
    "    # Simulate memory-intensive operations\n",
    "\n",
    "    # Data transformation    df['complex_metric'] = df['o_totalprice'] * np.log(df['o_totalprice'] + 1)\n",
    "\n",
    "    # Data transformation    df['percentile_rank'] = df['o_totalprice'].rank(pct=True)\n",
    "    \n",
    "    return df.to_dict('records')\n",
    "\n",
    "# Apply with optimized batch size for memory management\n",
    "memory_optimized_orders = enriched_orders.map_batches(\n",
    "    memory_intensive_etl,\n",
    "    num_cpus=1.0,  # Fewer concurrent tasks for memory management\n",
    "    batch_size=500,  # Smaller batches for memory efficiency\n",
    "    batch_format=\"pandas\"\n",
    ")\n",
    "\n",
    "print(f\"Memory-optimized processing: {memory_optimized_orders.count():,} records\")\n",
    "\n",
    "# Large batch processing for I/O-intensive operationsdef io_intensive_etl(batch):\n",
    "    \"\"\"I/O-intensive ETL transformation.\"\"\"\n",
    "    # Simulate I/O operations\n",
    "    processed_records = []\n",
    "    for record in batch:\n",
    "        processed_record = {\n",
    "            **record,\n",
    "            'processing_timestamp': datetime.now().isoformat(),\n",
    "            'batch_id': str(uuid.uuid4())[:8]\n",
    "        }\n",
    "        processed_records.append(processed_record)\n",
    "    \n",
    "    return processed_records\n",
    "\n",
    "# Apply with optimized batch size for I/O efficiency\n",
    "io_optimized_orders = enriched_orders.map_batches(\n",
    "    io_intensive_etl,\n",
    "    num_cpus=0.25,  # Higher concurrency for I/O operations\n",
    "    batch_size=2000  # Larger batches for I/O efficiency\n",
    ", batch_format=\"pandas\")\n",
    "\n",
    "print(f\"I/O-optimized processing: {io_optimized_orders.count():,} records\")\n",
    "```\n",
    "\n",
    "### Column selection and schema optimization\n",
    "\n",
    "``` python\n",
    "# ETL Optimization: Column pruning for performance\n",
    "print(\"Applying column selection optimization...\")\n",
    "\n",
    "# Select only essential columns for downstream processing\n",
    "essential_customer_columns = customers_ds.select_columns([\n",
    "    \"c_custkey\", \"c_name\", \"c_mktsegment\", \"c_acctbal\", \"c_nationkey\"\n",
    "])\n",
    "\n",
    "essential_order_columns = enriched_orders.select_columns([\n",
    "    \"o_orderkey\", \"o_custkey\", \"o_totalprice\", \"o_orderdate\", \n",
    "    \"order_year\", \"revenue_tier\", \"is_large_order\"\n",
    "])\n",
    "\n",
    "print(f\"Column optimization:\")\n",
    "print(f\"  Customer columns: {len(essential_customer_columns.schema().names)}\")\n",
    "print(f\"  Order columns: {len(essential_order_columns.schema().names)}\")\n",
    "\n",
    "# Optimized join with selected columns\n",
    "optimized_join = essential_customer_columns.join(\n",
    "    essential_order_columns,\n",
    "    left_key=\"c_custkey\",\n",
    "    right_key=\"o_custkey\"\n",
    ")\n",
    "\n",
    "print(f\"Optimized join completed: {optimized_join.count():,} records\")\n",
    "```\n",
    "\n",
    "## Step 4: Large-Scale ETL Patterns\n",
    "\n",
    "Production ETL systems must handle billions of records efficiently. This section demonstrates Ray Data patterns for large-scale data processing including distributed aggregations, multi-dimensional analysis, and data warehouse integration.\n",
    "\n",
    "### Why scale matters in ETL\n",
    "\n",
    "As data volumes grow, ETL approaches must evolve:\n",
    "\n",
    "| Data Scale | Traditional ETL Challenge | Ray Data Solution |\n",
    "|----------------|---------------------------------|------------------------|\n",
    "| **\\< 100 GB** | Single machine sufficient | Ray Data still faster with parallelism |\n",
    "| **100 GB - 1 TB** | Memory constraints appear | Streaming execution prevents OOMs |\n",
    "| **1 TB - 10 TB** | Processing takes hours/days | Distributed processing reduces time |\n",
    "| **\\> 10 TB** | May not complete | Scales horizontally across cluster |\n",
    "\n",
    "**Scaling dimensions**:\n",
    "- **Data volume**: From gigabytes to petabytes\n",
    "- **Cluster size**: From single node to hundreds of nodes\n",
    "- **Complexity**: From simple transforms to complex business logic\n",
    "- **Concurrency**: From sequential to massively parallel\n",
    "\n",
    "### Distributed Aggregations\n",
    "\n",
    "``` python\n",
    "# Large-scale aggregations using Ray Data native operations\n",
    "print(\"Performing large-scale distributed aggregations...\")\n",
    "\n",
    "# Multi-dimensional aggregations for business intelligence\n",
    "comprehensive_metrics = optimized_join.groupby([\"c_mktsegment\", \"order_year\", \"revenue_tier\"]).aggregate(\n",
    "    Count(),\n",
    "    Sum(\"o_totalprice\"),\n",
    "    Mean(\"o_totalprice\"),\n",
    "    Max(\"o_totalprice\"),\n",
    "    Mean(\"c_acctbal\")\n",
    ")\n",
    "\n",
    "print(\"Comprehensive Business Metrics:\")\n",
    "# Display comprehensive business metrics\n",
    "print(\"Comprehensive Business Metrics:\")\n",
    "print(comprehensive_metrics.limit(20).to_pandas())\n",
    "\n",
    "# Time-series aggregations for trend analysis\n",
    "yearly_trends = optimized_join.groupby(\"order_year\").aggregate(\n",
    "    Count(),\n",
    "    Sum(\"o_totalprice\"),\n",
    "    Mean(\"o_totalprice\")\n",
    ")\n",
    "\n",
    "print(\"Yearly Trends Analysis:\")\n",
    "# Display yearly trends\n",
    "print(\"Yearly Trends Analysis:\")\n",
    "print(yearly_trends.limit(10).to_pandas())\n",
    "\n",
    "# Customer segment performance analysis\n",
    "segment_performance = optimized_join.groupby([\"c_mktsegment\", \"revenue_tier\"]).aggregate(\n",
    "    Count(),\n",
    "    Sum(\"o_totalprice\"),\n",
    "    Mean(\"c_acctbal\")\n",
    ")\n",
    "\n",
    "print(\"Customer Segment Performance:\")\n",
    "# Display segment performance\n",
    "print(\"Customer Segment Performance:\")\n",
    "print(segment_performance.limit(10).to_pandas())\n",
    "```\n",
    "\n",
    "### ETL pipeline optimization\n",
    "\n",
    "``` python\n",
    "# Demonstrate optimized ETL pipeline patterns\n",
    "print(\"Building optimized ETL pipeline...\")\n",
    "\n",
    "def create_optimized_etl_pipeline():\n",
    "    \"\"\"Create optimized ETL pipeline with Ray Data best practices.\"\"\"\n",
    "    \n",
    "    # Extract: Optimized data loading with column selection\n",
    "    customers = ray.data.read_parquet(\n",
    "        f\"{TPCH_S3_PATH}/customer\",\n",
    "        columns=[\"c_custkey\", \"c_name\", \"c_mktsegment\", \"c_acctbal\", \"c_nationkey\"],\n",
    "        num_cpus=0.025  # High I/O concurrency\n",
    "    )\n",
    "    \n",
    "    orders = ray.data.read_parquet(\n",
    "        f\"{TPCH_S3_PATH}/orders\",\n",
    "        columns=[\"o_orderkey\", \"o_custkey\", \"o_totalprice\", \"o_orderdate\", \"o_orderstatus\"],\n",
    "        num_cpus=0.025\n",
    "    )\n",
    "    \n",
    "    # Transform: Apply business logic with optimized batch processing\n",
    "    enriched_customers = customers.map_batches(\n",
    "        lambda batch: [\n",
    "            {\n",
    "                **record,\n",
    "                \"customer_tier\": \"premium\" if record[\"c_acctbal\"] > 5000 else \"standard\",\n",
    "                \"market_priority\": \"high\" if record[\"c_mktsegment\"] in [\"BUILDING\", \"AUTOMOBILE\"] else \"medium\"\n",
    "            }\n",
    "            for record in batch\n",
    "        ],\n",
    "        num_cpus=0.5,  # Medium complexity transformation\n",
    "        batch_size=1000\n",
    "    , batch_format=\"pandas\")\n",
    "    \n",
    "    # Transform: Order processing with time dimensions\n",
    "    processed_orders = orders.map_batches(\n",
    "        lambda batch: [\n",
    "            {\n",
    "                **record,\n",
    "                \"order_year\": int(record[\"o_orderdate\"][:4], batch_format=\"pandas\"),\n",
    "                \"order_size\": \"large\" if record[\"o_totalprice\"] > 200000 else \"small\",\n",
    "                \"processing_timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            for record in batch\n",
    "        ],\n",
    "        num_cpus=0.5,\n",
    "        batch_size=1000\n",
    "    )\n",
    "    \n",
    "    # Load: Join and aggregate for analytics output\n",
    "    final_analytics = enriched_customers.join(\n",
    "        processed_orders,\n",
    "        left_key=\"c_custkey\",\n",
    "        right_key=\"o_custkey\"\n",
    "    ).groupby([\"customer_tier\", \"order_year\"]).aggregate(\n",
    "        Count(),\n",
    "        Sum(\"o_totalprice\"),\n",
    "        Mean(\"c_acctbal\")\n",
    "    )\n",
    "    \n",
    "    return final_analytics\n",
    "\n",
    "# Execute optimized ETL pipeline\n",
    "optimized_results = create_optimized_etl_pipeline()\n",
    "print(\"Optimized ETL Pipeline Results:\")\n",
    "# Display optimized pipeline results\n",
    "print(\"Optimized ETL Pipeline Results:\")\n",
    "print(optimized_results.limit(10).to_pandas())\n",
    "```\n",
    "\n",
    "### Large-scale data processing\n",
    "\n",
    "``` python\n",
    "# Process large datasets with optimization techniques\n",
    "print(\"Demonstrating large-scale data processing...\")\n",
    "\n",
    "# Load larger TPC-H scale factor for performance testing\n",
    "large_orders = ray.data.read_parquet(\n",
    "    f\"{TPCH_S3_PATH}/lineitem\",  # Largest TPC-H table\n",
    "    columns=[\"l_orderkey\", \"l_partkey\", \"l_quantity\", \"l_extendedprice\", \"l_discount\"],\n",
    "    num_cpus=0.025  # High concurrency for large dataset\n",
    ")\n",
    "\n",
    "print(f\"Large dataset loaded: {large_orders.count():,} line items\")\n",
    "\n",
    "# Apply distributed transformations for large-scale processingdef calculate_line_metrics(batch):\n",
    "    \"\"\"Calculate line item business metrics.\"\"\"\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(batch)\n",
    "    \n",
    "    # Business calculations\n",
    "    df['discounted_price'] = df['l_extendedprice'] * (1 - df['l_discount'])\n",
    "    df['revenue_impact'] = df['discounted_price'] * df['l_quantity']\n",
    "    df['volume_category'] = pd.cut(\n",
    "        df['l_quantity'],\n",
    "        bins=[0, 10, 50, 100, float('inf')],\n",
    "        labels=['Low', 'Medium', 'High', 'Bulk']\n",
    "    )\n",
    "    \n",
    "    return df.to_dict('records')\n",
    "\n",
    "# Process large dataset with optimized settings\n",
    "processed_lineitems = large_orders.map_batches(\n",
    "    calculate_line_metrics,\n",
    "    num_cpus=0.5,  # Balanced processing\n",
    "    batch_size=1000,\n",
    "    batch_format=\"pandas\"\n",
    ")\n",
    "\n",
    "# Large-scale aggregations\n",
    "revenue_analysis = processed_lineitems.groupby(\"volume_category\").aggregate(\n",
    "    Count(),\n",
    "    Sum(\"revenue_impact\"),\n",
    "    Mean(\"discounted_price\")\n",
    ")\n",
    "\n",
    "print(\"Large-Scale Revenue Analysis:\")\n",
    "# Display revenue analysis results\n",
    "print(\"Large-Scale Revenue Analysis:\")\n",
    "print(revenue_analysis.limit(10).to_pandas())\n",
    "```\n",
    "\n",
    "### ETL output and data warehouse integration\n",
    "\n",
    "``` python\n",
    "# Write ETL results to data warehouse formats\n",
    "print(\"Writing ETL results to data warehouse...\")\n",
    "\n",
    "# Write customer analytics with partitioning\n",
    "enriched_customers.write_parquet(\n",
    "    \"/tmp/etl_warehouse/customers/\",\n",
    "    partition_cols=[\"customer_tier\"],\n",
    "    compression=\"snappy\",\n",
    "    num_cpus=0.1\n",
    ")\n",
    "\n",
    "# Write order analytics with time-based partitioning\n",
    "processed_orders.write_parquet(\n",
    "    \"/tmp/etl_warehouse/orders/\",\n",
    "    partition_cols=[\"order_year\"],\n",
    "    compression=\"snappy\",\n",
    "    num_cpus=0.1\n",
    ")\n",
    "\n",
    "# Write aggregated analytics for BI tools\n",
    "final_analytics = optimized_join.groupby([\"c_mktsegment\", \"revenue_tier\", \"order_year\"]).aggregate(\n",
    "    Count(),\n",
    "    Sum(\"o_totalprice\"),\n",
    "    Mean(\"o_totalprice\"),\n",
    "    Mean(\"c_acctbal\")\n",
    ")\n",
    "\n",
    "final_analytics.write_parquet(\n",
    "    \"/tmp/etl_warehouse/analytics/\",\n",
    "    partition_cols=[\"order_year\"],\n",
    "    compression=\"snappy\",\n",
    "    num_cpus=0.1\n",
    ")\n",
    "\n",
    "print(\"ETL warehouse output completed\")\n",
    "```\n",
    "\n",
    "### Performance monitoring and validation\n",
    "\n",
    "``` python\n",
    "# Validate ETL pipeline performance\n",
    "print(\"Validating ETL pipeline performance...\")\n",
    "\n",
    "# Read back and verify outputs\n",
    "customer_verification = ray.data.read_parquet(\n",
    "    \"/tmp/etl_warehouse/customers/\",\n",
    "    num_cpus=0.025\n",
    ")\n",
    "\n",
    "order_verification = ray.data.read_parquet(\n",
    "    \"/tmp/etl_warehouse/orders/\",\n",
    "    num_cpus=0.025\n",
    ")\n",
    "\n",
    "analytics_verification = ray.data.read_parquet(\n",
    "    \"/tmp/etl_warehouse/analytics/\",\n",
    "    num_cpus=0.025\n",
    ")\n",
    "\n",
    "print(f\"ETL Pipeline Verification:\")\n",
    "print(f\"  Customer records: {customer_verification.count():,}\")\n",
    "print(f\"  Order records: {order_verification.count():,}\")\n",
    "print(f\"  Analytics records: {analytics_verification.count():,}\")\n",
    "\n",
    "# Display sample results\n",
    "sample_analytics = analytics_verification.take(5)\n",
    "print(\"Sample ETL Analytics Results:\")\n",
    "for i, record in enumerate(sample_analytics):\n",
    "    print(f\"  {i+1}. Segment: {record['c_mktsegment']}, Tier: {record['revenue_tier']}, \"\n",
    "          f\"Year: {record['order_year']}, Orders: {record['count()']}, Revenue: ${record['sum(o_totalprice)']:,.0f}\")\n",
    "```\n",
    "\n",
    "## ETL pipeline execution results\n",
    "\n",
    "### Processing metrics\n",
    "\n",
    "This ETL pipeline processed TPC-H benchmark data demonstrating production-scale capabilities:\n",
    "\n",
    "| Dataset | Records Processed | Ray Data Operation | Purpose |\n",
    "|------------|-----------------------|-------------------------|------------|\n",
    "| **Customers** | 1.5M rows | `read_parquet()` | Customer master data |\n",
    "| **Orders** | 15M rows | `read_parquet()` | Order transactions |\n",
    "| **LineItems** | 60M rows | `read_parquet()` | Detailed line items |\n",
    "| **Final Analytics** | Aggregated | `groupby().aggregate()` | Business intelligence |\n",
    "\n",
    "### Ray Data operations demonstrated\n",
    "\n",
    "**Data loading and extraction**:\n",
    "- `read_parquet()` with column pruning - Optimized data loading\n",
    "- High I/O concurrency with `num_cpus=0.025` - Maximum parallelism\n",
    "\n",
    "**Data transformations**:\n",
    "- `map_batches()` - Distributed transformations with business logic\n",
    "- `filter()` with expressions API - Advanced filtering\n",
    "- `select_columns()` - Schema optimization for performance\n",
    "\n",
    "**Data integration**:\n",
    "- `join()` - Distributed joins across datasets\n",
    "- `groupby().aggregate()` - Large-scale aggregations\n",
    "- Native aggregations: `Count()`, `Sum()`, `Mean()`, `Max()`\n",
    "\n",
    "**Data output**:\n",
    "- `write_parquet()` - Data warehouse output with partitioning\n",
    "- Compression and optimization for query performance\n",
    "\n",
    "### ETL optimization techniques applied\n",
    "\n",
    "| Technique | Implementation | Benefit |\n",
    "|-----------------------|-------------------------------|-------------------|\n",
    "| **Column Pruning** | Specify columns in `read_parquet()` | 60-95% I/O reduction |\n",
    "| **Early Filtering** | Use `filter()` after read | Reduce data volume early |\n",
    "| **Resource Allocation** | Stage-specific `num_cpus` values | Balanced parallelism |\n",
    "| **Batch Sizing** | Appropriate batch sizes | Memory management |\n",
    "| **Partitioned Output** | `partition_cols` parameter | Query optimization |\n",
    "| **Compression** | Snappy compression | Storage efficiency |\n",
    "\n",
    "### Production ETL patterns demonstrated\n",
    "\n",
    "This template showcases enterprise-ready patterns:\n",
    "- TPC-H benchmark for standardized testing\n",
    "- Business logic transformations with real data\n",
    "- Data quality and validation checks\n",
    "- Analytics-ready output formats (partitioned Parquet)\n",
    "- Performance monitoring with progress bars\n",
    "- Proper resource cleanup\n",
    "\n",
    "### ETL pipeline performance visualization\n",
    "\n",
    "``` python\n",
    "# Visualize ETL pipeline performance using utility functions\n",
    "from util.viz_utils import (\n",
    "    visualize_etl_performance,\n",
    "    create_interactive_etl_pipeline,\n",
    "    create_data_lineage_diagram,\n",
    "    create_tpch_schema_diagram\n",
    ")\n",
    "\n",
    "# Generate ETL performance visualization\n",
    "fig = visualize_etl_performance()\n",
    "print(\"ETL performance visualization created\")\n",
    "\n",
    "# Create interactive pipeline dashboard\n",
    "pipeline_results = {\n",
    "    'stages': ['Extract', 'Transform', 'Load'],\n",
    "    'records': [1500000, 1200000, 1200000],\n",
    "    'processing_time': [12, 45, 10]\n",
    "}\n",
    "interactive_dashboard = create_interactive_etl_pipeline(pipeline_results)\n",
    "interactive_dashboard.write_html('etl_pipeline_dashboard.html')\n",
    "print(\"Interactive ETL pipeline dashboard saved\")\n",
    "\n",
    "# Create data lineage visualization\n",
    "lineage_diagram = create_data_lineage_diagram()\n",
    "lineage_diagram.write_html('data_lineage.html')\n",
    "print(\"Data lineage diagram saved\")\n",
    "\n",
    "# Create TPC-H schema diagram\n",
    "schema_diagram = create_tpch_schema_diagram()\n",
    "schema_diagram.write_html('tpch_schema.html')\n",
    "print(\"TPC-H schema diagram saved\")\n",
    "```\n",
    "\n",
    "**Interactive visualizations created:**\n",
    "- **ETL pipeline dashboard**: Shows data volumes, processing times, and resource utilization\n",
    "- **Data lineage diagram**: Sankey chart showing data flow through ETL stages\n",
    "- **TPC-H schema**: Visual representation of table relationships\n",
    "- **Performance metrics**: Interactive charts for throughput and optimization impact\n",
    "\n",
    "### Ray Data’s streaming execution model\n",
    "\n",
    "Ray Data uniquely combines the best aspects of structured streaming and batch processing:\n",
    "\n",
    "#### How streaming execution works\n",
    "\n",
    "**Traditional Batch Processing:**\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/cko-2025-q1/batch-processing.png\" width=\"800\" alt=\"Traditional Batch Processing\">\n",
    "\n",
    "    Traditional approach problems:\n",
    "    ┌──────┐   ┌──────┐   ┌──────┐   ┌──────┐\n",
    "    │ Read │ → │ Proc │ → │ Agg  │ → │Write │\n",
    "    │ ALL  │   │ ALL  │   │ ALL  │   │ ALL  │\n",
    "    └──────┘   └──────┘   └──────┘   └──────┘\n",
    "      Wait       Wait       Wait       Wait\n",
    "      \n",
    "     High memory (load everything)\n",
    "     No parallelism across stages\n",
    "     Long time to first result\n",
    "\n",
    "**Ray Data Streaming Execution:**\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/cko-2025-q1/pipelining.png\" width=\"800\" alt=\"Ray Data Streaming Execution\">\n",
    "\n",
    "    ┌──────┐   ┌──────┐   ┌──────┐   ┌──────┐\n",
    "    │Block1│ → │Block1│ → │Block1│ → │Block1│\n",
    "    │Block2│ → │Block2│ → │Block2│ → │Block2│ (Parallel!)\n",
    "    │Block3│ → │Block3│ → │Block3│ → │Block3│\n",
    "    └──────┘   └──────┘   └──────┘   └──────┘\n",
    "      Read      Process    Aggregate   Write\n",
    "\n",
    "     Low memory (process in blocks)\n",
    "     Pipeline parallelism (all stages active)\n",
    "     Fast time to first result\n",
    "\n",
    "#### Best of structured streaming\n",
    "\n",
    "From structured streaming systems (Spark Structured Streaming, Flink), Ray Data inherits:\n",
    "\n",
    "| Streaming Benefit | How Ray Data Implements It | ETL Advantage |\n",
    "|----------------------|--------------------------------|-------------------|\n",
    "| **Low memory footprint** | Process 128 MB blocks, not full dataset | Handle TB datasets with GB clusters |\n",
    "| **Pipeline parallelism** | All stages run simultaneously | Better resource utilization |\n",
    "| **Backpressure control** | Automatic flow management | Prevents memory overflow |\n",
    "| **Incremental results** | Output available immediately | Faster feedback |\n",
    "\n",
    "#### Best of batch processing\n",
    "\n",
    "From batch processing systems (Spark SQL, pandas), Ray Data inherits:\n",
    "\n",
    "| Batch Benefit | How Ray Data Implements It | ETL Advantage |\n",
    "|-------------------|----------------------------------|--------------------|\n",
    "| **Simple API** | Familiar operations (filter, join, groupby) | Easy to learn and use |\n",
    "| **High throughput** | Optimized block sizes (128 MB) | Efficient processing |\n",
    "| **Rich transformations** | Full SQL-like operations | Complex business logic |\n",
    "| **No complexity** | No windowing or watermarking | Simpler code |\n",
    "\n",
    "#### Unique Ray Data advantages\n",
    "\n",
    "What makes Ray Data’s approach special:\n",
    "\n",
    "**Automatic optimization**:\n",
    "- No manual micro-batch tuning required\n",
    "- Intelligent backpressure between stages\n",
    "- Dynamic resource allocation\n",
    "\n",
    "**Unified API**:\n",
    "- Same code for batch and streaming\n",
    "- No separate APIs to learn\n",
    "- Consistent behavior\n",
    "\n",
    "**Example: Streaming execution in ETL**\n",
    "\n",
    "``` python\n",
    "# This pipeline runs all stages simultaneously:\n",
    "etl_results = (\n",
    "    # Stage 1: Read (starts immediately)\n",
    "    ray.data.read_parquet(\"s3://data/\",\n",
    "    num_cpus=0.025\n",
    ")\n",
    "    \n",
    "    # Stage 2: Filter (processes blocks as they arrive)  \n",
    "    .filter(lambda x: x[\"valid\"] == True, num_cpus=0.1)\n",
    "    \n",
    "    # Stage 3: Transform (runs in parallel with read/filter)\n",
    "    .map_batches(enrich_data, num_cpus=0.5, batch_format=\"pandas\")\n",
    "    \n",
    "    # Stage 4: Write (starts as soon as first blocks ready)\n",
    "    .write_parquet(\"s3://output/\",\n",
    "    num_cpus=0.1\n",
    ")\n",
    ")\n",
    "\n",
    "# All stages active simultaneously - pipeline parallelism!\n",
    "# Block 1 can be writing while Block 10 is being read\n",
    "# Memory stays constant regardless of total dataset size\n",
    "```\n",
    "\n",
    "**Why this matters**:\n",
    "- Process 100 TB with 64 GB cluster memory\n",
    "- Results available while pipeline still running\n",
    "- All CPUs active across all stages\n",
    "- No manual tuning of batch sizes or windows\n",
    "\n",
    "### Datasets and blocks\n",
    "\n",
    "Ray Data processes data in **blocks** - the basic units of distributed data processing:\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/dataset-arch.svg\" width=\"700\" alt=\"Ray Data Block Architecture\">\n",
    "\n",
    "**Key concepts:**\n",
    "- **Blocks**: Disjoint subsets of rows stored as PyArrow Tables or Pandas DataFrames\n",
    "- **Block size**: Defaults to 128 MB, configurable via `DataContext.target_max_block_size`\n",
    "- **Distributed storage**: Blocks stored in Ray Object Store for efficient sharing\n",
    "- **Streaming processing**: Blocks processed independently for scalability\n",
    "\n",
    "**Why blocks matter for ETL:**\n",
    "- **Memory efficiency**: Process 128 MB at a time, not full dataset\n",
    "- **Parallelism**: Each block processed independently across cluster\n",
    "- **Scalability**: Same code works for GB or TB datasets\n",
    "- **Performance**: Optimal block size balances overhead vs throughput\n",
    "\n",
    "### Ray memory model\n",
    "\n",
    "Ray manages memory in two key areas:\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/memory.svg\" width=\"600\" alt=\"Ray Memory Model\">\n",
    "\n",
    "**1. Object Store Memory (30% of node memory):**\n",
    "- Stores blocks as shared memory objects\n",
    "- Enables zero-copy data sharing between tasks\n",
    "- Automatic spilling to disk when full\n",
    "- Used for passing data between ETL stages\n",
    "\n",
    "**2. Task Execution Memory (remaining memory):**\n",
    "- Used by workers to execute ETL transformations\n",
    "- Allocated from worker heap\n",
    "- Released after task completion\n",
    "\n",
    "**Why this matters for ETL:**\n",
    "- **Resource planning**: Understand memory budgets for large joins\n",
    "- **Performance tuning**: Avoid object store pressure through proper `num_cpus`\n",
    "- **Cluster sizing**: Balance object store vs execution memory needs\n",
    "\n",
    "### Operators and resource management\n",
    "\n",
    "Ray Data uses **physical operators** to execute your ETL pipeline:\n",
    "\n",
    "**Common operators for ETL:**\n",
    "- **TaskPoolMapOperator**: Executes transformations using Ray tasks\n",
    "- **ActorPoolMapOperator**: Executes transformations using Ray actors (for stateful ops)\n",
    "- **AllToAllOperator**: Handles shuffles for joins and groupby operations\n",
    "\n",
    "**Operator fusion:**\n",
    "Ray Data automatically fuses consecutive map operations for efficiency:\n",
    "\n",
    "``` python\n",
    "# These two operations get fused into a single task\n",
    "ds.map_batches(transform1).map_batches(transform2)\n",
    "# Becomes: TaskPoolMapOperator[transform1->transform2]\n",
    "# Result: No data transfer between operations, faster execution\n",
    "```\n",
    "\n",
    "**Resource management and backpressure:**\n",
    "- **Dynamic allocation**: Resources distributed across operators automatically\n",
    "- **Backpressure**: Prevents memory overflow by throttling upstream operators\n",
    "- **Configurable limits**: Set via `DataContext.execution_options.resource_limits`\n",
    "\n",
    "**Why this matters for ETL:**\n",
    "- **Automatic optimization**: No manual tuning of micro-batches\n",
    "- **Memory safety**: Backpressure prevents OOM errors\n",
    "- **Resource efficiency**: Dynamic allocation maximizes cluster utilization\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "``` python\n",
    "# Cleanup Ray resources following best practices\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "print(\"Ray shutdown completed\")\n",
    "```\n",
    "\n",
    "## Ray Data ETL operations summary\n",
    "\n",
    "### Native operations demonstrated\n",
    "\n",
    "This template showcases the full range of Ray Data’s native ETL operations:\n",
    "\n",
    "| Operation Category | Functions Used | Purpose | Performance Benefit |\n",
    "|----------------------|------------------|-----------|----------------------|\n",
    "| **Data Loading** | `read_parquet()` | Extract data from sources | Parallel I/O with column pruning |\n",
    "| **Filtering** | `filter()` with expressions | Remove unwanted data | Push-down optimization |\n",
    "| **Transformations** | `map_batches()` | Apply business logic | Distributed processing |\n",
    "| **Joins** | `join()` | Combine datasets | Scalable distributed joins |\n",
    "| **Aggregations** | `groupby().aggregate()` | Calculate metrics | Parallel aggregation |\n",
    "| **Column Operations** | `select_columns()` | Schema optimization | Reduce memory usage |\n",
    "| **Output** | `write_parquet()` | Load to warehouse | Partitioned, compressed |\n",
    "\n",
    "### ETL pipeline optimization techniques applied\n",
    "\n",
    "| Technique | Implementation | Performance Impact | When to Use |\n",
    "|--------------|-------------------|-----------------------|-----------------|\n",
    "| **Column Pruning** | `columns=[\"col1\", \"col2\"]` in read operations | 60-95% I/O reduction | Always for large datasets |\n",
    "| **num_cpus Tuning** | Stage-specific values (0.025-2.0) | Balanced resource utilization | When CPU \\<80% |\n",
    "| **Batch Sizing** | `batch_size=500-2000` | Memory management | For memory-intensive ops |\n",
    "| **Early Filtering** | `filter()` immediately after read | Data volume reduction | When you can filter early |\n",
    "| **Expression API** | `col()` and `lit()` for filters | Query optimization | For complex filtering |\n",
    "| **Partitioned Output** | `partition_cols=[\"date\"]` | Query performance | For time-series data |\n",
    "| **Compression** | `compression=\"snappy\"` | Storage efficiency | All warehouse outputs |\n",
    "\n",
    "### Key takeaways\n",
    "\n",
    "**Ray Data ETL advantages**:\n",
    "- **Parallel processing**: Distribute operations across cluster automatically\n",
    "- **Memory efficiency**: Stream processing without full dataset materialization\n",
    "- **Native operations**: Optimized implementations for common ETL patterns\n",
    "- **Scalability**: Handle datasets from gigabytes to petabytes with same code\n",
    "- **Performance**: Proper resource allocation delivers optimal throughput\n",
    "\n",
    "**Best practices demonstrated**:\n",
    "- Use `read_parquet()` with column pruning for efficient extraction\n",
    "- Apply filters early using expressions API for data reduction\n",
    "- Specify `num_cpus` based on operation complexity\n",
    "- Use `map_batches()` with batch_format=“pandas” for transformations\n",
    "- Use native `join()` and `groupby()` operations\n",
    "- Write with partitioning and compression for warehouse optimization\n",
    "- Monitor progress with Ray Dashboard and progress bars\n",
    "\n",
    "**Production patterns**:\n",
    "- **TPC-H benchmark**: Industry-standard for testing and learning\n",
    "- **Column selection**: Only read/process what you need\n",
    "- **Resource allocation**: Stage-specific `num_cpus` values\n",
    "- **Memory management**: Batch sizes and block sizes for stability\n",
    "- **Data warehouse**: Partitioned, compressed Parquet output\n",
    "- **Validation**: Verify outputs and data quality\n",
    "\n",
    "This template covers complete ETL workflows from fundamental concepts to production optimization techniques using real benchmark data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
