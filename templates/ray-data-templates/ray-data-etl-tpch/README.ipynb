{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d3427c1",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# TPC-H ETL benchmark with Ray Data\n",
    "\n",
    "**Time to complete**: 35 min | **Difficulty**: Intermediate | **Prerequisites**: Python, data processing experience\n",
    "\n",
    "This template demonstrates how to build production-ready ETL pipelines using Ray Data for distributed data processing. You'll learn to process large-scale datasets efficiently while understanding the underlying architecture that makes Ray Data powerful for both traditional ETL and modern AI workloads.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Prerequisites and Setup](#prerequisites-and-setup) (3 min)\n",
    "2. [Quick Start: Your First ETL Pipeline](#quick-start-your-first-etl-pipeline) (5 min)\n",
    "3. [Understanding Ray Data and ETL](#understanding-ray-data-and-etl) (4 min)\n",
    "4. [Ray Data Architecture & Concepts](#ray-data-architecture--concepts) (5 min)\n",
    "5. [Extract: Reading Data from Multiple Sources](#extract-reading-data-from-multiple-sources) (6 min)\n",
    "6. [Transform: Processing and Cleaning Data](#transform-processing-and-cleaning-data) (8 min)\n",
    "7. [Load: Writing Data to Destinations](#load-writing-data-to-destinations) (4 min)\n",
    "8. [Advanced ETL Patterns and Optimizations](#advanced-etl-patterns-and-optimizations) (5 min)\n",
    "9. [Performance Monitoring and Best Practices](#performance-monitoring-and-best-practices) (3 min)\n",
    "10. [Troubleshooting and Production Considerations](#troubleshooting-and-production-considerations) (2 min)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this template, you will understand:\n",
    "\n",
    "**Why ETL matters for modern data teams:**\n",
    "- ETL is the foundation of data-driven decision making, enabling businesses to transform raw data into actionable insights\n",
    "- Modern ETL must handle both structured business data and unstructured AI content at scale\n",
    "- Traditional ETL tools struggle with Python-native workflows and multimodal data processing\n",
    "\n",
    "**Ray Data's ETL superpowers:**\n",
    "- **Unified Platform**: Process traditional ETL and AI workloads on the same infrastructure\n",
    "- **Python-Native Performance**: No JVM overhead - pure Python performance at scale\n",
    "- **Automatic Scaling**: Seamlessly scale from single machines to thousands of cores\n",
    "- **Streaming Architecture**: Handle datasets larger than cluster memory with ease\n",
    "- **Production Ready**: Built-in fault tolerance, monitoring, and enterprise features\n",
    "\n",
    "**What you'll be able to do:**\n",
    "- Build scalable ETL pipelines that process millions of records efficiently\n",
    "- Optimize performance using Ray Data's advanced features like operator fusion\n",
    "- Handle real-world data quality issues and implement robust error handling\n",
    "- Deploy production-ready ETL pipelines with proper monitoring and observability\n",
    "- Choose the right Ray Data operations for different ETL scenarios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530af088",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Prerequisites and Setup\n",
    "\n",
    "### Prerequisites Checklist\n",
    "\n",
    "Before starting this template, ensure you have:\n",
    "\n",
    "- [ ] **Python 3.8+** installed on your system\n",
    "- [ ] **Basic Python knowledge** including functions, classes, and data structures\n",
    "- [ ] **Data processing experience** with pandas, numpy, or similar libraries\n",
    "- [ ] **Understanding of ETL concepts** (Extract, Transform, Load)\n",
    "- [ ] **Access to a Ray cluster** or ability to run Ray locally\n",
    "- [ ] **8GB+ RAM** recommended for optimal performance\n",
    "- [ ] **Internet connection** for downloading dependencies and data\n",
    "\n",
    "### System Requirements\n",
    "\n",
    "**Minimum Requirements:**\n",
    "- CPU: 4 cores\n",
    "- RAM: 8GB\n",
    "- Storage: 2GB free space\n",
    "- Python: 3.8+\n",
    "\n",
    "**Recommended for Production:**\n",
    "- CPU: 16+ cores\n",
    "- RAM: 32GB+\n",
    "- Storage: 10GB+ free space\n",
    "- Python: 3.9+\n",
    "\n",
    "### Installation and Verification\n",
    "\n",
    "Let's verify your environment and install required dependencies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa22fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:38:22,820\tINFO worker.py:1771 -- Connecting to existing Ray cluster at address: 10.0.248.128:6379...\n",
      "2025-08-28 00:38:22,832\tINFO worker.py:1942 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-g3cs331q2c6ppx12j9ir5htn6l.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2025-08-28 00:38:22,835\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_08c553810358104c2d14814f4a773e46518ed2d6.zip' (0.11MiB) to Ray cluster...\n",
      "2025-08-28 00:38:22,836\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_08c553810358104c2d14814f4a773e46518ed2d6.zip'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray version: 2.49.0\n",
      "Ray cluster resources: {'anyscale/node-group:8CPU-32GB': 10.0, 'anyscale/provider:aws': 11.0, 'CPU': 88.0, 'object_store_memory': 105541541064.0, 'memory': 377957122048.0, 'anyscale/cpu_only:true': 11.0, 'node:10.0.246.59': 1.0, 'anyscale/region:us-west-2': 11.0, 'node:10.0.228.88': 1.0, 'node:10.0.195.252': 1.0, 'anyscale/node-group:head': 1.0, 'node:__internal_head__': 1.0, 'node:10.0.248.128': 1.0, 'node:10.0.222.37': 1.0, 'node:10.0.228.223': 1.0, 'node:10.0.234.204': 1.0, 'node:10.0.218.75': 1.0, 'node:10.0.198.227': 1.0, 'node:10.0.208.38': 1.0, 'node:10.0.238.242': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "from typing import Dict, Any, List, Tuple, Union\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Third-party imports\n",
    "import ray\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "from ray.data import DataContext, Dataset\n",
    "from ray.data.aggregate import Count, Mean, Sum, Min, Max\n",
    "\n",
    "# Configure logging for better debugging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def verify_environment() -> bool:\n",
    "    \"\"\"Verify that the environment meets requirements.\"\"\"\n",
    "    try:\n",
    "        # Check Python version\n",
    "        python_version = sys.version_info\n",
    "        if python_version.major < 3 or (python_version.major == 3 and python_version.minor < 8):\n",
    "            logger.error(f\"Python 3.8+ required, found {python_version.major}.{python_version.minor}\")\n",
    "            return False\n",
    "        \n",
    "        # Check available memory (approximate)\n",
    "        import psutil\n",
    "        memory_gb = psutil.virtual_memory().total / (1024**3)\n",
    "        if memory_gb < 4:\n",
    "            logger.warning(f\"Low memory detected: {memory_gb:.1f}GB. 8GB+ recommended.\")\n",
    "        \n",
    "        logger.info(f\"Environment verification passed - Python {python_version.major}.{python_version.minor}, {memory_gb:.1f}GB RAM\")\n",
    "        return True\n",
    "        \n",
    "    except ImportError as e:\n",
    "        logger.error(f\"Missing required package: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Environment verification failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Verify environment before proceeding\n",
    "if not verify_environment():\n",
    "    raise RuntimeError(\"Environment verification failed. Please check prerequisites.\")\n",
    "\n",
    "# Configure Ray Data for optimal performance\n",
    "DataContext.get_current().enable_progress_bars = False\n",
    "\n",
    "# Initialize Ray with error handling\n",
    "try:\n",
    "    if not ray.is_initialized():\n",
    "        ray.init(\n",
    "            ignore_reinit_error=True,\n",
    "            logging_level=logging.INFO,\n",
    "            include_dashboard=True\n",
    "        )\n",
    "    logger.info(\"Ray cluster initialized successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to initialize Ray: {e}\")\n",
    "    raise\n",
    "\n",
    "# Display cluster information\n",
    "print(\"=\"*60)\n",
    "print(\"RAY DATA ETL TEMPLATE - ENVIRONMENT STATUS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Ray version: {ray.__version__}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Cluster resources: {ray.cluster_resources()}\")\n",
    "print(f\"Dashboard URL: {ray.get_dashboard_url()}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb976466",
   "metadata": {},
   "source": [
    "## Quick Start: Your First ETL Pipeline\n",
    "\n",
    "Let's build a complete ETL pipeline in just 5 minutes to see Ray Data in action!\n",
    "\n",
    "### Step 1: Create Sample Data\n",
    "\n",
    "We'll start with a simple dataset to demonstrate core ETL concepts:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e225c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample customer data\n",
    "def generate_sample_customers(num_customers: int = 1000) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Generate realistic sample customer data for ETL demonstration.\"\"\"\n",
    "    np.random.seed(42)  # For reproducible results\n",
    "    \n",
    "    customers = []\n",
    "    for i in range(num_customers):\n",
    "        customer = {\n",
    "            'customer_id': f\"CUST_{i:06d}\",\n",
    "            'name': f\"Customer {i}\",\n",
    "            'email': f\"customer{i}@example.com\",\n",
    "            'age': np.random.randint(18, 80),\n",
    "            'income': np.random.normal(50000, 20000),\n",
    "            'city': np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix']),\n",
    "            'signup_date': datetime.now() - timedelta(days=np.random.randint(0, 365)),\n",
    "            'is_premium': np.random.choice([True, False], p=[0.3, 0.7])\n",
    "        }\n",
    "        customers.append(customer)\n",
    "    \n",
    "    return customers\n",
    "\n",
    "# Generate and create Ray dataset\n",
    "sample_customers = generate_sample_customers(1000)\n",
    "customers_ds = ray.data.from_items(sample_customers)\n",
    "\n",
    "print(f\"Created dataset with {customers_ds.count()} customers\")\n",
    "print(f\"Schema: {customers_ds.schema()}\")\n",
    "print(\"\\nSample records:\")\n",
    "customers_ds.take(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f04de34",
   "metadata": {},
   "source": [
    "### Step 2: Transform Data\n",
    "\n",
    "Now let's apply some business logic transformations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21667e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform customer data with business logic\n",
    "def transform_customer_batch(batch: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Apply business transformations to customer data.\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    df = pd.DataFrame(batch)\n",
    "    \n",
    "    # Calculate derived fields\n",
    "    df['income_tier'] = pd.cut(df['income'], \n",
    "                              bins=[0, 30000, 60000, 100000, float('inf')], \n",
    "                              labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "    \n",
    "    df['age_group'] = pd.cut(df['age'], \n",
    "                            bins=[0, 25, 35, 50, 65, 100], \n",
    "                            labels=['18-25', '26-35', '36-50', '51-65', '65+'])\n",
    "    \n",
    "    df['customer_value'] = df['income'] * (1.2 if df['is_premium'] else 1.0)\n",
    "    df['days_since_signup'] = (datetime.now() - pd.to_datetime(df['signup_date'])).dt.days\n",
    "    \n",
    "    return df.to_dict('records')\n",
    "\n",
    "# Apply transformations\n",
    "transformed_customers = customers_ds.map_batches(\n",
    "    transform_customer_batch,\n",
    "    batch_size=100,\n",
    "    concurrency=4\n",
    ")\n",
    "\n",
    "print(\"Transformation completed!\")\n",
    "print(f\"Transformed dataset schema: {transformed_customers.schema()}\")\n",
    "print(\"\\nSample transformed records:\")\n",
    "transformed_customers.take(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c0fb3c",
   "metadata": {},
   "source": [
    "### Step 3: Aggregate and Analyze\n",
    "\n",
    "Let's perform some business intelligence aggregations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23b3724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform business intelligence aggregations\n",
    "customer_analytics = (\n",
    "    transformed_customers\n",
    "    .groupby(\"city\")\n",
    "    .aggregate(\n",
    "        Count().alias(\"customer_count\"),\n",
    "        Mean(\"income\").alias(\"avg_income\"),\n",
    "        Mean(\"customer_value\").alias(\"avg_customer_value\"),\n",
    "        Sum(\"customer_value\").alias(\"total_value\")\n",
    "    )\n",
    "    .sort(\"total_value\", descending=True)\n",
    ")\n",
    "\n",
    "print(\"Customer Analytics by City:\")\n",
    "print(\"=\"*50)\n",
    "customer_analytics.show(5)\n",
    "\n",
    "# Calculate overall metrics\n",
    "total_customers = transformed_customers.count()\n",
    "premium_customers = transformed_customers.filter(lambda x: x[\"is_premium\"]).count()\n",
    "avg_income = transformed_customers.aggregate(Mean(\"income\"))\n",
    "\n",
    "print(f\"\\nOverall Metrics:\")\n",
    "print(f\"Total customers: {total_customers:,}\")\n",
    "print(f\"Premium customers: {premium_customers:,} ({premium_customers/total_customers*100:.1f}%)\")\n",
    "print(f\"Average income: ${avg_income:.2f}\")\n",
    "\n",
    "print(\"\\nQuick Start Complete! You've built your first Ray Data ETL pipeline.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416a0c3c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Understanding Ray Data and ETL\n",
    "\n",
    "### The Challenge: Modern Data Processing Complexity\n",
    "\n",
    "**The Problem:**\n",
    "Modern data teams face unprecedented challenges in building scalable ETL pipelines:\n",
    "\n",
    "- **Scale Requirements**: Processing terabytes to petabytes of data daily\n",
    "- **Technology Fragmentation**: Multiple tools for different data types and processing needs\n",
    "- **Python Performance**: Traditional distributed systems (Spark, Dask) have JVM overhead\n",
    "- **AI Integration**: Need to process both structured business data and unstructured AI content\n",
    "- **Operational Complexity**: Managing clusters, monitoring performance, handling failures\n",
    "- **Cost Management**: Balancing performance with infrastructure costs\n",
    "\n",
    "**Real-World Impact:**\n",
    "- **Amazon**: Migrated exabyte-scale workloads, saving $120M annually\n",
    "- **Netflix**: Processes 100x more data for recommendation systems\n",
    "- **Financial Services**: Handles fraud detection at massive scale with real-time requirements\n",
    "\n",
    "### The Solution: Ray Data's Unified Platform\n",
    "\n",
    "**Ray Data** is a distributed data processing library that addresses these challenges:\n",
    "\n",
    "**Core Capabilities:**\n",
    "- **Unified Processing**: Handle traditional ETL and AI workloads on the same platform\n",
    "- **Python-Native Performance**: No JVM overhead - pure Python performance at scale\n",
    "- **Automatic Scaling**: Seamlessly scale from single machines to thousands of cores\n",
    "- **Streaming Architecture**: Process datasets larger than cluster memory\n",
    "- **Production Ready**: Built-in fault tolerance, monitoring, and enterprise features\n",
    "\n",
    "**Technical Advantages:**\n",
    "- **Lazy Evaluation**: Optimizes entire pipelines before execution\n",
    "- **Operator Fusion**: Combines compatible operations for efficiency\n",
    "- **Memory Management**: Intelligent caching and streaming for large datasets\n",
    "- **Fault Tolerance**: Automatic recovery from node failures\n",
    "- **Resource Optimization**: Dynamic resource allocation based on workload\n",
    "\n",
    "### The Impact: Transformative Results\n",
    "\n",
    "**Performance Improvements:**\n",
    "- **5.1x faster** processing with RayTurbo optimizations\n",
    "- **82% cost reduction** compared to traditional solutions\n",
    "- **90%+ GPU utilization** for AI workloads\n",
    "- **Sub-second latency** for real-time processing\n",
    "\n",
    "**Business Value:**\n",
    "- **Faster Time-to-Insight**: Reduce data processing time from hours to minutes\n",
    "- **Cost Optimization**: Lower infrastructure costs through better resource utilization\n",
    "- **Operational Excellence**: Simplified deployment and monitoring\n",
    "- **Future-Proof Architecture**: Seamlessly evolve from ETL to AI workloads\n",
    "\n",
    "**Traditional & Current Workloads:**\n",
    "- **Business ETL**: Customer analytics, financial reporting, operational dashboards\n",
    "- **Classical ML**: Recommendation systems, fraud detection, predictive analytics\n",
    "- **Data Engineering**: Large-scale data cleaning, transformation, and aggregation\n",
    "\n",
    "**Next-Generation Workloads:**\n",
    "- **Multimodal AI**: Processing text, images, video, and audio together\n",
    "- **LLM Pipelines**: Fine-tuning, embedding generation, and batch inference\n",
    "- **Computer Vision**: Image preprocessing and model inference at scale\n",
    "- **Compound AI Systems**: Orchestrating multiple models and traditional ML\n",
    "\n",
    "### Ray Data vs Traditional Tools\n",
    "\n",
    "Let's understand how Ray Data compares to other data processing tools across traditional and modern workloads:\n",
    "\n",
    "| Feature | Ray Data | Pandas | Spark | Dask |\n",
    "|---------|----------|--------|-------|------|\n",
    "| **Scale** | Multi-machine | Single-machine | Multi-machine | Multi-machine |\n",
    "| **Memory Strategy** | Streaming | In-memory | Mixed | In-Memory |\n",
    "| **Python Performance** | Native (no JVM) | Native | JVM overhead | Native |\n",
    "| **CPU Clusters** | Excellent | Single-node | Excellent | Good |\n",
    "| **GPU Support** | Native | None | Limited | Limited |\n",
    "| **Classical ML** | Excellent | Limited | Limited | Good |\n",
    "| **Multimodal Data** | Optimized | Limited | Limited | Limited |\n",
    "| **Fault Tolerance** | Built-in | None | Built-in | Limited |\n",
    "\n",
    "### Real-World Impact Across All Workloads\n",
    "\n",
    "Organizations worldwide are seeing dramatic results with Ray Data for both traditional and advanced workloads:\n",
    "\n",
    "**Traditional ETL & Analytics:**\n",
    "- **Amazon**: Migrated an exabyte-scale workload from Spark to Ray Data, cutting costs by **82%** and saving **$120 million annually**\n",
    "- **Instacart**: Processing **100x more data** for recommendation systems and business analytics\n",
    "- **Financial Services**: Major banks using Ray Data for fraud detection and risk analytics at scale\n",
    "\n",
    "**Modern AI & ML:**\n",
    "- **Niantic**: Reduced code complexity by **85%** while scaling AR/VR data pipelines\n",
    "- **Canva**: Cut cloud costs in **half** while processing design assets and user data\n",
    "- **Pinterest**: Boosted GPU utilization to **90%+** for image processing and recommendations\n",
    "\n",
    "Ray Data provides a unified platform that excels at traditional ETL, classical ML, and next-generation AI workloads - eliminating the need for multiple specialized systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7280fcaf",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Ray Data Architecture & Concepts\n",
    "\n",
    "### Step 1: Understanding Ray Data's Architecture\n",
    "\n",
    "Ray Data's architecture is designed for the modern era of computing, addressing the unique challenges of AI and data processing:\n",
    "\n",
    "**Key Architectural Principles:**\n",
    "- **Python-Native**: No JVM overhead or serialization bottlenecks\n",
    "- **Heterogeneous Compute**: Seamlessly orchestrates CPUs, GPUs, and other accelerators\n",
    "- **Dynamic Workloads**: Adapts to varying compute needs in real-time\n",
    "- **Fault Tolerance**: Handles failures gracefully at massive scale\n",
    "\n",
    "### Step 2: Core Concepts\n",
    "\n",
    "**Datasets and Blocks:**\n",
    "A **Dataset** in Ray Data is a distributed collection of data divided into **blocks** - chunks that can be processed independently.\n",
    "\n",
    "**Block Characteristics:**\n",
    "- Each block contains 1-128 MB of data\n",
    "- Stored in Ray's distributed object store\n",
    "- Operations applied in parallel across the cluster\n",
    "- Block size affects performance (too small = overhead, too large = memory issues)\n",
    "\n",
    "**Execution Model:**\n",
    "Ray Data uses **lazy execution** by default - operations build a plan first, then execute for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5706ab60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:38:23,206\tINFO dataset.py:3248 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n",
      "2025-08-28 00:38:23,208\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_327_0\n",
      "2025-08-28 00:38:23,219\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_327_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:38:23,220\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_327_0: InputDataBuffer[Input] -> LimitOperator[limit=3]\n",
      "2025-08-28 00:38:23,222\tWARNING resource_manager.py:134 -- \u26a0\ufe0f  Ray's object store is configured to use only 27.9% of available memory (98.3GiB out of 352.0GiB total). For optimal Ray Data performance, we recommend setting the object store to at least 50% of available memory. You can do this by setting the 'object_store_memory' parameter when calling ray.init() or by setting the RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION environment variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: MaterializedDataset(\n",
      "   num_blocks=200,\n",
      "   num_rows=1000,\n",
      "   schema={id: int64, name: string, value: double}\n",
      ")\n",
      "Number of blocks: 200\n",
      "Schema: Column  Type\n",
      "------  ----\n",
      "id      int64\n",
      "name    string\n",
      "value   double\n",
      "\n",
      "First 3 rows:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:38:24,741\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_327_0 execution finished in 1.52 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 0: {'id': 0, 'name': 'name_0', 'value': 0.10075406674150811}\n",
      "Row 1: {'id': 1, 'name': 'name_1', 'value': 0.05100088950394521}\n",
      "Row 2: {'id': 2, 'name': 'name_2', 'value': 0.467035054122913}\n"
     ]
    }
   ],
   "source": [
    "# Let's create a simple dataset to understand blocks\n",
    "# Create sample data\n",
    "import numpy as np\n",
    "\n",
    "data = [{\"id\": i, \"name\": f\"name_{i}\", \"value\": float(np.random.rand())} for i in range(1000)]\n",
    "ds = ray.data.from_items(data)\n",
    "\n",
    "print(f\"Dataset: {ds}\")\n",
    "print(f\"Number of blocks: {ds.num_blocks()}\")\n",
    "print(f\"Schema: {ds.schema()}\")\n",
    "\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "for i, row in enumerate(ds.take(3)):\n",
    "    print(f\"Row {i}: {row}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366da2ff",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### 2. Execution Model\n",
    "\n",
    "Ray Data uses **lazy execution** by default, meaning operations are not executed immediately but are planned and optimized before execution.\n",
    "\n",
    "**Lazy Execution Benefits:**\n",
    "- **Optimization**: Ray Data can optimize the entire pipeline before execution\n",
    "- **Memory efficiency**: Only necessary data is loaded into memory\n",
    "- **Fault tolerance**: Can restart from intermediate points if failures occur\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b> Understanding Execution:</b><br>\n",
    "<b>Lazy:</b> Build a plan first, then execute (default)<br>\n",
    "<b>Eager:</b> Execute operations immediately as they're called<br><br>\n",
    "Lazy execution allows Ray Data to optimize your entire pipeline for better performance!\n",
    "</div>\n",
    "\n",
    "Ray Data also utilizes **streaming execution**, which helps combine the best of structured streaming and batch inference.  Streaming execution doesn\u2019t wait for one operator to complete to start the next. Each operator takes in and outputs a stream of blocks. This approach allows you to process datasets that are too large to fit in your cluster\u2019s memory and can be \n",
    "\n",
    "![Streaming Execution](https://images.ctfassets.net/xjan103pcp94/6EpeVQ743xNDZZfZyEfKFr/16069060673a8667b08be34aa828cc60/image8.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c62bdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:38:24,844\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_329_0\n",
      "2025-08-28 00:38:24,848\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_329_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:38:24,848\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_329_0: InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(transform)] -> LimitOperator[limit=5]\n",
      "2025-08-28 00:38:25,332\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_329_0 execution finished in 0.48 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def transform(batch: pd.DataFrame) -> pd.DataFrame:\n",
    "    batch = batch.copy()\n",
    "    batch[\"value\"] = batch[\"value\"] * 2\n",
    "    batch = batch[batch[\"value\"] > 1.0]\n",
    "    batch[\"category\"] = np.where(batch[\"value\"] > 1.5, \"high\", \"medium\")\n",
    "    return batch\n",
    "\n",
    "ds_lazy = ds.map_batches(transform, batch_format=\"pandas\")  # still lazy\n",
    "\n",
    "# Calling take() to actually execute \n",
    "result = ds_lazy.take(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54595129",
   "metadata": {},
   "source": [
    "#### 3. Lazy vs Eager Operations\n",
    "\n",
    "Understanding which operations are lazy vs eager is crucial for performance optimization:\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> Lazy Operations (Default):</b><br>\n",
    "These operations build a computation plan but don't execute immediately. They allow Ray Data to optimize the entire pipeline before execution.\n",
    "</div>\n",
    "\n",
    "**Lazy Operations (Build Plan, Execute Later):**\n",
    "- **Data Reading:** `read_parquet()`, `read_csv()`, `read_json()`, `from_pandas()`\n",
    "- **Transformations:** `map_batches()`, `map()`, `flat_map()`\n",
    "- **Filtering:** `filter()`, `select_columns()`, `drop_columns()`\n",
    "- **Joins:** `join()`, `union()`\n",
    "- **Aggregations:** `groupby().aggregate()`, `groupby().sum()`, `groupby().mean()`\n",
    "- **Sorting:** `sort()`, `sort_by()`\n",
    "- **Sampling:** `sample()`, `random_sample()`\n",
    "- **Repartitioning:** `repartition()`, `coalesce()`\n",
    "- **Window Operations:** `rolling()`, `window()`\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b> Eager Operations (Execute Immediately):</b><br>\n",
    "These operations trigger immediate execution and materialize results. Use them strategically to control when computation happens.\n",
    "</div>\n",
    "\n",
    "**Eager Operations (Execute Immediately):**\n",
    "- **Materialization:** `materialize()` - Force execution and cache results\n",
    "- **Data Access:** `take()`, `take_batch()`, `show()`, `to_pandas()`, `to_arrow()`\n",
    "- **Counts:** `count()`, `num_rows()`, `num_blocks()`\n",
    "- **Schema:** `schema()` - Returns schema without materializing data\n",
    "- **Writing:** `write_parquet()`, `write_csv()`, `write_json()`\n",
    "- **Iteration:** `iter_batches()`, `iter_rows()`\n",
    "- **Statistics:** `stats()`, `mean()`, `std()`, `min()`, `max()`\n",
    "\n",
    "**Best Practices:**\n",
    "- **Use lazy operations** for building complex pipelines\n",
    "- **Use eager operations** only when you need immediate results\n",
    "- **Batch eager operations** to minimize execution overhead\n",
    "- **Use `materialize()`** to cache expensive intermediate results\n",
    "\n",
    "**Example: Lazy vs Eager Usage**\n",
    "```python\n",
    "# Lazy pipeline (no execution yet)\n",
    "lazy_pipeline = (\n",
    "    ds\n",
    "    .filter(lambda x: x[\"value\"] > 0.5)  # Lazy\n",
    "    .map_batches(transform)              # Lazy\n",
    "    .groupby(\"category\")                 # Lazy\n",
    "    .aggregate(Count(), Mean(\"value\"))   # Lazy\n",
    ")\n",
    "\n",
    "# Eager execution (triggers computation)\n",
    "result = lazy_pipeline.take(10)          # Eager - executes entire pipeline\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f12cce6",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Part 3: Extract - Reading Data\n",
    "\n",
    "The **Extract** phase involves reading data from various sources. Ray Data provides built-in connectors for many common data sources and makes it easy to scale data reading across a distributed cluster, especially for the **multimodal data** that powers modern AI applications.\n",
    "\n",
    "### The Multimodal Data Revolution\n",
    "\n",
    "Today's AI applications process vastly more complex data than traditional ETL pipelines:\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> The Scale of Modern Data:</b>\n",
    "<ul>\n",
    "    <li><b>Unstructured Data Growth:</b> Now outpaces structured data by 10x+ in most organizations</li>\n",
    "    <li><b>Video Processing:</b> Companies like OpenAI (Sora), Pinterest, and Apple process petabytes of multimodal data daily</li>\n",
    "    <li><b>Foundation Models:</b> Require processing millions of images, videos, and documents</li>\n",
    "    <li><b>AI-Powered Processing:</b> Every aspect of data processing is becoming AI-enhanced</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "### How Ray Data Reads Data Under the Hood\n",
    "\n",
    "When you read data with Ray Data, here's what happens:\n",
    "\n",
    "1. **File Discovery**: Ray Data discovers all files matching your path pattern\n",
    "2. **Task Creation**: Files are distributed across Ray tasks (typically one file per task)\n",
    "3. **Parallel Reading**: Multiple tasks read files simultaneously across the cluster\n",
    "4. **Block Creation**: Each task creates data blocks stored in Ray's object store\n",
    "5. **Lazy Planning**: The dataset is created but data isn't loaded until needed\n",
    "\n",
    "This architecture enables Ray Data to efficiently handle both traditional structured data and modern unstructured formats that power AI applications.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Built-in Data Sources:</b>\n",
    "<ul>\n",
    "    <li><b>Structured:</b> Parquet, CSV, JSON, Arrow</li>\n",
    "    <li><b>Unstructured:</b> Images, Videos, Audio, Binary files</li>\n",
    "    <li><b>Databases:</b> MongoDB, MySQL, PostgreSQL, Snowflake</li>\n",
    "    <li><b>Cloud Storage:</b> S3, GCS, Azure Blob Storage</li>\n",
    "    <li><b>Data Lakes:</b> Delta Lake, Iceberg (via RayTurbo)</li>\n",
    "    <li><b>ML Formats:</b> TensorFlow Records, PyTorch datasets</li>\n",
    "    <li><b>Memory:</b> Python lists, NumPy arrays, Pandas DataFrames</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "### Enterprise-Grade Data Connectivity\n",
    "\n",
    "For enterprise environments, **Anyscale** provides additional connectors and optimizations:\n",
    "- **Enhanced Security**: Integration with enterprise identity systems\n",
    "- **Governance Controls**: Data lineage and access controls\n",
    "- **Performance Optimization**: RayTurbo's streaming metadata fetching provides up to **4.5x faster** data loading\n",
    "- **Hybrid Deployment**: Support for Kubernetes, on-premises, and multi-cloud environments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aa5e8c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Accessing TPC-H Benchmark Data for Our ETL Examples\n",
    "\n",
    "We'll use the industry-standard TPC-H benchmark dataset. This provides realistic enterprise-scale data that's used by companies worldwide to evaluate data processing systems and represents real business scenarios with complex relationships between customers, orders, suppliers, and products.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d452fc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TPC-H Schema (8 Tables):\n",
      "\tCUSTOMER:\tCustomer master data with demographics and market segments\n",
      "\tORDERS:\t\tOrder header information with dates, priorities, and status\n",
      "\tLINEITEM:\tDetailed line items for each order (largest table ~6B rows)\n",
      "\tPART:\t\tParts catalog with specifications and retail prices\n",
      "\tSUPPLIER:\tSupplier information including contact details and geography\n",
      "\tPARTSUPP:\tPart-supplier relationships with costs and availability\n",
      "\tNATION:\t\tNation reference data with geographic regions\n",
      "\tREGION:\t\tRegional groupings for geographic analysis\n"
     ]
    }
   ],
   "source": [
    "# Using TPC-H Benchmark Dataset - Industry Standard for Data Processing\n",
    "# TPC-H is the gold standard benchmark for decision support systems and analytics\n",
    "\n",
    "# TPC-H S3 data location\n",
    "TPCH_S3_PATH = \"s3://ray-benchmark-data/tpch/parquet/sf10\"\n",
    "\n",
    "# TPC-H Schema Overview\n",
    "tpch_tables = {\n",
    "    \"customer\": \"Customer master data with demographics and market segments\",\n",
    "    \"orders\": \"\\tOrder header information with dates, priorities, and status\",\n",
    "    \"lineitem\": \"Detailed line items for each order (largest table ~6B rows)\",\n",
    "    \"part\": \"\\tParts catalog with specifications and retail prices\", \n",
    "    \"supplier\": \"Supplier information including contact details and geography\",\n",
    "    \"partsupp\": \"Part-supplier relationships with costs and availability\",\n",
    "    \"nation\": \"\\tNation reference data with geographic regions\",\n",
    "    \"region\": \"\\tRegional groupings for geographic analysis\"\n",
    "}\n",
    "\n",
    "print(f\"\\n TPC-H Schema (8 Tables):\")\n",
    "for table, description in tpch_tables.items():\n",
    "    print(f\"\\t{table.upper()}:\\t{description}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb12d31",
   "metadata": {},
   "source": [
    "### Step 3: Real-World Data Sources\n",
    "\n",
    "Let's work with actual public datasets that demonstrate enterprise ETL patterns:\n",
    "\n",
    "**Public Data Sources We'll Use:**\n",
    "- **TPC-H Benchmark**: Industry-standard benchmark for data warehousing\n",
    "- **Yahoo Finance**: Real financial data for time series analysis\n",
    "- **Open Datasets**: Various public datasets for different use cases\n",
    "\n",
    "### Step 4: Data Quality Validation\n",
    "\n",
    "Before processing any data, it's crucial to validate data quality:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4af1eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Validation Functions\n",
    "def validate_data_quality(dataset: Dataset, dataset_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"Comprehensive data quality validation for any dataset.\"\"\"\n",
    "    logger.info(f\"Validating data quality for {dataset_name}...\")\n",
    "    \n",
    "    validation_results = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'total_records': 0,\n",
    "        'null_counts': {},\n",
    "        'data_types': {},\n",
    "        'duplicate_count': 0,\n",
    "        'quality_score': 0.0,\n",
    "        'issues': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Get basic statistics\n",
    "        validation_results['total_records'] = dataset.count()\n",
    "        validation_results['data_types'] = dataset.schema()\n",
    "        \n",
    "        # Sample data for detailed analysis\n",
    "        sample_data = dataset.take(1000)\n",
    "        if not sample_data:\n",
    "            validation_results['issues'].append(\"No data available for validation\")\n",
    "            return validation_results\n",
    "        \n",
    "        # Convert to pandas for analysis\n",
    "        import pandas as pd\n",
    "        df = pd.DataFrame(sample_data)\n",
    "        \n",
    "        # Check for null values\n",
    "        null_counts = df.isnull().sum()\n",
    "        validation_results['null_counts'] = null_counts.to_dict()\n",
    "        \n",
    "        # Check for duplicates\n",
    "        validation_results['duplicate_count'] = df.duplicated().sum()\n",
    "        \n",
    "        # Calculate quality score (0-100)\n",
    "        total_cells = len(df) * len(df.columns)\n",
    "        null_cells = null_counts.sum()\n",
    "        duplicate_rows = validation_results['duplicate_count']\n",
    "        \n",
    "        quality_score = max(0, 100 - (null_cells / total_cells * 50) - (duplicate_rows / len(df) * 50))\n",
    "        validation_results['quality_score'] = quality_score\n",
    "        \n",
    "        # Identify specific issues\n",
    "        if null_cells > 0:\n",
    "            validation_results['issues'].append(f\"Found {null_cells} null values\")\n",
    "        if duplicate_rows > 0:\n",
    "            validation_results['issues'].append(f\"Found {duplicate_rows} duplicate rows\")\n",
    "        if quality_score < 80:\n",
    "            validation_results['issues'].append(\"Data quality score below 80%\")\n",
    "        \n",
    "        logger.info(f\"Data quality validation completed for {dataset_name}\")\n",
    "        logger.info(f\"Quality score: {quality_score:.1f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Data quality validation failed for {dataset_name}: {e}\")\n",
    "        validation_results['issues'].append(f\"Validation error: {str(e)}\")\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "def display_validation_results(results: Dict[str, Any]) -> None:\n",
    "    \"\"\"Display data quality validation results in a readable format.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DATA QUALITY REPORT: {results['dataset_name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total Records: {results['total_records']:,}\")\n",
    "    print(f\"Quality Score: {results['quality_score']:.1f}%\")\n",
    "    print(f\"Duplicate Rows: {results['duplicate_count']:,}\")\n",
    "    \n",
    "    if results['null_counts']:\n",
    "        print(f\"\\nNull Value Counts:\")\n",
    "        for column, null_count in results['null_counts'].items():\n",
    "            if null_count > 0:\n",
    "                print(f\"  {column}: {null_count:,}\")\n",
    "    \n",
    "    if results['issues']:\n",
    "        print(f\"\\nIssues Found:\")\n",
    "        for issue in results['issues']:\n",
    "            print(f\"  WARNING: {issue}\")\n",
    "    else:\n",
    "        print(f\"\\nNo data quality issues detected\")\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "# Validate our sample customer data\n",
    "customer_validation = validate_data_quality(customers_ds, \"Sample Customer Data\")\n",
    "display_validation_results(customer_validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec25ee5b",
   "metadata": {},
   "source": [
    "### Step 5: Real Financial Data Processing\n",
    "\n",
    "Let's work with actual financial data to demonstrate time series ETL patterns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49190e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real Financial Data Processing\n",
    "def generate_financial_data(symbols: List[str], days: int = 365) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Generate realistic financial data for ETL demonstration.\"\"\"\n",
    "    import yfinance as yf\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    financial_data = []\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=days)\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        try:\n",
    "            # Download real stock data\n",
    "            ticker = yf.Ticker(symbol)\n",
    "            hist = ticker.history(start=start_date, end=end_date)\n",
    "            \n",
    "            for date, row in hist.iterrows():\n",
    "                financial_data.append({\n",
    "                    'symbol': symbol,\n",
    "                    'date': date.strftime('%Y-%m-%d'),\n",
    "                    'open': float(row['Open']),\n",
    "                    'high': float(row['High']),\n",
    "                    'low': float(row['Low']),\n",
    "                    'close': float(row['Close']),\n",
    "                    'volume': int(row['Volume']),\n",
    "                    'adj_close': float(row['Close']),  # Simplified for demo\n",
    "                    'daily_return': 0.0,  # Will calculate below\n",
    "                    'volatility': 0.0,    # Will calculate below\n",
    "                    'market_cap': 0.0     # Will calculate below\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not fetch data for {symbol}: {e}\")\n",
    "            # Generate synthetic data as fallback\n",
    "            base_price = 100.0\n",
    "            for i in range(days):\n",
    "                date = (start_date + timedelta(days=i)).strftime('%Y-%m-%d')\n",
    "                price_change = np.random.normal(0, 0.02)  # 2% daily volatility\n",
    "                base_price *= (1 + price_change)\n",
    "                \n",
    "                financial_data.append({\n",
    "                    'symbol': symbol,\n",
    "                    'date': date,\n",
    "                    'open': base_price * 0.99,\n",
    "                    'high': base_price * 1.02,\n",
    "                    'low': base_price * 0.98,\n",
    "                    'close': base_price,\n",
    "                    'volume': np.random.randint(1000000, 10000000),\n",
    "                    'adj_close': base_price,\n",
    "                    'daily_return': price_change,\n",
    "                    'volatility': 0.0,\n",
    "                    'market_cap': 0.0\n",
    "                })\n",
    "    \n",
    "    return financial_data\n",
    "\n",
    "# Generate financial data for major stocks\n",
    "stock_symbols = ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'TSLA']\n",
    "logger.info(f\"Generating financial data for {len(stock_symbols)} stocks...\")\n",
    "\n",
    "financial_data = generate_financial_data(stock_symbols, days=90)  # 3 months of data\n",
    "financial_ds = ray.data.from_items(financial_data)\n",
    "\n",
    "print(f\"Generated financial dataset with {financial_ds.count():,} records\")\n",
    "print(f\"Schema: {financial_ds.schema()}\")\n",
    "\n",
    "# Validate financial data quality\n",
    "financial_validation = validate_data_quality(financial_ds, \"Financial Market Data\")\n",
    "display_validation_results(financial_validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42808c96",
   "metadata": {},
   "source": [
    "### Step 6: Schema Documentation and Analysis\n",
    "\n",
    "Understanding your data schema is crucial for effective ETL processing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dae5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Schema Analysis\n",
    "def analyze_dataset_schema(dataset: Dataset, dataset_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"Perform comprehensive schema analysis for a dataset.\"\"\"\n",
    "    logger.info(f\"Analyzing schema for {dataset_name}...\")\n",
    "    \n",
    "    schema_info = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'total_columns': 0,\n",
    "        'column_details': {},\n",
    "        'data_types': {},\n",
    "        'memory_usage': 0,\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Get schema information\n",
    "        schema = dataset.schema()\n",
    "        schema_info['data_types'] = schema\n",
    "        schema_info['total_columns'] = len(schema)\n",
    "        \n",
    "        # Sample data for detailed analysis\n",
    "        sample_data = dataset.take(1000)\n",
    "        if sample_data:\n",
    "            import pandas as pd\n",
    "            df = pd.DataFrame(sample_data)\n",
    "            \n",
    "            # Analyze each column\n",
    "            for column in df.columns:\n",
    "                col_info = {\n",
    "                    'dtype': str(df[column].dtype),\n",
    "                    'null_count': int(df[column].isnull().sum()),\n",
    "                    'unique_count': int(df[column].nunique()),\n",
    "                    'memory_usage': int(df[column].memory_usage(deep=True)),\n",
    "                    'sample_values': df[column].dropna().head(3).tolist()\n",
    "                }\n",
    "                schema_info['column_details'][column] = col_info\n",
    "            \n",
    "            # Calculate total memory usage\n",
    "            schema_info['memory_usage'] = df.memory_usage(deep=True).sum()\n",
    "            \n",
    "            # Generate recommendations\n",
    "            for column, details in schema_info['column_details'].items():\n",
    "                if details['null_count'] > len(df) * 0.1:  # More than 10% nulls\n",
    "                    schema_info['recommendations'].append(f\"Column '{column}' has {details['null_count']} null values - consider data cleaning\")\n",
    "                \n",
    "                if details['dtype'] == 'object' and details['unique_count'] < len(df) * 0.1:\n",
    "                    schema_info['recommendations'].append(f\"Column '{column}' has low cardinality - consider categorical encoding\")\n",
    "                \n",
    "                if 'date' in column.lower() and details['dtype'] == 'object':\n",
    "                    schema_info['recommendations'].append(f\"Column '{column}' appears to be a date but stored as string - consider datetime conversion\")\n",
    "        \n",
    "        logger.info(f\"Schema analysis completed for {dataset_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Schema analysis failed for {dataset_name}: {e}\")\n",
    "        schema_info['recommendations'].append(f\"Analysis error: {str(e)}\")\n",
    "    \n",
    "    return schema_info\n",
    "\n",
    "def display_schema_analysis(schema_info: Dict[str, Any]) -> None:\n",
    "    \"\"\"Display comprehensive schema analysis results.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"SCHEMA ANALYSIS: {schema_info['dataset_name']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total Columns: {schema_info['total_columns']}\")\n",
    "    print(f\"Memory Usage: {schema_info['memory_usage'] / 1024:.1f} KB (sample)\")\n",
    "    \n",
    "    print(f\"\\nColumn Details:\")\n",
    "    print(f\"{'Column Name':<20} {'Data Type':<15} {'Nulls':<8} {'Unique':<8} {'Sample Values'}\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    \n",
    "    for column, details in schema_info['column_details'].items():\n",
    "        sample_str = str(details['sample_values'][:2]) if details['sample_values'] else 'N/A'\n",
    "        print(f\"{column:<20} {details['dtype']:<15} {details['null_count']:<8} {details['unique_count']:<8} {sample_str}\")\n",
    "    \n",
    "    if schema_info['recommendations']:\n",
    "        print(f\"\\nRecommendations:\")\n",
    "        for i, rec in enumerate(schema_info['recommendations'], 1):\n",
    "            print(f\"  {i}. {rec}\")\n",
    "    \n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "# Analyze schemas for both datasets\n",
    "customer_schema = analyze_dataset_schema(customers_ds, \"Customer Data\")\n",
    "display_schema_analysis(customer_schema)\n",
    "\n",
    "financial_schema = analyze_dataset_schema(financial_ds, \"Financial Data\")\n",
    "display_schema_analysis(financial_schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5d6919",
   "metadata": {},
   "source": [
    "### Step 7: Advanced Performance Optimization\n",
    "\n",
    "Let's demonstrate specific optimization techniques for Ray Data ETL pipelines:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd8a288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Performance Optimization Techniques\n",
    "def benchmark_operation(dataset: Dataset, operation_name: str, operation_func, *args, **kwargs) -> Dict[str, Any]:\n",
    "    \"\"\"Benchmark a Ray Data operation and return performance metrics.\"\"\"\n",
    "    import time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    start_memory = ray.cluster_resources().get('memory', 0)\n",
    "    \n",
    "    try:\n",
    "        result = operation_func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        end_memory = ray.cluster_resources().get('memory', 0)\n",
    "        \n",
    "        # Get result count if possible\n",
    "        try:\n",
    "            record_count = result.count() if hasattr(result, 'count') else len(result) if hasattr(result, '__len__') else 0\n",
    "        except:\n",
    "            record_count = 0\n",
    "        \n",
    "        execution_time = end_time - start_time\n",
    "        throughput = record_count / execution_time if execution_time > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'operation': operation_name,\n",
    "            'execution_time': execution_time,\n",
    "            'record_count': record_count,\n",
    "            'throughput': throughput,\n",
    "            'memory_used': end_memory - start_memory,\n",
    "            'success': True\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        end_time = time.time()\n",
    "        return {\n",
    "            'operation': operation_name,\n",
    "            'execution_time': end_time - start_time,\n",
    "            'record_count': 0,\n",
    "            'throughput': 0,\n",
    "            'memory_used': 0,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def optimize_batch_processing(dataset: Dataset) -> Dict[str, Any]:\n",
    "    \"\"\"Demonstrate batch size optimization for different operations.\"\"\"\n",
    "    logger.info(\"Testing batch size optimization...\")\n",
    "    \n",
    "    batch_sizes = [100, 500, 1000, 2000, 5000]\n",
    "    results = {}\n",
    "    \n",
    "    def test_transform(batch):\n",
    "        import pandas as pd\n",
    "        df = pd.DataFrame(batch)\n",
    "        df['processed'] = df['income'] * 1.1  # Simple transformation\n",
    "        return df.to_dict('records')\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        test_ds = dataset.map_batches(test_transform, batch_size=batch_size)\n",
    "        result = benchmark_operation(dataset, f\"map_batches_batch_{batch_size}\", lambda: test_ds.take(100))\n",
    "        results[f\"batch_{batch_size}\"] = result\n",
    "    \n",
    "    return results\n",
    "\n",
    "def optimize_concurrency(dataset: Dataset) -> Dict[str, Any]:\n",
    "    \"\"\"Demonstrate concurrency optimization for different operations.\"\"\"\n",
    "    logger.info(\"Testing concurrency optimization...\")\n",
    "    \n",
    "    concurrency_levels = [1, 2, 4, 8, 16]\n",
    "    results = {}\n",
    "    \n",
    "    def test_transform(batch):\n",
    "        import pandas as pd\n",
    "        df = pd.DataFrame(batch)\n",
    "        df['processed'] = df['income'] * 1.1\n",
    "        return df.to_dict('records')\n",
    "    \n",
    "    for concurrency in concurrency_levels:\n",
    "        test_ds = dataset.map_batches(test_transform, concurrency=concurrency)\n",
    "        result = benchmark_operation(dataset, f\"map_batches_concurrency_{concurrency}\", lambda: test_ds.take(100))\n",
    "        results[f\"concurrency_{concurrency}\"] = result\n",
    "    \n",
    "    return results\n",
    "\n",
    "def demonstrate_operator_fusion(dataset: Dataset) -> Dict[str, Any]:\n",
    "    \"\"\"Demonstrate the benefits of operator fusion.\"\"\"\n",
    "    logger.info(\"Demonstrating operator fusion...\")\n",
    "    \n",
    "    # Non-fused operations (separate steps)\n",
    "    def non_fused_pipeline():\n",
    "        step1 = dataset.filter(lambda x: x['income'] > 30000)\n",
    "        step2 = step1.map_batches(lambda batch: batch)  # Identity transform\n",
    "        step3 = step2.select_columns(['customer_id', 'income', 'city'])\n",
    "        return step3.take(100)\n",
    "    \n",
    "    # Fused operations (single pipeline)\n",
    "    def fused_pipeline():\n",
    "        return (dataset\n",
    "                .filter(lambda x: x['income'] > 30000)\n",
    "                .map_batches(lambda batch: batch)\n",
    "                .select_columns(['customer_id', 'income', 'city'])\n",
    "                .take(100))\n",
    "    \n",
    "    non_fused_result = benchmark_operation(dataset, \"Non-Fused Pipeline\", non_fused_pipeline)\n",
    "    fused_result = benchmark_operation(dataset, \"Fused Pipeline\", fused_pipeline)\n",
    "    \n",
    "    return {\n",
    "        'non_fused': non_fused_result,\n",
    "        'fused': fused_result,\n",
    "        'improvement': (non_fused_result['execution_time'] - fused_result['execution_time']) / non_fused_result['execution_time'] * 100 if non_fused_result['execution_time'] > 0 else 0\n",
    "    }\n",
    "\n",
    "# Run performance optimization tests\n",
    "logger.info(\"Starting performance optimization analysis...\")\n",
    "\n",
    "# Test batch size optimization\n",
    "batch_results = optimize_batch_processing(customers_ds)\n",
    "print(\"\\nBatch Size Optimization Results:\")\n",
    "print(f\"{'Batch Size':<12} {'Time (s)':<10} {'Throughput':<12} {'Success'}\")\n",
    "print(\"-\" * 50)\n",
    "for batch_name, result in batch_results.items():\n",
    "    batch_size = batch_name.split('_')[1]\n",
    "    print(f\"{batch_size:<12} {result['execution_time']:<10.3f} {result['throughput']:<12.0f} {result['success']}\")\n",
    "\n",
    "# Test concurrency optimization\n",
    "concurrency_results = optimize_concurrency(customers_ds)\n",
    "print(\"\\nConcurrency Optimization Results:\")\n",
    "print(f\"{'Concurrency':<12} {'Time (s)':<10} {'Throughput':<12} {'Success'}\")\n",
    "print(\"-\" * 50)\n",
    "for conc_name, result in concurrency_results.items():\n",
    "    conc_level = conc_name.split('_')[1]\n",
    "    print(f\"{conc_level:<12} {result['execution_time']:<10.3f} {result['throughput']:<12.0f} {result['success']}\")\n",
    "\n",
    "# Test operator fusion\n",
    "fusion_results = demonstrate_operator_fusion(customers_ds)\n",
    "print(f\"\\nOperator Fusion Results:\")\n",
    "print(f\"Non-Fused Time: {fusion_results['non_fused']['execution_time']:.3f}s\")\n",
    "print(f\"Fused Time: {fusion_results['fused']['execution_time']:.3f}s\")\n",
    "print(f\"Performance Improvement: {fusion_results['improvement']:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aafc4a",
   "metadata": {},
   "source": [
    "### Step 8: Resource Monitoring and Observability\n",
    "\n",
    "Effective monitoring is crucial for production ETL pipelines:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d92ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Resource Monitoring\n",
    "def monitor_cluster_resources() -> Dict[str, Any]:\n",
    "    \"\"\"Monitor current cluster resource utilization.\"\"\"\n",
    "    try:\n",
    "        resources = ray.cluster_resources()\n",
    "        nodes = ray.nodes()\n",
    "        \n",
    "        monitoring_data = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'total_cpus': resources.get('CPU', 0),\n",
    "            'total_memory_gb': resources.get('memory', 0) / (1024**3),\n",
    "            'object_store_memory_gb': resources.get('object_store_memory', 0) / (1024**3),\n",
    "            'num_nodes': len(nodes),\n",
    "            'node_details': []\n",
    "        }\n",
    "        \n",
    "        for node in nodes:\n",
    "            node_info = {\n",
    "                'node_id': node['NodeID'],\n",
    "                'alive': node['Alive'],\n",
    "                'resources': node['Resources'],\n",
    "                'cpu_usage': node['Resources'].get('CPU', 0),\n",
    "                'memory_usage': node['Resources'].get('memory', 0) / (1024**3)\n",
    "            }\n",
    "            monitoring_data['node_details'].append(node_info)\n",
    "        \n",
    "        return monitoring_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Resource monitoring failed: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "def display_resource_status(monitoring_data: Dict[str, Any]) -> None:\n",
    "    \"\"\"Display current resource utilization status.\"\"\"\n",
    "    if 'error' in monitoring_data:\n",
    "        print(f\"ERROR: Resource monitoring error: {monitoring_data['error']}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CLUSTER RESOURCE STATUS - {monitoring_data['timestamp']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total Nodes: {monitoring_data['num_nodes']}\")\n",
    "    print(f\"Total CPUs: {monitoring_data['total_cpus']:.1f}\")\n",
    "    print(f\"Total Memory: {monitoring_data['total_memory_gb']:.1f} GB\")\n",
    "    print(f\"Object Store: {monitoring_data['object_store_memory_gb']:.1f} GB\")\n",
    "    \n",
    "    print(f\"\\nNode Details:\")\n",
    "    print(f\"{'Node ID':<20} {'Status':<8} {'CPUs':<8} {'Memory (GB)':<12}\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    \n",
    "    for node in monitoring_data['node_details']:\n",
    "        status = \"Alive\" if node['alive'] else \"Dead\"\n",
    "        print(f\"{node['node_id'][:18]:<20} {status:<8} {node['cpu_usage']:<8.1f} {node['memory_usage']:<12.1f}\")\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "def create_performance_dashboard_data(dataset: Dataset, operation_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"Create data for performance dashboard visualization.\"\"\"\n",
    "    import time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    start_resources = monitor_cluster_resources()\n",
    "    \n",
    "    # Perform a sample operation\n",
    "    sample_result = dataset.take(100)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    end_resources = monitor_cluster_resources()\n",
    "    \n",
    "    execution_time = end_time - start_time\n",
    "    memory_delta = end_resources.get('total_memory_gb', 0) - start_resources.get('total_memory_gb', 0)\n",
    "    \n",
    "    return {\n",
    "        'operation': operation_name,\n",
    "        'execution_time': execution_time,\n",
    "        'memory_delta_gb': memory_delta,\n",
    "        'records_processed': len(sample_result),\n",
    "        'throughput_per_second': len(sample_result) / execution_time if execution_time > 0 else 0,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "def generate_performance_report(operations_data: List[Dict[str, Any]]) -> None:\n",
    "    \"\"\"Generate a comprehensive performance report.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PERFORMANCE REPORT\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"{'Operation':<25} {'Time (s)':<10} {'Records':<10} {'Throughput/s':<12} {'Memory \u0394':<10}\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    \n",
    "    total_time = 0\n",
    "    total_records = 0\n",
    "    \n",
    "    for op_data in operations_data:\n",
    "        print(f\"{op_data['operation']:<25} {op_data['execution_time']:<10.3f} {op_data['records_processed']:<10} {op_data['throughput_per_second']:<12.0f} {op_data['memory_delta_gb']:<10.3f}\")\n",
    "        total_time += op_data['execution_time']\n",
    "        total_records += op_data['records_processed']\n",
    "    \n",
    "    print(f\"{'-'*70}\")\n",
    "    print(f\"{'TOTAL':<25} {total_time:<10.3f} {total_records:<10} {total_records/total_time if total_time > 0 else 0:<12.0f} {'N/A':<10}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "# Monitor current cluster resources\n",
    "current_resources = monitor_cluster_resources()\n",
    "display_resource_status(current_resources)\n",
    "\n",
    "# Create performance monitoring data for different operations\n",
    "performance_data = []\n",
    "\n",
    "# Test different operations\n",
    "operations_to_test = [\n",
    "    (\"Filter Operation\", lambda: customers_ds.filter(lambda x: x['income'] > 50000)),\n",
    "    (\"Map Batches\", lambda: customers_ds.map_batches(lambda batch: batch)),\n",
    "    (\"Group By\", lambda: customers_ds.groupby('city').aggregate(Count())),\n",
    "    (\"Sample Data\", lambda: customers_ds.take(100))\n",
    "]\n",
    "\n",
    "for op_name, op_func in operations_to_test:\n",
    "    try:\n",
    "        op_data = create_performance_dashboard_data(customers_ds, op_name)\n",
    "        performance_data.append(op_data)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Performance monitoring failed for {op_name}: {e}\")\n",
    "\n",
    "# Generate performance report\n",
    "generate_performance_report(performance_data)\n",
    "\n",
    "# Display Ray dashboard information\n",
    "dashboard_url = ray.get_dashboard_url()\n",
    "print(f\"\\nRay Dashboard: {dashboard_url}\")\n",
    "print(f\"Use the dashboard to monitor real-time cluster performance\")\n",
    "print(f\"Look for task execution, memory usage, and resource utilization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f58ee58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.12/site-packages/ray/data/_internal/datasource/parquet_datasource.py:750: FutureWarning: The default `file_extensions` for `read_parquet` will change from `None` to ['parquet'] after Ray 2.43, and your dataset contains files that don't match the new `file_extensions`. To maintain backwards compatibility, set `file_extensions=None` explicitly.\n",
      "  warnings.warn(\n",
      "2025-08-28 00:38:25,879\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_332_0\n",
      "2025-08-28 00:38:25,894\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_332_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:38:25,895\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_332_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)] -> LimitOperator[limit=1]\n",
      "\u001b[36m(MapBatches(transform) pid=292427)\u001b[0m Error calculating size for column 'name': cannot call `vectorize` on size 0 inputs unless `otypes` is set\n",
      "\u001b[36m(MapBatches(transform) pid=292427)\u001b[0m Error calculating size for column 'category': cannot call `vectorize` on size 0 inputs unless `otypes` is set\n",
      "\u001b[36m(MapBatches(transform) pid=292427)\u001b[0m Error calculating size for column 'name': cannot call `vectorize` on size 0 inputs unless `otypes` is set\n",
      "\u001b[36m(MapBatches(transform) pid=292427)\u001b[0m Error calculating size for column 'category': cannot call `vectorize` on size 0 inputs unless `otypes` is set\n",
      "2025-08-28 00:38:28,263\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_332_0 execution finished in 2.37 seconds\n",
      "2025-08-28 00:38:28,271\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_334_0\n",
      "2025-08-28 00:38:28,279\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_334_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:38:28,280\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_334_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)] -> LimitOperator[limit=1] -> TaskPoolMapOperator[Project]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TPC-H Customer Master Data (Traditional ETL):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:38:29,861\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_334_0 execution finished in 1.58 seconds\n",
      "2025-08-28 00:38:29,867\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_335_0\n",
      "2025-08-28 00:38:29,876\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_335_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:38:29,876\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_335_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)->Project] -> AggregateNumRows[AggregateNumRows]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema: Column        Type\n",
      "------        ----\n",
      "c_custkey     int64\n",
      "c_name        string\n",
      "c_address     string\n",
      "c_nationkey   int64\n",
      "c_phone       string\n",
      "c_acctbal     double\n",
      "c_mktsegment  string\n",
      "c_comment     string\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:38:30,710\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_335_0 execution finished in 0.83 seconds\n",
      "2025-08-28 00:38:30,719\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_336_0\n",
      "2025-08-28 00:38:30,729\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_336_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:38:30,729\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_336_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)] -> LimitOperator[limit=25] -> TaskPoolMapOperator[Project]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total customers: 1,500,000\n",
      "Sample customer records:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:38:32,123\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_336_0 execution finished in 1.39 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_custkey</th>\n",
       "      <th>c_name</th>\n",
       "      <th>c_address</th>\n",
       "      <th>c_nationkey</th>\n",
       "      <th>c_phone</th>\n",
       "      <th>c_acctbal</th>\n",
       "      <th>c_mktsegment</th>\n",
       "      <th>c_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Customer#000000001</td>\n",
       "      <td>IVhzIApeRb ot,c,E</td>\n",
       "      <td>15</td>\n",
       "      <td>25-989-741-2988</td>\n",
       "      <td>711.56</td>\n",
       "      <td>BUILDING</td>\n",
       "      <td>to the even, regular platelets. regular, ironi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Customer#000000002</td>\n",
       "      <td>XSTf4,NCwDVaWNe6tEgvwfmRchLXak</td>\n",
       "      <td>13</td>\n",
       "      <td>23-768-687-3665</td>\n",
       "      <td>121.65</td>\n",
       "      <td>AUTOMOBILE</td>\n",
       "      <td>l accounts. blithely ironic theodolites integr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Customer#000000003</td>\n",
       "      <td>MG9kdTD2WBHm</td>\n",
       "      <td>1</td>\n",
       "      <td>11-719-748-3364</td>\n",
       "      <td>7498.12</td>\n",
       "      <td>AUTOMOBILE</td>\n",
       "      <td>deposits eat slyly ironic, even instructions....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Customer#000000004</td>\n",
       "      <td>XxVSJsLAGtn</td>\n",
       "      <td>4</td>\n",
       "      <td>14-128-190-5944</td>\n",
       "      <td>2866.83</td>\n",
       "      <td>MACHINERY</td>\n",
       "      <td>requests. final, regular ideas sleep final accou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Customer#000000005</td>\n",
       "      <td>KvpyuHCplrB84WgAiGV6sYpZq7Tj</td>\n",
       "      <td>3</td>\n",
       "      <td>13-750-942-6364</td>\n",
       "      <td>794.47</td>\n",
       "      <td>HOUSEHOLD</td>\n",
       "      <td>n accounts will have to unwind. foxes cajole a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Customer#000000006</td>\n",
       "      <td>sKZz0CsnMD7mp4Xd0YrBvx,LREYKUWAh yVn</td>\n",
       "      <td>20</td>\n",
       "      <td>30-114-968-4951</td>\n",
       "      <td>7638.57</td>\n",
       "      <td>AUTOMOBILE</td>\n",
       "      <td>tions. even deposits boost according to the sl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Customer#000000007</td>\n",
       "      <td>TcGe5gaZNgVePxU5kRrvXBfkasDTea</td>\n",
       "      <td>18</td>\n",
       "      <td>28-190-982-9759</td>\n",
       "      <td>9561.95</td>\n",
       "      <td>AUTOMOBILE</td>\n",
       "      <td>ainst the ironic, express theodolites. express...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Customer#000000008</td>\n",
       "      <td>I0B10bB0AymmC, 0PrRYBCP1yGJ8xcBPmWhl5</td>\n",
       "      <td>17</td>\n",
       "      <td>27-147-574-9335</td>\n",
       "      <td>6819.74</td>\n",
       "      <td>BUILDING</td>\n",
       "      <td>among the slyly regular theodolites kindle bli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Customer#000000009</td>\n",
       "      <td>xKiAFTjUsCuxfeleNqefumTrjS</td>\n",
       "      <td>8</td>\n",
       "      <td>18-338-906-3675</td>\n",
       "      <td>8324.07</td>\n",
       "      <td>FURNITURE</td>\n",
       "      <td>r theodolites according to the requests wake t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Customer#000000010</td>\n",
       "      <td>6LrEaV6KR6PLVcgl2ArL Q3rqzLzcT1 v2</td>\n",
       "      <td>5</td>\n",
       "      <td>15-741-346-9870</td>\n",
       "      <td>2753.54</td>\n",
       "      <td>HOUSEHOLD</td>\n",
       "      <td>es regular deposits haggle. fur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Customer#000000011</td>\n",
       "      <td>PkWS 3HlXqwTuzrKg633BEi</td>\n",
       "      <td>23</td>\n",
       "      <td>33-464-151-3439</td>\n",
       "      <td>-272.60</td>\n",
       "      <td>BUILDING</td>\n",
       "      <td>ckages. requests sleep slyly. quickly even pin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Customer#000000012</td>\n",
       "      <td>9PWKuhzT4Zr1Q</td>\n",
       "      <td>13</td>\n",
       "      <td>23-791-276-1263</td>\n",
       "      <td>3396.49</td>\n",
       "      <td>HOUSEHOLD</td>\n",
       "      <td>to the carefully final braids. blithely regul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Customer#000000013</td>\n",
       "      <td>nsXQu0oVjD7PM659uC3SRSp</td>\n",
       "      <td>3</td>\n",
       "      <td>13-761-547-5974</td>\n",
       "      <td>3857.34</td>\n",
       "      <td>BUILDING</td>\n",
       "      <td>ounts sleep carefully after the close frays. c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Customer#000000014</td>\n",
       "      <td>KXkletMlL2JQEA</td>\n",
       "      <td>1</td>\n",
       "      <td>11-845-129-3851</td>\n",
       "      <td>5266.30</td>\n",
       "      <td>FURNITURE</td>\n",
       "      <td>, ironic packages across the unus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Customer#000000015</td>\n",
       "      <td>YtWggXoOLdwdo7b0y,BZaGUQMLJMX1Y,EC,6Dn</td>\n",
       "      <td>23</td>\n",
       "      <td>33-687-542-7601</td>\n",
       "      <td>2788.52</td>\n",
       "      <td>HOUSEHOLD</td>\n",
       "      <td>platelets. regular deposits detect asymptotes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Customer#000000016</td>\n",
       "      <td>cYiaeMLZSMAOQ2 d0W,</td>\n",
       "      <td>10</td>\n",
       "      <td>20-781-609-3107</td>\n",
       "      <td>4681.03</td>\n",
       "      <td>FURNITURE</td>\n",
       "      <td>kly silent courts. thinly regular theodolites ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Customer#000000017</td>\n",
       "      <td>izrh 6jdqtp2eqdtbkswDD8SG4SzXruMfIXyR7</td>\n",
       "      <td>2</td>\n",
       "      <td>12-970-682-3487</td>\n",
       "      <td>6.34</td>\n",
       "      <td>AUTOMOBILE</td>\n",
       "      <td>packages wake! blithely even pint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Customer#000000018</td>\n",
       "      <td>3txGO AiuFux3zT0Z9NYaFRnZt</td>\n",
       "      <td>6</td>\n",
       "      <td>16-155-215-1315</td>\n",
       "      <td>5494.43</td>\n",
       "      <td>BUILDING</td>\n",
       "      <td>s sleep. carefully even instructions nag furio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Customer#000000019</td>\n",
       "      <td>uc,3bHIx84H,wdrmLOjVsiqXCq2tr</td>\n",
       "      <td>18</td>\n",
       "      <td>28-396-526-5053</td>\n",
       "      <td>8914.71</td>\n",
       "      <td>HOUSEHOLD</td>\n",
       "      <td>nag. furiously careful packages are slyly at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Customer#000000020</td>\n",
       "      <td>JrPk8Pqplj4Ne</td>\n",
       "      <td>22</td>\n",
       "      <td>32-957-234-8742</td>\n",
       "      <td>7603.40</td>\n",
       "      <td>FURNITURE</td>\n",
       "      <td>g alongside of the special excuses-- fluffily ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Customer#000000021</td>\n",
       "      <td>XYmVpr9yAHDEn</td>\n",
       "      <td>8</td>\n",
       "      <td>18-902-614-8344</td>\n",
       "      <td>1428.25</td>\n",
       "      <td>MACHINERY</td>\n",
       "      <td>quickly final accounts integrate blithely fur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Customer#000000022</td>\n",
       "      <td>QI6p41,FNs5k7RZoCCVPUTkUdYpB</td>\n",
       "      <td>3</td>\n",
       "      <td>13-806-545-9701</td>\n",
       "      <td>591.98</td>\n",
       "      <td>MACHINERY</td>\n",
       "      <td>s nod furiously above the furiously ironic ide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Customer#000000023</td>\n",
       "      <td>OdY W13N7Be3OC5MpgfmcYss0Wn6TKT</td>\n",
       "      <td>3</td>\n",
       "      <td>13-312-472-8245</td>\n",
       "      <td>3332.02</td>\n",
       "      <td>HOUSEHOLD</td>\n",
       "      <td>deposits. special deposits cajole slyly. fluff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Customer#000000024</td>\n",
       "      <td>HXAFgIAyjxtdqwimt13Y3OZO 4xeLe7U8PqG</td>\n",
       "      <td>13</td>\n",
       "      <td>23-127-851-8031</td>\n",
       "      <td>9255.67</td>\n",
       "      <td>MACHINERY</td>\n",
       "      <td>into beans. fluffily final ideas haggle fluffily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Customer#000000025</td>\n",
       "      <td>Hp8GyFQgGHFYSilH5tBfe</td>\n",
       "      <td>12</td>\n",
       "      <td>22-603-468-3533</td>\n",
       "      <td>7133.70</td>\n",
       "      <td>FURNITURE</td>\n",
       "      <td>y. accounts sleep ruthlessly according to the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    c_custkey              c_name                               c_address  \\\n",
       "0           1  Customer#000000001                       IVhzIApeRb ot,c,E   \n",
       "1           2  Customer#000000002          XSTf4,NCwDVaWNe6tEgvwfmRchLXak   \n",
       "2           3  Customer#000000003                            MG9kdTD2WBHm   \n",
       "3           4  Customer#000000004                             XxVSJsLAGtn   \n",
       "4           5  Customer#000000005            KvpyuHCplrB84WgAiGV6sYpZq7Tj   \n",
       "5           6  Customer#000000006    sKZz0CsnMD7mp4Xd0YrBvx,LREYKUWAh yVn   \n",
       "6           7  Customer#000000007          TcGe5gaZNgVePxU5kRrvXBfkasDTea   \n",
       "7           8  Customer#000000008   I0B10bB0AymmC, 0PrRYBCP1yGJ8xcBPmWhl5   \n",
       "8           9  Customer#000000009              xKiAFTjUsCuxfeleNqefumTrjS   \n",
       "9          10  Customer#000000010      6LrEaV6KR6PLVcgl2ArL Q3rqzLzcT1 v2   \n",
       "10         11  Customer#000000011                 PkWS 3HlXqwTuzrKg633BEi   \n",
       "11         12  Customer#000000012                           9PWKuhzT4Zr1Q   \n",
       "12         13  Customer#000000013                 nsXQu0oVjD7PM659uC3SRSp   \n",
       "13         14  Customer#000000014                         KXkletMlL2JQEA    \n",
       "14         15  Customer#000000015  YtWggXoOLdwdo7b0y,BZaGUQMLJMX1Y,EC,6Dn   \n",
       "15         16  Customer#000000016                     cYiaeMLZSMAOQ2 d0W,   \n",
       "16         17  Customer#000000017  izrh 6jdqtp2eqdtbkswDD8SG4SzXruMfIXyR7   \n",
       "17         18  Customer#000000018              3txGO AiuFux3zT0Z9NYaFRnZt   \n",
       "18         19  Customer#000000019           uc,3bHIx84H,wdrmLOjVsiqXCq2tr   \n",
       "19         20  Customer#000000020                           JrPk8Pqplj4Ne   \n",
       "20         21  Customer#000000021                           XYmVpr9yAHDEn   \n",
       "21         22  Customer#000000022            QI6p41,FNs5k7RZoCCVPUTkUdYpB   \n",
       "22         23  Customer#000000023         OdY W13N7Be3OC5MpgfmcYss0Wn6TKT   \n",
       "23         24  Customer#000000024    HXAFgIAyjxtdqwimt13Y3OZO 4xeLe7U8PqG   \n",
       "24         25  Customer#000000025                   Hp8GyFQgGHFYSilH5tBfe   \n",
       "\n",
       "    c_nationkey          c_phone  c_acctbal c_mktsegment  \\\n",
       "0            15  25-989-741-2988     711.56     BUILDING   \n",
       "1            13  23-768-687-3665     121.65   AUTOMOBILE   \n",
       "2             1  11-719-748-3364    7498.12   AUTOMOBILE   \n",
       "3             4  14-128-190-5944    2866.83    MACHINERY   \n",
       "4             3  13-750-942-6364     794.47    HOUSEHOLD   \n",
       "5            20  30-114-968-4951    7638.57   AUTOMOBILE   \n",
       "6            18  28-190-982-9759    9561.95   AUTOMOBILE   \n",
       "7            17  27-147-574-9335    6819.74     BUILDING   \n",
       "8             8  18-338-906-3675    8324.07    FURNITURE   \n",
       "9             5  15-741-346-9870    2753.54    HOUSEHOLD   \n",
       "10           23  33-464-151-3439    -272.60     BUILDING   \n",
       "11           13  23-791-276-1263    3396.49    HOUSEHOLD   \n",
       "12            3  13-761-547-5974    3857.34     BUILDING   \n",
       "13            1  11-845-129-3851    5266.30    FURNITURE   \n",
       "14           23  33-687-542-7601    2788.52    HOUSEHOLD   \n",
       "15           10  20-781-609-3107    4681.03    FURNITURE   \n",
       "16            2  12-970-682-3487       6.34   AUTOMOBILE   \n",
       "17            6  16-155-215-1315    5494.43     BUILDING   \n",
       "18           18  28-396-526-5053    8914.71    HOUSEHOLD   \n",
       "19           22  32-957-234-8742    7603.40    FURNITURE   \n",
       "20            8  18-902-614-8344    1428.25    MACHINERY   \n",
       "21            3  13-806-545-9701     591.98    MACHINERY   \n",
       "22            3  13-312-472-8245    3332.02    HOUSEHOLD   \n",
       "23           13  23-127-851-8031    9255.67    MACHINERY   \n",
       "24           12  22-603-468-3533    7133.70    FURNITURE   \n",
       "\n",
       "                                            c_comment  \n",
       "0   to the even, regular platelets. regular, ironi...  \n",
       "1   l accounts. blithely ironic theodolites integr...  \n",
       "2    deposits eat slyly ironic, even instructions....  \n",
       "3    requests. final, regular ideas sleep final accou  \n",
       "4   n accounts will have to unwind. foxes cajole a...  \n",
       "5   tions. even deposits boost according to the sl...  \n",
       "6   ainst the ironic, express theodolites. express...  \n",
       "7   among the slyly regular theodolites kindle bli...  \n",
       "8   r theodolites according to the requests wake t...  \n",
       "9                     es regular deposits haggle. fur  \n",
       "10  ckages. requests sleep slyly. quickly even pin...  \n",
       "11   to the carefully final braids. blithely regul...  \n",
       "12  ounts sleep carefully after the close frays. c...  \n",
       "13                  , ironic packages across the unus  \n",
       "14   platelets. regular deposits detect asymptotes...  \n",
       "15  kly silent courts. thinly regular theodolites ...  \n",
       "16                  packages wake! blithely even pint  \n",
       "17  s sleep. carefully even instructions nag furio...  \n",
       "18   nag. furiously careful packages are slyly at ...  \n",
       "19  g alongside of the special excuses-- fluffily ...  \n",
       "20   quickly final accounts integrate blithely fur...  \n",
       "21  s nod furiously above the furiously ironic ide...  \n",
       "22  deposits. special deposits cajole slyly. fluff...  \n",
       "23   into beans. fluffily final ideas haggle fluffily  \n",
       "24  y. accounts sleep ruthlessly according to the ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read TPC-H Customer Master Data (Traditional Structured Data Processing)\n",
    "customers_ds = ray.data.read_parquet(f\"{TPCH_S3_PATH}/customer\")\n",
    "\n",
    "customers_ds = customers_ds.drop_columns([\"column8\"])\n",
    "customers_ds = customers_ds.rename_columns([\n",
    "\"c_custkey\",\n",
    "\"c_name\",\n",
    "\"c_address\",\n",
    "\"c_nationkey\",\n",
    "\"c_phone\",\n",
    "\"c_acctbal\",\n",
    "\"c_mktsegment\",\n",
    "\"c_comment\",\n",
    "])\n",
    "\n",
    "print(\" TPC-H Customer Master Data (Traditional ETL):\")\n",
    "print(f\"Schema: {customers_ds.schema()}\")\n",
    "print(f\"Total customers: {customers_ds.count():,}\")\n",
    "print(\"Sample customer records:\")\n",
    "customers_ds.limit(25).to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac2e6f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:38:32,246\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_338_0\n",
      "2025-08-28 00:38:32,257\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_338_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:38:32,258\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_338_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)->Project] -> AllToAllOperator[Aggregate] -> LimitOperator[limit=1]\n",
      "2025-08-28 00:38:33,199\tWARNING streaming_executor_state.py:790 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: c_mktsegment: string\n",
      "count(): int64\n",
      "mean(c_acctbal): double, new schema: None. This may lead to unexpected behavior.\n",
      "2025-08-28 00:38:33,211\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_338_0 execution finished in 0.95 seconds\n",
      "2025-08-28 00:38:33,219\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_340_0\n",
      "2025-08-28 00:38:33,228\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_340_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:38:33,229\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_340_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)->Project] -> AllToAllOperator[Aggregate] -> LimitOperator[limit=5] -> TaskPoolMapOperator[Project]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer Market Segment Distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:38:34,066\tWARNING streaming_executor_state.py:790 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: c_mktsegment: string\n",
      "count(): int64\n",
      "mean(c_acctbal): double, new schema: None. This may lead to unexpected behavior.\n",
      "2025-08-28 00:38:34,099\tWARNING streaming_executor_state.py:790 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: c_mktsegment: string\n",
      "customer_count: int64\n",
      "avg_account_balance: double, new schema: . This may lead to unexpected behavior.\n",
      "2025-08-28 00:38:34,112\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_340_0 execution finished in 0.88 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'c_mktsegment': 'AUTOMOBILE', 'customer_count': 300036, 'avg_account_balance': 4496.230541701662}\n",
      "{'c_mktsegment': 'BUILDING', 'customer_count': 300276, 'avg_account_balance': 4505.869852402457}\n",
      "{'c_mktsegment': 'FURNITURE', 'customer_count': 299496, 'avg_account_balance': 4500.162798034031}\n",
      "{'c_mktsegment': 'HOUSEHOLD', 'customer_count': 299751, 'avg_account_balance': 4499.8627405746765}\n",
      "{'c_mktsegment': 'MACHINERY', 'customer_count': 300441, 'avg_account_balance': 4492.427445488464}\n"
     ]
    }
   ],
   "source": [
    "from ray.data.aggregate import Count, Mean\n",
    "\n",
    "segment_analysis = customers_ds.groupby(\"c_mktsegment\").aggregate(\n",
    "    Count(),             # counts rows per segment\n",
    "    Mean(\"c_acctbal\"),   # average account balance\n",
    "    ).rename_columns([\"c_mktsegment\", \"customer_count\", \"avg_account_balance\"]\n",
    ")\n",
    "\n",
    "print(\"Customer Market Segment Distribution:\")\n",
    "segment_analysis.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc44024c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:38:34,219\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_343_0\n",
      "2025-08-28 00:38:34,227\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_343_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:38:34,227\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_343_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> LimitOperator[limit=1] -> TaskPoolMapOperator[ReadFiles]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPC-H Nations Reference Data:\n",
      "Loading geographic data for customer demographics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:38:34,541\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_343_0 execution finished in 0.31 seconds\n",
      "2025-08-28 00:38:34,548\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_345_0\n",
      "2025-08-28 00:38:34,554\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_345_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:38:34,554\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_345_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[MapBatches(count_rows)]\n",
      "2025-08-28 00:38:34,811\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_345_0 execution finished in 0.26 seconds\n",
      "2025-08-28 00:38:34,817\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_346_0\n",
      "2025-08-28 00:38:34,825\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_346_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:38:34,825\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_346_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> LimitOperator[limit=5] -> TaskPoolMapOperator[ReadFiles]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nations: 25\n",
      "\n",
      " Sample nation records:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:38:35,119\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_346_0 execution finished in 0.29 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_nationkey': 0, 'n_name': 'ALGERIA', 'n_regionkey': 0, 'n_comment': ' haggle. carefully final deposits detect slyly agai'}\n",
      "{'n_nationkey': 1, 'n_name': 'ARGENTINA', 'n_regionkey': 1, 'n_comment': 'al foxes promise slyly according to the regular accounts. bold requests alon'}\n",
      "{'n_nationkey': 2, 'n_name': 'BRAZIL', 'n_regionkey': 1, 'n_comment': 'y alongside of the pending deposits. carefully special packages are about the ironic forges. slyly special '}\n",
      "{'n_nationkey': 3, 'n_name': 'CANADA', 'n_regionkey': 1, 'n_comment': 'eas hang ironic, silent packages. slyly regular packages are furiously over the tithes. fluffily bold'}\n",
      "{'n_nationkey': 4, 'n_name': 'EGYPT', 'n_regionkey': 4, 'n_comment': 'y above the carefully unusual theodolites. final dugouts are quickly across the furiously regular d'}\n"
     ]
    }
   ],
   "source": [
    "# Geographic Reference Data - Nations Table\n",
    "print(\"TPC-H Nations Reference Data:\")\n",
    "print(\"Loading geographic data for customer demographics...\")\n",
    "\n",
    "nation_ds = ray.data.read_parquet(f\"{TPCH_S3_PATH}/nation\")\n",
    "\n",
    "nation_ds = (\n",
    "    nation_ds\n",
    "    .select_columns([\"column0\", \"column1\", \"column2\", \"column3\"])\n",
    "    .rename_columns([\"n_nationkey\", \"n_name\", \"n_regionkey\", \"n_comment\"])\n",
    ")\n",
    "\n",
    "print(f\"Total nations: {nation_ds.count():,}\")\n",
    "\n",
    "print(\"\\n Sample nation records:\")\n",
    "nation_ds.limit(25).to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4eab8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:38:35,231\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_350_0\n",
      "2025-08-28 00:38:35,247\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_350_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:38:35,248\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_350_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)->Project], InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> JoinOperatorWithPolars[Join(num_partitions=100)] -> AllToAllOperator[Aggregate] -> LimitOperator[limit=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Customer Demographics by Nation:\n",
      "   Joining customer and nation data for geographic analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:38:40,128\tWARNING streaming_executor_state.py:790 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: n_name: string\n",
      "count(): int64\n",
      "mean(c_acctbal): double\n",
      "sum(c_acctbal): double, new schema: None. This may lead to unexpected behavior.\n",
      "2025-08-28 00:38:40,240\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_350_0 execution finished in 4.99 seconds\n",
      "2025-08-28 00:38:40,305\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_353_0\n",
      "2025-08-28 00:38:40,398\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_353_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:38:40,399\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_353_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(drop_columns)->Project], InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> JoinOperatorWithPolars[Join(num_partitions=100)] -> AllToAllOperator[Aggregate] -> TaskPoolMapOperator[Project] -> AllToAllOperator[Sort] -> LimitOperator[limit=10]\n",
      "2025-08-28 00:38:48,896\tWARNING streaming_executor_state.py:790 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: n_name: string\n",
      "count(): int64\n",
      "mean(c_acctbal): double\n",
      "sum(c_acctbal): double, new schema: None. This may lead to unexpected behavior.\n",
      "2025-08-28 00:38:49,089\tWARNING streaming_executor_state.py:790 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: n_name: string\n",
      "customer_count: int64\n",
      "avg_balance: double\n",
      "total_balance: double, new schema: . This may lead to unexpected behavior.\n",
      "2025-08-28 00:38:50,143\tWARNING streaming_executor_state.py:790 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: n_name: string\n",
      "customer_count: int64\n",
      "avg_balance: double\n",
      "total_balance: double, new schema: . This may lead to unexpected behavior.\n",
      "2025-08-28 00:38:50,295\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_353_0 execution finished in 9.89 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_name': 'ETHIOPIA', 'customer_count': 60471, 'avg_balance': 4512.16504423608, 'total_balance': 272855132.39}\n",
      "{'n_name': 'UNITED KINGDOM', 'customer_count': 60381, 'avg_balance': 4482.881766946556, 'total_balance': 270680883.97}\n",
      "{'n_name': 'FRANCE', 'customer_count': 60316, 'avg_balance': 4483.745413488959, 'total_balance': 270441588.36}\n",
      "{'n_name': 'INDONESIA', 'customer_count': 60236, 'avg_balance': 4506.17889501295, 'total_balance': 271434191.9200001}\n",
      "{'n_name': 'INDIA', 'customer_count': 60215, 'avg_balance': 4505.563116167067, 'total_balance': 271302483.03999996}\n",
      "{'n_name': 'GERMANY', 'customer_count': 60153, 'avg_balance': 4488.417533456352, 'total_balance': 269991779.89}\n",
      "{'n_name': 'IRAN', 'customer_count': 60101, 'avg_balance': 4512.225304737025, 'total_balance': 271189253.03999996}\n",
      "{'n_name': 'CHINA', 'customer_count': 60065, 'avg_balance': 4480.9787298759675, 'total_balance': 269149987.40999997}\n",
      "{'n_name': 'RUSSIA', 'customer_count': 60065, 'avg_balance': 4507.959750603513, 'total_balance': 270770602.42}\n",
      "{'n_name': 'IRAQ', 'customer_count': 60056, 'avg_balance': 4516.969680797922, 'total_balance': 271271131.15}\n"
     ]
    }
   ],
   "source": [
    "# Customer Demographics by Nation - Join Analysis\n",
    "print(\" Customer Demographics by Nation:\")\n",
    "print(\"   Joining customer and nation data for geographic analysis...\")\n",
    "\n",
    "from ray.data.aggregate import Count, Mean, Sum\n",
    "\n",
    "customer_nation_analysis = (customers_ds\n",
    "    .join(nation_ds, on=(\"c_nationkey\",), right_on=(\"n_nationkey\",), join_type=\"inner\", num_partitions=100)\n",
    "    .groupby(\"n_name\")\n",
    "    .aggregate(\n",
    "        Count(),\n",
    "        Mean(\"c_acctbal\"),\n",
    "        Sum(\"c_acctbal\"),\n",
    ")\n",
    ".rename_columns([\"n_name\", \"customer_count\", \"avg_balance\", \"total_balance\"])\n",
    ")\n",
    "\n",
    "customer_nation_analysis.sort(\"customer_count\", descending=True).limit(10).to_pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d92883",
   "metadata": {},
   "source": [
    "### Understanding Join Partitioning in Ray Data\n",
    "\n",
    "The `num_partitions` parameter in joins is crucial for performance optimization. Here's how to choose the right number:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Join Partitioning Guidelines:</b>\n",
    "<ul>\n",
    "    <li><b>Default:</b> Ray Data automatically chooses partitions based on cluster size and data volume</li>\n",
    "    <li><b>Rule of Thumb:</b> Start with 1-2 partitions per CPU core in your cluster</li>\n",
    "    <li><b>Memory Consideration:</b> Each partition should fit comfortably in memory</li>\n",
    "    <li><b>Data Skew:</b> More partitions help with skewed data distribution</li>\n",
    "    <li><b>Network Overhead:</b> Too many partitions can cause excessive network communication</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "**Choosing the Right Number of Partitions:**\n",
    "\n",
    "| Cluster Size | Data Volume | Recommended Partitions | Reasoning |\n",
    "|--------------|-------------|------------------------|-----------|\n",
    "| 4-8 cores | < 1GB | 4-8 | One partition per core |\n",
    "| 16-32 cores | 1-10GB | 16-32 | Balanced parallelism |\n",
    "| 64+ cores | 10GB+ | 64-128 | Avoid too many small partitions |\n",
    "| High skew | Any size | 2-4x cores | Handle data imbalance |\n",
    "\n",
    "**Performance Impact:**\n",
    "- **Too Few Partitions:** Underutilized cluster, slower processing\n",
    "- **Too Many Partitions:** Network overhead, task scheduling overhead\n",
    "- **Optimal Range:** 1-2 partitions per CPU core typically works well\n",
    "\n",
    "**Monitoring Join Performance:**\n",
    "```python\n",
    "# Check join execution plan\n",
    "print(\"Join execution plan:\")\n",
    "print(customer_nation_analysis._plan)\n",
    "\n",
    "# Monitor resource utilization during joins\n",
    "# Look for balanced task distribution in Ray dashboard\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "844e7cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:38:50,461\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_356_0\n",
      "2025-08-28 00:38:50,469\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_356_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:38:50,470\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_356_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> LimitOperator[limit=1] -> TaskPoolMapOperator[ReadFiles]\n",
      "2025-08-28 00:38:51,744\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_356_0 execution finished in 1.27 seconds\n",
      "2025-08-28 00:38:51,752\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_359_0\n",
      "2025-08-28 00:38:51,759\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_359_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:38:51,760\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_359_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> LimitOperator[limit=3] -> TaskPoolMapOperator[ReadFiles]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TPC-H Orders Data (Enterprise Transaction Processing):\n",
      "\n",
      " Sample order records:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:38:53,657\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_359_0 execution finished in 1.90 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'o_orderkey': 1, 'o_custkey': 369001, 'o_orderstatus': 'O', 'o_totalprice': 186600.18, 'o_orderdate': datetime.date(1996, 1, 2), 'o_orderpriority': '5-LOW', 'o_clerk': 'Clerk#000009506', 'o_shippriority': 0, 'o_comment': 'nstructions sleep furiously among '}\n",
      "{'o_orderkey': 2, 'o_custkey': 780017, 'o_orderstatus': 'O', 'o_totalprice': 66219.63, 'o_orderdate': datetime.date(1996, 12, 1), 'o_orderpriority': '1-URGENT', 'o_clerk': 'Clerk#000008792', 'o_shippriority': 0, 'o_comment': ' foxes. pending accounts at the pending, silent asymptot'}\n",
      "{'o_orderkey': 3, 'o_custkey': 1233140, 'o_orderstatus': 'F', 'o_totalprice': 270741.97, 'o_orderdate': datetime.date(1993, 10, 14), 'o_orderpriority': '5-LOW', 'o_clerk': 'Clerk#000009543', 'o_shippriority': 0, 'o_comment': 'sly final accounts boost. carefully regular ideas cajole carefully. depos'}\n"
     ]
    }
   ],
   "source": [
    "# Read TPC-H High-Volume Transactional Data (Orders + Line Items)\n",
    "\n",
    "# Read Orders table (header information)\n",
    "orders_ds = ray.data.read_parquet(f\"{TPCH_S3_PATH}/orders\")\n",
    "\n",
    "orders_ds = (orders_ds\n",
    "    .select_columns([f\"column{i}\" for i in range(9)])\n",
    "    .rename_columns([\n",
    "        \"o_orderkey\",\n",
    "        \"o_custkey\",\n",
    "        \"o_orderstatus\",\n",
    "        \"o_totalprice\",\n",
    "        \"o_orderdate\",\n",
    "        \"o_orderpriority\",\n",
    "        \"o_clerk\",\n",
    "        \"o_shippriority\",\n",
    "        \"o_comment\",\n",
    "    ])\n",
    ")\n",
    "\n",
    "print(\"TPC-H Orders Data (Enterprise Transaction Processing):\")\n",
    "print(\"Sample order records:\")\n",
    "orders_ds.limit(10).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08962a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:38:53,780\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_362_0\n",
      "2025-08-28 00:38:53,788\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_362_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:38:53,789\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_362_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> LimitOperator[limit=1] -> TaskPoolMapOperator[ReadFiles]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:38:55,331\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_362_0 execution finished in 1.54 seconds\n",
      "2025-08-28 00:38:55,338\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_364_0\n",
      "2025-08-28 00:38:55,345\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_364_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:38:55,345\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_364_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> LimitOperator[limit=1] -> TaskPoolMapOperator[ReadFiles]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TPC-H Line Items Data (Detailed Transaction Processing):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:38:56,747\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_364_0 execution finished in 1.40 seconds\n",
      "2025-08-28 00:38:56,753\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_365_0\n",
      "2025-08-28 00:38:56,761\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_365_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:38:56,762\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_365_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[MapBatches(count_rows)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Schema: Column           Type\n",
      "------           ----\n",
      "l_orderkey       int64\n",
      "l_partkey        int64\n",
      "l_suppkey        int64\n",
      "l_linenumber     int64\n",
      "l_quantity       int64\n",
      "l_extendedprice  double\n",
      "l_discount       double\n",
      "l_tax            double\n",
      "l_returnflag     string\n",
      "l_linestatus     string\n",
      "l_shipdate       date32[day]\n",
      "l_commitdate     date32[day]\n",
      "l_receiptdate    date32[day]\n",
      "l_shipinstruct   string\n",
      "l_shipmode       string\n",
      "l_comment        string\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:38:57,257\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_365_0 execution finished in 0.50 seconds\n",
      "2025-08-28 00:38:57,264\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_366_0\n",
      "2025-08-28 00:38:57,271\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_366_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:38:57,272\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_366_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> LimitOperator[limit=3] -> TaskPoolMapOperator[ReadFiles]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total line items: 59,986,052\n",
      "\n",
      " Sample line item records:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:38:59,309\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_366_0 execution finished in 2.04 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'l_orderkey': 6000001, 'l_partkey': 1278570, 'l_suppkey': 53607, 'l_linenumber': 1, 'l_quantity': 45, 'l_extendedprice': 69682.95, 'l_discount': 0.09, 'l_tax': 0.05, 'l_returnflag': 'N', 'l_linestatus': 'O', 'l_shipdate': datetime.date(1998, 10, 20), 'l_commitdate': datetime.date(1998, 9, 10), 'l_receiptdate': datetime.date(1998, 11, 15), 'l_shipinstruct': 'COLLECT COD', 'l_shipmode': 'SHIP', 'l_comment': 'es haggle blithely above the silent ac'}\n",
      "{'l_orderkey': 6000001, 'l_partkey': 921150, 'l_suppkey': 96178, 'l_linenumber': 2, 'l_quantity': 47, 'l_extendedprice': 55042.17, 'l_discount': 0.08, 'l_tax': 0.02, 'l_returnflag': 'N', 'l_linestatus': 'O', 'l_shipdate': datetime.date(1998, 9, 8), 'l_commitdate': datetime.date(1998, 8, 31), 'l_receiptdate': datetime.date(1998, 9, 15), 'l_shipinstruct': 'COLLECT COD', 'l_shipmode': 'AIR', 'l_comment': 'counts. furio'}\n",
      "{'l_orderkey': 6000002, 'l_partkey': 1832878, 'l_suppkey': 7933, 'l_linenumber': 1, 'l_quantity': 13, 'l_extendedprice': 23540.14, 'l_discount': 0.01, 'l_tax': 0.07, 'l_returnflag': 'N', 'l_linestatus': 'O', 'l_shipdate': datetime.date(1996, 10, 29), 'l_commitdate': datetime.date(1996, 11, 29), 'l_receiptdate': datetime.date(1996, 10, 31), 'l_shipinstruct': 'DELIVER IN PERSON', 'l_shipmode': 'AIR', 'l_comment': 'ackages haggle slyly. bold, exp'}\n"
     ]
    }
   ],
   "source": [
    "# Read Line Items table (detailed transaction data - largest table in TPC-H)\n",
    "lineitem_ds = ray.data.read_parquet(f\"{TPCH_S3_PATH}/lineitem\")\n",
    "\n",
    "lineitem_cols = [f\"column{str(i).zfill(2)}\" for i in range(16)]\n",
    "lineitem_ds = (lineitem_ds\n",
    "    .select_columns(lineitem_cols)\n",
    "    .rename_columns([\n",
    "        \"l_orderkey\",\n",
    "        \"l_partkey\",\n",
    "        \"l_suppkey\",\n",
    "        \"l_linenumber\",\n",
    "        \"l_quantity\",\n",
    "        \"l_extendedprice\",\n",
    "        \"l_discount\",\n",
    "        \"l_tax\",\n",
    "        \"l_returnflag\",\n",
    "        \"l_linestatus\",\n",
    "        \"l_shipdate\",\n",
    "        \"l_commitdate\",\n",
    "        \"l_receiptdate\",\n",
    "        \"l_shipinstruct\",\n",
    "        \"l_shipmode\",\n",
    "        \"l_comment\",\n",
    "    ])\n",
    ")\n",
    "#lineitem_ds = lineitem_ds.limit(10000)\n",
    "\n",
    "print(f\"TPC-H Line Items Data (Detailed Transaction Processing):\")\n",
    "print(f\"Schema: {lineitem_ds.schema()}\")\n",
    "print(f\"Total line items: {lineitem_ds.count():,}\")\n",
    "\n",
    "print(\"\\n Sample line item records:\")\n",
    "lineitem_ds.limit(25).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca038c3e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Part 4: Transform - Processing Data\n",
    "\n",
    "The **Transform** phase is where the real data processing happens. Ray Data provides several transformation operations that can be applied to datasets, and understanding how they work under the hood is key to building efficient ETL pipelines that power modern AI applications.\n",
    "\n",
    "### Transformations for the AI Era\n",
    "\n",
    "Modern AI workloads require more than traditional data transformations. Ray Data is designed for the era of **compound AI systems** and **agentic workflows** where:\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> AI-Powered Transformations:</b>\n",
    "<ul>\n",
    "    <li><b>Multimodal Processing:</b> Simultaneously process text, images, video, and audio</li>\n",
    "    <li><b>Model Inference:</b> Embed ML models directly into transformation pipelines</li>\n",
    "    <li><b>GPU Acceleration:</b> Seamlessly utilize both CPU and GPU resources</li>\n",
    "    <li><b>Compound AI:</b> Orchestrate multiple models and traditional ML within single workflows</li>\n",
    "    <li><b>AI-Enhanced ETL:</b> Use AI to optimize every aspect of data processing</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "### How Ray Data Processes Transformations\n",
    "\n",
    "When you apply transformations with Ray Data:\n",
    "\n",
    "1. **Task Distribution**: Transformations are distributed across Ray tasks/actors\n",
    "2. **Block-level Processing**: Each task processes one or more blocks independently  \n",
    "3. **Streaming Execution**: Blocks flow through the pipeline without waiting for all data\n",
    "4. **Operator Fusion**: Compatible operations are automatically combined for efficiency\n",
    "5. **Heterogeneous Compute**: Intelligently schedules CPU and GPU work\n",
    "6. **Fault Tolerance**: Failed tasks are automatically retried\n",
    "\n",
    "This architecture enables Ray Data to handle everything from traditional business logic to cutting-edge AI inference within the same pipeline.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Transformation Categories:</b>\n",
    "<ul>\n",
    "    <li><b>Row-wise operations:</b> <code>map()</code> - Transform individual rows</li>\n",
    "    <li><b>Batch operations:</b> <code>map_batches()</code> - Transform groups of rows (ideal for ML inference)</li>\n",
    "    <li><b>Filtering:</b> <code>filter()</code> - Remove rows based on conditions</li>\n",
    "    <li><b>Aggregations:</b> <code>groupby()</code> - Group and aggregate data</li>\n",
    "    <li><b>Joins:</b> <code>join()</code> - Combine datasets</li>\n",
    "    <li><b>AI Operations:</b> Embed models for inference, embeddings, and feature extraction</li>\n",
    "    <li><b>Shuffling:</b> <code>random_shuffle()</code>, <code>sort()</code> - Reorder data</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "While most other data frameworks support similar map operations through UDFs, with Ray Data, these are treated as first-class supported features. Instead of just arbitrary operations sent to partitions, Ray Data has several key advantages:\n",
    "- Each task can have task-level concurrency and hardware allocation set instead of just global settings for all UDFs.\n",
    "- These tasks support PyArrow, pandas, and NumPy format, which provides easy integrations to the rest of the Python ecosystem.\n",
    "- These tasks support stateful actors, which supports initializing expensive steps like downloading an AI model, only once per replica instead of per-invocation.\n",
    "- For more advanced use cases, Ray Core can be run inside of the task, supporting nested parallelism algorithms. This is useful for HPC-style applications where we need to have complicated compute tasks on top of big data.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> GPU Optimizations:</b>\n",
    "<ul>\n",
    "    <li><b>Nvidia RAPIDS:</b> <code>map_batches()</code> - Pandas ETL operations can be sped up using the Nvidia cuDF library to run the slower sections of ETL logic onto GPUs.</li>\n",
    "    <li><b>Batch Inference:</b> <code>map_batches()</code> - GPU AI batch inference can be used for unstructured data ingestion or LLM processing, amongst many other use cases</li>\n",
    "    <li><b>AI Training:</b> - Many data pipelines, such as time series analysis, train many small models over sections of the data. These smaller ML models, such as XGBoost models, can be trained using GPUs for faster performance</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "In this tutorial, we are going to focus on traditional CPU-based ETL workloads, but there are other templates available for batch inference using GPUs if you are interested in learning further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c2aedd",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Practical ETL Transformations\n",
    "\n",
    "Let's implement common ETL transformations using our e-commerce data:\n",
    "\n",
    "#### 1. Data Enrichment with Business Logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab097e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:39:20,752\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_377_0\n",
      "2025-08-28 00:39:20,760\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_377_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:39:20,761\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_377_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> LimitOperator[limit=1000] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(traditional_etl_enrichment_tpch)] -> LimitOperator[limit=25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional ETL Results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:39:26,971\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_377_0 execution finished in 6.21 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>o_orderkey</th>\n",
       "      <th>o_custkey</th>\n",
       "      <th>o_orderstatus</th>\n",
       "      <th>o_totalprice</th>\n",
       "      <th>o_orderdate</th>\n",
       "      <th>o_orderpriority</th>\n",
       "      <th>o_clerk</th>\n",
       "      <th>o_shippriority</th>\n",
       "      <th>o_comment</th>\n",
       "      <th>order_year</th>\n",
       "      <th>...</th>\n",
       "      <th>quarter_name</th>\n",
       "      <th>month_name</th>\n",
       "      <th>revenue_tier</th>\n",
       "      <th>priority_weight</th>\n",
       "      <th>weighted_revenue</th>\n",
       "      <th>is_urgent</th>\n",
       "      <th>is_large_order</th>\n",
       "      <th>requires_expedited_processing</th>\n",
       "      <th>days_to_process</th>\n",
       "      <th>is_peak_season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4423681</td>\n",
       "      <td>325555</td>\n",
       "      <td>O</td>\n",
       "      <td>59347.44</td>\n",
       "      <td>1996-12-08</td>\n",
       "      <td>1-URGENT</td>\n",
       "      <td>Clerk#000000386</td>\n",
       "      <td>0</td>\n",
       "      <td>ng the accounts. slyly express pinto beans sleep</td>\n",
       "      <td>1996</td>\n",
       "      <td>...</td>\n",
       "      <td>Q4</td>\n",
       "      <td>December</td>\n",
       "      <td>Medium</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59347.440</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1803</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4423682</td>\n",
       "      <td>1075774</td>\n",
       "      <td>F</td>\n",
       "      <td>48526.32</td>\n",
       "      <td>1993-06-03</td>\n",
       "      <td>2-HIGH</td>\n",
       "      <td>Clerk#000006158</td>\n",
       "      <td>0</td>\n",
       "      <td>deposits. special requests integrate blithely...</td>\n",
       "      <td>1993</td>\n",
       "      <td>...</td>\n",
       "      <td>Q2</td>\n",
       "      <td>June</td>\n",
       "      <td>Small</td>\n",
       "      <td>0.8</td>\n",
       "      <td>38821.056</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>519</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4423683</td>\n",
       "      <td>1015111</td>\n",
       "      <td>F</td>\n",
       "      <td>211218.00</td>\n",
       "      <td>1994-09-03</td>\n",
       "      <td>1-URGENT</td>\n",
       "      <td>Clerk#000008441</td>\n",
       "      <td>0</td>\n",
       "      <td>ackages. carefully express pains boost. bold d...</td>\n",
       "      <td>1994</td>\n",
       "      <td>...</td>\n",
       "      <td>Q3</td>\n",
       "      <td>September</td>\n",
       "      <td>Large</td>\n",
       "      <td>1.0</td>\n",
       "      <td>211218.000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>976</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4423684</td>\n",
       "      <td>1448251</td>\n",
       "      <td>F</td>\n",
       "      <td>73936.26</td>\n",
       "      <td>1994-06-24</td>\n",
       "      <td>5-LOW</td>\n",
       "      <td>Clerk#000007302</td>\n",
       "      <td>0</td>\n",
       "      <td>fully around the unusual accounts. care</td>\n",
       "      <td>1994</td>\n",
       "      <td>...</td>\n",
       "      <td>Q2</td>\n",
       "      <td>June</td>\n",
       "      <td>Medium</td>\n",
       "      <td>0.2</td>\n",
       "      <td>14787.252</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>905</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4423685</td>\n",
       "      <td>224824</td>\n",
       "      <td>O</td>\n",
       "      <td>60869.04</td>\n",
       "      <td>1995-12-04</td>\n",
       "      <td>4-NOT SPECIFIED</td>\n",
       "      <td>Clerk#000005898</td>\n",
       "      <td>0</td>\n",
       "      <td>pending accounts integrate q</td>\n",
       "      <td>1995</td>\n",
       "      <td>...</td>\n",
       "      <td>Q4</td>\n",
       "      <td>December</td>\n",
       "      <td>Medium</td>\n",
       "      <td>0.4</td>\n",
       "      <td>24347.616</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1433</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4423686</td>\n",
       "      <td>94375</td>\n",
       "      <td>O</td>\n",
       "      <td>55521.21</td>\n",
       "      <td>1998-04-11</td>\n",
       "      <td>1-URGENT</td>\n",
       "      <td>Clerk#000006090</td>\n",
       "      <td>0</td>\n",
       "      <td>o beans wake slyly along the even packa</td>\n",
       "      <td>1998</td>\n",
       "      <td>...</td>\n",
       "      <td>Q2</td>\n",
       "      <td>April</td>\n",
       "      <td>Medium</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55521.210</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2292</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4423687</td>\n",
       "      <td>637748</td>\n",
       "      <td>P</td>\n",
       "      <td>96743.73</td>\n",
       "      <td>1995-04-13</td>\n",
       "      <td>4-NOT SPECIFIED</td>\n",
       "      <td>Clerk#000003578</td>\n",
       "      <td>0</td>\n",
       "      <td>he silent, sly packages sleep accordi</td>\n",
       "      <td>1995</td>\n",
       "      <td>...</td>\n",
       "      <td>Q2</td>\n",
       "      <td>April</td>\n",
       "      <td>Medium</td>\n",
       "      <td>0.4</td>\n",
       "      <td>38697.492</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1198</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4423712</td>\n",
       "      <td>1115005</td>\n",
       "      <td>P</td>\n",
       "      <td>309802.49</td>\n",
       "      <td>1995-04-16</td>\n",
       "      <td>1-URGENT</td>\n",
       "      <td>Clerk#000009508</td>\n",
       "      <td>0</td>\n",
       "      <td>ounts. furiously bold accou</td>\n",
       "      <td>1995</td>\n",
       "      <td>...</td>\n",
       "      <td>Q2</td>\n",
       "      <td>April</td>\n",
       "      <td>Enterprise</td>\n",
       "      <td>1.0</td>\n",
       "      <td>309802.490</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1201</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4423713</td>\n",
       "      <td>364759</td>\n",
       "      <td>F</td>\n",
       "      <td>279348.18</td>\n",
       "      <td>1992-02-08</td>\n",
       "      <td>1-URGENT</td>\n",
       "      <td>Clerk#000005668</td>\n",
       "      <td>0</td>\n",
       "      <td>as. instructions about the quickly ironic foxe</td>\n",
       "      <td>1992</td>\n",
       "      <td>...</td>\n",
       "      <td>Q1</td>\n",
       "      <td>February</td>\n",
       "      <td>Large</td>\n",
       "      <td>1.0</td>\n",
       "      <td>279348.180</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>38</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4423714</td>\n",
       "      <td>4279</td>\n",
       "      <td>F</td>\n",
       "      <td>122838.22</td>\n",
       "      <td>1993-09-12</td>\n",
       "      <td>5-LOW</td>\n",
       "      <td>Clerk#000009293</td>\n",
       "      <td>0</td>\n",
       "      <td>ss accounts. blithe</td>\n",
       "      <td>1993</td>\n",
       "      <td>...</td>\n",
       "      <td>Q3</td>\n",
       "      <td>September</td>\n",
       "      <td>Medium</td>\n",
       "      <td>0.2</td>\n",
       "      <td>24567.644</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>620</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4423715</td>\n",
       "      <td>1391221</td>\n",
       "      <td>F</td>\n",
       "      <td>240515.93</td>\n",
       "      <td>1992-12-24</td>\n",
       "      <td>3-MEDIUM</td>\n",
       "      <td>Clerk#000002392</td>\n",
       "      <td>0</td>\n",
       "      <td>s; carefully bold packages solve slyly. specia...</td>\n",
       "      <td>1992</td>\n",
       "      <td>...</td>\n",
       "      <td>Q4</td>\n",
       "      <td>December</td>\n",
       "      <td>Large</td>\n",
       "      <td>0.6</td>\n",
       "      <td>144309.558</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>358</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4423716</td>\n",
       "      <td>236656</td>\n",
       "      <td>F</td>\n",
       "      <td>182232.10</td>\n",
       "      <td>1992-11-02</td>\n",
       "      <td>5-LOW</td>\n",
       "      <td>Clerk#000006940</td>\n",
       "      <td>0</td>\n",
       "      <td>regular pinto beans. regula</td>\n",
       "      <td>1992</td>\n",
       "      <td>...</td>\n",
       "      <td>Q4</td>\n",
       "      <td>November</td>\n",
       "      <td>Large</td>\n",
       "      <td>0.2</td>\n",
       "      <td>36446.420</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>306</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4423717</td>\n",
       "      <td>944855</td>\n",
       "      <td>O</td>\n",
       "      <td>55194.24</td>\n",
       "      <td>1996-10-05</td>\n",
       "      <td>1-URGENT</td>\n",
       "      <td>Clerk#000003250</td>\n",
       "      <td>0</td>\n",
       "      <td>elets! sly requests wake carefully</td>\n",
       "      <td>1996</td>\n",
       "      <td>...</td>\n",
       "      <td>Q4</td>\n",
       "      <td>October</td>\n",
       "      <td>Medium</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55194.240</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1739</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4423718</td>\n",
       "      <td>1163669</td>\n",
       "      <td>F</td>\n",
       "      <td>145746.18</td>\n",
       "      <td>1994-03-24</td>\n",
       "      <td>3-MEDIUM</td>\n",
       "      <td>Clerk#000006591</td>\n",
       "      <td>0</td>\n",
       "      <td>s wake furiously express excuses. regular, pen...</td>\n",
       "      <td>1994</td>\n",
       "      <td>...</td>\n",
       "      <td>Q1</td>\n",
       "      <td>March</td>\n",
       "      <td>Medium</td>\n",
       "      <td>0.6</td>\n",
       "      <td>87447.708</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>813</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4423719</td>\n",
       "      <td>773638</td>\n",
       "      <td>O</td>\n",
       "      <td>140131.93</td>\n",
       "      <td>1997-02-22</td>\n",
       "      <td>2-HIGH</td>\n",
       "      <td>Clerk#000000979</td>\n",
       "      <td>0</td>\n",
       "      <td>kages nag along the pending ideas. even, expre...</td>\n",
       "      <td>1997</td>\n",
       "      <td>...</td>\n",
       "      <td>Q1</td>\n",
       "      <td>February</td>\n",
       "      <td>Medium</td>\n",
       "      <td>0.8</td>\n",
       "      <td>112105.544</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1879</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4423744</td>\n",
       "      <td>506038</td>\n",
       "      <td>F</td>\n",
       "      <td>416004.61</td>\n",
       "      <td>1993-03-21</td>\n",
       "      <td>4-NOT SPECIFIED</td>\n",
       "      <td>Clerk#000008859</td>\n",
       "      <td>0</td>\n",
       "      <td>t pinto beans x-ray carefully furiously regula...</td>\n",
       "      <td>1993</td>\n",
       "      <td>...</td>\n",
       "      <td>Q1</td>\n",
       "      <td>March</td>\n",
       "      <td>Enterprise</td>\n",
       "      <td>0.4</td>\n",
       "      <td>166401.844</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>445</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4423745</td>\n",
       "      <td>1477240</td>\n",
       "      <td>O</td>\n",
       "      <td>227181.93</td>\n",
       "      <td>1995-09-30</td>\n",
       "      <td>2-HIGH</td>\n",
       "      <td>Clerk#000001033</td>\n",
       "      <td>0</td>\n",
       "      <td>ly whithout the final deposits;</td>\n",
       "      <td>1995</td>\n",
       "      <td>...</td>\n",
       "      <td>Q3</td>\n",
       "      <td>September</td>\n",
       "      <td>Large</td>\n",
       "      <td>0.8</td>\n",
       "      <td>181745.544</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1368</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4423746</td>\n",
       "      <td>1471403</td>\n",
       "      <td>O</td>\n",
       "      <td>153590.22</td>\n",
       "      <td>1995-06-25</td>\n",
       "      <td>5-LOW</td>\n",
       "      <td>Clerk#000008571</td>\n",
       "      <td>0</td>\n",
       "      <td>es. accounts wake furiously about the dep</td>\n",
       "      <td>1995</td>\n",
       "      <td>...</td>\n",
       "      <td>Q2</td>\n",
       "      <td>June</td>\n",
       "      <td>Large</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30718.044</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1271</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4423747</td>\n",
       "      <td>866546</td>\n",
       "      <td>O</td>\n",
       "      <td>24009.81</td>\n",
       "      <td>1997-06-09</td>\n",
       "      <td>5-LOW</td>\n",
       "      <td>Clerk#000008660</td>\n",
       "      <td>0</td>\n",
       "      <td>foxes. theodolites according to the furious, r</td>\n",
       "      <td>1997</td>\n",
       "      <td>...</td>\n",
       "      <td>Q2</td>\n",
       "      <td>June</td>\n",
       "      <td>Small</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4801.962</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1986</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4423748</td>\n",
       "      <td>528055</td>\n",
       "      <td>O</td>\n",
       "      <td>141626.74</td>\n",
       "      <td>1996-06-11</td>\n",
       "      <td>1-URGENT</td>\n",
       "      <td>Clerk#000000618</td>\n",
       "      <td>0</td>\n",
       "      <td>ly regular sentiments integrate unusual reques...</td>\n",
       "      <td>1996</td>\n",
       "      <td>...</td>\n",
       "      <td>Q2</td>\n",
       "      <td>June</td>\n",
       "      <td>Medium</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141626.740</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1623</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4423749</td>\n",
       "      <td>994843</td>\n",
       "      <td>F</td>\n",
       "      <td>107573.21</td>\n",
       "      <td>1992-07-30</td>\n",
       "      <td>2-HIGH</td>\n",
       "      <td>Clerk#000008555</td>\n",
       "      <td>0</td>\n",
       "      <td>its wake furiously blit</td>\n",
       "      <td>1992</td>\n",
       "      <td>...</td>\n",
       "      <td>Q3</td>\n",
       "      <td>July</td>\n",
       "      <td>Medium</td>\n",
       "      <td>0.8</td>\n",
       "      <td>86058.568</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>211</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4423750</td>\n",
       "      <td>1308635</td>\n",
       "      <td>O</td>\n",
       "      <td>52547.58</td>\n",
       "      <td>1997-07-25</td>\n",
       "      <td>5-LOW</td>\n",
       "      <td>Clerk#000003457</td>\n",
       "      <td>0</td>\n",
       "      <td>ffily ideas. stealthily even packages nag. car...</td>\n",
       "      <td>1997</td>\n",
       "      <td>...</td>\n",
       "      <td>Q3</td>\n",
       "      <td>July</td>\n",
       "      <td>Medium</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10509.516</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2032</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4423751</td>\n",
       "      <td>1216394</td>\n",
       "      <td>F</td>\n",
       "      <td>106017.64</td>\n",
       "      <td>1993-04-22</td>\n",
       "      <td>2-HIGH</td>\n",
       "      <td>Clerk#000007135</td>\n",
       "      <td>0</td>\n",
       "      <td>slyly. slyly slow sheaves sleep enticingly. pe...</td>\n",
       "      <td>1993</td>\n",
       "      <td>...</td>\n",
       "      <td>Q2</td>\n",
       "      <td>April</td>\n",
       "      <td>Medium</td>\n",
       "      <td>0.8</td>\n",
       "      <td>84814.112</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>477</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4423776</td>\n",
       "      <td>421148</td>\n",
       "      <td>O</td>\n",
       "      <td>107647.22</td>\n",
       "      <td>1997-07-08</td>\n",
       "      <td>3-MEDIUM</td>\n",
       "      <td>Clerk#000007691</td>\n",
       "      <td>0</td>\n",
       "      <td>pending, silent dolphins according to the furi...</td>\n",
       "      <td>1997</td>\n",
       "      <td>...</td>\n",
       "      <td>Q3</td>\n",
       "      <td>July</td>\n",
       "      <td>Medium</td>\n",
       "      <td>0.6</td>\n",
       "      <td>64588.332</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2015</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4423777</td>\n",
       "      <td>1229882</td>\n",
       "      <td>F</td>\n",
       "      <td>130557.34</td>\n",
       "      <td>1992-05-05</td>\n",
       "      <td>3-MEDIUM</td>\n",
       "      <td>Clerk#000009091</td>\n",
       "      <td>0</td>\n",
       "      <td>ickly. slyly silent instructions serve blithely</td>\n",
       "      <td>1992</td>\n",
       "      <td>...</td>\n",
       "      <td>Q2</td>\n",
       "      <td>May</td>\n",
       "      <td>Medium</td>\n",
       "      <td>0.6</td>\n",
       "      <td>78334.404</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>125</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25 rows \u00d7 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    o_orderkey  o_custkey o_orderstatus  o_totalprice o_orderdate  \\\n",
       "0      4423681     325555             O      59347.44  1996-12-08   \n",
       "1      4423682    1075774             F      48526.32  1993-06-03   \n",
       "2      4423683    1015111             F     211218.00  1994-09-03   \n",
       "3      4423684    1448251             F      73936.26  1994-06-24   \n",
       "4      4423685     224824             O      60869.04  1995-12-04   \n",
       "5      4423686      94375             O      55521.21  1998-04-11   \n",
       "6      4423687     637748             P      96743.73  1995-04-13   \n",
       "7      4423712    1115005             P     309802.49  1995-04-16   \n",
       "8      4423713     364759             F     279348.18  1992-02-08   \n",
       "9      4423714       4279             F     122838.22  1993-09-12   \n",
       "10     4423715    1391221             F     240515.93  1992-12-24   \n",
       "11     4423716     236656             F     182232.10  1992-11-02   \n",
       "12     4423717     944855             O      55194.24  1996-10-05   \n",
       "13     4423718    1163669             F     145746.18  1994-03-24   \n",
       "14     4423719     773638             O     140131.93  1997-02-22   \n",
       "15     4423744     506038             F     416004.61  1993-03-21   \n",
       "16     4423745    1477240             O     227181.93  1995-09-30   \n",
       "17     4423746    1471403             O     153590.22  1995-06-25   \n",
       "18     4423747     866546             O      24009.81  1997-06-09   \n",
       "19     4423748     528055             O     141626.74  1996-06-11   \n",
       "20     4423749     994843             F     107573.21  1992-07-30   \n",
       "21     4423750    1308635             O      52547.58  1997-07-25   \n",
       "22     4423751    1216394             F     106017.64  1993-04-22   \n",
       "23     4423776     421148             O     107647.22  1997-07-08   \n",
       "24     4423777    1229882             F     130557.34  1992-05-05   \n",
       "\n",
       "    o_orderpriority          o_clerk  o_shippriority  \\\n",
       "0          1-URGENT  Clerk#000000386               0   \n",
       "1            2-HIGH  Clerk#000006158               0   \n",
       "2          1-URGENT  Clerk#000008441               0   \n",
       "3             5-LOW  Clerk#000007302               0   \n",
       "4   4-NOT SPECIFIED  Clerk#000005898               0   \n",
       "5          1-URGENT  Clerk#000006090               0   \n",
       "6   4-NOT SPECIFIED  Clerk#000003578               0   \n",
       "7          1-URGENT  Clerk#000009508               0   \n",
       "8          1-URGENT  Clerk#000005668               0   \n",
       "9             5-LOW  Clerk#000009293               0   \n",
       "10         3-MEDIUM  Clerk#000002392               0   \n",
       "11            5-LOW  Clerk#000006940               0   \n",
       "12         1-URGENT  Clerk#000003250               0   \n",
       "13         3-MEDIUM  Clerk#000006591               0   \n",
       "14           2-HIGH  Clerk#000000979               0   \n",
       "15  4-NOT SPECIFIED  Clerk#000008859               0   \n",
       "16           2-HIGH  Clerk#000001033               0   \n",
       "17            5-LOW  Clerk#000008571               0   \n",
       "18            5-LOW  Clerk#000008660               0   \n",
       "19         1-URGENT  Clerk#000000618               0   \n",
       "20           2-HIGH  Clerk#000008555               0   \n",
       "21            5-LOW  Clerk#000003457               0   \n",
       "22           2-HIGH  Clerk#000007135               0   \n",
       "23         3-MEDIUM  Clerk#000007691               0   \n",
       "24         3-MEDIUM  Clerk#000009091               0   \n",
       "\n",
       "                                            o_comment  order_year  ...  \\\n",
       "0    ng the accounts. slyly express pinto beans sleep        1996  ...   \n",
       "1    deposits. special requests integrate blithely...        1993  ...   \n",
       "2   ackages. carefully express pains boost. bold d...        1994  ...   \n",
       "3             fully around the unusual accounts. care        1994  ...   \n",
       "4                        pending accounts integrate q        1995  ...   \n",
       "5             o beans wake slyly along the even packa        1998  ...   \n",
       "6               he silent, sly packages sleep accordi        1995  ...   \n",
       "7                         ounts. furiously bold accou        1995  ...   \n",
       "8      as. instructions about the quickly ironic foxe        1992  ...   \n",
       "9                                 ss accounts. blithe        1993  ...   \n",
       "10  s; carefully bold packages solve slyly. specia...        1992  ...   \n",
       "11                        regular pinto beans. regula        1992  ...   \n",
       "12                 elets! sly requests wake carefully        1996  ...   \n",
       "13  s wake furiously express excuses. regular, pen...        1994  ...   \n",
       "14  kages nag along the pending ideas. even, expre...        1997  ...   \n",
       "15  t pinto beans x-ray carefully furiously regula...        1993  ...   \n",
       "16                    ly whithout the final deposits;        1995  ...   \n",
       "17          es. accounts wake furiously about the dep        1995  ...   \n",
       "18     foxes. theodolites according to the furious, r        1997  ...   \n",
       "19  ly regular sentiments integrate unusual reques...        1996  ...   \n",
       "20                            its wake furiously blit        1992  ...   \n",
       "21  ffily ideas. stealthily even packages nag. car...        1997  ...   \n",
       "22  slyly. slyly slow sheaves sleep enticingly. pe...        1993  ...   \n",
       "23  pending, silent dolphins according to the furi...        1997  ...   \n",
       "24    ickly. slyly silent instructions serve blithely        1992  ...   \n",
       "\n",
       "    quarter_name  month_name  revenue_tier  priority_weight weighted_revenue  \\\n",
       "0             Q4    December        Medium              1.0        59347.440   \n",
       "1             Q2        June         Small              0.8        38821.056   \n",
       "2             Q3   September         Large              1.0       211218.000   \n",
       "3             Q2        June        Medium              0.2        14787.252   \n",
       "4             Q4    December        Medium              0.4        24347.616   \n",
       "5             Q2       April        Medium              1.0        55521.210   \n",
       "6             Q2       April        Medium              0.4        38697.492   \n",
       "7             Q2       April    Enterprise              1.0       309802.490   \n",
       "8             Q1    February         Large              1.0       279348.180   \n",
       "9             Q3   September        Medium              0.2        24567.644   \n",
       "10            Q4    December         Large              0.6       144309.558   \n",
       "11            Q4    November         Large              0.2        36446.420   \n",
       "12            Q4     October        Medium              1.0        55194.240   \n",
       "13            Q1       March        Medium              0.6        87447.708   \n",
       "14            Q1    February        Medium              0.8       112105.544   \n",
       "15            Q1       March    Enterprise              0.4       166401.844   \n",
       "16            Q3   September         Large              0.8       181745.544   \n",
       "17            Q2        June         Large              0.2        30718.044   \n",
       "18            Q2        June         Small              0.2         4801.962   \n",
       "19            Q2        June        Medium              1.0       141626.740   \n",
       "20            Q3        July        Medium              0.8        86058.568   \n",
       "21            Q3        July        Medium              0.2        10509.516   \n",
       "22            Q2       April        Medium              0.8        84814.112   \n",
       "23            Q3        July        Medium              0.6        64588.332   \n",
       "24            Q2         May        Medium              0.6        78334.404   \n",
       "\n",
       "   is_urgent is_large_order  requires_expedited_processing  days_to_process  \\\n",
       "0       True          False                           True             1803   \n",
       "1       True          False                           True              519   \n",
       "2       True           True                           True              976   \n",
       "3      False          False                          False              905   \n",
       "4      False          False                          False             1433   \n",
       "5       True          False                           True             2292   \n",
       "6      False          False                          False             1198   \n",
       "7       True           True                           True             1201   \n",
       "8       True           True                           True               38   \n",
       "9      False          False                          False              620   \n",
       "10     False           True                           True              358   \n",
       "11     False          False                          False              306   \n",
       "12      True          False                           True             1739   \n",
       "13     False          False                          False              813   \n",
       "14      True          False                           True             1879   \n",
       "15     False           True                           True              445   \n",
       "16      True           True                           True             1368   \n",
       "17     False          False                          False             1271   \n",
       "18     False          False                          False             1986   \n",
       "19      True          False                           True             1623   \n",
       "20      True          False                           True              211   \n",
       "21     False          False                          False             2032   \n",
       "22      True          False                           True              477   \n",
       "23     False          False                          False             2015   \n",
       "24     False          False                          False              125   \n",
       "\n",
       "    is_peak_season  \n",
       "0             True  \n",
       "1            False  \n",
       "2            False  \n",
       "3            False  \n",
       "4             True  \n",
       "5            False  \n",
       "6            False  \n",
       "7            False  \n",
       "8            False  \n",
       "9            False  \n",
       "10            True  \n",
       "11            True  \n",
       "12           False  \n",
       "13           False  \n",
       "14           False  \n",
       "15           False  \n",
       "16           False  \n",
       "17           False  \n",
       "18           False  \n",
       "19           False  \n",
       "20           False  \n",
       "21           False  \n",
       "22           False  \n",
       "23           False  \n",
       "24           False  \n",
       "\n",
       "[25 rows x 24 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def traditional_etl_enrichment_tpch(batch):\n",
    "    \"\"\"\n",
    "    Traditional ETL transformations for TPC-H business intelligence and reporting\n",
    "    This demonstrates classic data warehouse-style transformations on enterprise data\n",
    "    \"\"\"\n",
    "    #df = batch.to_pandas() if hasattr(batch, 'to_pandas') else pd.DataFrame(batch)\n",
    "    df = batch\n",
    "    \n",
    "    # Parse order date and create time dimensions (standard BI practice)\n",
    "    df['o_orderdate'] = pd.to_datetime(df['o_orderdate'])\n",
    "    df['order_year'] = df['o_orderdate'].dt.year\n",
    "    df['order_quarter'] = df['o_orderdate'].dt.quarter\n",
    "    df['order_month'] = df['o_orderdate'].dt.month\n",
    "    df['order_day_of_week'] = df['o_orderdate'].dt.dayofweek\n",
    "    \n",
    "    # Business day classifications (common in traditional ETL)\n",
    "    df['is_weekend'] = df['order_day_of_week'].isin([5, 6])\n",
    "    df['quarter_name'] = 'Q' + df['order_quarter'].astype(str)\n",
    "    df['month_name'] = df['o_orderdate'].dt.month_name()\n",
    "    \n",
    "    # Revenue and profit calculations (standard BI metrics)\n",
    "    df['revenue_tier'] = pd.cut(\n",
    "        df['o_totalprice'],\n",
    "        bins=[0, 50000, 150000, 300000, float('inf')],\n",
    "        labels=['Small', 'Medium', 'Large', 'Enterprise']\n",
    "    )\n",
    "    \n",
    "    # Order priority business rules (TPC-H specific)\n",
    "    priority_weights = {\n",
    "        '1-URGENT': 1.0,\n",
    "        '2-HIGH': 0.8,\n",
    "        '3-MEDIUM': 0.6,\n",
    "        '4-NOT SPECIFIED': 0.4,\n",
    "        '5-LOW': 0.2\n",
    "    }\n",
    "    df['priority_weight'] = df['o_orderpriority'].map(priority_weights).fillna(0.4)\n",
    "    df['weighted_revenue'] = df['o_totalprice'] * df['priority_weight']\n",
    "    \n",
    "    # Order status analysis\n",
    "    df['is_urgent'] = df['o_orderpriority'].isin(['1-URGENT', '2-HIGH'])\n",
    "    df['is_large_order'] = df['o_totalprice'] > 200000\n",
    "    df['requires_expedited_processing'] = df['is_urgent'] | df['is_large_order']\n",
    "    \n",
    "    # Date-based business logic\n",
    "    df['days_to_process'] = (pd.to_datetime(df['o_orderdate']) - pd.Timestamp('1992-01-01')).dt.days\n",
    "    df['is_peak_season'] = df['order_month'].isin([11, 12])  # Nov-Dec peak\n",
    "    \n",
    "    return df\n",
    "\n",
    "def ml_ready_feature_engineering_tpch(batch):\n",
    "    \"\"\"\n",
    "    Modern ML feature engineering for TPC-H data\n",
    "    This prepares enterprise data for machine learning models\n",
    "    \"\"\"\n",
    "    #df = batch.to_pandas() if hasattr(batch, 'to_pandas') else pd.DataFrame(batch)\n",
    "    df = batch\n",
    "    \n",
    "    # Temporal features for ML models\n",
    "    df['days_since_epoch'] = (df['o_orderdate'] - pd.Timestamp('1992-01-01')).dt.days\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['order_month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['order_month'] / 12)\n",
    "    df['quarter_sin'] = np.sin(2 * np.pi * df['order_quarter'] / 4)\n",
    "    df['quarter_cos'] = np.cos(2 * np.pi * df['order_quarter'] / 4)\n",
    "    \n",
    "    # Priority encoding for ML (one-hot style features)\n",
    "    for priority in ['1-URGENT', '2-HIGH', '3-MEDIUM']:\n",
    "        df[f'is_priority_{priority.split(\"-\")[0]}'] = (df['o_orderpriority'] == priority).astype(int)\n",
    "    \n",
    "    # Revenue-based features (common in ML)\n",
    "    df['log_total_price'] = np.log1p(df['o_totalprice'])  # Log transformation for ML\n",
    "    df['revenue_per_priority'] = df['o_totalprice'] * df['priority_weight']\n",
    "    df['weekend_large_order'] = (df['is_weekend'] & df['is_large_order']).astype(int)\n",
    "    \n",
    "    # Time-series features for predictive modeling\n",
    "    df['year_normalized'] = (df['order_year'] - df['order_year'].min()) / (df['order_year'].max() - df['order_year'].min())\n",
    "    df['seasonal_revenue_multiplier'] = np.where(df['is_peak_season'], 1.2, 1.0)\n",
    "    \n",
    "    # Customer key features (for customer analytics)\n",
    "    df['customer_id_mod_100'] = df['o_custkey'] % 100  # Simple customer segmentation feature\n",
    "    \n",
    "    return df\n",
    "\n",
    "traditional_enriched = orders_ds.map_batches(\n",
    "    traditional_etl_enrichment_tpch,\n",
    "    batch_format=\"pandas\",\n",
    "    batch_size=1024  # Larger batches for efficiency \n",
    ")\n",
    "\n",
    "print(\"Traditional ETL Results:\")\n",
    "traditional_enriched.limit(25).to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db2816c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:39:27,181\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_379_0\n",
      "2025-08-28 00:39:27,191\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_379_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:39:27,191\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_379_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> LimitOperator[limit=1000] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(traditional_etl_enrichment_tpch)->MapBatches(ml_ready_feature_engineering_tpch)] -> LimitOperator[limit=3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ML-Ready Feature Engineering (Next-Generation Capabilities):\n",
      "   Adding ML features for predictive analytics on enterprise transaction data...\n",
      "\n",
      " ML-Ready Data Sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:39:32,975\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_379_0 execution finished in 5.78 seconds\n",
      "2025-08-28 00:39:32,987\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_380_0\n",
      "2025-08-28 00:39:32,998\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_380_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:39:32,998\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_380_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> LimitOperator[limit=1000] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(traditional_etl_enrichment_tpch)->MapBatches(ml_ready_feature_engineering_tpch)] -> LimitOperator[limit=25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'o_orderkey': np.int64(24000001), 'o_custkey': np.int64(1494782), 'o_orderstatus': 'O', 'o_totalprice': np.float64(73193.66), 'o_orderdate': Timestamp('1997-03-03 00:00:00'), 'o_orderpriority': '1-URGENT', 'o_clerk': 'Clerk#000008224', 'o_shippriority': np.int64(0), 'o_comment': 'deas. carefully final pinto beans cajole a', 'order_year': np.int32(1997), 'order_quarter': np.int32(1), 'order_month': np.int32(3), 'order_day_of_week': np.int32(0), 'is_weekend': np.False_, 'quarter_name': 'Q1', 'month_name': 'March', 'revenue_tier': 'Medium', 'priority_weight': np.float64(1.0), 'weighted_revenue': np.float64(73193.66), 'is_urgent': np.True_, 'is_large_order': np.False_, 'requires_expedited_processing': np.True_, 'days_to_process': np.int64(1888), 'is_peak_season': np.False_, 'days_since_epoch': np.int64(1888), 'month_sin': np.float64(1.0), 'month_cos': np.float64(6.123233995736766e-17), 'quarter_sin': np.float64(1.0), 'quarter_cos': np.float64(6.123233995736766e-17), 'is_priority_1': np.int64(1), 'is_priority_2': np.int64(0), 'is_priority_3': np.int64(0), 'log_total_price': np.float64(11.20087774646869), 'revenue_per_priority': np.float64(73193.66), 'weekend_large_order': np.int64(0), 'year_normalized': np.float64(0.8333333333333334), 'seasonal_revenue_multiplier': np.float64(1.0), 'customer_id_mod_100': np.int64(82)}\n",
      "{'o_orderkey': np.int64(24000002), 'o_custkey': np.int64(793336), 'o_orderstatus': 'F', 'o_totalprice': np.float64(67691.93), 'o_orderdate': Timestamp('1993-01-21 00:00:00'), 'o_orderpriority': '1-URGENT', 'o_clerk': 'Clerk#000004358', 'o_shippriority': np.int64(0), 'o_comment': ' deposits. finally ironic ', 'order_year': np.int32(1993), 'order_quarter': np.int32(1), 'order_month': np.int32(1), 'order_day_of_week': np.int32(3), 'is_weekend': np.False_, 'quarter_name': 'Q1', 'month_name': 'January', 'revenue_tier': 'Medium', 'priority_weight': np.float64(1.0), 'weighted_revenue': np.float64(67691.93), 'is_urgent': np.True_, 'is_large_order': np.False_, 'requires_expedited_processing': np.True_, 'days_to_process': np.int64(386), 'is_peak_season': np.False_, 'days_since_epoch': np.int64(386), 'month_sin': np.float64(0.49999999999999994), 'month_cos': np.float64(0.8660254037844387), 'quarter_sin': np.float64(1.0), 'quarter_cos': np.float64(6.123233995736766e-17), 'is_priority_1': np.int64(1), 'is_priority_2': np.int64(0), 'is_priority_3': np.int64(0), 'log_total_price': np.float64(11.122737022132414), 'revenue_per_priority': np.float64(67691.93), 'weekend_large_order': np.int64(0), 'year_normalized': np.float64(0.16666666666666666), 'seasonal_revenue_multiplier': np.float64(1.0), 'customer_id_mod_100': np.int64(36)}\n",
      "{'o_orderkey': np.int64(24000003), 'o_custkey': np.int64(82214), 'o_orderstatus': 'O', 'o_totalprice': np.float64(59537.05), 'o_orderdate': Timestamp('1998-05-24 00:00:00'), 'o_orderpriority': '2-HIGH', 'o_clerk': 'Clerk#000008735', 'o_shippriority': np.int64(0), 'o_comment': 'ymptotes wake. furiously daring courts cajole slyly across the ironic Tir', 'order_year': np.int32(1998), 'order_quarter': np.int32(2), 'order_month': np.int32(5), 'order_day_of_week': np.int32(6), 'is_weekend': np.True_, 'quarter_name': 'Q2', 'month_name': 'May', 'revenue_tier': 'Medium', 'priority_weight': np.float64(0.8), 'weighted_revenue': np.float64(47629.64000000001), 'is_urgent': np.True_, 'is_large_order': np.False_, 'requires_expedited_processing': np.True_, 'days_to_process': np.int64(2335), 'is_peak_season': np.False_, 'days_since_epoch': np.int64(2335), 'month_sin': np.float64(0.49999999999999994), 'month_cos': np.float64(-0.8660254037844387), 'quarter_sin': np.float64(1.2246467991473532e-16), 'quarter_cos': np.float64(-1.0), 'is_priority_1': np.int64(0), 'is_priority_2': np.int64(1), 'is_priority_3': np.int64(0), 'log_total_price': np.float64(10.994370882941736), 'revenue_per_priority': np.float64(47629.64000000001), 'weekend_large_order': np.int64(0), 'year_normalized': np.float64(1.0), 'seasonal_revenue_multiplier': np.float64(1.0), 'customer_id_mod_100': np.int64(14)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:39:38,035\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_380_0 execution finished in 5.04 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>o_orderkey</th>\n",
       "      <th>o_custkey</th>\n",
       "      <th>o_orderstatus</th>\n",
       "      <th>o_totalprice</th>\n",
       "      <th>o_orderdate</th>\n",
       "      <th>o_orderpriority</th>\n",
       "      <th>o_clerk</th>\n",
       "      <th>o_shippriority</th>\n",
       "      <th>o_comment</th>\n",
       "      <th>order_year</th>\n",
       "      <th>...</th>\n",
       "      <th>quarter_cos</th>\n",
       "      <th>is_priority_1</th>\n",
       "      <th>is_priority_2</th>\n",
       "      <th>is_priority_3</th>\n",
       "      <th>log_total_price</th>\n",
       "      <th>revenue_per_priority</th>\n",
       "      <th>weekend_large_order</th>\n",
       "      <th>year_normalized</th>\n",
       "      <th>seasonal_revenue_multiplier</th>\n",
       "      <th>customer_id_mod_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4423681</td>\n",
       "      <td>325555</td>\n",
       "      <td>O</td>\n",
       "      <td>59347.44</td>\n",
       "      <td>1996-12-08</td>\n",
       "      <td>1-URGENT</td>\n",
       "      <td>Clerk#000000386</td>\n",
       "      <td>0</td>\n",
       "      <td>ng the accounts. slyly express pinto beans sleep</td>\n",
       "      <td>1996</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.991181</td>\n",
       "      <td>59347.440</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.2</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4423682</td>\n",
       "      <td>1075774</td>\n",
       "      <td>F</td>\n",
       "      <td>48526.32</td>\n",
       "      <td>1993-06-03</td>\n",
       "      <td>2-HIGH</td>\n",
       "      <td>Clerk#000006158</td>\n",
       "      <td>0</td>\n",
       "      <td>deposits. special requests integrate blithely...</td>\n",
       "      <td>1993</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10.789882</td>\n",
       "      <td>38821.056</td>\n",
       "      <td>0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4423683</td>\n",
       "      <td>1015111</td>\n",
       "      <td>F</td>\n",
       "      <td>211218.00</td>\n",
       "      <td>1994-09-03</td>\n",
       "      <td>1-URGENT</td>\n",
       "      <td>Clerk#000008441</td>\n",
       "      <td>0</td>\n",
       "      <td>ackages. carefully express pains boost. bold d...</td>\n",
       "      <td>1994</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.260651</td>\n",
       "      <td>211218.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4423684</td>\n",
       "      <td>1448251</td>\n",
       "      <td>F</td>\n",
       "      <td>73936.26</td>\n",
       "      <td>1994-06-24</td>\n",
       "      <td>5-LOW</td>\n",
       "      <td>Clerk#000007302</td>\n",
       "      <td>0</td>\n",
       "      <td>fully around the unusual accounts. care</td>\n",
       "      <td>1994</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.210972</td>\n",
       "      <td>14787.252</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4423685</td>\n",
       "      <td>224824</td>\n",
       "      <td>O</td>\n",
       "      <td>60869.04</td>\n",
       "      <td>1995-12-04</td>\n",
       "      <td>4-NOT SPECIFIED</td>\n",
       "      <td>Clerk#000005898</td>\n",
       "      <td>0</td>\n",
       "      <td>pending accounts integrate q</td>\n",
       "      <td>1995</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.016496</td>\n",
       "      <td>24347.616</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.2</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4423686</td>\n",
       "      <td>94375</td>\n",
       "      <td>O</td>\n",
       "      <td>55521.21</td>\n",
       "      <td>1998-04-11</td>\n",
       "      <td>1-URGENT</td>\n",
       "      <td>Clerk#000006090</td>\n",
       "      <td>0</td>\n",
       "      <td>o beans wake slyly along the even packa</td>\n",
       "      <td>1998</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.924538</td>\n",
       "      <td>55521.210</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4423687</td>\n",
       "      <td>637748</td>\n",
       "      <td>P</td>\n",
       "      <td>96743.73</td>\n",
       "      <td>1995-04-13</td>\n",
       "      <td>4-NOT SPECIFIED</td>\n",
       "      <td>Clerk#000003578</td>\n",
       "      <td>0</td>\n",
       "      <td>he silent, sly packages sleep accordi</td>\n",
       "      <td>1995</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.479831</td>\n",
       "      <td>38697.492</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4423712</td>\n",
       "      <td>1115005</td>\n",
       "      <td>P</td>\n",
       "      <td>309802.49</td>\n",
       "      <td>1995-04-16</td>\n",
       "      <td>1-URGENT</td>\n",
       "      <td>Clerk#000009508</td>\n",
       "      <td>0</td>\n",
       "      <td>ounts. furiously bold accou</td>\n",
       "      <td>1995</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.643693</td>\n",
       "      <td>309802.490</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4423713</td>\n",
       "      <td>364759</td>\n",
       "      <td>F</td>\n",
       "      <td>279348.18</td>\n",
       "      <td>1992-02-08</td>\n",
       "      <td>1-URGENT</td>\n",
       "      <td>Clerk#000005668</td>\n",
       "      <td>0</td>\n",
       "      <td>as. instructions about the quickly ironic foxe</td>\n",
       "      <td>1992</td>\n",
       "      <td>...</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.540218</td>\n",
       "      <td>279348.180</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4423714</td>\n",
       "      <td>4279</td>\n",
       "      <td>F</td>\n",
       "      <td>122838.22</td>\n",
       "      <td>1993-09-12</td>\n",
       "      <td>5-LOW</td>\n",
       "      <td>Clerk#000009293</td>\n",
       "      <td>0</td>\n",
       "      <td>ss accounts. blithe</td>\n",
       "      <td>1993</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.718632</td>\n",
       "      <td>24567.644</td>\n",
       "      <td>0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4423715</td>\n",
       "      <td>1391221</td>\n",
       "      <td>F</td>\n",
       "      <td>240515.93</td>\n",
       "      <td>1992-12-24</td>\n",
       "      <td>3-MEDIUM</td>\n",
       "      <td>Clerk#000002392</td>\n",
       "      <td>0</td>\n",
       "      <td>s; carefully bold packages solve slyly. specia...</td>\n",
       "      <td>1992</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.390546</td>\n",
       "      <td>144309.558</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.2</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4423716</td>\n",
       "      <td>236656</td>\n",
       "      <td>F</td>\n",
       "      <td>182232.10</td>\n",
       "      <td>1992-11-02</td>\n",
       "      <td>5-LOW</td>\n",
       "      <td>Clerk#000006940</td>\n",
       "      <td>0</td>\n",
       "      <td>regular pinto beans. regula</td>\n",
       "      <td>1992</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.113042</td>\n",
       "      <td>36446.420</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.2</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4423717</td>\n",
       "      <td>944855</td>\n",
       "      <td>O</td>\n",
       "      <td>55194.24</td>\n",
       "      <td>1996-10-05</td>\n",
       "      <td>1-URGENT</td>\n",
       "      <td>Clerk#000003250</td>\n",
       "      <td>0</td>\n",
       "      <td>elets! sly requests wake carefully</td>\n",
       "      <td>1996</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.918632</td>\n",
       "      <td>55194.240</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4423718</td>\n",
       "      <td>1163669</td>\n",
       "      <td>F</td>\n",
       "      <td>145746.18</td>\n",
       "      <td>1994-03-24</td>\n",
       "      <td>3-MEDIUM</td>\n",
       "      <td>Clerk#000006591</td>\n",
       "      <td>0</td>\n",
       "      <td>s wake furiously express excuses. regular, pen...</td>\n",
       "      <td>1994</td>\n",
       "      <td>...</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11.889629</td>\n",
       "      <td>87447.708</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4423719</td>\n",
       "      <td>773638</td>\n",
       "      <td>O</td>\n",
       "      <td>140131.93</td>\n",
       "      <td>1997-02-22</td>\n",
       "      <td>2-HIGH</td>\n",
       "      <td>Clerk#000000979</td>\n",
       "      <td>0</td>\n",
       "      <td>kages nag along the pending ideas. even, expre...</td>\n",
       "      <td>1997</td>\n",
       "      <td>...</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.850347</td>\n",
       "      <td>112105.544</td>\n",
       "      <td>0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4423744</td>\n",
       "      <td>506038</td>\n",
       "      <td>F</td>\n",
       "      <td>416004.61</td>\n",
       "      <td>1993-03-21</td>\n",
       "      <td>4-NOT SPECIFIED</td>\n",
       "      <td>Clerk#000008859</td>\n",
       "      <td>0</td>\n",
       "      <td>t pinto beans x-ray carefully furiously regula...</td>\n",
       "      <td>1993</td>\n",
       "      <td>...</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.938454</td>\n",
       "      <td>166401.844</td>\n",
       "      <td>1</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4423745</td>\n",
       "      <td>1477240</td>\n",
       "      <td>O</td>\n",
       "      <td>227181.93</td>\n",
       "      <td>1995-09-30</td>\n",
       "      <td>2-HIGH</td>\n",
       "      <td>Clerk#000001033</td>\n",
       "      <td>0</td>\n",
       "      <td>ly whithout the final deposits;</td>\n",
       "      <td>1995</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12.333511</td>\n",
       "      <td>181745.544</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4423746</td>\n",
       "      <td>1471403</td>\n",
       "      <td>O</td>\n",
       "      <td>153590.22</td>\n",
       "      <td>1995-06-25</td>\n",
       "      <td>5-LOW</td>\n",
       "      <td>Clerk#000008571</td>\n",
       "      <td>0</td>\n",
       "      <td>es. accounts wake furiously about the dep</td>\n",
       "      <td>1995</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.942050</td>\n",
       "      <td>30718.044</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4423747</td>\n",
       "      <td>866546</td>\n",
       "      <td>O</td>\n",
       "      <td>24009.81</td>\n",
       "      <td>1997-06-09</td>\n",
       "      <td>5-LOW</td>\n",
       "      <td>Clerk#000008660</td>\n",
       "      <td>0</td>\n",
       "      <td>foxes. theodolites according to the furious, r</td>\n",
       "      <td>1997</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.086259</td>\n",
       "      <td>4801.962</td>\n",
       "      <td>0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4423748</td>\n",
       "      <td>528055</td>\n",
       "      <td>O</td>\n",
       "      <td>141626.74</td>\n",
       "      <td>1996-06-11</td>\n",
       "      <td>1-URGENT</td>\n",
       "      <td>Clerk#000000618</td>\n",
       "      <td>0</td>\n",
       "      <td>ly regular sentiments integrate unusual reques...</td>\n",
       "      <td>1996</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.860957</td>\n",
       "      <td>141626.740</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4423749</td>\n",
       "      <td>994843</td>\n",
       "      <td>F</td>\n",
       "      <td>107573.21</td>\n",
       "      <td>1992-07-30</td>\n",
       "      <td>2-HIGH</td>\n",
       "      <td>Clerk#000008555</td>\n",
       "      <td>0</td>\n",
       "      <td>its wake furiously blit</td>\n",
       "      <td>1992</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.585936</td>\n",
       "      <td>86058.568</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4423750</td>\n",
       "      <td>1308635</td>\n",
       "      <td>O</td>\n",
       "      <td>52547.58</td>\n",
       "      <td>1997-07-25</td>\n",
       "      <td>5-LOW</td>\n",
       "      <td>Clerk#000003457</td>\n",
       "      <td>0</td>\n",
       "      <td>ffily ideas. stealthily even packages nag. car...</td>\n",
       "      <td>1997</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.869493</td>\n",
       "      <td>10509.516</td>\n",
       "      <td>0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4423751</td>\n",
       "      <td>1216394</td>\n",
       "      <td>F</td>\n",
       "      <td>106017.64</td>\n",
       "      <td>1993-04-22</td>\n",
       "      <td>2-HIGH</td>\n",
       "      <td>Clerk#000007135</td>\n",
       "      <td>0</td>\n",
       "      <td>slyly. slyly slow sheaves sleep enticingly. pe...</td>\n",
       "      <td>1993</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.571370</td>\n",
       "      <td>84814.112</td>\n",
       "      <td>0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4423776</td>\n",
       "      <td>421148</td>\n",
       "      <td>O</td>\n",
       "      <td>107647.22</td>\n",
       "      <td>1997-07-08</td>\n",
       "      <td>3-MEDIUM</td>\n",
       "      <td>Clerk#000007691</td>\n",
       "      <td>0</td>\n",
       "      <td>pending, silent dolphins according to the furi...</td>\n",
       "      <td>1997</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11.586624</td>\n",
       "      <td>64588.332</td>\n",
       "      <td>0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4423777</td>\n",
       "      <td>1229882</td>\n",
       "      <td>F</td>\n",
       "      <td>130557.34</td>\n",
       "      <td>1992-05-05</td>\n",
       "      <td>3-MEDIUM</td>\n",
       "      <td>Clerk#000009091</td>\n",
       "      <td>0</td>\n",
       "      <td>ickly. slyly silent instructions serve blithely</td>\n",
       "      <td>1992</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11.779575</td>\n",
       "      <td>78334.404</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25 rows \u00d7 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    o_orderkey  o_custkey o_orderstatus  o_totalprice o_orderdate  \\\n",
       "0      4423681     325555             O      59347.44  1996-12-08   \n",
       "1      4423682    1075774             F      48526.32  1993-06-03   \n",
       "2      4423683    1015111             F     211218.00  1994-09-03   \n",
       "3      4423684    1448251             F      73936.26  1994-06-24   \n",
       "4      4423685     224824             O      60869.04  1995-12-04   \n",
       "5      4423686      94375             O      55521.21  1998-04-11   \n",
       "6      4423687     637748             P      96743.73  1995-04-13   \n",
       "7      4423712    1115005             P     309802.49  1995-04-16   \n",
       "8      4423713     364759             F     279348.18  1992-02-08   \n",
       "9      4423714       4279             F     122838.22  1993-09-12   \n",
       "10     4423715    1391221             F     240515.93  1992-12-24   \n",
       "11     4423716     236656             F     182232.10  1992-11-02   \n",
       "12     4423717     944855             O      55194.24  1996-10-05   \n",
       "13     4423718    1163669             F     145746.18  1994-03-24   \n",
       "14     4423719     773638             O     140131.93  1997-02-22   \n",
       "15     4423744     506038             F     416004.61  1993-03-21   \n",
       "16     4423745    1477240             O     227181.93  1995-09-30   \n",
       "17     4423746    1471403             O     153590.22  1995-06-25   \n",
       "18     4423747     866546             O      24009.81  1997-06-09   \n",
       "19     4423748     528055             O     141626.74  1996-06-11   \n",
       "20     4423749     994843             F     107573.21  1992-07-30   \n",
       "21     4423750    1308635             O      52547.58  1997-07-25   \n",
       "22     4423751    1216394             F     106017.64  1993-04-22   \n",
       "23     4423776     421148             O     107647.22  1997-07-08   \n",
       "24     4423777    1229882             F     130557.34  1992-05-05   \n",
       "\n",
       "    o_orderpriority          o_clerk  o_shippriority  \\\n",
       "0          1-URGENT  Clerk#000000386               0   \n",
       "1            2-HIGH  Clerk#000006158               0   \n",
       "2          1-URGENT  Clerk#000008441               0   \n",
       "3             5-LOW  Clerk#000007302               0   \n",
       "4   4-NOT SPECIFIED  Clerk#000005898               0   \n",
       "5          1-URGENT  Clerk#000006090               0   \n",
       "6   4-NOT SPECIFIED  Clerk#000003578               0   \n",
       "7          1-URGENT  Clerk#000009508               0   \n",
       "8          1-URGENT  Clerk#000005668               0   \n",
       "9             5-LOW  Clerk#000009293               0   \n",
       "10         3-MEDIUM  Clerk#000002392               0   \n",
       "11            5-LOW  Clerk#000006940               0   \n",
       "12         1-URGENT  Clerk#000003250               0   \n",
       "13         3-MEDIUM  Clerk#000006591               0   \n",
       "14           2-HIGH  Clerk#000000979               0   \n",
       "15  4-NOT SPECIFIED  Clerk#000008859               0   \n",
       "16           2-HIGH  Clerk#000001033               0   \n",
       "17            5-LOW  Clerk#000008571               0   \n",
       "18            5-LOW  Clerk#000008660               0   \n",
       "19         1-URGENT  Clerk#000000618               0   \n",
       "20           2-HIGH  Clerk#000008555               0   \n",
       "21            5-LOW  Clerk#000003457               0   \n",
       "22           2-HIGH  Clerk#000007135               0   \n",
       "23         3-MEDIUM  Clerk#000007691               0   \n",
       "24         3-MEDIUM  Clerk#000009091               0   \n",
       "\n",
       "                                            o_comment  order_year  ...  \\\n",
       "0    ng the accounts. slyly express pinto beans sleep        1996  ...   \n",
       "1    deposits. special requests integrate blithely...        1993  ...   \n",
       "2   ackages. carefully express pains boost. bold d...        1994  ...   \n",
       "3             fully around the unusual accounts. care        1994  ...   \n",
       "4                        pending accounts integrate q        1995  ...   \n",
       "5             o beans wake slyly along the even packa        1998  ...   \n",
       "6               he silent, sly packages sleep accordi        1995  ...   \n",
       "7                         ounts. furiously bold accou        1995  ...   \n",
       "8      as. instructions about the quickly ironic foxe        1992  ...   \n",
       "9                                 ss accounts. blithe        1993  ...   \n",
       "10  s; carefully bold packages solve slyly. specia...        1992  ...   \n",
       "11                        regular pinto beans. regula        1992  ...   \n",
       "12                 elets! sly requests wake carefully        1996  ...   \n",
       "13  s wake furiously express excuses. regular, pen...        1994  ...   \n",
       "14  kages nag along the pending ideas. even, expre...        1997  ...   \n",
       "15  t pinto beans x-ray carefully furiously regula...        1993  ...   \n",
       "16                    ly whithout the final deposits;        1995  ...   \n",
       "17          es. accounts wake furiously about the dep        1995  ...   \n",
       "18     foxes. theodolites according to the furious, r        1997  ...   \n",
       "19  ly regular sentiments integrate unusual reques...        1996  ...   \n",
       "20                            its wake furiously blit        1992  ...   \n",
       "21  ffily ideas. stealthily even packages nag. car...        1997  ...   \n",
       "22  slyly. slyly slow sheaves sleep enticingly. pe...        1993  ...   \n",
       "23  pending, silent dolphins according to the furi...        1997  ...   \n",
       "24    ickly. slyly silent instructions serve blithely        1992  ...   \n",
       "\n",
       "     quarter_cos  is_priority_1  is_priority_2  is_priority_3 log_total_price  \\\n",
       "0   1.000000e+00              1              0              0       10.991181   \n",
       "1  -1.000000e+00              0              1              0       10.789882   \n",
       "2  -1.836970e-16              1              0              0       12.260651   \n",
       "3  -1.000000e+00              0              0              0       11.210972   \n",
       "4   1.000000e+00              0              0              0       11.016496   \n",
       "5  -1.000000e+00              1              0              0       10.924538   \n",
       "6  -1.000000e+00              0              0              0       11.479831   \n",
       "7  -1.000000e+00              1              0              0       12.643693   \n",
       "8   6.123234e-17              1              0              0       12.540218   \n",
       "9  -1.836970e-16              0              0              0       11.718632   \n",
       "10  1.000000e+00              0              0              1       12.390546   \n",
       "11  1.000000e+00              0              0              0       12.113042   \n",
       "12  1.000000e+00              1              0              0       10.918632   \n",
       "13  6.123234e-17              0              0              1       11.889629   \n",
       "14  6.123234e-17              0              1              0       11.850347   \n",
       "15  6.123234e-17              0              0              0       12.938454   \n",
       "16 -1.836970e-16              0              1              0       12.333511   \n",
       "17 -1.000000e+00              0              0              0       11.942050   \n",
       "18 -1.000000e+00              0              0              0       10.086259   \n",
       "19 -1.000000e+00              1              0              0       11.860957   \n",
       "20 -1.836970e-16              0              1              0       11.585936   \n",
       "21 -1.836970e-16              0              0              0       10.869493   \n",
       "22 -1.000000e+00              0              1              0       11.571370   \n",
       "23 -1.836970e-16              0              0              1       11.586624   \n",
       "24 -1.000000e+00              0              0              1       11.779575   \n",
       "\n",
       "   revenue_per_priority weekend_large_order  year_normalized  \\\n",
       "0             59347.440                   0         0.666667   \n",
       "1             38821.056                   0         0.166667   \n",
       "2            211218.000                   1         0.333333   \n",
       "3             14787.252                   0         0.333333   \n",
       "4             24347.616                   0         0.500000   \n",
       "5             55521.210                   0         1.000000   \n",
       "6             38697.492                   0         0.500000   \n",
       "7            309802.490                   1         0.500000   \n",
       "8            279348.180                   1         0.000000   \n",
       "9             24567.644                   0         0.166667   \n",
       "10           144309.558                   0         0.000000   \n",
       "11            36446.420                   0         0.000000   \n",
       "12            55194.240                   0         0.666667   \n",
       "13            87447.708                   0         0.333333   \n",
       "14           112105.544                   0         0.833333   \n",
       "15           166401.844                   1         0.166667   \n",
       "16           181745.544                   1         0.500000   \n",
       "17            30718.044                   0         0.500000   \n",
       "18             4801.962                   0         0.833333   \n",
       "19           141626.740                   0         0.666667   \n",
       "20            86058.568                   0         0.000000   \n",
       "21            10509.516                   0         0.833333   \n",
       "22            84814.112                   0         0.166667   \n",
       "23            64588.332                   0         0.833333   \n",
       "24            78334.404                   0         0.000000   \n",
       "\n",
       "    seasonal_revenue_multiplier  customer_id_mod_100  \n",
       "0                           1.2                   55  \n",
       "1                           1.0                   74  \n",
       "2                           1.0                   11  \n",
       "3                           1.0                   51  \n",
       "4                           1.2                   24  \n",
       "5                           1.0                   75  \n",
       "6                           1.0                   48  \n",
       "7                           1.0                    5  \n",
       "8                           1.0                   59  \n",
       "9                           1.0                   79  \n",
       "10                          1.2                   21  \n",
       "11                          1.2                   56  \n",
       "12                          1.0                   55  \n",
       "13                          1.0                   69  \n",
       "14                          1.0                   38  \n",
       "15                          1.0                   38  \n",
       "16                          1.0                   40  \n",
       "17                          1.0                    3  \n",
       "18                          1.0                   46  \n",
       "19                          1.0                   55  \n",
       "20                          1.0                   43  \n",
       "21                          1.0                   35  \n",
       "22                          1.0                   94  \n",
       "23                          1.0                   48  \n",
       "24                          1.0                   82  \n",
       "\n",
       "[25 rows x 38 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ML-Ready Feature Engineering (Preparing enterprise data for model training/inference)\n",
    "print(\"ML-Ready Feature Engineering (Next-Generation Capabilities):\")\n",
    "print(\"Adding ML features for predictive analytics on enterprise transaction data...\")\n",
    "\n",
    "enriched_orders = traditional_enriched.map_batches(\n",
    "    ml_ready_feature_engineering_tpch,\n",
    "    batch_format=\"pandas\",\n",
    "    batch_size=10000\n",
    ")\n",
    "enriched_orders.limit(25).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8400913b",
   "metadata": {},
   "source": [
    "### Understanding Operator Fusion in Ray Data\n",
    "\n",
    "Operator fusion is a key optimization technique that combines multiple operations into single tasks, reducing overhead and improving performance.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> What is Operator Fusion?</b><br>\n",
    "Operator fusion combines compatible operations into single tasks, reducing:\n",
    "<ul>\n",
    "    <li><b>Task Scheduling Overhead:</b> Fewer tasks to schedule and manage</li>\n",
    "    <li><b>Memory Transfers:</b> Intermediate results stay in memory</li>\n",
    "    <li><b>Network Communication:</b> Less data movement between nodes</li>\n",
    "    <li><b>Serialization Costs:</b> Fewer serialization/deserialization cycles</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "**When Operator Fusion Happens:**\n",
    "\n",
    "Ray Data automatically fuses operators when these conditions are met:\n",
    "\n",
    "1. **Compatible Resource Requirements:**\n",
    "   - Same CPU/GPU requirements\n",
    "   - Compatible memory usage patterns\n",
    "   - No conflicting resource constraints\n",
    "\n",
    "2. **Sequential Operations:**\n",
    "   - Operations that can be chained together\n",
    "   - No data dependencies that prevent fusion\n",
    "   - Compatible input/output formats\n",
    "\n",
    "3. **Compatible Execution Contexts:**\n",
    "   - Same execution environment\n",
    "   - Compatible error handling\n",
    "   - Similar retry policies\n",
    "\n",
    "**Operations That Fuse Well:**\n",
    "\n",
    "| Operation Type | Fuses With | Benefits |\n",
    "|----------------|------------|----------|\n",
    "| `map_batches()` | `map_batches()`, `filter()` | Reduce task overhead |\n",
    "| `filter()` | `map_batches()`, `select_columns()` | Memory efficiency |\n",
    "| `select_columns()` | `map_batches()`, `filter()` | Column pruning |\n",
    "| `map()` | `map()`, `filter()` | Simple transformations |\n",
    "| `flat_map()` | `map()`, `filter()` | Data reshaping |\n",
    "\n",
    "**Operations That Don't Fuse:**\n",
    "\n",
    "- **All-to-All Operations:** `groupby()`, `join()`, `repartition()`\n",
    "- **Materialization:** `take()`, `to_pandas()`, `write_*()`\n",
    "- **Cross-Node Operations:** Operations requiring data shuffling\n",
    "- **Resource-Intensive Operations:** GPU operations with CPU operations\n",
    "\n",
    "**Monitoring Operator Fusion:**\n",
    "\n",
    "```python\n",
    "# Check if operations are fused\n",
    "ds = ray.data.from_items([{\"id\": i, \"value\": i*2} for i in range(1000)])\n",
    "\n",
    "# Build a pipeline\n",
    "pipeline = (\n",
    "    ds\n",
    "    .map_batches(lambda batch: batch)  # Should fuse\n",
    "    .filter(lambda x: x[\"value\"] > 100)  # Should fuse\n",
    "    .map_batches(lambda batch: batch)  # Should fuse\n",
    ")\n",
    "\n",
    "# Check execution plan\n",
    "print(\"Execution plan:\")\n",
    "print(pipeline._plan)\n",
    "\n",
    "# Look for fused operators in the plan\n",
    "# Fused operations appear as single operators\n",
    "```\n",
    "\n",
    "**Forcing Fusion vs Preventing Fusion:**\n",
    "\n",
    "```python\n",
    "# Ray Data automatically fuses when beneficial\n",
    "# You generally don't need to manually control fusion\n",
    "\n",
    "# However, you can influence fusion through:\n",
    "# 1. Batch size consistency\n",
    "pipeline = (\n",
    "    ds\n",
    "    .map_batches(func1, batch_size=1000)  # Consistent batch size\n",
    "    .map_batches(func2, batch_size=1000)  # Helps fusion\n",
    ")\n",
    "\n",
    "# 2. Resource requirements\n",
    "pipeline = (\n",
    "    ds\n",
    "    .map_batches(cpu_func, num_cpus=1)    # CPU operation\n",
    "    .map_batches(cpu_func2, num_cpus=1)   # Fuses with CPU operation\n",
    ")\n",
    "\n",
    "# 3. Memory usage patterns\n",
    "pipeline = (\n",
    "    ds\n",
    "    .map_batches(low_memory_func)         # Low memory\n",
    "    .map_batches(low_memory_func2)        # Fuses with low memory\n",
    ")\n",
    "```\n",
    "\n",
    "**Performance Impact of Fusion:**\n",
    "\n",
    "| Scenario | Without Fusion | With Fusion | Improvement |\n",
    "|----------|----------------|-------------|-------------|\n",
    "| **Simple Pipeline** | 3 tasks | 1 task | 3x fewer tasks |\n",
    "| **Memory Transfer** | 2x data movement | 1x data movement | 2x less I/O |\n",
    "| **Scheduling Overhead** | High | Low | Significant reduction |\n",
    "| **End-to-End Latency** | Higher | Lower | 20-50% improvement |\n",
    "\n",
    "**Best Practices for Fusion:**\n",
    "\n",
    "1. **Design for Fusion:**\n",
    "   - Use consistent batch sizes\n",
    "   - Keep resource requirements similar\n",
    "   - Avoid unnecessary materialization\n",
    "\n",
    "2. **Monitor Fusion:**\n",
    "   - Check execution plans\n",
    "   - Use Ray dashboard to monitor task distribution\n",
    "   - Profile performance with and without fusion\n",
    "\n",
    "3. **Optimize for Fusion:**\n",
    "   - Group compatible operations together\n",
    "   - Avoid breaking fusion with materialization\n",
    "   - Use appropriate batch sizes\n",
    "\n",
    "**Example: Fusion in Action**\n",
    "\n",
    "```python\n",
    "# This pipeline will have multiple fused operations\n",
    "optimized_pipeline = (\n",
    "    ray.data.read_parquet(\"data.parquet\")\n",
    "    .filter(lambda x: x[\"value\"] > 0)           # Fuses with next operation\n",
    "    .map_batches(transform_batch)               # Fuses with previous and next\n",
    "    .select_columns([\"id\", \"value\", \"category\"]) # Fuses with previous\n",
    "    .map_batches(add_derived_fields)            # Fuses with previous\n",
    "    .filter(lambda x: x[\"category\"] == \"A\")     # Fuses with previous\n",
    ")\n",
    "\n",
    "# Check the execution plan to see fusion\n",
    "print(\"Fused execution plan:\")\n",
    "print(optimized_pipeline._plan)\n",
    "```\n",
    "\n",
    "**Troubleshooting Fusion Issues:**\n",
    "\n",
    "If operations aren't fusing as expected:\n",
    "\n",
    "1. **Check Resource Requirements:**\n",
    "   ```python\n",
    "   # Different resource requirements prevent fusion\n",
    "   ds.map_batches(cpu_func, num_cpus=1)      # CPU\n",
    "   ds.map_batches(gpu_func, num_gpus=1)      # GPU - won't fuse\n",
    "   ```\n",
    "\n",
    "2. **Check Batch Sizes:**\n",
    "   ```python\n",
    "   # Inconsistent batch sizes can prevent fusion\n",
    "   ds.map_batches(func1, batch_size=1000)    # 1000\n",
    "   ds.map_batches(func2, batch_size=500)     # 500 - may not fuse\n",
    "   ```\n",
    "\n",
    "3. **Check Data Dependencies:**\n",
    "   ```python\n",
    "   # Some operations inherently can't fuse\n",
    "   ds.groupby(\"category\")                    # All-to-all operation\n",
    "   ds.map_batches(transform)                 # Won't fuse with groupby\n",
    "   ```\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Fusion Optimization Summary:</b><br>\n",
    "Ray Data automatically optimizes your pipelines through operator fusion. Focus on writing clean, efficient code and let Ray Data handle the optimization. Monitor execution plans to understand how your operations are being fused and optimized.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a925b36",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### 2. Aggregations and Analytics\n",
    "\n",
    "Aggregations are essential for creating summary statistics and business metrics. Ray Data's `groupby()` operations distribute the computation across the cluster.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b> Under the Hood - GroupBy Operations:</b><br>\n",
    "When you perform a GroupBy operation, Ray Data:<br>\n",
    "<ol>\n",
    "    <li><b>Shuffle Phase:</b> Data is redistributed so all records with the same key end up on the same node</li>\n",
    "    <li><b>Local Aggregation:</b> Each node performs aggregation on its subset of data</li>\n",
    "    <li><b>Result Collection:</b> Final aggregated results are collected</li>\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "Let's make several new datasets by aggregating data together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9467e698",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:39:38,211\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_383_0\n",
      "2025-08-28 00:39:38,221\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_383_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:39:38,221\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_383_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> LimitOperator[limit=1000] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(traditional_etl_enrichment_tpch)->MapBatches(ml_ready_feature_engineering_tpch)] -> AllToAllOperator[Aggregate] -> LimitOperator[limit=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executive Dashboard (Traditional BI on TPC-H):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:40:01,086\tWARNING streaming_executor_state.py:790 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: order_quarter: int64\n",
      "count(): int64\n",
      "sum(o_totalprice): double\n",
      "mean(o_totalprice): double\n",
      "sum(weighted_revenue): double\n",
      "mean(is_urgent): double, new schema: None. This may lead to unexpected behavior.\n",
      "2025-08-28 00:40:01,123\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_383_0 execution finished in 22.90 seconds\n",
      "2025-08-28 00:40:01,130\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_385_0\n",
      "2025-08-28 00:40:01,139\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_385_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:40:01,139\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_385_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> LimitOperator[limit=1000] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(traditional_etl_enrichment_tpch)->MapBatches(ml_ready_feature_engineering_tpch)] -> AllToAllOperator[Aggregate] -> LimitOperator[limit=25] -> TaskPoolMapOperator[Project]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quarterly Business Performance:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:40:23,711\tWARNING streaming_executor_state.py:790 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: order_quarter: int64\n",
      "count(): int64\n",
      "sum(o_totalprice): double\n",
      "mean(o_totalprice): double\n",
      "sum(weighted_revenue): double\n",
      "mean(is_urgent): double, new schema: None. This may lead to unexpected behavior.\n",
      "2025-08-28 00:40:23,895\tWARNING streaming_executor_state.py:790 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: order_quarter: int64\n",
      "total_orders: int64\n",
      "total_revenue: double\n",
      "avg_order_value: double\n",
      "weighted_revenue: double\n",
      "urgent_order_percentage: double, new schema: PandasBlockSchema(names=[], types=[]). This may lead to unexpected behavior.\n",
      "2025-08-28 00:40:23,930\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_385_0 execution finished in 22.79 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_quarter</th>\n",
       "      <th>total_orders</th>\n",
       "      <th>total_revenue</th>\n",
       "      <th>avg_order_value</th>\n",
       "      <th>weighted_revenue</th>\n",
       "      <th>urgent_order_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3939930</td>\n",
       "      <td>5.954367e+11</td>\n",
       "      <td>151128.743367</td>\n",
       "      <td>3.573891e+11</td>\n",
       "      <td>0.399966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3971040</td>\n",
       "      <td>5.999666e+11</td>\n",
       "      <td>151085.499182</td>\n",
       "      <td>3.600278e+11</td>\n",
       "      <td>0.400345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3648472</td>\n",
       "      <td>5.509344e+11</td>\n",
       "      <td>151004.153324</td>\n",
       "      <td>3.307208e+11</td>\n",
       "      <td>0.400452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3440558</td>\n",
       "      <td>5.199605e+11</td>\n",
       "      <td>151126.804122</td>\n",
       "      <td>3.120457e+11</td>\n",
       "      <td>0.400078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   order_quarter  total_orders  total_revenue  avg_order_value  \\\n",
       "0              1       3939930   5.954367e+11    151128.743367   \n",
       "1              2       3971040   5.999666e+11    151085.499182   \n",
       "2              3       3648472   5.509344e+11    151004.153324   \n",
       "3              4       3440558   5.199605e+11    151126.804122   \n",
       "\n",
       "   weighted_revenue  urgent_order_percentage  \n",
       "0      3.573891e+11                 0.399966  \n",
       "1      3.600278e+11                 0.400345  \n",
       "2      3.307208e+11                 0.400452  \n",
       "3      3.120457e+11                 0.400078  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.data.aggregate import Count, Mean, Sum, Max\n",
    "\n",
    "#Executive Summary Dashboard - typical BI metrics on enterprise data\n",
    "print(\"Executive Dashboard (Traditional BI on TPC-H):\")\n",
    "executive_summary = (enriched_orders\n",
    "    .groupby(\"order_quarter\")\n",
    "    .aggregate(\n",
    "        Count(),\n",
    "        Sum(\"o_totalprice\"),\n",
    "        Mean(\"o_totalprice\"),\n",
    "        Sum(\"weighted_revenue\"),\n",
    "        Mean(\"is_urgent\"),\n",
    "    )\n",
    "    .rename_columns([\n",
    "        \"order_quarter\",\n",
    "        \"total_orders\",\n",
    "        \"total_revenue\",\n",
    "        \"avg_order_value\",\n",
    "        \"weighted_revenue\",\n",
    "        \"urgent_order_percentage\",\n",
    "    ])\n",
    ")\n",
    "\n",
    "print(\"Quarterly Business Performance:\")\n",
    "executive_summary.limit(25).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6ffea18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:40:24,067\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_388_0\n",
      "2025-08-28 00:40:24,082\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_388_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:40:24,083\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_388_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> LimitOperator[limit=1000] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(traditional_etl_enrichment_tpch)->MapBatches(ml_ready_feature_engineering_tpch)] -> AllToAllOperator[Aggregate] -> LimitOperator[limit=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operational Analytics (Enterprise Process Optimization):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:40:45,640\tWARNING streaming_executor_state.py:790 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: revenue_tier: string\n",
      "count(): int64\n",
      "sum(o_totalprice): double\n",
      "mean(priority_weight): double\n",
      "mean(requires_expedited_processing): double\n",
      "sum(is_peak_season): int64, new schema: None. This may lead to unexpected behavior.\n",
      "2025-08-28 00:40:45,689\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_388_0 execution finished in 21.61 seconds\n",
      "2025-08-28 00:40:45,696\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_390_0\n",
      "2025-08-28 00:40:45,704\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_390_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:40:45,705\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_390_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> LimitOperator[limit=1000] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(traditional_etl_enrichment_tpch)->MapBatches(ml_ready_feature_engineering_tpch)] -> AllToAllOperator[Aggregate] -> LimitOperator[limit=25] -> TaskPoolMapOperator[Project]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance by Revenue Tier:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:41:07,447\tWARNING streaming_executor_state.py:790 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: revenue_tier: string\n",
      "count(): int64\n",
      "sum(o_totalprice): double\n",
      "mean(priority_weight): double\n",
      "mean(requires_expedited_processing): double\n",
      "sum(is_peak_season): int64, new schema: None. This may lead to unexpected behavior.\n",
      "2025-08-28 00:41:07,626\tWARNING streaming_executor_state.py:790 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: revenue_tier: string\n",
      "order_volume: int64\n",
      "total_revenue: double\n",
      "avg_priority_weight: double\n",
      "expedited_processing_rate: double\n",
      "peak_season_orders: int64, new schema: PandasBlockSchema(names=[], types=[]). This may lead to unexpected behavior.\n",
      "2025-08-28 00:41:07,655\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_390_0 execution finished in 21.95 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>revenue_tier</th>\n",
       "      <th>order_volume</th>\n",
       "      <th>total_revenue</th>\n",
       "      <th>avg_priority_weight</th>\n",
       "      <th>expedited_processing_rate</th>\n",
       "      <th>peak_season_orders</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Enterprise</td>\n",
       "      <td>854969</td>\n",
       "      <td>2.868012e+11</td>\n",
       "      <td>0.600440</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>130174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Large</td>\n",
       "      <td>6322157</td>\n",
       "      <td>1.351880e+12</td>\n",
       "      <td>0.600089</td>\n",
       "      <td>0.745465</td>\n",
       "      <td>962126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Medium</td>\n",
       "      <td>5706652</td>\n",
       "      <td>5.680665e+11</td>\n",
       "      <td>0.600149</td>\n",
       "      <td>0.400155</td>\n",
       "      <td>867669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Small</td>\n",
       "      <td>2116222</td>\n",
       "      <td>5.955084e+10</td>\n",
       "      <td>0.600166</td>\n",
       "      <td>0.400108</td>\n",
       "      <td>322330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  revenue_tier  order_volume  total_revenue  avg_priority_weight  \\\n",
       "0   Enterprise        854969   2.868012e+11             0.600440   \n",
       "1        Large       6322157   1.351880e+12             0.600089   \n",
       "2       Medium       5706652   5.680665e+11             0.600149   \n",
       "3        Small       2116222   5.955084e+10             0.600166   \n",
       "\n",
       "   expedited_processing_rate  peak_season_orders  \n",
       "0                   1.000000              130174  \n",
       "1                   0.745465              962126  \n",
       "2                   0.400155              867669  \n",
       "3                   0.400108              322330  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Operational Analytics - business process optimization\n",
    "print(\"Operational Analytics (Enterprise Process Optimization):\")\n",
    "operational_metrics = (enriched_orders\n",
    "    .groupby(\"revenue_tier\")\n",
    "    .aggregate(\n",
    "        Count(),\n",
    "        Sum(\"o_totalprice\"),\n",
    "        Mean(\"priority_weight\"),\n",
    "        Mean(\"requires_expedited_processing\"),\n",
    "        Sum(\"is_peak_season\"),\n",
    "    )\n",
    "    .rename_columns([\n",
    "        \"revenue_tier\",\n",
    "        \"order_volume\",\n",
    "        \"total_revenue\",\n",
    "        \"avg_priority_weight\",\n",
    "        \"expedited_processing_rate\",\n",
    "        \"peak_season_orders\",\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Uncomment for viewing dataset output\n",
    "print(\"Performance by Revenue Tier:\")\n",
    "operational_metrics.limit(25).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ff11085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:41:07,751\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_393_0\n",
      "2025-08-28 00:41:07,760\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_393_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:41:07,761\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_393_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> LimitOperator[limit=1000] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(traditional_etl_enrichment_tpch)->MapBatches(ml_ready_feature_engineering_tpch)] -> AllToAllOperator[Aggregate] -> LimitOperator[limit=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Priority-Based Analysis (Order Management Insights):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:41:29,471\tWARNING streaming_executor_state.py:790 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: o_orderpriority: string\n",
      "count(): int64\n",
      "sum(o_totalprice): double\n",
      "mean(o_totalprice): double\n",
      "mean(is_large_order): double\n",
      "mean(is_weekend): double, new schema: None. This may lead to unexpected behavior.\n",
      "2025-08-28 00:41:29,508\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_393_0 execution finished in 21.75 seconds\n",
      "2025-08-28 00:41:29,514\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_396_0\n",
      "2025-08-28 00:41:29,524\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_396_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:41:29,524\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_396_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> LimitOperator[limit=1000] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(traditional_etl_enrichment_tpch)->MapBatches(ml_ready_feature_engineering_tpch)] -> AllToAllOperator[Aggregate] -> TaskPoolMapOperator[Project] -> AllToAllOperator[Sort] -> LimitOperator[limit=25]\n",
      "2025-08-28 00:41:50,905\tWARNING streaming_executor_state.py:790 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: o_orderpriority: string\n",
      "count(): int64\n",
      "sum(o_totalprice): double\n",
      "mean(o_totalprice): double\n",
      "mean(is_large_order): double\n",
      "mean(is_weekend): double, new schema: None. This may lead to unexpected behavior.\n",
      "2025-08-28 00:41:51,040\tWARNING streaming_executor_state.py:790 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: o_orderpriority: string\n",
      "priority_orders: int64\n",
      "priority_revenue: double\n",
      "avg_order_value: double\n",
      "large_order_rate: double\n",
      "weekend_order_rate: double, new schema: PandasBlockSchema(names=[], types=[]). This may lead to unexpected behavior.\n",
      "2025-08-28 00:41:51,414\tWARNING streaming_executor_state.py:790 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: PandasBlockSchema(names=['o_orderpriority', 'priority_orders', 'priority_revenue', 'avg_order_value', 'large_order_rate', 'weekend_order_rate'], types=[dtype('O'), dtype('int64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64')]), new schema: PandasBlockSchema(names=[], types=[]). This may lead to unexpected behavior.\n",
      "2025-08-28 00:41:51,489\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_396_0 execution finished in 21.96 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>o_orderpriority</th>\n",
       "      <th>priority_orders</th>\n",
       "      <th>priority_revenue</th>\n",
       "      <th>avg_order_value</th>\n",
       "      <th>large_order_rate</th>\n",
       "      <th>weekend_order_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-URGENT</td>\n",
       "      <td>3003093</td>\n",
       "      <td>4.537567e+11</td>\n",
       "      <td>151096.441152</td>\n",
       "      <td>0.299924</td>\n",
       "      <td>0.286203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2-HIGH</td>\n",
       "      <td>3000061</td>\n",
       "      <td>4.534886e+11</td>\n",
       "      <td>151159.783341</td>\n",
       "      <td>0.300221</td>\n",
       "      <td>0.285980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4-NOT SPECIFIED</td>\n",
       "      <td>3000260</td>\n",
       "      <td>4.532757e+11</td>\n",
       "      <td>151078.819384</td>\n",
       "      <td>0.299571</td>\n",
       "      <td>0.286167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3-MEDIUM</td>\n",
       "      <td>2998940</td>\n",
       "      <td>4.529252e+11</td>\n",
       "      <td>151028.424989</td>\n",
       "      <td>0.299468</td>\n",
       "      <td>0.286202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5-LOW</td>\n",
       "      <td>2997646</td>\n",
       "      <td>4.528520e+11</td>\n",
       "      <td>151069.216228</td>\n",
       "      <td>0.299619</td>\n",
       "      <td>0.285374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   o_orderpriority  priority_orders  priority_revenue  avg_order_value  \\\n",
       "0         1-URGENT          3003093      4.537567e+11    151096.441152   \n",
       "1           2-HIGH          3000061      4.534886e+11    151159.783341   \n",
       "2  4-NOT SPECIFIED          3000260      4.532757e+11    151078.819384   \n",
       "3         3-MEDIUM          2998940      4.529252e+11    151028.424989   \n",
       "4            5-LOW          2997646      4.528520e+11    151069.216228   \n",
       "\n",
       "   large_order_rate  weekend_order_rate  \n",
       "0          0.299924            0.286203  \n",
       "1          0.300221            0.285980  \n",
       "2          0.299571            0.286167  \n",
       "3          0.299468            0.286202  \n",
       "4          0.299619            0.285374  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Priority-Based Analysis - enterprise order management\n",
    "print(\"\\n Priority-Based Analysis (Order Management Insights):\")\n",
    "priority_performance = (enriched_orders\n",
    "    .groupby(\"o_orderpriority\")\n",
    "    .aggregate(\n",
    "        Count(),\n",
    "        Sum(\"o_totalprice\"),\n",
    "        Mean(\"o_totalprice\"),\n",
    "        Mean(\"is_large_order\"),\n",
    "        Mean(\"is_weekend\"),\n",
    "    )\n",
    "    .rename_columns([\n",
    "        \"o_orderpriority\",\n",
    "        \"priority_orders\",\n",
    "        \"priority_revenue\",\n",
    "        \"avg_order_value\",\n",
    "        \"large_order_rate\",\n",
    "        \"weekend_order_rate\",\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Uncomment for viewing dataset output\n",
    "# print(\"Performance by Order Priority:\")\n",
    "priority_performance.sort(\"priority_revenue\", descending=True).limit(25).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4049636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:41:51,602\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_399_0\n",
      "2025-08-28 00:41:51,611\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_399_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:41:51,611\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_399_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> LimitOperator[limit=1000] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(traditional_etl_enrichment_tpch)->MapBatches(ml_ready_feature_engineering_tpch)] -> AllToAllOperator[Aggregate] -> LimitOperator[limit=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Analysis (Time-Series Business Intelligence):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:42:12,674\tWARNING streaming_executor_state.py:790 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: order_year: int64\n",
      "count(): int64\n",
      "sum(o_totalprice): double\n",
      "mean(o_totalprice): double\n",
      "mean(is_peak_season): double\n",
      "mean(is_large_order): double, new schema: None. This may lead to unexpected behavior.\n",
      "2025-08-28 00:42:12,710\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_399_0 execution finished in 21.10 seconds\n",
      "2025-08-28 00:42:12,717\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_402_0\n",
      "2025-08-28 00:42:12,726\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_402_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:42:12,727\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_402_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> LimitOperator[limit=1000] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(traditional_etl_enrichment_tpch)->MapBatches(ml_ready_feature_engineering_tpch)] -> AllToAllOperator[Aggregate] -> TaskPoolMapOperator[Project] -> AllToAllOperator[Sort] -> LimitOperator[limit=25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year-over-Year Performance:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:42:34,273\tWARNING streaming_executor_state.py:790 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: order_year: int64\n",
      "count(): int64\n",
      "sum(o_totalprice): double\n",
      "mean(o_totalprice): double\n",
      "mean(is_peak_season): double\n",
      "mean(is_large_order): double, new schema: None. This may lead to unexpected behavior.\n",
      "2025-08-28 00:42:34,415\tWARNING streaming_executor_state.py:790 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: order_year: int64\n",
      "yearly_orders: int64\n",
      "yearly_revenue: double\n",
      "avg_order_value: double\n",
      "peak_season_rate: double\n",
      "large_order_percentage: double, new schema: PandasBlockSchema(names=[], types=[]). This may lead to unexpected behavior.\n",
      "2025-08-28 00:42:34,873\tWARNING streaming_executor_state.py:790 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: PandasBlockSchema(names=['order_year', 'yearly_orders', 'yearly_revenue', 'avg_order_value', 'peak_season_rate', 'large_order_percentage'], types=[dtype('int64'), dtype('int64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64')]), new schema: PandasBlockSchema(names=[], types=[]). This may lead to unexpected behavior.\n",
      "2025-08-28 00:42:34,944\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_402_0 execution finished in 22.21 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_year</th>\n",
       "      <th>yearly_orders</th>\n",
       "      <th>yearly_revenue</th>\n",
       "      <th>avg_order_value</th>\n",
       "      <th>peak_season_rate</th>\n",
       "      <th>large_order_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1992</td>\n",
       "      <td>2281205</td>\n",
       "      <td>3.444725e+11</td>\n",
       "      <td>151004.621657</td>\n",
       "      <td>0.167060</td>\n",
       "      <td>0.299643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1993</td>\n",
       "      <td>2276638</td>\n",
       "      <td>3.440619e+11</td>\n",
       "      <td>151127.204812</td>\n",
       "      <td>0.167049</td>\n",
       "      <td>0.299833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1994</td>\n",
       "      <td>2275919</td>\n",
       "      <td>3.440890e+11</td>\n",
       "      <td>151186.860472</td>\n",
       "      <td>0.167508</td>\n",
       "      <td>0.300342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1995</td>\n",
       "      <td>2275575</td>\n",
       "      <td>3.437713e+11</td>\n",
       "      <td>151070.064800</td>\n",
       "      <td>0.166866</td>\n",
       "      <td>0.299505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1996</td>\n",
       "      <td>2281938</td>\n",
       "      <td>3.447880e+11</td>\n",
       "      <td>151094.386277</td>\n",
       "      <td>0.166771</td>\n",
       "      <td>0.299821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1997</td>\n",
       "      <td>2275511</td>\n",
       "      <td>3.436590e+11</td>\n",
       "      <td>151024.990596</td>\n",
       "      <td>0.166723</td>\n",
       "      <td>0.299462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1998</td>\n",
       "      <td>1333214</td>\n",
       "      <td>2.014564e+11</td>\n",
       "      <td>151105.820581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.299689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   order_year  yearly_orders  yearly_revenue  avg_order_value  \\\n",
       "0        1992        2281205    3.444725e+11    151004.621657   \n",
       "1        1993        2276638    3.440619e+11    151127.204812   \n",
       "2        1994        2275919    3.440890e+11    151186.860472   \n",
       "3        1995        2275575    3.437713e+11    151070.064800   \n",
       "4        1996        2281938    3.447880e+11    151094.386277   \n",
       "5        1997        2275511    3.436590e+11    151024.990596   \n",
       "6        1998        1333214    2.014564e+11    151105.820581   \n",
       "\n",
       "   peak_season_rate  large_order_percentage  \n",
       "0          0.167060                0.299643  \n",
       "1          0.167049                0.299833  \n",
       "2          0.167508                0.300342  \n",
       "3          0.166866                0.299505  \n",
       "4          0.166771                0.299821  \n",
       "5          0.166723                0.299462  \n",
       "6          0.000000                0.299689  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Temporal Business Analysis - time-series insights\n",
    "print(\"Temporal Analysis (Time-Series Business Intelligence):\")\n",
    "temporal_intelligence = (enriched_orders\n",
    "    .groupby(\"order_year\")\n",
    "    .aggregate(\n",
    "        Count(),\n",
    "        Sum(\"o_totalprice\"),\n",
    "        Mean(\"o_totalprice\"),\n",
    "        Mean(\"is_peak_season\"),\n",
    "        Mean(\"is_large_order\"),\n",
    "    )\n",
    "    .rename_columns([\n",
    "        \"order_year\",\n",
    "        \"yearly_orders\",\n",
    "        \"yearly_revenue\",\n",
    "        \"avg_order_value\",\n",
    "        \"peak_season_rate\",\n",
    "        \"large_order_percentage\",\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Uncomment for viewing dataset output\n",
    "print(\"Year-over-Year Performance:\")\n",
    "temporal_intelligence.sort(\"order_year\").limit(25).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be63c5e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Part 5: Load - Writing Data\n",
    "\n",
    "The **Load** phase involves writing the processed data to destination systems. Ray Data supports writing to various formats and destinations, and understanding how this works helps you optimize for your use case.\n",
    "\n",
    "### How Ray Data Writes Data\n",
    "\n",
    "When you write data with Ray Data:\n",
    "\n",
    "1. **Parallel Writing**: Multiple tasks write data simultaneously across the cluster\n",
    "2. **Partitioned Output**: Data is written as multiple files (one per block typically)\n",
    "3. **Format Optimization**: Ray Data optimizes the writing process for each format\n",
    "4. **Streaming Writes**: Large datasets can be written without loading everything into memory\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Supported Output Formats:</b>\n",
    "<ul>\n",
    "    <li><b>Files:</b> Parquet, CSV, JSON</li>\n",
    "    <li><b>Databases:</b> MongoDB, MySQL, PostgreSQL</li>\n",
    "    <li><b>Cloud Storage:</b> S3, GCS, Azure Blob Storage</li>\n",
    "    <li><b>Lakehouse Formats:</b> Delta Lake (coming soon), Iceberg, Hudi</li>\n",
    "    <li><b>Custom:</b> Implement your own writers using <code>FileBasedDatasource</code></li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1eb17987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:42:35,135\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_404_0\n",
      "2025-08-28 00:42:35,148\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_404_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:42:35,149\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_404_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> LimitOperator[limit=1000] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(traditional_etl_enrichment_tpch)->MapBatches(ml_ready_feature_engineering_tpch)->Write]\n",
      "2025-08-28 00:42:35,170\tWARNING progress_bar.py:120 -- Truncating long operator name to 100 characters. To disable this behavior, set `ray.data.DataContext.get_current().DEFAULT_ENABLE_PROGRESS_BAR_NAME_TRUNCATION = False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Writing TPC-H processed data to various formats...\n",
      " Writing enriched TPC-H orders to Parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:43:02,396\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_404_0 execution finished in 27.25 seconds\n",
      "2025-08-28 00:43:02,448\tINFO dataset.py:4871 -- Data sink Parquet finished. 15000000 rows and 8.0GB data written.\n",
      "2025-08-28 00:43:02,454\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_407_0\n",
      "2025-08-28 00:43:02,474\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_407_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:43:02,475\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_407_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> LimitOperator[limit=1000] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(traditional_etl_enrichment_tpch)->MapBatches(ml_ready_feature_engineering_tpch)] -> AllToAllOperator[Aggregate] -> TaskPoolMapOperator[Project->Write]\n",
      "2025-08-28 00:43:23,835\tWARNING streaming_executor_state.py:790 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: order_quarter: int64\n",
      "count(): int64\n",
      "sum(o_totalprice): double\n",
      "mean(o_totalprice): double\n",
      "sum(weighted_revenue): double\n",
      "mean(is_urgent): double, new schema: None. This may lead to unexpected behavior.\n",
      "2025-08-28 00:43:23,994\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_407_0 execution finished in 21.52 seconds\n",
      "2025-08-28 00:43:24,033\tINFO dataset.py:4871 -- Data sink Parquet finished. 4 rows and 8.7KB data written.\n",
      "2025-08-28 00:43:24,039\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_410_0\n",
      "2025-08-28 00:43:24,053\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_410_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:43:24,054\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_410_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> LimitOperator[limit=1000] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(traditional_etl_enrichment_tpch)->MapBatches(ml_ready_feature_engineering_tpch)] -> AllToAllOperator[Aggregate] -> TaskPoolMapOperator[Project->Write]\n",
      "2025-08-28 00:43:45,353\tWARNING streaming_executor_state.py:790 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: revenue_tier: string\n",
      "count(): int64\n",
      "sum(o_totalprice): double\n",
      "mean(priority_weight): double\n",
      "mean(requires_expedited_processing): double\n",
      "sum(is_peak_season): int64, new schema: None. This may lead to unexpected behavior.\n",
      "2025-08-28 00:43:45,512\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_410_0 execution finished in 21.46 seconds\n",
      "2025-08-28 00:43:45,556\tINFO dataset.py:4871 -- Data sink Parquet finished. 4 rows and 9.1KB data written.\n",
      "2025-08-28 00:43:45,565\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_413_0\n",
      "2025-08-28 00:43:45,579\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_413_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:43:45,580\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_413_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> LimitOperator[limit=1000] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(traditional_etl_enrichment_tpch)->MapBatches(ml_ready_feature_engineering_tpch)] -> AllToAllOperator[Aggregate] -> TaskPoolMapOperator[Project->Write]\n",
      "2025-08-28 00:44:07,091\tWARNING streaming_executor_state.py:790 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: o_orderpriority: string\n",
      "count(): int64\n",
      "sum(o_totalprice): double\n",
      "mean(o_totalprice): double\n",
      "mean(is_large_order): double\n",
      "mean(is_weekend): double, new schema: None. This may lead to unexpected behavior.\n",
      "2025-08-28 00:44:07,253\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_413_0 execution finished in 21.67 seconds\n",
      "2025-08-28 00:44:07,294\tINFO dataset.py:4871 -- Data sink Parquet finished. 5 rows and 8.6KB data written.\n",
      "2025-08-28 00:44:07,300\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_416_0\n",
      "2025-08-28 00:44:07,313\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_416_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:44:07,314\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_416_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> LimitOperator[limit=1000] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(traditional_etl_enrichment_tpch)->MapBatches(ml_ready_feature_engineering_tpch)] -> AllToAllOperator[Aggregate] -> TaskPoolMapOperator[Project->Write]\n",
      "2025-08-28 00:44:28,398\tWARNING streaming_executor_state.py:790 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: order_year: int64\n",
      "count(): int64\n",
      "sum(o_totalprice): double\n",
      "mean(o_totalprice): double\n",
      "mean(is_peak_season): double\n",
      "mean(is_large_order): double, new schema: None. This may lead to unexpected behavior.\n",
      "2025-08-28 00:44:28,553\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_416_0 execution finished in 21.24 seconds\n",
      "2025-08-28 00:44:28,594\tINFO dataset.py:4871 -- Data sink Parquet finished. 7 rows and 8.4KB data written.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +10m44s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n"
     ]
    }
   ],
   "source": [
    "# Create output directories for TPC-H processed data\n",
    "import os\n",
    "OUTPUT_PATH = \"/mnt/cluster_storage/tpch_etl_output\"\n",
    "\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "print(\" Writing TPC-H processed data to various formats...\")\n",
    "\n",
    "# Write enriched TPC-H orders to Parquet (best for large enterprise datasets)\n",
    "print(\" Writing enriched TPC-H orders to Parquet...\")\n",
    "enriched_orders.write_parquet(f\"{OUTPUT_PATH}/enriched_orders\")\n",
    "executive_summary.write_parquet(f\"{OUTPUT_PATH}/executive_summary\")\n",
    "operational_metrics.write_parquet(f\"{OUTPUT_PATH}/operational_metrics\")\n",
    "priority_performance.write_parquet(f\"{OUTPUT_PATH}/priority_performance\")\n",
    "temporal_intelligence.write_parquet(f\"{OUTPUT_PATH}/temporal_intelligence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab5b9cc",
   "metadata": {},
   "source": [
    "## Performance Monitoring and Best Practices\n",
    "\n",
    "### Step 1: Monitoring Ray Data Performance\n",
    "\n",
    "**Key Performance Metrics to Track:**\n",
    "- **Throughput**: Records processed per second\n",
    "- **Memory Usage**: Peak and average memory consumption\n",
    "- **CPU Utilization**: Core usage across the cluster\n",
    "- **I/O Performance**: Read/write speeds and network usage\n",
    "- **Task Execution Time**: Individual operation performance\n",
    "\n",
    "### Step 2: Performance Optimization Techniques\n",
    "\n",
    "**Batch Size Tuning:**\n",
    "```python\n",
    "# Optimal batch sizes for different operations\n",
    "optimal_batch_sizes = {\n",
    "    'map_batches': 1000,      # Good for most transformations\n",
    "    'filter': 10000,          # Can handle larger batches\n",
    "    'groupby': 5000,          # Balance between memory and parallelism\n",
    "    'join': 2000              # Smaller batches for complex joins\n",
    "}\n",
    "```\n",
    "\n",
    "**Resource Allocation:**\n",
    "```python\n",
    "# Configure resources for different workloads\n",
    "ray.data.DataContext.get_current().execution_options = {\n",
    "    'concurrency': 4,         # Number of parallel tasks\n",
    "    'batch_size': 1000,       # Records per batch\n",
    "    'num_cpus': 1,           # CPUs per task\n",
    "    'num_gpus': 0            # GPUs per task (if needed)\n",
    "}\n",
    "```\n",
    "\n",
    "### Step 3: Production Best Practices\n",
    "\n",
    "**Error Handling:**\n",
    "- Implement comprehensive try-catch blocks\n",
    "- Use retry mechanisms for transient failures\n",
    "- Log errors with sufficient context for debugging\n",
    "\n",
    "**Resource Management:**\n",
    "- Always call `ray.shutdown()` when done\n",
    "- Monitor memory usage to prevent OOM errors\n",
    "- Use streaming operations for large datasets\n",
    "\n",
    "**Monitoring and Alerting:**\n",
    "- Set up Ray dashboard monitoring\n",
    "- Configure alerts for high memory usage\n",
    "- Track processing times and throughput\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544cb23a",
   "metadata": {},
   "source": [
    "## Troubleshooting and Production Considerations\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "**Issue 1: Out of Memory Errors**\n",
    "```python\n",
    "# Problem: Dataset too large for available memory\n",
    "# Solution: Use streaming operations\n",
    "ds = ray.data.read_parquet(\"large_file.parquet\")\n",
    "# Instead of: result = ds.to_pandas()  # This loads everything into memory\n",
    "# Use: result = ds.take(1000)  # This streams data\n",
    "```\n",
    "\n",
    "**Issue 2: Slow Performance**\n",
    "```python\n",
    "# Problem: Operations running slowly\n",
    "# Solutions:\n",
    "# 1. Increase concurrency\n",
    "ds.map_batches(func, concurrency=8)\n",
    "\n",
    "# 2. Optimize batch size\n",
    "ds.map_batches(func, batch_size=2000)\n",
    "\n",
    "# 3. Use native Ray Data operations when possible\n",
    "ds.filter(lambda x: x[\"value\"] > 100)  # Instead of map_batches for simple filters\n",
    "```\n",
    "\n",
    "**Issue 3: Task Failures**\n",
    "```python\n",
    "# Problem: Tasks failing due to errors\n",
    "# Solution: Add error handling\n",
    "def robust_transform(batch):\n",
    "    try:\n",
    "        # Your transformation logic\n",
    "        return transformed_batch\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Transform failed: {e}\")\n",
    "        return []  # Return empty batch to continue processing\n",
    "```\n",
    "\n",
    "### Production Deployment Checklist\n",
    "\n",
    "**Pre-Deployment:**\n",
    "- [ ] **Resource Planning**: Calculate memory and CPU requirements\n",
    "- [ ] **Error Handling**: Implement comprehensive error handling\n",
    "- [ ] **Logging**: Set up structured logging with appropriate levels\n",
    "- [ ] **Monitoring**: Configure Ray dashboard and external monitoring\n",
    "- [ ] **Testing**: Test with production-scale data volumes\n",
    "\n",
    "**Deployment:**\n",
    "- [ ] **Environment Variables**: Use environment variables for configuration\n",
    "- [ ] **Secrets Management**: Secure handling of database credentials and API keys\n",
    "- [ ] **Resource Limits**: Set appropriate CPU and memory limits\n",
    "- [ ] **Health Checks**: Implement health check endpoints\n",
    "- [ ] **Graceful Shutdown**: Handle shutdown signals properly\n",
    "\n",
    "**Post-Deployment:**\n",
    "- [ ] **Performance Monitoring**: Track key metrics continuously\n",
    "- [ ] **Alert Configuration**: Set up alerts for critical issues\n",
    "- [ ] **Log Analysis**: Regular analysis of logs for issues\n",
    "- [ ] **Capacity Planning**: Monitor resource usage trends\n",
    "- [ ] **Backup Strategy**: Implement data backup and recovery procedures\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**What You've Learned:**\n",
    "1. **Ray Data Fundamentals**: Understanding datasets, blocks, and lazy evaluation\n",
    "2. **ETL Pipeline Building**: Complete extract, transform, load workflows\n",
    "3. **Performance Optimization**: Tuning batch sizes, concurrency, and resources\n",
    "4. **Production Readiness**: Error handling, monitoring, and deployment best practices\n",
    "5. **Real-World Applications**: Processing enterprise-scale data efficiently\n",
    "\n",
    "**Next Steps:**\n",
    "1. **Explore Advanced Features**: Window functions, streaming data, GPU acceleration\n",
    "2. **Integrate with AI Workloads**: Multimodal data processing, model training pipelines\n",
    "3. **Deploy to Production**: Use Anyscale platform for enterprise deployments\n",
    "4. **Join the Community**: Connect with other Ray Data users and contributors\n",
    "\n",
    "**Resources:**\n",
    "- **Ray Documentation**: https://docs.ray.io/en/latest/data/data.html\n",
    "- **Ray Data Examples**: https://github.com/ray-project/ray/tree/master/python/ray/data/examples\n",
    "- **Community Support**: https://discuss.ray.io/\n",
    "- **Anyscale Platform**: https://www.anyscale.com/\n",
    "\n",
    "### Action Items\n",
    "\n",
    "**Immediate Actions:**\n",
    "1. **Experiment**: Try different batch sizes and concurrency settings\n",
    "2. **Profile**: Use Ray dashboard to monitor your pipeline performance\n",
    "3. **Scale**: Test with larger datasets to understand scaling behavior\n",
    "4. **Optimize**: Apply the performance techniques learned in this template\n",
    "\n",
    "**Long-term Goals:**\n",
    "1. **Production Deployment**: Deploy a real ETL pipeline using Ray Data\n",
    "2. **Team Training**: Share knowledge with your data engineering team\n",
    "3. **Community Contribution**: Contribute examples or improvements back to the community\n",
    "4. **Advanced Use Cases**: Explore AI/ML integration with Ray Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4422e673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup and Resource Management\n",
    "def cleanup_resources():\n",
    "    \"\"\"Properly clean up Ray resources and display final statistics.\"\"\"\n",
    "    try:\n",
    "        # Display final cluster statistics\n",
    "        print(\"=\"*60)\n",
    "        print(\"ETL PIPELINE COMPLETION SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Get cluster resources\n",
    "        resources = ray.cluster_resources()\n",
    "        print(f\"Cluster Resources Used:\")\n",
    "        print(f\"  CPU cores: {resources.get('CPU', 0):.1f}\")\n",
    "        print(f\"  Memory: {resources.get('memory', 0) / (1024**3):.1f} GB\")\n",
    "        print(f\"  Object Store: {resources.get('object_store_memory', 0) / (1024**3):.1f} GB\")\n",
    "        \n",
    "        # Display Ray dashboard URL\n",
    "        dashboard_url = ray.get_dashboard_url()\n",
    "        print(f\"\\nRay Dashboard: {dashboard_url}\")\n",
    "        \n",
    "        print(\"\\nETL Pipeline completed successfully!\")\n",
    "        print(\"Check the Ray dashboard for detailed performance metrics\")\n",
    "        print(\"Use the troubleshooting guide for production deployment\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during cleanup: {e}\")\n",
    "    finally:\n",
    "        # Always attempt to shutdown Ray\n",
    "        try:\n",
    "            if ray.is_initialized():\n",
    "                ray.shutdown()\n",
    "                logger.info(\"Ray cluster shutdown completed\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during Ray shutdown: {e}\")\n",
    "\n",
    "# Run cleanup\n",
    "cleanup_resources()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53a6d69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 00:38:59,398\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_369_0\n",
      "2025-08-28 00:38:59,406\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_369_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:38:59,407\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_369_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> LimitOperator[limit=1000] -> TaskPoolMapOperator[ReadFiles] -> AllToAllOperator[Aggregate] -> LimitOperator[limit=1]\n",
      "2025-08-28 00:39:02,044\tWARNING streaming_executor_state.py:790 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: o_orderpriority: string\n",
      "count(): int64\n",
      "mean(o_totalprice): double\n",
      "sum(o_totalprice): double, new schema: None. This may lead to unexpected behavior.\n",
      "2025-08-28 00:39:02,066\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_369_0 execution finished in 2.66 seconds\n",
      "2025-08-28 00:39:02,088\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_374_0\n",
      "2025-08-28 00:39:02,103\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_374_0. Full logs are in /tmp/ray/session_2025-08-27_22-21-09_261930_2324/logs/ray-data\n",
      "2025-08-28 00:39:02,106\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_374_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> LimitOperator[limit=1000] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[Map(<lambda>)] -> AllToAllOperator[Aggregate] -> LimitOperator[limit=1]\n",
      "2025-08-28 00:39:20,617\tWARNING streaming_executor_state.py:790 -- Operator produced a RefBundle with a different schema than the previous one. Previous schema: order_year: int64\n",
      "count(): int64\n",
      "sum(o_totalprice): double\n",
      "mean(o_totalprice): double, new schema: None. This may lead to unexpected behavior.\n",
      "2025-08-28 00:39:20,639\tINFO streaming_executor.py:279 -- \u2714\ufe0f  Dataset dataset_374_0 execution finished in 18.53 seconds\n"
     ]
    }
   ],
   "source": [
    "from ray.data.aggregate import Count, Mean, Sum\n",
    "\n",
    "# Order priority analysis - Ray Dataset API\n",
    "order_priority_analysis = (orders_ds\n",
    "    .groupby(\"o_orderpriority\")\n",
    "    .aggregate(\n",
    "        Count(),\n",
    "        Mean(\"o_totalprice\"),\n",
    "        Sum(\"o_totalprice\"),\n",
    "    )\n",
    "    .rename_columns([\"o_orderpriority\", \"order_count\", \"avg_total_price\", \"total_value\"])\n",
    ")\n",
    "\n",
    "# Time-based order analysis\n",
    "orders_with_year = orders_ds.map(lambda r: {**r, \"order_year\": r[\"o_orderdate\"].year})\n",
    "\n",
    "yearly_revenue = (orders_with_year\n",
    "    .groupby(\"order_year\")\n",
    "    .aggregate(\n",
    "        Count(),\n",
    "        Sum(\"o_totalprice\"),\n",
    "        Mean(\"o_totalprice\"),\n",
    "    )\n",
    "    .rename_columns([\"order_year\", \"yearly_orders\", \"yearly_revenue\", \"avg_order_value\"])\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966b2180",
   "metadata": {},
   "source": [
    "### Comprehensive Filter Operations\n",
    "\n",
    "Filtering is one of the most common operations in ETL pipelines. Here are comprehensive examples of different filtering patterns:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Filter Performance Tips:</b>\n",
    "<ul>\n",
    "    <li><b>Push Down Filters:</b> Apply filters early in the pipeline to reduce data volume</li>\n",
    "    <li><b>Indexed Columns:</b> Filter on indexed columns when possible for better performance</li>\n",
    "    <li><b>Batch Processing:</b> Use `map_batches()` for complex filtering logic</li>\n",
    "    <li><b>Memory Efficiency:</b> Combine multiple filters into single operations when possible</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "**1. Basic Value Filtering**\n",
    "```python\n",
    "# Simple value comparisons\n",
    "high_value_customers = customers_ds.filter(lambda x: x[\"c_acctbal\"] > 5000)\n",
    "recent_orders = orders_ds.filter(lambda x: x[\"o_orderdate\"] > \"2023-01-01\")\n",
    "urgent_orders = orders_ds.filter(lambda x: x[\"o_orderpriority\"] == \"1-URGENT\")\n",
    "```\n",
    "\n",
    "**2. Range Filtering**\n",
    "```python\n",
    "# Numeric ranges\n",
    "medium_balance = customers_ds.filter(lambda x: 1000 <= x[\"c_acctbal\"] <= 5000)\n",
    "recent_orders = orders_ds.filter(lambda x: \"2023-01-01\" <= x[\"o_orderdate\"] <= \"2023-12-31\")\n",
    "```\n",
    "\n",
    "**3. String Pattern Matching**\n",
    "```python\n",
    "# String contains/startswith/endswith\n",
    "building_customers = customers_ds.filter(lambda x: \"BUILDING\" in x[\"c_mktsegment\"])\n",
    "phone_area_555 = customers_ds.filter(lambda x: x[\"c_phone\"].startswith(\"555\"))\n",
    "```\n",
    "\n",
    "**4. Multiple Condition Filtering**\n",
    "```python\n",
    "# Complex boolean logic\n",
    "premium_customers = customers_ds.filter(\n",
    "    lambda x: x[\"c_acctbal\"] > 7500 and x[\"c_mktsegment\"] == \"AUTOMOBILE\"\n",
    ")\n",
    "\n",
    "high_priority_recent = orders_ds.filter(\n",
    "    lambda x: x[\"o_orderpriority\"] in [\"1-URGENT\", \"2-HIGH\"] and \n",
    "              x[\"o_orderdate\"] > \"2023-06-01\"\n",
    ")\n",
    "```\n",
    "\n",
    "**5. Null/None Value Filtering**\n",
    "```python\n",
    "# Filter out null values\n",
    "valid_customers = customers_ds.filter(lambda x: x[\"c_comment\"] is not None)\n",
    "non_empty_orders = orders_ds.filter(lambda x: x[\"o_comment\"] != \"\")\n",
    "```\n",
    "\n",
    "**6. List/Set Membership Filtering**\n",
    "```python\n",
    "# Filter by membership in lists\n",
    "target_segments = [\"AUTOMOBILE\", \"MACHINERY\", \"FURNITURE\"]\n",
    "target_customers = customers_ds.filter(lambda x: x[\"c_mktsegment\"] in target_segments)\n",
    "\n",
    "priority_levels = [\"1-URGENT\", \"2-HIGH\"]\n",
    "priority_orders = orders_ds.filter(lambda x: x[\"o_orderpriority\"] in priority_levels)\n",
    "```\n",
    "\n",
    "**7. Date/Time Filtering**\n",
    "```python\n",
    "# Date range filtering\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Last 30 days\n",
    "recent_cutoff = datetime.now() - timedelta(days=30)\n",
    "recent_orders = orders_ds.filter(lambda x: x[\"o_orderdate\"] > recent_cutoff)\n",
    "\n",
    "# Specific year/quarter\n",
    "q4_orders = orders_ds.filter(lambda x: x[\"o_orderdate\"].quarter == 4)\n",
    "```\n",
    "\n",
    "**8. Complex Business Logic Filtering**\n",
    "```python\n",
    "# Multi-step business logic\n",
    "def is_high_value_customer(customer):\n",
    "    return (\n",
    "        customer[\"c_acctbal\"] > 5000 and\n",
    "        customer[\"c_mktsegment\"] in [\"AUTOMOBILE\", \"MACHINERY\"] and\n",
    "        customer[\"c_comment\"] is not None and\n",
    "        len(customer[\"c_comment\"]) > 10\n",
    "    )\n",
    "\n",
    "high_value = customers_ds.filter(is_high_value_customer)\n",
    "```\n",
    "\n",
    "**9. Performance-Optimized Batch Filtering**\n",
    "```python\n",
    "# For complex filtering logic, use map_batches\n",
    "def complex_filter_batch(batch):\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(batch)\n",
    "    \n",
    "    # Complex filtering logic\n",
    "    filtered_df = df[\n",
    "        (df[\"c_acctbal\"] > 1000) &\n",
    "        (df[\"c_mktsegment\"].isin([\"AUTOMOBILE\", \"MACHINERY\"])) &\n",
    "        (df[\"c_phone\"].str.contains(\"555\", na=False)) &\n",
    "        (df[\"c_comment\"].str.len() > 5)\n",
    "    ]\n",
    "    \n",
    "    return filtered_df.to_dict('records')\n",
    "\n",
    "filtered_customers = customers_ds.map_batches(\n",
    "    complex_filter_batch,\n",
    "    batch_format=\"pandas\"\n",
    ")\n",
    "```\n",
    "\n",
    "**10. Filtering with Error Handling**\n",
    "```python\n",
    "# Robust filtering with error handling\n",
    "def safe_filter(record):\n",
    "    try:\n",
    "        return (\n",
    "            record[\"c_acctbal\"] is not None and\n",
    "            record[\"c_acctbal\"] > 0 and\n",
    "            record[\"c_mktsegment\"] in [\"AUTOMOBILE\", \"MACHINERY\", \"FURNITURE\"]\n",
    "        )\n",
    "    except (KeyError, TypeError, ValueError):\n",
    "        return False\n",
    "\n",
    "safe_filtered = customers_ds.filter(safe_filter)\n",
    "```\n",
    "\n",
    "**Filter Performance Monitoring:**\n",
    "```python\n",
    "# Monitor filter performance\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "filtered_ds = customers_ds.filter(lambda x: x[\"c_acctbal\"] > 5000)\n",
    "result = filtered_ds.take(100)\n",
    "filter_time = time.time() - start_time\n",
    "\n",
    "print(f\"Filtered {len(result)} records in {filter_time:.2f}s\")\n",
    "print(f\"Filter selectivity: {len(result) / customers_ds.count():.2%}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39b76d7",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Summary: Your Journey with Ray Data ETL\n",
    "\n",
    "Congratulations! You've completed a comprehensive journey through Ray Data for ETL. Let's summarize what you've learned and explore how to take your AI data pipelines to production.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> What You've Mastered:</b>\n",
    "<ul>\n",
    "    <li><b>Ray Data Fundamentals:</b> Blocks, lazy execution, streaming processing</li>\n",
    "    <li><b>Extract Phase:</b> Reading from multiple data sources efficiently, including multimodal data</li>\n",
    "    <li><b>Transform Phase:</b> Distributed data processing and feature engineering</li>\n",
    "    <li><b>Load Phase:</b> Writing to various destinations with optimization</li>\n",
    "    <li><b>Production Patterns:</b> Error handling, monitoring, and data quality</li>\n",
    "    <li><b>Performance Optimization:</b> Understanding bottlenecks and solutions</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "### When to Use Ray Data\n",
    "\n",
    "**Ray Data excels across the full spectrum of data workloads:**\n",
    "\n",
    "**Traditional ETL & Business Intelligence:**\n",
    "- **High-volume transaction processing** for e-commerce, finance, and operations\n",
    "- **Business intelligence** and executive reporting at scale\n",
    "- **Data warehouse** loading and transformation pipelines\n",
    "- **CPU cluster optimization** with pure Python performance (no JVM overhead)\n",
    "- **Traditional analytics** that need to scale beyond single-node tools\n",
    "\n",
    "**Modern ML & AI Workloads:**\n",
    "- **Feature engineering** for machine learning at scale\n",
    "- **Batch inference** on foundation models and LLMs\n",
    "- **Multimodal data processing** (text, images, video, audio)\n",
    "- **GPU-accelerated pipelines** for AI applications\n",
    "- **Real-time model serving** and inference workloads\n",
    "\n",
    "**Ray Data's Unified Platform Advantage:**\n",
    "- **One system** for both traditional ETL and cutting-edge AI\n",
    "- **Seamless evolution** from CPU-based analytics to GPU-powered AI\n",
    "- **No migration** required as your data needs grow and change\n",
    "- **Consistent APIs** whether processing structured business data or unstructured AI content\n",
    "\n",
    "**Ray is proven at scale:**\n",
    "- Processing **exabyte-scale** workloads\n",
    "- **1M+ clusters** orchestrated monthly across the Ray ecosystem\n",
    "- **$120M annual savings** achieved by leading enterprises\n",
    "- **Traditional workloads** running alongside **next-generation AI** on the same platform\n",
    "\n",
    "### From Open Source to Enterprise: Anyscale Platform\n",
    "\n",
    "While Ray Data open source provides powerful capabilities, **Anyscale** offers a unified AI platform for production deployments:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Anyscale: The Unified AI Platform</b>\n",
    "<ul>\n",
    "    <li><b>RayTurbo Runtime:</b> Up to 5.1x performance improvements over open source</li>\n",
    "    <li><b>Enterprise Governance:</b> Resource quotas, usage tracking, and advanced observability</li>\n",
    "    <li><b>AI Anywhere:</b> Deploy on Kubernetes, hybrid cloud, or any infrastructure</li>\n",
    "    <li><b>LLM Suite:</b> Complete capabilities for embeddings, fine-tuning, and serving</li>\n",
    "    <li><b>Marketplace Ready:</b> Available on AWS and GCP Marketplaces</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "### Production Deployment Options\n",
    "\n",
    "**Getting Started:**\n",
    "1. **Ray Open Source**: Perfect for development and smaller workloads\n",
    "2. **Anyscale Platform**: Enterprise features with RayTurbo optimizations\n",
    "3. **Marketplace Deployment**: One-click setup via AWS or GCP Marketplace\n",
    "\n",
    "### Key Architectural Insights\n",
    "\n",
    "Understanding how Ray Data works under the hood helps you build better pipelines:\n",
    "\n",
    "1. **AI-Native Architecture**: Purpose-built for Python, GPUs, and multimodal data\n",
    "2. **Streaming Execution**: Process datasets larger than cluster memory\n",
    "3. **Heterogeneous Compute**: Seamlessly orchestrate CPUs, GPUs, and other accelerators\n",
    "4. **Operator Fusion**: Combines compatible operations for efficiency\n",
    "5. **Enterprise Scalability**: Proven to scale to 8,000+ nodes\n",
    "\n",
    "### Production Readiness Checklist\n",
    "\n",
    "Before deploying Ray Data pipelines to production:\n",
    "\n",
    "-  **Architecture**: Choose between Ray OSS and Anyscale based on your needs\n",
    "-  **Performance**: Consider RayTurbo for production workloads requiring maximum efficiency\n",
    "-  **Governance**: Implement enterprise controls for AI sprawl and cost management\n",
    "-  **Security**: Leverage enterprise identity integration and access controls\n",
    "-  **Monitoring**: Use advanced observability tools for optimization insights\n",
    "-  **Scalability**: Test with realistic data volumes and cluster sizes\n",
    "\n",
    "### Join the Ray Ecosystem\n",
    "\n",
    "The Ray community is thriving with **1,000+ contributors** and growing:\n",
    "\n",
    "1. **Community**: Join the Ray Slack community for support and discussions\n",
    "2. **Learning**: Access Ray Summit sessions and technical deep-dives\n",
    "3. **Contributing**: Contribute to the fastest-growing AI infrastructure project\n",
    "4. **Enterprise Support**: Explore Anyscale for production deployments\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> One Platform for All Your Data Workloads</b><br>\n",
    "You now have the knowledge to build production-ready, scalable data pipelines that handle everything from traditional business ETL to cutting-edge AI applications. Whether you're processing millions of e-commerce transactions for business intelligence or preparing multimodal data for foundation models, Ray Data provides a unified platform that scales with your needs.<br><br>\n",
    "<b>Start with traditional ETL today, evolve to AI tomorrow - all on the same platform.</b> Ray Data and Anyscale eliminate the complexity of managing multiple systems as your data requirements grow.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e9e932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}