{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35b03769",
   "metadata": {},
   "source": [
    "# Geospatial data analysis with Ray Data\n",
    "\n",
    "**Time to complete**: 25 min | **Difficulty**: Intermediate | **Prerequisites**: Basic Python, understanding of coordinates\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "Create a scalable geospatial analysis pipeline that processes millions of location points across entire cities. Learn to find nearby businesses, calculate distances, and perform spatial clustering using Ray Data's distributed processing capabilities.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Data Creation](#step-1-setup-and-data-loading) (5 min)\n",
    "2. [Spatial Operations](#step-2-basic-spatial-operations) (8 min)\n",
    "3. [Distance Calculations](#step-3-distance-calculations-at-scale) (7 min)  \n",
    "4. [Visualization and Results](#step-4-visualization-and-analysis) (5 min)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "**Why geospatial analytics matters**: Location intelligence drives revenue across ride-sharing, delivery, and real estate industries through data-driven decision making. Understanding spatial patterns and relationships unlocks competitive advantages in location-based businesses.\n",
    "\n",
    "**Ray Data's spatial capabilities**: Distribute complex geographic calculations like spatial joins, clustering, and routing across distributed clusters. You'll learn how Ray Data handles the computational intensity of spatial operations at scale.\n",
    "\n",
    "**Real-world location applications**: Industry-standard techniques used by Uber, DoorDash, and Google Maps to process billions of location events daily. These patterns apply across transportation, logistics, and location-based services.\n",
    "\n",
    "**Advanced spatial analysis**: Master geofencing, hot spot detection, route optimization, and location-based recommendations at city scale. These techniques enable sophisticated location intelligence applications.\n",
    "\n",
    "**Production deployment patterns**: Real-time location processing, spatial indexing, and geographic data pipeline optimization strategies that enable enterprise-scale geospatial applications.\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Challenge**: Traditional geospatial analysis tools face significant limitations when processing large-scale location data. Single-machine processing overwhelms memory with millions of GPS coordinates, while spatial operations like proximity search and clustering become computationally expensive at scale.\n",
    "\n",
    "**Solution**: Ray Data provides distributed geospatial processing capabilities that automatically parallelize spatial calculations across multiple nodes. The framework handles datasets larger than cluster memory through streaming processing and integrates seamlessly with popular geospatial libraries.\n",
    "\n",
    "**Impact**: Leading location-based companies leverage distributed geospatial analytics for transformative business results. Uber processes millions of daily trips across cities using sophisticated spatial algorithms, while DoorDash optimizes delivery zones through distributed geographic calculations. These patterns enable real-time location processing at enterprise scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6f2838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Real-time spatial matching like Uber/Lyft\n",
    "def find_nearest_drivers(passenger_location, driver_locations, max_distance_km=5):\n",
    "    \"\"\"Find nearest available drivers using efficient spatial operations.\"\"\"\n",
    "    \n",
    "    import math\n",
    "    \n",
    "    def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "        \"\"\"Calculate haversine distance between two points.\"\"\"\n",
    "        R = 6371  # Earth's radius in kilometers\n",
    "        \n",
    "        lat1_rad, lon1_rad = math.radians(lat1), math.radians(lon1)\n",
    "        lat2_rad, lon2_rad = math.radians(lat2), math.radians(lon2)\n",
    "        \n",
    "        dlat = lat2_rad - lat1_rad\n",
    "        dlon = lon2_rad - lon1_rad\n",
    "        \n",
    "        a = math.sin(dlat/2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dlon/2)**2\n",
    "        c = 2 * math.asin(math.sqrt(a))\n",
    "        \n",
    "        return R * c\n",
    "    \n",
    "    # Find drivers within range\n",
    "    nearby_drivers = []\n",
    "    passenger_lat, passenger_lon = passenger_location\n",
    "    \n",
    "    for driver in driver_locations:\n",
    "        distance = calculate_distance(\n",
    "            passenger_lat, passenger_lon,\n",
    "            driver['latitude'], driver['longitude']\n",
    "        )\n",
    "        \n",
    "        if distance <= max_distance_km:\n",
    "            nearby_drivers.append({\n",
    "                'driver_id': driver['driver_id'],\n",
    "                'distance_km': distance,\n",
    "                'eta_minutes': distance * 2.5  # Estimated time\n",
    "            })\n",
    "    \n",
    "    # Sort by distance\n",
    "    nearby_drivers.sort(key=lambda x: x['distance_km'])\n",
    "    \n",
    "    return nearby_drivers[:3]  # Return top 3 nearest\n",
    "\n",
    "print(\"Real-time spatial matching capabilities enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711c381a",
   "metadata": {},
   "source": [
    "This distributed approach enables real-world applications across industries. Ride-sharing platforms find nearest drivers to passengers in real-time using spatial indexing. Retail companies analyze store locations and customer proximity for optimal placement strategies. Healthcare systems optimize emergency services and resource allocation through geographic analysis, while social applications provide location-based features and recommendations using spatial intelligence.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites Checklist\n",
    "\n",
    "Before starting this geospatial analysis template, ensure you have Python 3.8+ with basic geospatial processing experience and understanding of geographic coordinates. Knowledge of GIS concepts like projections and coordinate systems will help you understand the spatial transformations demonstrated.\n",
    "\n",
    "**Required setup**:\n",
    "- [ ] Python 3.8+ with geospatial processing experience\n",
    "- [ ] Understanding of latitude/longitude coordinate systems\n",
    "- [ ] Access to Ray cluster for distributed processing\n",
    "- [ ] 8GB+ RAM for processing geographic datasets\n",
    "- [ ] Optional: Experience with geospatial libraries (GeoPandas, Shapely)\n",
    "\n",
    "## Quick Start (3 minutes)\n",
    "\n",
    "Want to see geospatial processing in action immediately? This section demonstrates core spatial analysis concepts in just a few minutes.\n",
    "\n",
    "### Install Required Packages\n",
    "\n",
    "First, ensure you have the necessary geospatial libraries installed:\n",
    "\n",
    "```bash\n",
    "pip install \"ray[data]\" pandas numpy matplotlib seaborn plotly folium geopandas contextily\n",
    "```\n",
    "\n",
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f616807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Ray for distributed processing\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d849f9",
   "metadata": {},
   "source": [
    "### Create Sample Location Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d31a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample location data for major US cities\n",
    "print(\"Creating sample geospatial dataset...\")\n",
    "\n",
    "# Major US city coordinates\n",
    "# Load NYC taxi trip data for geospatial analysis\n",
    "taxi_data = ray.data.read_parquet(\n",
    "    \"s3://ray-benchmark-data/nyc-taxi/yellow_tripdata_2023-01.parquet\"\n",
    ").limit(50000)\n",
    "\n",
    "# Extract location points from taxi data using Ray Data map_batches\n",
    "def extract_taxi_locations(batch):\n",
    "    \"\"\"Extract taxi pickup locations for geospatial analysis.\"\"\"\n",
    "    df = pd.DataFrame(batch)\n",
    "    locations = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Extract pickup locations with valid NYC coordinates\n",
    "        if (row.get('pickup_latitude') is not None and \n",
    "            row.get('pickup_longitude') is not None and\n",
    "            40.0 <= row['pickup_latitude'] <= 41.0 and  # Valid NYC latitude range\n",
    "            -75.0 <= row['pickup_longitude'] <= -73.0):  # Valid NYC longitude range\n",
    "            \n",
    "            locations.append({\n",
    "                'location_id': f\"pickup_{row.name}\",\n",
    "                'lat': float(row['pickup_latitude']),\n",
    "                'lon': float(row['pickup_longitude']),\n",
    "                'type': 'taxi_pickup',\n",
    "                'borough': 'NYC',\n",
    "                'trip_distance': float(row.get('trip_distance', 0))\n",
    "            })\n",
    "    \n",
    "    return locations\n",
    "\n",
    "# Use Ray Data map_batches for efficient location extraction\n",
    "# Optimize batch size for memory efficiency with large datasets\n",
    "location_dataset = taxi_data.map_batches(\n",
    "    extract_taxi_locations,\n",
    "    batch_format=\"pandas\",\n",
    "    batch_size=500,  # Reduced batch size for memory efficiency\n",
    "    concurrency=4    # Increase concurrency for better parallelization\n",
    ").flatten()\n",
    "\n",
    "print(f\"Loaded NYC taxi location data: {location_dataset.count():,} location points\")\n",
    "\n",
    "### **NYC Geospatial Analysis Dashboard**\n",
    "\n",
    "Let's create comprehensive visualizations to understand spatial patterns and optimize location-based services:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306635f8",
   "metadata": {},
   "source": [
    "python\n",
    "# Create engaging NYC geospatial analysis dashboard\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_nyc_geospatial_dashboard():\n",
    "    \"\"\"Generate comprehensive NYC taxi geospatial analysis dashboard.\"\"\"\n",
    "    \n",
    "    # Convert Ray dataset to pandas for visualization\n",
    "    sample_locations = ds.take(10000)  # Sample for visualization\n",
    "    location_df = pd.DataFrame(sample_locations)\n",
    "    \n",
    "    # Create comprehensive analysis dashboard\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.suptitle('NYC Geospatial Analysis: Taxi Location Intelligence Dashboard', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Geographic distribution heatmap\n",
    "    ax1 = axes[0, 0]\n",
    "    # Create 2D histogram for pickup density\n",
    "    lat_bins = np.linspace(location_df['lat'].min(), location_df['lat'].max(), 50)\n",
    "    lon_bins = np.linspace(location_df['lon'].min(), location_df['lon'].max(), 50)\n",
    "    \n",
    "    H, xedges, yedges = np.histogram2d(location_df['lon'], location_df['lat'], bins=[lon_bins, lat_bins])\n",
    "    extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
    "    \n",
    "    im1 = ax1.imshow(H.T, origin='lower', extent=extent, cmap='hot', aspect='auto')\n",
    "    ax1.set_title('Taxi Pickup Density Heatmap', fontweight='bold')\n",
    "    ax1.set_xlabel('Longitude')\n",
    "    ax1.set_ylabel('Latitude')\n",
    "    plt.colorbar(im1, ax=ax1, label='Pickup Density')\n",
    "    \n",
    "    # 2. Trip distance distribution\n",
    "    ax2 = axes[0, 1]\n",
    "    trip_distances = location_df['trip_distance'].dropna()\n",
    "    trip_distances = trip_distances[trip_distances > 0]  # Remove zero distances\n",
    "    trip_distances = trip_distances[trip_distances < 50]  # Remove outliers\n",
    "    \n",
    "    ax2.hist(trip_distances, bins=30, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "    ax2.axvline(trip_distances.mean(), color='red', linestyle='--', \n",
    "               label=f'Mean: {trip_distances.mean():.2f} miles')\n",
    "    ax2.axvline(trip_distances.median(), color='orange', linestyle='--', \n",
    "               label=f'Median: {trip_distances.median():.2f} miles')\n",
    "    ax2.set_title('Trip Distance Distribution', fontweight='bold')\n",
    "    ax2.set_xlabel('Trip Distance (miles)')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Spatial clustering analysis\n",
    "    ax3 = axes[0, 2]\n",
    "    # Simple spatial clustering using lat/lon buckets\n",
    "    location_df['lat_bucket'] = pd.cut(location_df['lat'], bins=20, labels=False)\n",
    "    location_df['lon_bucket'] = pd.cut(location_df['lon'], bins=20, labels=False)\n",
    "    location_df['spatial_cluster'] = location_df['lat_bucket'] * 20 + location_df['lon_bucket']\n",
    "    \n",
    "    cluster_sizes = location_df.groupby('spatial_cluster').size().sort_values(ascending=False).head(10)\n",
    "    \n",
    "    bars3 = ax3.bar(range(len(cluster_sizes)), cluster_sizes.values, color='lightgreen')\n",
    "    ax3.set_title('Top 10 Spatial Clusters by Activity', fontweight='bold')\n",
    "    ax3.set_xlabel('Cluster ID')\n",
    "    ax3.set_ylabel('Number of Pickups')\n",
    "    ax3.set_xticks(range(len(cluster_sizes)))\n",
    "    ax3.set_xticklabels([f'C{i+1}' for i in range(len(cluster_sizes))])\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars3, cluster_sizes.values):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                f'{value}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. Latitude vs Longitude scatter with density\n",
    "    ax4 = axes[1, 0]\n",
    "    scatter = ax4.scatter(location_df['lon'], location_df['lat'], \n",
    "                         c=location_df['trip_distance'], cmap='viridis', \n",
    "                         alpha=0.6, s=10)\n",
    "    ax4.set_title('Geographic Distribution by Trip Distance', fontweight='bold')\n",
    "    ax4.set_xlabel('Longitude')\n",
    "    ax4.set_ylabel('Latitude')\n",
    "    cbar = plt.colorbar(scatter, ax=ax4)\n",
    "    cbar.set_label('Trip Distance (miles)')\n",
    "    \n",
    "    # 5. Geospatial efficiency analysis\n",
    "    ax5 = axes[1, 1]\n",
    "    # Calculate pickup efficiency by area\n",
    "    location_df['efficiency'] = location_df.groupby('spatial_cluster')['trip_distance'].transform('mean')\n",
    "    efficiency_by_cluster = location_df.groupby('spatial_cluster')['efficiency'].first().sort_values(ascending=False).head(8)\n",
    "    \n",
    "    bars5 = ax5.bar(range(len(efficiency_by_cluster)), efficiency_by_cluster.values, color='coral')\n",
    "    ax5.set_title('Average Trip Distance by Spatial Cluster', fontweight='bold')\n",
    "    ax5.set_xlabel('Cluster ID')\n",
    "    ax5.set_ylabel('Average Trip Distance (miles)')\n",
    "    ax5.set_xticks(range(len(efficiency_by_cluster)))\n",
    "    ax5.set_xticklabels([f'C{i+1}' for i in range(len(efficiency_by_cluster))])\n",
    "    \n",
    "    # 6. Spatial coverage analysis\n",
    "    ax6 = axes[1, 2]\n",
    "    # Analyze geographic coverage\n",
    "    lat_range = location_df['lat'].max() - location_df['lat'].min()\n",
    "    lon_range = location_df['lon'].max() - location_df['lon'].min()\n",
    "    coverage_area = lat_range * lon_range * 111 * 111  # Approximate km²\n",
    "    \n",
    "    metrics = ['Lat Range (°)', 'Lon Range (°)', 'Coverage (km²)', 'Density (pts/km²)']\n",
    "    values = [lat_range, lon_range, coverage_area, len(location_df) / coverage_area]\n",
    "    colors = ['lightblue', 'lightgreen', 'lightyellow', 'lightpink']\n",
    "    \n",
    "    bars6 = ax6.bar(range(len(metrics)), values, color=colors)\n",
    "    ax6.set_title('Spatial Coverage Metrics', fontweight='bold')\n",
    "    ax6.set_ylabel('Value')\n",
    "    ax6.set_xticks(range(len(metrics)))\n",
    "    ax6.set_xticklabels(metrics, rotation=45, ha='right')\n",
    "    ax6.set_yscale('log')  # Log scale due to different magnitude values\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars6, values):\n",
    "        height = bar.get_height()\n",
    "        ax6.text(bar.get_x() + bar.get_width()/2., height * 1.1,\n",
    "                f'{value:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"NYC Geospatial Analysis Summary:\")\n",
    "    print(f\"- Total pickup locations analyzed: {len(location_df):,}\")\n",
    "    print(f\"- Average trip distance: {trip_distances.mean():.2f} miles\")\n",
    "    print(f\"- Geographic coverage: {coverage_area:.2f} km²\")\n",
    "    print(f\"- Pickup density: {len(location_df) / coverage_area:.2f} pickups/km²\")\n",
    "    print(f\"- Most active cluster has {cluster_sizes.iloc[0]:,} pickups\")\n",
    "    print(f\"- Highest efficiency cluster averages {efficiency_by_cluster.iloc[0]:.2f} miles/trip\")\n",
    "\n",
    "# Create NYC geospatial analysis dashboard\n",
    "create_nyc_geospatial_dashboard()\n",
    "```\n",
    "\n",
    "This comprehensive geospatial analysis reveals patterns crucial for optimizing ride-sharing operations, delivery routing, and urban planning decisions.\n",
    "```\n",
    "\n",
    "### Interactive Geospatial Visualization Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2325efc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an engaging geospatial data visualization dashboard\n",
    "def create_geospatial_dashboard(dataset, sample_size=1000):\n",
    "    \"\"\"Generate a comprehensive geospatial data analysis dashboard.\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    \n",
    "    # Sample data for analysis\n",
    "    sample_data = dataset.take(sample_size)\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    \n",
    "    # Create comprehensive dashboard\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    gs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. Geographic Distribution Map\n",
    "    ax_map = fig.add_subplot(gs[0, :2])\n",
    "    \n",
    "    # Create scatter plot of locations\n",
    "    scatter = ax_map.scatter(df['lon'], df['lat'], c=df['type'].astype('category').cat.codes, \n",
    "                           cmap='tab10', alpha=0.6, s=20)\n",
    "    ax_map.set_title('Geographic Distribution of Locations', fontsize=14, fontweight='bold')\n",
    "    ax_map.set_xlabel('Longitude')\n",
    "    ax_map.set_ylabel('Latitude')\n",
    "    ax_map.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add city labels\n",
    "    city_centers = df.groupby('city')[['lat', 'lon']].mean()\n",
    "    for city, (lat, lon) in city_centers.iterrows():\n",
    "        ax_map.annotate(city, (lon, lat), xytext=(5, 5), textcoords='offset points',\n",
    "                       bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8),\n",
    "                       fontsize=8)\n",
    "    \n",
    "    # 2. Location Type Distribution\n",
    "    ax_types = fig.add_subplot(gs[0, 2:])\n",
    "    type_counts = df['type'].value_counts()\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(type_counts)))\n",
    "    \n",
    "    wedges, texts, autotexts = ax_types.pie(type_counts.values, labels=type_counts.index, \n",
    "                                           autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "    ax_types.set_title('Location Type Distribution', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # 3. City Coverage Analysis\n",
    "    ax_cities = fig.add_subplot(gs[1, :2])\n",
    "    city_counts = df['city'].value_counts()\n",
    "    bars = ax_cities.bar(range(len(city_counts)), city_counts.values,\n",
    "                        color=plt.cm.viridis(np.linspace(0, 1, len(city_counts))))\n",
    "    ax_cities.set_title('Location Count by City', fontsize=12, fontweight='bold')\n",
    "    ax_cities.set_ylabel('Number of Locations')\n",
    "    ax_cities.set_xticks(range(len(city_counts)))\n",
    "    ax_cities.set_xticklabels(city_counts.index, rotation=45)\n",
    "    \n",
    "    # Add count labels\n",
    "    for bar, count in zip(bars, city_counts.values):\n",
    "        height = bar.get_height()\n",
    "        ax_cities.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                      f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. Latitude Distribution\n",
    "    ax_lat = fig.add_subplot(gs[1, 2:])\n",
    "    ax_lat.hist(df['lat'], bins=30, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "    ax_lat.axvline(df['lat'].mean(), color='red', linestyle='--', linewidth=2,\n",
    "                  label=f'Mean: {df[\"lat\"].mean():.2f}°')\n",
    "    ax_lat.set_title('Latitude Distribution', fontsize=12, fontweight='bold')\n",
    "    ax_lat.set_xlabel('Latitude (degrees)')\n",
    "    ax_lat.set_ylabel('Frequency')\n",
    "    ax_lat.legend()\n",
    "    ax_lat.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Longitude Distribution\n",
    "    ax_lon = fig.add_subplot(gs[2, :2])\n",
    "    ax_lon.hist(df['lon'], bins=30, color='lightgreen', alpha=0.7, edgecolor='black')\n",
    "    ax_lon.axvline(df['lon'].mean(), color='red', linestyle='--', linewidth=2,\n",
    "                  label=f'Mean: {df[\"lon\"].mean():.2f}°')\n",
    "    ax_lon.set_title('Longitude Distribution', fontsize=12, fontweight='bold')\n",
    "    ax_lon.set_xlabel('Longitude (degrees)')\n",
    "    ax_lon.set_ylabel('Frequency')\n",
    "    ax_lon.legend()\n",
    "    ax_lon.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Type vs City Heatmap\n",
    "    ax_heatmap = fig.add_subplot(gs[2, 2:])\n",
    "    type_city_matrix = pd.crosstab(df['type'], df['city'])\n",
    "    sns.heatmap(type_city_matrix, annot=True, fmt='d', cmap='YlOrRd', ax=ax_heatmap)\n",
    "    ax_heatmap.set_title('Location Types by City', fontsize=12, fontweight='bold')\n",
    "    ax_heatmap.set_xlabel('City')\n",
    "    ax_heatmap.set_ylabel('Location Type')\n",
    "    \n",
    "    # 7. Geographic Bounds Analysis\n",
    "    ax_bounds = fig.add_subplot(gs[3, :2])\n",
    "    ax_bounds.axis('off')\n",
    "    \n",
    "    # Calculate geographic bounds\n",
    "    lat_min, lat_max = df['lat'].min(), df['lat'].max()\n",
    "    lon_min, lon_max = df['lon'].min(), df['lon'].max()\n",
    "    lat_range = lat_max - lat_min\n",
    "    lon_range = lon_max - lon_min\n",
    "    \n",
    "    bounds_text = \"🗺️ Geographic Analysis\\n\" + \"=\"*40 + \"\\n\"\n",
    "    bounds_text += f\"Total Locations: {len(df):,}\\n\"\n",
    "    bounds_text += f\"Latitude Range: {lat_min:.2f}° to {lat_max:.2f}°\\n\"\n",
    "    bounds_text += f\"Longitude Range: {lon_min:.2f}° to {lon_max:.2f}°\\n\"\n",
    "    bounds_text += f\"Latitude Span: {lat_range:.2f}°\\n\"\n",
    "    bounds_text += f\"Longitude Span: {lon_range:.2f}°\\n\"\n",
    "    bounds_text += f\"Cities Covered: {len(df['city'].unique())}\\n\"\n",
    "    bounds_text += f\"Location Types: {len(df['type'].unique())}\\n\"\n",
    "    \n",
    "    ax_bounds.text(0.05, 0.95, bounds_text, transform=ax_bounds.transAxes, \n",
    "                  fontsize=11, verticalalignment='top', fontfamily='monospace',\n",
    "                  bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightyellow\", alpha=0.8))\n",
    "    \n",
    "    # 8. Sample Data Table\n",
    "    ax_table = fig.add_subplot(gs[3, 2:])\n",
    "    ax_table.axis('off')\n",
    "    \n",
    "    # Create sample data table\n",
    "    sample_df = df.head(8)[['location_id', 'city', 'type', 'lat', 'lon']].copy()\n",
    "    \n",
    "    table_text = \"Sample Location Data\\n\" + \"=\"*70 + \"\\n\"\n",
    "    table_text += f\"{'ID':<12} {'City':<10} {'Type':<10} {'Lat':<8} {'Lon':<8}\\n\"\n",
    "    table_text += \"-\"*70 + \"\\n\"\n",
    "    \n",
    "    for _, row in sample_df.iterrows():\n",
    "        table_text += f\"{row['location_id']:<12} {row['city']:<10} {row['type']:<10} {row['lat']:<8.2f} {row['lon']:<8.2f}\\n\"\n",
    "    \n",
    "    ax_table.text(0.05, 0.95, table_text, transform=ax_table.transAxes, \n",
    "                 fontsize=9, verticalalignment='top', fontfamily='monospace',\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\", alpha=0.8))\n",
    "    \n",
    "    plt.suptitle('Geospatial Data Analysis Dashboard', fontsize=18, fontweight='bold', y=0.95)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print geospatial insights\n",
    "    print(f\"🌍 Geospatial Data Insights:\")\n",
    "    print(f\"   • Geographic coverage: {lat_range:.2f}° lat × {lon_range:.2f}° lon\")\n",
    "    print(f\"   • Location density: {len(df):,} points across {len(df['city'].unique())} cities\")\n",
    "    print(f\"   • Type diversity: {len(df['type'].unique())} different location types\")\n",
    "    print(f\"   • Coordinate range: ({lat_min:.2f}°, {lon_min:.2f}°) to ({lat_max:.2f}°, {lon_max:.2f}°)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate the geospatial dashboard\n",
    "geospatial_df = create_geospatial_dashboard(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de080f8",
   "metadata": {},
   "source": [
    "**Why This Dashboard Matters:**\n",
    "- **Geographic Understanding**: Visualize spatial distribution and coverage patterns\n",
    "- **Data Quality**: Verify coordinate validity and geographic bounds\n",
    "- **Pattern Recognition**: Identify clustering and distribution patterns across cities\n",
    "- **Type Analysis**: Understand location type diversity and city coverage\n",
    "\n",
    "## Step 1: Setup and Data Loading\n",
    "\n",
    "First, let's set up Ray and load our geospatial datasets. We'll create realistic point-of-interest (POI) data across major metropolitan areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a397624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import folium\n",
    "from folium.plugins import HeatMap, MarkerCluster\n",
    "from typing import Dict, Any\n",
    "import time\n",
    "\n",
    "# Initialize Ray - this creates our distributed computing cluster\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "print(\" Ray cluster initialized!\")\n",
    "print(f\" Available resources: {ray.cluster_resources()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de204e41",
   "metadata": {},
   "source": [
    "Now let's create our geospatial data generation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea35b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_geospatial_data():\n",
    "    \"\"\"\n",
    "    Load geospatial datasets for analysis.\n",
    "    \n",
    "    Returns:\n",
    "        ray.data.Dataset: Dataset containing point-of-interest data with coordinates,\n",
    "                         categories, and ratings for multiple metropolitan areas.\n",
    "                         \n",
    "    Note:\n",
    "        Uses reproducible random seed (42) for consistent results across runs.\n",
    "        Creates realistic POI distributions within major US metropolitan areas.\n",
    "    \"\"\"\n",
    "    print(\"Loading geospatial datasets...\")\n",
    "    \n",
    "    # Create sample POI data for major US metro areas\n",
    "    # Use fixed seed for reproducible results (rule #502)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    metro_areas = {\n",
    "        'NYC': {'lat': 40.7128, 'lon': -74.0060, 'radius': 0.5},\n",
    "        'LA': {'lat': 34.0522, 'lon': -118.2437, 'radius': 0.6},\n",
    "        'Chicago': {'lat': 41.8781, 'lon': -87.6298, 'radius': 0.4}\n",
    "    }\n",
    "    \n",
    "    poi_data = []\n",
    "    categories = ['restaurant', 'retail', 'hospital', 'school', 'bank']\n",
    "    \n",
    "    for metro, coords in metro_areas.items():\n",
    "        for i in range(1000):  # 1000 POIs per metro\n",
    "            angle = np.random.uniform(0, 2 * np.pi)\n",
    "            radius = np.random.uniform(0, coords['radius'])\n",
    "            \n",
    "            lat = coords['lat'] + radius * np.cos(angle)\n",
    "            lon = coords['lon'] + radius * np.sin(angle)\n",
    "            \n",
    "            poi_data.append({\n",
    "                'poi_id': f'{metro}_{i:04d}',\n",
    "                'name': f'Business_{i}',\n",
    "                'category': np.random.choice(categories),\n",
    "                'latitude': round(lat, 6),\n",
    "                'longitude': round(lon, 6),\n",
    "                'metro_area': metro,\n",
    "                'rating': np.random.uniform(1.0, 5.0)\n",
    "            })\n",
    "    \n",
    "    return ray.data.from_items(poi_data)\n",
    "\n",
    "# Load the dataset and measure performance\n",
    "start_time = time.time()\n",
    "poi_dataset = load_geospatial_data()\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "print(f\"⏱ Data loading took: {load_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad25d46",
   "metadata": {},
   "source": [
    "Inspect the dataset structure and validate our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c292fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(f\" Dataset size: {poi_dataset.count()} records\")\n",
    "print(f\" Schema: {poi_dataset.schema()}\")\n",
    "\n",
    "# Show sample data to verify it looks correct\n",
    "print(\"\\n Sample POI data:\")\n",
    "sample_data = poi_dataset.take(5)\n",
    "for i, poi in enumerate(sample_data):\n",
    "    print(f\"  {i+1}. {poi['name']} ({poi['category']}) at {poi['latitude']:.4f}, {poi['longitude']:.4f}\")\n",
    "\n",
    "# Comprehensive data validation (rule #218: Include comprehensive data validation)\n",
    "print(f\"\\n Data validation:\")\n",
    "\n",
    "# Validate coordinate ranges\n",
    "valid_coords = poi_dataset.filter(\n",
    "    lambda x: x['latitude'] is not None and x['longitude'] is not None and\n",
    "              -90 <= x['latitude'] <= 90 and -180 <= x['longitude'] <= 180\n",
    ").count()\n",
    "\n",
    "print(f\"  - Valid coordinates: {valid_coords} / {poi_dataset.count()}\")\n",
    "print(f\"  - Metro areas covered: {len(set([poi['metro_area'] for poi in poi_dataset.take(100)]))}\")\n",
    "\n",
    "# Additional validation checks\n",
    "sample_data = poi_dataset.take(100)\n",
    "categories = set([poi['category'] for poi in sample_data])\n",
    "ratings = [poi['rating'] for poi in sample_data if poi['rating'] is not None]\n",
    "\n",
    "print(f\"  - Categories found: {len(categories)} ({list(categories)})\")\n",
    "print(f\"  - Rating range: {min(ratings):.1f} - {max(ratings):.1f}\")\n",
    "\n",
    "# Validate data integrity\n",
    "if valid_coords != poi_dataset.count():\n",
    "    print(\"  Warning: Some POIs have invalid coordinates\")\n",
    "if len(categories) == 0:\n",
    "    raise ValueError(\"No valid categories found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae89336f",
   "metadata": {},
   "source": [
    "** What just happened?**\n",
    "- Created 3,000 realistic POI locations across 3 major cities\n",
    "- Each POI has coordinates, category, and rating information\n",
    "- Data is distributed across Ray workers for parallel processing\n",
    "- We validated our data to ensure it's ready for analysis\n",
    "\n",
    "## Step 2: Basic Spatial Operations\n",
    "\n",
    "Now let's perform basic spatial operations using Ray Data's distributed processing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d924f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZED: Reduce pandas usage and use native Ray Data operations where possible\n",
    "def calculate_distance_metrics(batch: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Calculate distance-based metrics with optimized processing.\"\"\"\n",
    "    # Group records by metro area using native Python (avoid pandas groupby)\n",
    "    metro_groups = {}\n",
    "    for record in batch:\n",
    "        metro = record.get('metro_area', 'unknown')\n",
    "        if metro not in metro_groups:\n",
    "            metro_groups[metro] = []\n",
    "        metro_groups[metro].append(record)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for metro, pois in metro_groups.items():\n",
    "        if not pois:\n",
    "            continue\n",
    "            \n",
    "        # Calculate center point using native operations\n",
    "        total_lat = sum(poi.get('latitude', 0) for poi in pois)\n",
    "        total_lon = sum(poi.get('longitude', 0) for poi in pois)\n",
    "        poi_count = len(pois)\n",
    "        center_lat = total_lat / poi_count\n",
    "        center_lon = total_lon / poi_count\n",
    "        \n",
    "        # Use vectorized haversine calculation for efficiency\n",
    "        distances = []\n",
    "        for poi in pois:\n",
    "            lat1, lon1 = np.radians(center_lat), np.radians(center_lon)\n",
    "            lat2, lon2 = np.radians(poi.get('latitude', 0)), np.radians(poi.get('longitude', 0))\n",
    "            \n",
    "            dlat = lat2 - lat1\n",
    "            dlon = lon2 - lon1\n",
    "            a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "            c = 2 * np.arcsin(np.sqrt(a))\n",
    "            dist = 6371 * c  # Earth's radius in kilometers\n",
    "            distances.append(dist)\n",
    "        \n",
    "        results.append({\n",
    "            'metro_area': metro,\n",
    "            'center_lat': center_lat,\n",
    "            'center_lon': center_lon,\n",
    "            'avg_distance_from_center': np.mean(distances),\n",
    "            'max_distance_from_center': np.max(distances),\n",
    "            'poi_count': poi_count\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Process with optimized batch processing\n",
    "distance_analysis = poi_dataset.map_batches(\n",
    "    calculate_distance_metrics,\n",
    "    batch_size=2000,    # Larger batch size for efficiency\n",
    "    concurrency=4       # Increased concurrency\n",
    ")\n",
    "\n",
    "print(\"Distance Analysis Results:\")\n",
    "distance_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4111769",
   "metadata": {},
   "source": [
    "## Step 3: Advanced Spatial Operations with Ray Data\n",
    "\n",
    "Ray Data provides powerful native operations that are perfect for geospatial analysis. Let's demonstrate filtering, grouping, joining, and aggregating spatial data using Ray Data's distributed capabilities.\n",
    "\n",
    "### Spatial Filtering and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff22cd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PRACTICE: Use Ray Data expressions API for optimized spatial queries\n",
    "from ray.data.expressions import col, lit\n",
    "\n",
    "# Find high-rated restaurants in NYC using expressions API\n",
    "high_rated_restaurants = poi_dataset.filter(\n",
    "    (col('category') == lit('restaurant')) & \n",
    "    (col('rating') > lit(4.0)) & \n",
    "    (col('metro_area') == lit('NYC'))\n",
    ")\n",
    "\n",
    "print(f\"High-rated NYC restaurants: {high_rated_restaurants.count()} found\")\n",
    "\n",
    "# Filter POIs within specific geographic bounds using expressions\n",
    "manhattan_bounds = poi_dataset.filter(\n",
    "    (col('latitude') >= lit(40.7000)) & \n",
    "    (col('latitude') <= lit(40.8000)) &\n",
    "    (col('longitude') >= lit(-74.0200)) & \n",
    "    (col('longitude') <= lit(-73.9000)) &\n",
    "    (col('metro_area') == lit('NYC'))\n",
    ")\n",
    "\n",
    "print(f\"POIs in Manhattan bounds: {manhattan_bounds.count()} locations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54827dc",
   "metadata": {},
   "source": [
    "### Spatial Aggregations and Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66764440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Ray Data's native groupby() for distributed spatial aggregations\n",
    "print(\"Performing distributed spatial aggregations...\")\n",
    "\n",
    "# Group by metro area and category using Ray Data native operations\n",
    "category_analysis = poi_dataset.groupby(['metro_area', 'category']).count()\n",
    "print(\"POI Count by Metro and Category:\")\n",
    "category_analysis.show(15)\n",
    "\n",
    "# Calculate spatial statistics by metro area\n",
    "from ray.data.aggregate import Mean, Max, Min, Count\n",
    "spatial_stats = poi_dataset.groupby('metro_area').aggregate(\n",
    "    Count('poi_id'),\n",
    "    Mean('rating'),\n",
    "    Mean('latitude'),\n",
    "    Mean('longitude')\n",
    ")\n",
    "\n",
    "print(\"\\nSpatial Statistics by Metro Area:\")\n",
    "spatial_stats.show()\n",
    "\n",
    "# Advanced aggregation: Category distribution analysis\n",
    "category_distribution = poi_dataset.groupby('category').aggregate(\n",
    "    Count('poi_id'),\n",
    "    Mean('rating'),\n",
    "    Max('rating'),\n",
    "    Min('rating')\n",
    ")\n",
    "\n",
    "print(\"\\nCategory Distribution Analysis:\")\n",
    "category_distribution.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87678b5a",
   "metadata": {},
   "source": [
    "## Step 4: Advanced Spatial Joins and Analysis\n",
    "\n",
    "Now let's demonstrate Ray Data's powerful join operations for complex spatial analysis. We'll create demographic data and join it with our POI data to understand location patterns.\n",
    "\n",
    "### Creating Demographic Data for Spatial Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7d8803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create demographic data that we can join with POI data\n",
    "def create_demographic_data():\n",
    "    \"\"\"Create realistic demographic data for spatial joins.\"\"\"\n",
    "    np.random.seed(42)  # Reproducible results\n",
    "    \n",
    "    demographics = []\n",
    "    metro_areas = ['NYC', 'LA', 'Chicago']\n",
    "    \n",
    "    for metro in metro_areas:\n",
    "        for zone_id in range(10):  # 10 zones per metro\n",
    "            demographics.append({\n",
    "                'metro_area': metro,\n",
    "                'zone_id': f'{metro}_zone_{zone_id}',\n",
    "                'population': np.random.randint(50000, 500000),\n",
    "                'median_income': np.random.randint(35000, 150000),\n",
    "                'age_median': np.random.uniform(25, 45),\n",
    "                'density_per_km2': np.random.randint(1000, 15000)\n",
    "            })\n",
    "    \n",
    "    return ray.data.from_items(demographics)\n",
    "\n",
    "# Create demographic dataset\n",
    "demographic_data = create_demographic_data()\n",
    "print(f\"Created demographic data: {demographic_data.count()} zones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20607855",
   "metadata": {},
   "source": [
    "### Spatial Joins with Ray Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085d5daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform distributed spatial join using Ray Data's native join operation\n",
    "print(\"Performing spatial join between POIs and demographics...\")\n",
    "\n",
    "# Join POI data with demographic data by metro area\n",
    "spatial_join_result = poi_dataset.join(\n",
    "    demographic_data,\n",
    "    key='metro_area',  # Join on metro area\n",
    "    join_type='inner'  # Inner join for complete matches\n",
    ")\n",
    "\n",
    "print(f\"Spatial join completed: {spatial_join_result.count()} enriched records\")\n",
    "\n",
    "# Show sample of joined data\n",
    "joined_sample = spatial_join_result.take(3)\n",
    "for i, record in enumerate(joined_sample):\n",
    "    print(f\"  {i+1}. {record['name']} in {record['metro_area']} (Pop: {record['population']:,}, Income: ${record['median_income']:,})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c141889",
   "metadata": {},
   "source": [
    "### Advanced Spatial Analytics with Ray Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96552547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Ray Data's native sort() for geographic ranking\n",
    "print(\"Ranking locations by spatial accessibility...\")\n",
    "\n",
    "# Sort POIs by rating within each metro area\n",
    "top_rated_pois = spatial_join_result.sort(['metro_area', 'rating'], descending=[False, True])\n",
    "\n",
    "print(\"Top-rated POIs by metro area:\")\n",
    "top_pois_sample = top_rated_pois.take(10)\n",
    "for poi in top_pois_sample:\n",
    "    print(f\"  {poi['name']} ({poi['category']}) - Rating: {poi['rating']:.1f} in {poi['metro_area']}\")\n",
    "\n",
    "# Use Ray Data's union() operation to combine datasets\n",
    "print(\"\\nCombining multiple geographic datasets...\")\n",
    "\n",
    "# Create additional POI data for demonstration\n",
    "additional_pois = ray.data.from_items([\n",
    "    {'poi_id': 'new_001', 'name': 'Central Park', 'category': 'park', \n",
    "     'latitude': 40.7829, 'longitude': -73.9654, 'metro_area': 'NYC', 'rating': 4.8},\n",
    "    {'poi_id': 'new_002', 'name': 'Golden Gate Bridge', 'category': 'landmark', \n",
    "     'latitude': 37.8199, 'longitude': -122.4783, 'metro_area': 'SF', 'rating': 4.9}\n",
    "])\n",
    "\n",
    "# Combine datasets using Ray Data's union operation\n",
    "combined_pois = poi_dataset.union(additional_pois)\n",
    "print(f\"Combined POI dataset: {combined_pois.count()} total locations\")\n",
    "\n",
    "# Use Ray Data's limit() for efficient sampling\n",
    "geographic_sample = combined_pois.limit(1000)\n",
    "print(f\"Geographic sample created: {geographic_sample.count()} locations\")\n",
    "\n",
    "# Use Ray Data's select() operation to focus on specific columns\n",
    "spatial_coords = combined_pois.select_columns(['latitude', 'longitude', 'metro_area', 'category'])\n",
    "print(f\"Selected spatial coordinates: {spatial_coords.count()} records\")\n",
    "\n",
    "# Chain multiple Ray Data operations for complex spatial pipeline\n",
    "spatial_pipeline_result = (combined_pois\n",
    "    .filter(lambda x: x['rating'] > 3.0)  # Filter high-quality locations\n",
    "    .groupby('category')                   # Group by POI category\n",
    "    .count()                              # Count POIs per category\n",
    "    .sort('count', descending=True)       # Sort by count\n",
    "    .limit(10)                            # Take top 10 categories\n",
    ")\n",
    "\n",
    "print(\"\\nTop POI categories by count:\")\n",
    "spatial_pipeline_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30db513e",
   "metadata": {},
   "source": [
    "### Distributed Spatial Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a8ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAnalyzer:\n",
    "    \"\"\"Spatial analysis predictor class.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the spatial analyzer.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, batch: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Perform spatial analysis on a batch of POIs.\"\"\"\n",
    "        df = pd.DataFrame(batch)\n",
    "        \n",
    "        analysis_results = []\n",
    "        \n",
    "        for metro in df['metro_area'].unique():\n",
    "            metro_data = df[df['metro_area'] == metro]\n",
    "            \n",
    "            # Analyze service accessibility\n",
    "            hospitals = metro_data[metro_data['category'] == 'hospital']\n",
    "            schools = metro_data[metro_data['category'] == 'school']\n",
    "            \n",
    "            analysis_results.append({\n",
    "                'metro_area': metro,\n",
    "                'total_pois': len(metro_data),\n",
    "                'hospital_count': len(hospitals),\n",
    "                'school_count': len(schools),\n",
    "                'hospital_density': len(hospitals) / len(metro_data) if len(metro_data) > 0 else 0,\n",
    "                'avg_rating': metro_data['rating'].mean()\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(analysis_results).to_dict('list')\n",
    "\n",
    "# Apply spatial analysis\n",
    "spatial_results = poi_dataset.map_batches(\n",
    "    SpatialAnalyzer,\n",
    "    concurrency=2,\n",
    "    batch_size=1500\n",
    ")\n",
    "\n",
    "print(\"Spatial Analysis Results:\")\n",
    "spatial_results.show()\n",
    "\n",
    "# Save spatial analysis results using Ray Data's native write operations\n",
    "print(\"Saving geospatial analysis results...\")\n",
    "\n",
    "# Write enriched POI data to Parquet for efficient storage\n",
    "spatial_join_result.write_parquet(\"s3://your-bucket/geospatial-analysis/enriched-pois/\")\n",
    "\n",
    "# Write category analysis results  \n",
    "category_distribution.write_parquet(\"s3://your-bucket/geospatial-analysis/category-stats/\")\n",
    "\n",
    "# Write top locations for business intelligence\n",
    "top_rated_pois.limit(100).write_json(\"s3://your-bucket/geospatial-analysis/top-locations.json\")\n",
    "\n",
    "print(\"Geospatial analysis results saved using Ray Data native write operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3790885a",
   "metadata": {},
   "source": [
    "## Step 5: Interactive Visualizations and Results\n",
    "\n",
    "Let's create stunning interactive visualizations to understand our spatial data:\n",
    "\n",
    "### 5.1: Interactive Heatmaps and Density Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a32924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_heatmap(dataset):\n",
    "    \"\"\"Create interactive heatmap using Folium.\"\"\"\n",
    "    print(\"Creating interactive heatmap...\")\n",
    "    \n",
    "    # Convert to pandas for visualization\n",
    "    poi_df = dataset.to_pandas()\n",
    "    \n",
    "    # Create base map centered on NYC\n",
    "    center_lat = poi_df['latitude'].mean()\n",
    "    center_lon = poi_df['longitude'].mean()\n",
    "    \n",
    "    # Create Folium map with multiple tile layers\n",
    "    m = folium.Map(\n",
    "        location=[center_lat, center_lon], \n",
    "        zoom_start=10,\n",
    "        tiles=None\n",
    "    )\n",
    "    \n",
    "    # Add multiple tile layers for better visualization\n",
    "    folium.TileLayer('OpenStreetMap').add_to(m)\n",
    "    folium.TileLayer('CartoDB Positron').add_to(m)\n",
    "    folium.TileLayer('CartoDB Dark_Matter').add_to(m)\n",
    "    \n",
    "    # Create heatmap data\n",
    "    heat_data = [[row['latitude'], row['longitude']] for _, row in poi_df.iterrows()]\n",
    "    \n",
    "    # Add heatmap layer\n",
    "    HeatMap(\n",
    "        heat_data,\n",
    "        min_opacity=0.2,\n",
    "        radius=15,\n",
    "        blur=15,\n",
    "        max_zoom=1,\n",
    "        name='POI Density Heatmap'\n",
    "    ).add_to(m)\n",
    "    \n",
    "    # Add marker clusters for detailed view\n",
    "    marker_cluster = MarkerCluster(name='POI Markers').add_to(m)\n",
    "    \n",
    "    # Add markers for each POI with category-based colors\n",
    "    category_colors = {\n",
    "        'restaurant': 'red',\n",
    "        'retail': 'blue', \n",
    "        'hospital': 'green',\n",
    "        'school': 'orange',\n",
    "        'bank': 'purple'\n",
    "    }\n",
    "    \n",
    "    for _, poi in poi_df.head(100).iterrows():  # Show first 100 for performance\n",
    "        color = category_colors.get(poi['category'], 'gray')\n",
    "        folium.Marker(\n",
    "            [poi['latitude'], poi['longitude']],\n",
    "            popup=f\"<b>{poi['name']}</b><br>Category: {poi['category']}<br>Rating: {poi['rating']:.1f}\",\n",
    "            tooltip=f\"{poi['category']}: {poi['name']}\",\n",
    "            icon=folium.Icon(color=color)\n",
    "        ).add_to(marker_cluster)\n",
    "    \n",
    "    # Add layer control\n",
    "    folium.LayerControl().add_to(m)\n",
    "    \n",
    "    # Save map\n",
    "    map_file = \"poi_heatmap.html\"\n",
    "    m.save(map_file)\n",
    "    print(f\"Interactive heatmap saved as {map_file}\")\n",
    "    \n",
    "    return m\n",
    "\n",
    "# Create the interactive heatmap\n",
    "heatmap = create_interactive_heatmap(poi_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c862d75",
   "metadata": {},
   "source": [
    "### 5.2: 3D Density Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44588999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_3d_density_plot(dataset):\n",
    "    \"\"\"Create 3D density visualization using Plotly.\"\"\"\n",
    "    print(\"Creating 3D density visualization...\")\n",
    "    \n",
    "    poi_df = dataset.to_pandas()\n",
    "    \n",
    "    # Create 3D scatter plot with density\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add scatter plot for each metro area\n",
    "    for metro in poi_df['metro_area'].unique():\n",
    "        metro_data = poi_df[poi_df['metro_area'] == metro]\n",
    "        \n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=metro_data['longitude'],\n",
    "            y=metro_data['latitude'], \n",
    "            z=metro_data['rating'],\n",
    "            mode='markers',\n",
    "            name=f'{metro} POIs',\n",
    "            marker=dict(\n",
    "                size=4,\n",
    "                opacity=0.7,\n",
    "                color=metro_data['rating'],\n",
    "                colorscale='Viridis',\n",
    "                showscale=True,\n",
    "                colorbar=dict(title=\"Rating\")\n",
    "            ),\n",
    "            text=[f\"Name: {name}<br>Category: {cat}<br>Rating: {rating:.1f}\" \n",
    "                  for name, cat, rating in zip(metro_data['name'], metro_data['category'], metro_data['rating'])],\n",
    "            hovertemplate=\"<b>%{text}</b><br>Lat: %{y:.4f}<br>Lon: %{x:.4f}<extra></extra>\"\n",
    "        ))\n",
    "    \n",
    "    # Create density surface\n",
    "    fig.add_trace(go.Mesh3d(\n",
    "        x=poi_df['longitude'],\n",
    "        y=poi_df['latitude'],\n",
    "        z=poi_df['rating'],\n",
    "        alphahull=5,\n",
    "        opacity=0.1,\n",
    "        color='lightblue',\n",
    "        name='Density Surface'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"3D POI Distribution and Rating Analysis\",\n",
    "        scene=dict(\n",
    "            xaxis_title=\"Longitude\",\n",
    "            yaxis_title=\"Latitude\", \n",
    "            zaxis_title=\"Rating\",\n",
    "            camera=dict(\n",
    "                up=dict(x=0, y=0, z=1),\n",
    "                center=dict(x=0, y=0, z=0),\n",
    "                eye=dict(x=1.5, y=1.5, z=1.5)\n",
    "            )\n",
    "        ),\n",
    "        width=800,\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    # Save as HTML\n",
    "    fig.write_html(\"3d_poi_density.html\")\n",
    "    print(\"3D visualization saved as 3d_poi_density.html\")\n",
    "    \n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create 3D density plot\n",
    "density_3d = create_3d_density_plot(poi_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7726b762",
   "metadata": {},
   "source": [
    "### 5.3: Advanced Statistical Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2c5b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_statistical_visualizations(dataset):\n",
    "    \"\"\"Create comprehensive statistical visualizations.\"\"\"\n",
    "    print(\"Creating statistical visualizations...\")\n",
    "    \n",
    "# Convert results to pandas for visualization\n",
    "spatial_df = spatial_results.to_pandas()\n",
    "    poi_df = dataset.to_pandas()\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # Create comprehensive dashboard\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. POI Distribution by Metro and Category (Heatmap)\n",
    "    ax1 = plt.subplot(3, 3, 1)\n",
    "    category_metro = poi_df.groupby(['metro_area', 'category']).size().unstack(fill_value=0)\n",
    "    sns.heatmap(category_metro, annot=True, fmt='d', cmap='YlOrRd', ax=ax1)\n",
    "    ax1.set_title('POI Distribution Heatmap\\n(Metro Area vs Category)', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # 2. Rating Distribution by Category (Violin Plot)\n",
    "    ax2 = plt.subplot(3, 3, 2)\n",
    "    sns.violinplot(data=poi_df, x='category', y='rating', ax=ax2)\n",
    "    ax2.set_title('Rating Distribution by Category', fontsize=12, fontweight='bold')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. Geographic Scatter with Density\n",
    "    ax3 = plt.subplot(3, 3, 3)\n",
    "    scatter = ax3.scatter(poi_df['longitude'], poi_df['latitude'], \n",
    "                         c=poi_df['rating'], s=30, alpha=0.6, cmap='viridis')\n",
    "    ax3.set_title('Geographic Distribution with Ratings', fontsize=12, fontweight='bold')\n",
    "    ax3.set_xlabel('Longitude')\n",
    "    ax3.set_ylabel('Latitude')\n",
    "    plt.colorbar(scatter, ax=ax3, label='Rating')\n",
    "    \n",
    "    # 4. Total POIs by Metro (Enhanced Bar Chart)\n",
    "    ax4 = plt.subplot(3, 3, 4)\n",
    "    metro_counts = poi_df['metro_area'].value_counts()\n",
    "    bars = ax4.bar(metro_counts.index, metro_counts.values, \n",
    "                   color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "    ax4.set_title('Total POIs by Metro Area', fontsize=12, fontweight='bold')\n",
    "    ax4.set_ylabel('Number of POIs')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height + 10,\n",
    "                f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 5. Hospital Density Analysis\n",
    "    ax5 = plt.subplot(3, 3, 5)\n",
    "    if len(spatial_df) > 0:\n",
    "        bars = ax5.bar(spatial_df['metro_area'], spatial_df['hospital_density'], \n",
    "                       color=['#FF9F43', '#10AC84', '#5F27CD'])\n",
    "        ax5.set_title('Hospital Density by Metro Area', fontsize=12, fontweight='bold')\n",
    "        ax5.set_ylabel('Hospitals per Total POIs')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax5.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                    f'{height:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 6. Rating vs Distance from Center\n",
    "    ax6 = plt.subplot(3, 3, 6)\n",
    "    # Calculate distance from center for each metro\n",
    "    for metro in poi_df['metro_area'].unique():\n",
    "        metro_data = poi_df[poi_df['metro_area'] == metro]\n",
    "        center_lat = metro_data['latitude'].mean()\n",
    "        center_lon = metro_data['longitude'].mean()\n",
    "        \n",
    "        distances = []\n",
    "        for _, poi in metro_data.iterrows():\n",
    "            dist = np.sqrt((poi['latitude'] - center_lat)**2 + \n",
    "                          (poi['longitude'] - center_lon)**2) * 111  # km approximation\n",
    "            distances.append(dist)\n",
    "        \n",
    "        ax6.scatter(distances, metro_data['rating'], alpha=0.6, label=metro, s=20)\n",
    "    \n",
    "    ax6.set_title('Rating vs Distance from Center', fontsize=12, fontweight='bold')\n",
    "    ax6.set_xlabel('Distance from Center (km)')\n",
    "    ax6.set_ylabel('Rating')\n",
    "    ax6.legend()\n",
    "    \n",
    "    # 7. Category Distribution (Donut Chart)\n",
    "    ax7 = plt.subplot(3, 3, 7)\n",
    "    category_counts = poi_df['category'].value_counts()\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(category_counts)))\n",
    "    wedges, texts, autotexts = ax7.pie(category_counts.values, labels=category_counts.index, \n",
    "                                      autopct='%1.1f%%', colors=colors, startangle=90,\n",
    "                                      wedgeprops=dict(width=0.5))\n",
    "    ax7.set_title('POI Category Distribution', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # 8. Rating Trends by Metro\n",
    "    ax8 = plt.subplot(3, 3, 8)\n",
    "    metro_ratings = poi_df.groupby('metro_area')['rating'].agg(['mean', 'std']).reset_index()\n",
    "    x_pos = np.arange(len(metro_ratings))\n",
    "    \n",
    "    bars = ax8.bar(x_pos, metro_ratings['mean'], yerr=metro_ratings['std'], \n",
    "                   capsize=5, color=['#E17055', '#00B894', '#6C5CE7'])\n",
    "    ax8.set_title('Average Ratings by Metro Area', fontsize=12, fontweight='bold')\n",
    "    ax8.set_xlabel('Metro Area')\n",
    "    ax8.set_ylabel('Average Rating')\n",
    "    ax8.set_xticks(x_pos)\n",
    "    ax8.set_xticklabels(metro_ratings['metro_area'])\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        ax8.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "                f'{height:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 9. Correlation Matrix\n",
    "    ax9 = plt.subplot(3, 3, 9)\n",
    "    # Create correlation matrix for numerical columns\n",
    "    numeric_cols = poi_df.select_dtypes(include=[np.number]).columns\n",
    "    correlation_matrix = poi_df[numeric_cols].corr()\n",
    "    \n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=ax9,\n",
    "                square=True, fmt='.2f')\n",
    "    ax9.set_title('Feature Correlation Matrix', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout(pad=3.0)\n",
    "    plt.savefig('geospatial_analysis_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "    \n",
    "    print(\"Statistical visualizations saved as 'geospatial_analysis_dashboard.png'\")\n",
    "\n",
    "# Create statistical visualizations\n",
    "create_statistical_visualizations(poi_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27f80c8",
   "metadata": {},
   "source": [
    "### 5.4: Interactive Plotly Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0531a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_dashboard(dataset):\n",
    "    \"\"\"Create interactive Plotly dashboard.\"\"\"\n",
    "    print(\"Creating interactive Plotly dashboard...\")\n",
    "    \n",
    "    poi_df = dataset.to_pandas()\n",
    "    \n",
    "    # Create subplots\n",
    "    from plotly.subplots import make_subplots\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Geographic Distribution', 'Category Analysis', \n",
    "                       'Rating Distribution', 'Metro Comparison'),\n",
    "        specs=[[{\"type\": \"scattermapbox\"}, {\"type\": \"bar\"}],\n",
    "               [{\"type\": \"histogram\"}, {\"type\": \"box\"}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Geographic scatter map\n",
    "    fig.add_trace(\n",
    "        go.Scattermapbox(\n",
    "            lat=poi_df['latitude'],\n",
    "            lon=poi_df['longitude'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=8,\n",
    "                color=poi_df['rating'],\n",
    "                colorscale='Viridis',\n",
    "                showscale=True,\n",
    "                colorbar=dict(title=\"Rating\", x=0.45)\n",
    "            ),\n",
    "            text=[f\"Name: {name}<br>Category: {cat}<br>Rating: {rating:.1f}\" \n",
    "                  for name, cat, rating in zip(poi_df['name'], poi_df['category'], poi_df['rating'])],\n",
    "            hovertemplate=\"<b>%{text}</b><br>Lat: %{lat:.4f}<br>Lon: %{lon:.4f}<extra></extra>\",\n",
    "            name=\"POIs\"\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Category bar chart\n",
    "    category_counts = poi_df['category'].value_counts()\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=category_counts.index,\n",
    "            y=category_counts.values,\n",
    "            marker_color='lightblue',\n",
    "            name=\"Categories\"\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Rating histogram\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=poi_df['rating'],\n",
    "            nbinsx=20,\n",
    "            marker_color='lightgreen',\n",
    "            name=\"Rating Distribution\"\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Box plot by metro\n",
    "    for metro in poi_df['metro_area'].unique():\n",
    "        metro_data = poi_df[poi_df['metro_area'] == metro]\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=metro_data['rating'],\n",
    "                name=metro,\n",
    "                boxpoints='all',\n",
    "                jitter=0.3,\n",
    "                pointpos=-1.8\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title_text=\"Interactive Geospatial Analysis Dashboard\",\n",
    "        title_x=0.5,\n",
    "        height=800,\n",
    "        showlegend=False,\n",
    "        mapbox=dict(\n",
    "            style=\"open-street-map\",\n",
    "            center=dict(lat=poi_df['latitude'].mean(), lon=poi_df['longitude'].mean()),\n",
    "            zoom=8\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Update axes titles\n",
    "    fig.update_xaxes(title_text=\"Category\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Count\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Rating\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Frequency\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Rating\", row=2, col=2)\n",
    "    \n",
    "    # Save and show\n",
    "    fig.write_html(\"interactive_geospatial_dashboard.html\")\n",
    "    print(\"Interactive dashboard saved as 'interactive_geospatial_dashboard.html'\")\n",
    "    fig.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create interactive dashboard\n",
    "dashboard = create_interactive_dashboard(poi_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07d3f26",
   "metadata": {},
   "source": [
    "## Step 6: Saving Results\n",
    "\n",
    "Save your processed geospatial data for further analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72716ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "# Save results to parquet format\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "# Save spatial analysis results\n",
    "spatial_results.write_parquet(f\"local://{temp_dir}/spatial_analysis\")\n",
    "print(f\"Results saved to {temp_dir}/spatial_analysis\")\n",
    "\n",
    "# Save category analysis\n",
    "category_analysis.write_parquet(f\"local://{temp_dir}/category_analysis\")\n",
    "print(f\"Category analysis saved to {temp_dir}/category_analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eaa79a",
   "metadata": {},
   "source": [
    "## Performance Tips\n",
    "\n",
    "When working with large geospatial datasets:\n",
    "\n",
    "1. **Batch Size**: Use appropriate batch sizes based on your data size and available memory\n",
    "2. **Concurrency**: Set concurrency based on your cluster size and CPU cores\n",
    "3. **Memory Management**: Use streaming operations for very large datasets\n",
    "4. **Spatial Indexing**: Consider spatial indexing for complex geometric operations\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "Common issues and solutions:\n",
    "\n",
    "- **Out of Memory**: Reduce batch size or increase cluster resources\n",
    "- **Slow Performance**: Check concurrency settings and cluster utilization\n",
    "- **Coordinate System Issues**: Ensure consistent coordinate reference systems\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To extend this example:\n",
    "- Load real geospatial data from sources like OpenStreetMap or government APIs\n",
    "- Implement more complex spatial operations using specialized libraries\n",
    "- Add streaming processing for real-time geospatial data\n",
    "- Integrate with mapping services for visualization\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ead4c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up Ray resources when finished with geospatial analysis\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "    print(\"Ray cluster resources cleaned up successfully\")\n",
    "    print(\"Geospatial analysis pipeline completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67731e8",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "**Ray Data Native Operations Mastered**:\n",
    "- `read_parquet()` for loading geospatial datasets efficiently\n",
    "- `map_batches()` for distributed spatial calculations and transformations\n",
    "- `filter()` for spatial queries and geographic bounds filtering\n",
    "- `groupby()` and `aggregate()` for spatial statistics and analysis\n",
    "- `join()` for combining POI data with demographic information\n",
    "- `sort()` for geographic ranking and accessibility analysis\n",
    "- `union()` for combining multiple geographic datasets\n",
    "- `select_columns()` for focusing on spatial coordinates\n",
    "- `limit()` and `take()` for efficient spatial sampling\n",
    "- `write_parquet()` and `write_json()` for saving analysis results\n",
    "\n",
    "**Geospatial Processing Excellence**: Ray Data's distributed architecture enables processing millions of location points with sophisticated spatial operations that scale horizontally across clusters while maintaining memory efficiency through streaming execution.\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting Common Issues\n",
    "\n",
    "### **Problem: \"Memory errors with large datasets\"**\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8584b4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce batch size for memory-intensive operations\n",
    "ds.map_batches(spatial_function, batch_size=100, concurrency=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c49ca87",
   "metadata": {},
   "source": [
    "### **Problem: \"Slow distance calculations\"**\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9b07f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use vectorized operations for better performance\n",
    "import numpy as np\n",
    "# Vectorized haversine distance is much faster than loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c6aad5",
   "metadata": {},
   "source": [
    "### **Problem: \"Coordinate system issues\"**\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c150fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always validate coordinate ranges\n",
    "def validate_coordinates(lat, lon):\n",
    "    return -90 <= lat <= 90 and -180 <= lon <= 180"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245850fe",
   "metadata": {},
   "source": [
    "### **Debug and Monitoring Capabilities** (rule #200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a3a163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable debug mode for detailed logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# Ray Data debugging utilities\n",
    "def debug_dataset_info(dataset, name=\"dataset\"):\n",
    "    \"\"\"Debug utility to inspect dataset characteristics.\"\"\"\n",
    "    print(f\"\\n=== Debug Info for {name} ===\")\n",
    "    print(f\"Record count: {dataset.count()}\")\n",
    "    print(f\"Schema: {dataset.schema()}\")\n",
    "    print(f\"Sample record: {dataset.take(1)[0] if dataset.count() > 0 else 'No records'}\")\n",
    "    \n",
    "    # Check for common issues\n",
    "    try:\n",
    "        sample_batch = dataset.take_batch(10)\n",
    "        print(f\"Batch extraction: Success ({len(sample_batch)} records)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Batch extraction: Failed - {e}\")\n",
    "\n",
    "# Example usage: debug_dataset_info(poi_dataset, \"POI dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5fa1d8",
   "metadata": {},
   "source": [
    "## Troubleshooting Common Issues\n",
    "\n",
    "### **Problem: \"Invalid coordinate values\"**\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c39670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate coordinates before processing\n",
    "def validate_coordinates(batch):\n",
    "    valid_records = []\n",
    "    for record in batch:\n",
    "        lat = record.get('latitude', 0)\n",
    "        lon = record.get('longitude', 0)\n",
    "        if -90 <= lat <= 90 and -180 <= lon <= 180:\n",
    "            valid_records.append(record)\n",
    "    return valid_records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4a86ce",
   "metadata": {},
   "source": [
    "### **Problem: \"Memory issues with large spatial datasets\"**\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df52a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use smaller batch sizes for memory-intensive spatial calculations\n",
    "dataset.map_batches(spatial_function, batch_size=500, concurrency=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9176dd6e",
   "metadata": {},
   "source": [
    "### **Problem: \"Slow distance calculations\"**\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1ef957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use vectorized operations for better performance\n",
    "import numpy as np\n",
    "\n",
    "def fast_haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Vectorized haversine distance calculation.\"\"\"\n",
    "    R = 6371  # Earth's radius in kilometers\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    return 2 * R * np.arcsin(np.sqrt(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43d0dc3",
   "metadata": {},
   "source": [
    "### **Performance Optimization Tips**\n",
    "\n",
    "1. **Batch Processing**: Process locations in batches of 1000-5000 for optimal performance\n",
    "2. **Spatial Indexing**: Use spatial indexing for nearest neighbor searches\n",
    "3. **Coordinate Validation**: Always validate lat/lon ranges before processing\n",
    "4. **Memory Management**: Monitor memory usage for large spatial datasets\n",
    "5. **Parallel Processing**: Leverage Ray's automatic parallelization for spatial operations\n",
    "\n",
    "### **Performance Considerations**\n",
    "\n",
    "Ray Data provides several advantages for geospatial processing:\n",
    "- **Parallel computation**: Distance calculations are distributed across multiple workers\n",
    "- **Memory efficiency**: Large coordinate datasets are processed in manageable chunks\n",
    "- **Scalability**: The same code patterns work for neighborhood-scale to continental-scale analysis\n",
    "- **Automatic optimization**: Ray Data handles data partitioning and load balancing automatically\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps and Extensions\n",
    "\n",
    "### **Try These Advanced Features**\n",
    "1. **Real Datasets**: Use OpenStreetMap data or Census TIGER files\n",
    "2. **Spatial Joins**: Join POI data with demographic or economic data\n",
    "3. **Clustering Analysis**: Group POIs by spatial proximity and characteristics\n",
    "4. **Route Optimization**: Calculate optimal routes between multiple POIs\n",
    "5. **Heatmap Generation**: Create density maps and spatial visualizations\n",
    "\n",
    "### **Production Considerations**\n",
    "- **Coordinate System Management**: Handle different coordinate reference systems\n",
    "- **Spatial Indexing**: Implement R-tree or other spatial indexes for performance\n",
    "- **Real-Time Processing**: Adapt for streaming location data\n",
    "- **Privacy Protection**: Implement location privacy and anonymization\n",
    "- **Scalability**: Handle continental or global-scale spatial analysis\n",
    "\n",
    "### **Community Support** (rule #123)\n",
    "\n",
    "**Getting Help**:\n",
    "- [Ray Data GitHub Discussions](https://github.com/ray-project/ray/discussions)\n",
    "- [Ray Slack Community](https://forms.gle/9TSdDYUgxYs8SA9e8)\n",
    "- [Stack Overflow - Ray Data](https://stackoverflow.com/questions/tagged/ray-data)\n",
    "- [Ray Data Examples Repository](https://github.com/ray-project/ray/tree/master/python/ray/data/examples)\n",
    "\n",
    "### **Related Ray Data Templates**\n",
    "- **Ray Data Large-Scale ETL Optimization**: Optimize spatial data pipelines\n",
    "- **Ray Data Data Quality Monitoring**: Validate spatial data quality\n",
    "- **Ray Data Batch Inference Optimization**: Optimize spatial ML models\n",
    "\n",
    "## Cleanup and Resource Management\n",
    "\n",
    "Always clean up Ray resources when done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e8b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up Ray resources\n",
    "ray.shutdown()\n",
    "print(\"Ray cluster shutdown complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8e26db",
   "metadata": {},
   "source": [
    "** Congratulations!** You've successfully built a scalable geospatial analysis pipeline with Ray Data!\n",
    "\n",
    "These spatial processing techniques scale from city-level to continental-level analysis with the same code patterns.\n",
    "- Ray Data integrates well with existing geospatial Python libraries"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
