{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "717c34eb",
      "metadata": {},
      "source": [
        "# Large-scale ETL optimization with Ray Data\n",
        "\n",
        "**Time to complete**: 35 min | **Difficulty**: Intermediate | **Prerequisites**: Understanding of ETL concepts, data processing experience\n",
        "\n",
        "## What You'll Build\n",
        "\n",
        "Create an ETL pipeline that processes large datasets using Ray Data's distributed processing capabilities. Learn optimization techniques that help production ETL jobs scale reliably.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [ETL Data Creation](#step-1-creating-sample-etl-data) (8 min)\n",
        "2. [Optimized Transformations](#step-2-efficient-data-transformations) (12 min)\n",
        "3. [Parallel Processing](#step-3-distributed-etl-operations) (10 min)\n",
        "4. [Performance Monitoring](#step-4-etl-performance-optimization) (5 min)\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "**Why ETL optimization matters**: The difference between fast and slow data pipelines directly impacts business agility and operational costs at enterprise scale. Understanding optimization techniques enables data teams to deliver insights faster while reducing infrastructure costs.\n",
        "\n",
        "**Ray Data's ETL capabilities**: Native operations for distributed processing that automatically optimize memory, CPU, and I/O utilization. You'll learn how Ray Data's architecture enables efficient processing of massive datasets.\n",
        "\n",
        "**Real-world optimization patterns**: Techniques used by companies like Netflix and Airbnb to process petabytes of data daily for business intelligence demonstrate the practical application of distributed ETL optimization.\n",
        "\n",
        "**Performance tuning strategies**: Memory management, parallel processing, and resource configuration patterns for production ETL workloads.\n",
        "\n",
        "## Overview\n",
        "\n",
        "**The Challenge**: Traditional ETL tools struggle with modern data volumes. Processing terabytes of data can take days, creating bottlenecks in data-driven organizations.\n",
        "\n",
        "**The Solution**: Ray Data's distributed architecture and optimized operations enable efficient processing of large datasets through parallel computation.\n",
        "\n",
        "**Enterprise ETL at Scale**\n",
        "\n",
        "Leading technology companies demonstrate the critical importance of optimized ETL processing. Data warehouse companies like Snowflake process petabytes of data daily for business intelligence applications, requiring sophisticated distributed ETL pipelines that maintain sub-second query response times. E-commerce giants like Amazon process billions of transactions continuously for real-time recommendation systems, where ETL latency directly impacts revenue and customer experience.\n",
        "\n",
        "Social media platforms like Facebook process trillions of user events for content ranking and advertising optimization, demanding ETL systems that handle massive data volumes while preserving data quality and consistency. Ride-sharing companies like Uber process millions of trips for dynamic pricing and driver matching algorithms, where ETL efficiency affects operational costs and service reliability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a457bd89",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: High-volume transaction processing like Amazon\n",
        "def process_transaction_stream(batch):\n",
        "    \"\"\"Process e-commerce transactions for real-time analytics.\"\"\"\n",
        "    processed_transactions = []\n",
        "    \n",
        "    for transaction in batch:\n",
        "        # Calculate derived fields for analytics\n",
        "        processed_transaction = {\n",
        "            'transaction_id': transaction['id'],\n",
        "            'customer_segment': 'premium' if transaction['amount'] > 500 else 'standard',\n",
        "            'product_category': transaction['product_type'],\n",
        "            'revenue_impact': transaction['amount'] * transaction['quantity'],\n",
        "            'processing_timestamp': transaction['timestamp'],\n",
        "            'geographic_region': transaction['shipping_region']\n",
        "        }\n",
        "        \n",
        "        processed_transactions.append(processed_transaction)\n",
        "    \n",
        "    return processed_transactions\n",
        "\n",
        "# Demonstrate high-volume ETL processing\n",
        "print(\"Enterprise-scale transaction processing enabled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f3979d6",
      "metadata": {},
      "source": [
        "These implementations showcase how distributed ETL systems enable real-time business intelligence and operational efficiency at unprecedented scale.\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisites Checklist\n",
        "\n",
        "Before starting, ensure you have:\n",
        "- [ ] Understanding of ETL (Extract, Transform, Load) concepts\n",
        "- [ ] Experience with data processing and transformations\n",
        "- [ ] Familiarity with distributed computing concepts\n",
        "- [ ] Python environment with sufficient memory (8GB+ recommended)\n",
        "\n",
        "## Quick Start (3 minutes)\n",
        "\n",
        "Want to see high-performance ETL immediately? This section demonstrates the core concepts in just a few minutes.\n",
        "\n",
        "### Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "192c7e9f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import ray\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Initialize Ray for distributed processing\n",
        "ray.init(ignore_reinit_error=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b9cd9d9",
      "metadata": {},
      "source": [
        "### Load ETL dataset\n",
        "\n",
        "Load a real dataset or a prepared local subset for ETL processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa1bafa6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ray.data import read_parquet\n",
        "\n",
        "print(\"Loading ETL input dataset...\")\n",
        "ds = read_parquet(\n",
        "    \"s3://anonymous@nyc-tlc/trip_data/\",\n",
        ")\n",
        "print(f\"Created ETL dataset with {ds.count():,} records\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39ce961c",
      "metadata": {},
      "source": [
        "**What just happened?**\n",
        "- Loaded a real dataset into a Ray Dataset\n",
        "- Validated record count and schema quickly\n",
        "\n",
        "### Quick ETL Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a554ac4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick ETL demonstration\n",
        "print(\"Running quick ETL transformation...\")\n",
        "result = ds.map_batches(lambda batch: [\n",
        "    {**record, \"total_value\": record[\"amount\"] * record[\"quantity\"]} \n",
        "    for record in batch\n",
        "], batch_size=1000)\n",
        "\n",
        "sample_results = result.take(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e97b3b7d",
      "metadata": {},
      "source": [
        "### Display Processed Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f21c3aa1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display results in a visually appealing table format\n",
        "print(\"Sample Processed Records:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"{'Order ID':<12} {'Customer':<12} {'Product':<10} {'Qty':<4} {'Amount':<8} {'Total Value':<12}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for record in sample_results:\n",
        "    print(f\"{record['order_id']:<12} {record['customer_id']:<12} {record['product_id']:<10} \"\n",
        "          f\"{record['quantity']:<4} ${record['amount']:<7.2f} ${record['total_value']:<11.2f}\")\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print(f\"Dataset Summary: {ds.count():,} total records ready for advanced ETL processing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25dbac12",
      "metadata": {},
      "source": [
        "### Data distribution analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc0e35ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show data distribution for better understanding\n",
        "regions = [r['region'] for r in sample_results]\n",
        "print(f\"Regional Distribution (sample): {dict(pd.Series(regions).value_counts())}\")\n",
        "\n",
        "# Calculate and display basic statistics\n",
        "amounts = [r['amount'] for r in sample_results]\n",
        "print(f\"Amount Statistics (sample): Min=${min(amounts):.2f}, Max=${max(amounts):.2f}, Avg=${np.mean(amounts):.2f}\")\n",
        "\n",
        "print(f\"\\nReady for advanced ETL processing!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc8efc40",
      "metadata": {},
      "source": [
        "**Key takeaways from quick start:**\n",
        "- Ray Data handles large datasets efficiently through distributed processing\n",
        "- Simple transformations can be applied using `map_batches()` \n",
        "- Results can be displayed in professional, readable formats\n",
        "- The same patterns scale from thousands to millions of records\n",
        "\n",
        "## Why ETL Performance Matters\n",
        "\n",
        "**The Scale Challenge**:\n",
        "- **Volume**: Modern companies generate terabytes of data daily\n",
        "- **Velocity**: Business decisions require real-time or near-real-time data\n",
        "- **Complexity**: Data comes from dozens of sources in different formats\n",
        "- **Cost**: Slow ETL means expensive compute resources running longer\n",
        "\n",
        "**Performance Considerations**:\n",
        "\n",
        "| ETL Approach | Characteristics | Business Impact |\n",
        "|--------------|----------------|-----------------|\n",
        "| **Traditional ETL** | Single-machine processing | Limited scalability, resource constraints |\n",
        "| **Ray Data ETL** | Distributed parallel processing | Horizontal scalability, efficient resource utilization |\n",
        "| **Key Difference** | Distributed vs. centralized | Better resource utilization and scalability |\n",
        "\n",
        "## Use Case: E-commerce Data Warehouse ETL\n",
        "\n",
        "We'll build an ETL pipeline that processes:\n",
        "- **Customer Data**: Demographics, preferences, segments (10M+ records)\n",
        "- **Transaction Data**: Orders, payments, refunds (100M+ records)  \n",
        "- **Product Data**: Catalog, inventory, pricing (1M+ records)\n",
        "- **Behavioral Data**: Clicks, views, searches (1B+ records)\n",
        "\n",
        "The pipeline will:\n",
        "1. Extract data from multiple sources in parallel\n",
        "2. Apply data quality validation and cleansing\n",
        "3. Perform complex joins and aggregations\n",
        "4. Generate business intelligence metrics\n",
        "5. Load results to analytical data stores\n",
        "\n",
        "## Architecture\n",
        "\n",
        "### **Ray Data ETL processing architecture**\n",
        "\n",
        "```\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502                           Enterprise Data Sources                                \u2502\n",
        "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u2502\n",
        "\u2502  \u2502   Customer   \u2502 \u2502    Orders    \u2502 \u2502   Products   \u2502 \u2502  Behavioral  \u2502           \u2502\n",
        "\u2502  \u2502     Data     \u2502 \u2502     Data     \u2502 \u2502     Data     \u2502 \u2502     Data     \u2502           \u2502\n",
        "\u2502  \u2502   (10M+)     \u2502 \u2502   (100M+)    \u2502 \u2502    (1M+)     \u2502 \u2502    (1B+)     \u2502           \u2502\n",
        "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "                                        \u2502\n",
        "                                        \u25bc\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502                         Ray Data Ingestion Layer                                \u2502\n",
        "\u2502  \u2022 ray.data.read_parquet() \u2022 ray.data.read_csv() \u2022 ray.data.read_json()       \u2502\n",
        "\u2502  \u2022 Distributed loading across cluster \u2022 Automatic partitioning                \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "                                        \u2502\n",
        "                                        \u25bc\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502                     Parallel ETL Processing Engine                              \u2502\n",
        "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n",
        "\u2502  \u2502   Validation    \u2502 \u2502   Cleansing     \u2502 \u2502   Enrichment    \u2502                  \u2502\n",
        "\u2502  \u2502 \u2022 Data quality  \u2502 \u2502 \u2022 Deduplication \u2502 \u2502 \u2022 Joins         \u2502                  \u2502\n",
        "\u2502  \u2502 \u2022 Schema checks \u2502 \u2502 \u2022 Normalization \u2502 \u2502 \u2022 Calculations  \u2502                  \u2502\n",
        "\u2502  \u2502 \u2022 Business rules\u2502 \u2502 \u2022 Type casting  \u2502 \u2502 \u2022 Aggregations  \u2502                  \u2502\n",
        "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "                                        \u2502\n",
        "                                        \u25bc\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502                        Analytics & Storage Layer                                \u2502\n",
        "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n",
        "\u2502  \u2502 Data Warehouse  \u2502 \u2502   OLAP Cubes    \u2502 \u2502    Reports      \u2502                  \u2502\n",
        "\u2502  \u2502 \u2022 Partitioned   \u2502 \u2502 \u2022 Aggregated    \u2502 \u2502 \u2022 Dashboards    \u2502                  \u2502\n",
        "\u2502  \u2502 \u2022 Optimized     \u2502 \u2502 \u2022 Indexed       \u2502 \u2502 \u2022 Alerts        \u2502                  \u2502\n",
        "\u2502  \u2502 \u2022 Compressed    \u2502 \u2502 \u2022 Cached        \u2502 \u2502 \u2022 Insights      \u2502                  \u2502\n",
        "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "```\n",
        "\n",
        "### **Ray Data Advantages for ETL**\n",
        "\n",
        "| Traditional ETL Approach | Ray Data ETL Approach | Key Difference |\n",
        "|---------------------------|----------------------|----------------|\n",
        "| **Single-machine processing** | Distributed across multiple CPU cores | Horizontal scalability |\n",
        "| **Sequential operations** | Parallel processing pipeline | Concurrent execution |\n",
        "| **Manual resource management** | Automatic scaling and load balancing | Simplified operations |\n",
        "| **Complex infrastructure setup** | Native Ray Data operations | Streamlined development |\n",
        "| **Limited fault tolerance** | Built-in error recovery and retries | Enhanced reliability |\n",
        "\n",
        "## Key Components\n",
        "\n",
        "### 1. **Parallel Data Extraction**\n",
        "- `ray.data.read_parquet()` for efficient columnar data\n",
        "- `ray.data.read_csv()` for structured text data\n",
        "- `ray.data.read_json()` for semi-structured data\n",
        "- Optimized file reading with block size tuning\n",
        "\n",
        "### 2. **Native Data Transformations**\n",
        "- `dataset.map()` for row-wise transformations\n",
        "- `dataset.map_batches()` for vectorized operations\n",
        "- `dataset.filter()` for data selection\n",
        "- `dataset.flat_map()` for one-to-many transformations\n",
        "\n",
        "### 3. **Distributed Aggregations**\n",
        "- `dataset.groupby()` for aggregation operations\n",
        "- Native sorting and ranking operations\n",
        "- Statistical calculations and metrics\n",
        "- Cross-dataset joins and correlations\n",
        "\n",
        "### 4. **Optimized Data Loading**\n",
        "- `dataset.write_parquet()` for analytical workloads\n",
        "- `dataset.write_csv()` for reporting systems\n",
        "- Partitioned writing for optimal query performance\n",
        "- Compression and encoding optimization\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Ray cluster with sufficient memory and CPU cores\n",
        "- Python 3.8+ with Ray Data\n",
        "- Access to large datasets (multi-GB recommended)\n",
        "- Basic understanding of ETL concepts and data processing\n",
        "\n",
        "## Installation\n",
        "\n",
        "```bash\n",
        "pip install ray[data] pyarrow fastparquet\n",
        "pip install numpy pandas\n",
        "pip install boto3 s3fs\n",
        "pip install matplotlib seaborn plotly networkx psutil\n",
        "```\n",
        "\n",
        "## Quick Start\n",
        "\n",
        "### 1. **Load Large Datasets with Ray Data Native Operations**\n",
        "\n",
        "Let's load real-world datasets using Ray Data's native reading capabilities.\n",
        "\n",
        "**Import Required Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "003fe5e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "from typing import Dict, Any\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Third-party imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33249bbd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ray Data imports\n",
        "import ray\n",
        "from ray.data import read_parquet, read_csv\n",
        "\n",
        "print(\"All libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d53d2b02",
      "metadata": {},
      "source": [
        "**Initialize Ray with Optimized Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad4c3958",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Ray with optimized configuration for large-scale ETL\n",
        "ray.init(\n",
        "    object_store_memory=10_000_000_000,  # 10GB object store\n",
        "    _memory=20_000_000_000               # 20GB heap memory\n",
        ")\n",
        "\n",
        "print(\"Ray cluster initialized with optimized ETL configuration\")\n",
        "print(f\"Available resources: {ray.cluster_resources()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2080ac57",
      "metadata": {},
      "source": [
        "**Load NYC Taxi Data (public dataset)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90704bf1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load NYC Taxi data - publicly available, large scale dataset\n",
        "print(\"Loading NYC Taxi dataset...\")\n",
        "taxi_data = read_parquet(\n",
        "    \"s3://anonymous@nyc-tlc/trip_data/\",\n",
        "    columns=[\"pickup_datetime\", \"dropoff_datetime\", \"passenger_count\", \n",
        "             \"trip_distance\", \"fare_amount\", \"total_amount\"]\n",
        ")\n",
        "\n",
        "print(f\"NYC Taxi data loaded: {taxi_data.count():,} trip records\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7db2dd42",
      "metadata": {},
      "source": [
        "**Load Amazon Reviews Data (text + structured)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15531f1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Amazon product reviews - text + structured data\n",
        "print(\"Loading Amazon Reviews dataset...\")\n",
        "reviews_data = read_parquet(\n",
        "    \"s3://anonymous@amazon-reviews-pds/parquet/\",\n",
        "    columns=[\"review_date\", \"star_rating\", \"review_body\", \"product_category\"]\n",
        ")\n",
        "\n",
        "print(f\"Amazon Reviews loaded: {reviews_data.count():,} review records\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73bc00ff",
      "metadata": {},
      "source": [
        "**Load US Census Data (demographic information)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81835105",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load US Census data - demographic information\n",
        "print(\"Loading US Census dataset...\")\n",
        "census_data = read_csv(\"s3://anonymous@uscensus-grp/acs/2021_5yr_data.csv\")\n",
        "\n",
        "print(f\"US Census data loaded: {census_data.count():,} demographic records\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dad109e2",
      "metadata": {},
      "source": [
        "# Display dataset information with visual formatting\n",
        "datasets_info = [\n",
        "    (\"Taxi Data\", taxi_data.count(), taxi_data.schema()),\n",
        "    (\"Reviews Data\", reviews_data.count(), reviews_data.schema()),\n",
        "    (\"Census Data\", census_data.count(), census_data.schema())\n",
        "]\n",
        "\n",
        "print(\"Loaded Datasets Summary:\")\n",
        "print(\"=\" * 100)\n",
        "print(f\"{'Dataset':<15} {'Record Count':<15} {'Schema Preview':<50}\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "for name, count, schema in datasets_info:\n",
        "    # Get first few column names for schema preview\n",
        "    schema_preview = str(schema)[:47] + \"...\" if len(str(schema)) > 50 else str(schema)\n",
        "    print(f\"{name:<15} {count:<15,} {schema_preview:<50}\")\n",
        "\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Display sample records from each dataset\n",
        "print(\"\\nSample Data Preview:\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "# Taxi data sample\n",
        "taxi_sample = taxi_data.take(2)\n",
        "print(\"Taxi Data Sample:\")\n",
        "for i, record in enumerate(taxi_sample):\n",
        "    pickup = record.get('pickup_datetime', 'N/A')\n",
        "    fare = record.get('fare_amount', 0)\n",
        "    distance = record.get('trip_distance', 0)\n",
        "    print(f\"  {i+1}. Pickup: {pickup}, Fare: ${fare:.2f}, Distance: {distance:.1f}mi\")\n",
        "\n",
        "# Reviews data sample  \n",
        "reviews_sample = reviews_data.take(2)\n",
        "print(\"\\nReviews Data Sample:\")\n",
        "for i, record in enumerate(reviews_sample):\n",
        "    rating = record.get('star_rating', 'N/A')\n",
        "    category = record.get('product_category', 'N/A')\n",
        "    body_preview = str(record.get('review_body', ''))[:60] + \"...\" if len(str(record.get('review_body', ''))) > 60 else str(record.get('review_body', ''))\n",
        "    print(f\"  {i+1}. Rating: {rating} stars, Category: {category}\")\n",
        "    print(f\"      Review: {body_preview}\")\n",
        "\n",
        "print(\"-\" * 100)\n",
        "\n",
        "# Data format efficiency demonstration\n",
        "print(\"\\nData Format Efficiency:\")\n",
        "print(\"Using Parquet format for taxi and reviews data (optimal for analytics)\")\n",
        "print(\"Using CSV for census data (consider converting to Parquet for better performance)\")\n",
        "\n",
        "# Example: Convert CSV to Parquet for better performance\n",
        "# census_parquet = census_data.write_parquet(\"s3://your-bucket/census_optimized/\")\n",
        "```\n",
        "\n",
        "### 2. **Data Quality and Validation with Native Operations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91e53250",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate Ray Data native operations with expressions API\n",
        "from ray.data.expressions import col, lit\n",
        "print(\"Using Ray Data native operations with optimized expressions...\")\n",
        "\n",
        "# BEST PRACTICE: Use expressions API for better performance\n",
        "valid_taxi_trips = taxi_data.filter(\n",
        "    (col(\"fare_amount\") > lit(0)) & (col(\"trip_distance\") > lit(0))\n",
        ")\n",
        "\n",
        "# Native groupby operations for aggregation\n",
        "from ray.data.aggregate import Mean\n",
        "trip_stats = valid_taxi_trips.groupby(\"passenger_count\").aggregate(\n",
        "    Mean(\"fare_amount\"),\n",
        "    Mean(\"trip_distance\")\n",
        ")\n",
        "\n",
        "# Native sorting for ordered results  \n",
        "sorted_trips = valid_taxi_trips.sort(\"fare_amount\", descending=True)\n",
        "\n",
        "# Advanced filtering with expressions for business rules\n",
        "high_value_trips = valid_taxi_trips.filter(\n",
        "    (col(\"fare_amount\") > lit(50)) & \n",
        "    (col(\"trip_distance\") > lit(10)) &\n",
        "    (col(\"passenger_count\") >= lit(2))\n",
        ")\n",
        "\n",
        "print(f\"Filtered to {valid_taxi_trips.count()} valid trips\")\n",
        "print(f\"Grouped statistics by passenger count\")\n",
        "print(f\"Sorted trips by fare amount\")\n",
        "\n",
        "# Use Ray Data native operations for data quality checks\n",
        "def validate_taxi_data(batch):\n",
        "    \"\"\"Validate taxi trip data using business rules.\"\"\"\n",
        "    valid_records = []\n",
        "    \n",
        "    for record in batch:\n",
        "        # Apply validation rules\n",
        "        is_valid = True\n",
        "        validation_errors = []\n",
        "        \n",
        "        # Check fare amount\n",
        "        if record.get('fare_amount', 0) < 0:\n",
        "            is_valid = False\n",
        "            validation_errors.append(\"Negative fare amount\")\n",
        "        \n",
        "        # Check trip distance\n",
        "        if record.get('trip_distance', 0) < 0:\n",
        "            is_valid = False\n",
        "            validation_errors.append(\"Negative trip distance\")\n",
        "        \n",
        "        # Check passenger count\n",
        "        if record.get('passenger_count', 0) <= 0 or record.get('passenger_count', 0) > 6:\n",
        "            is_valid = False\n",
        "            validation_errors.append(\"Invalid passenger count\")\n",
        "        \n",
        "        # Add validation metadata\n",
        "        validated_record = {\n",
        "            **record,\n",
        "            'is_valid': is_valid,\n",
        "            'validation_errors': validation_errors,\n",
        "            'validation_timestamp': pd.Timestamp.now().isoformat()\n",
        "        }\n",
        "        \n",
        "        valid_records.append(validated_record)\n",
        "    \n",
        "    return valid_records\n",
        "\n",
        "# Apply validation using Ray Data native map_batches\n",
        "validated_taxi = taxi_data.map_batches(\n",
        "    validate_taxi_data,\n",
        "    batch_size=10000,\n",
        "    concurrency=8\n",
        ")\n",
        "\n",
        "# Filter to only valid records using native filter operation\n",
        "clean_taxi_data = validated_taxi.filter(lambda record: record['is_valid'])\n",
        "\n",
        "print(f\"Clean taxi data: {clean_taxi_data.count()} records\")\n",
        "\n",
        "# Display data quality summary in a visual format\n",
        "print(\"\\nData Quality Summary:\")\n",
        "print(\"=\" * 60)\n",
        "total_records = taxi_data.count()\n",
        "clean_records = clean_taxi_data.count()\n",
        "invalid_records = total_records - clean_records\n",
        "\n",
        "print(f\"{'Metric':<25} {'Count':<10} {'Percentage':<12}\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'Total Records':<25} {total_records:<10,} {'100.0%':<12}\")\n",
        "print(f\"{'Valid Records':<25} {clean_records:<10,} {clean_records/total_records*100:<11.1f}%\")\n",
        "print(f\"{'Invalid Records':<25} {invalid_records:<10,} {invalid_records/total_records*100:<11.1f}%\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Sample clean records for inspection\n",
        "sample_clean = clean_taxi_data.take(3)\n",
        "print(f\"\\nSample Clean Records:\")\n",
        "print(\"-\" * 100)\n",
        "for i, record in enumerate(sample_clean):\n",
        "    fare = record.get('fare_amount', 0)\n",
        "    distance = record.get('trip_distance', 0)\n",
        "    passengers = record.get('passenger_count', 0)\n",
        "    print(f\"{i+1}. Fare: ${fare:.2f}, Distance: {distance:.1f}mi, Passengers: {passengers}, Valid: {record.get('is_valid', False)}\")\n",
        "print(\"-\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79f99232",
      "metadata": {},
      "source": [
        "### 3. **Large-Scale Aggregations with Native GroupBy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8166a85a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# BEST PRACTICE: Use Ray Data native groupby operations instead of pandas\n",
        "from ray.data.aggregate import Count, Sum, Mean, Std\n",
        "\n",
        "# First, extract date from pickup_datetime using add_column\n",
        "def extract_pickup_date(batch):\n",
        "    \"\"\"Extract date from pickup_datetime for groupby operations.\"\"\"\n",
        "    import pandas as pd\n",
        "    \n",
        "    # Convert pickup_datetime to date for each record\n",
        "    enhanced_records = []\n",
        "    for record in batch:\n",
        "        try:\n",
        "            pickup_datetime = pd.to_datetime(record.get('pickup_datetime'))\n",
        "            pickup_date = pickup_datetime.date().isoformat()\n",
        "            enhanced_records.append({\n",
        "                **record,\n",
        "                'pickup_date': pickup_date\n",
        "            })\n",
        "        except Exception:\n",
        "            # Keep original record if date parsing fails\n",
        "            enhanced_records.append({\n",
        "                **record,\n",
        "                'pickup_date': '1970-01-01'  # Default date for invalid entries\n",
        "            })\n",
        "    \n",
        "    return enhanced_records\n",
        "\n",
        "# Add pickup_date column using Ray Data native operations\n",
        "taxi_with_dates = clean_taxi_data.map_batches(\n",
        "    extract_pickup_date,\n",
        "    batch_size=10000,\n",
        "    concurrency=6\n",
        ")\n",
        "\n",
        "# Use Ray Data native groupby with aggregation functions - MUCH MORE EFFICIENT\n",
        "daily_metrics = taxi_with_dates.groupby(\"pickup_date\").aggregate(\n",
        "    Count(),  # Trip count\n",
        "    Sum(\"fare_amount\"),  # Total fare revenue\n",
        "    Mean(\"fare_amount\"),  # Average fare\n",
        "    Std(\"fare_amount\"),   # Fare standard deviation\n",
        "    Sum(\"trip_distance\"), # Total distance\n",
        "    Mean(\"trip_distance\"), # Average distance\n",
        "    Sum(\"passenger_count\"), # Total passengers\n",
        "    Sum(\"total_amount\")    # Total revenue including tips\n",
        ")\n",
        "\n",
        "# Add computed columns using native Ray Data operations\n",
        "from ray.data.expressions import col, lit\n",
        "\n",
        "daily_metrics_enhanced = daily_metrics.add_column(\n",
        "    \"avg_fare_per_mile\", \n",
        "    col(\"sum(fare_amount)\") / col(\"sum(trip_distance)\")\n",
        ").add_column(\n",
        "    \"revenue_per_trip\",\n",
        "    col(\"sum(total_amount)\") / col(\"count()\")\n",
        ")\n",
        "\n",
        "print(f\"Daily metrics: {daily_metrics.count()} records\")\n",
        "\n",
        "# Display daily metrics in a visually appealing format\n",
        "sample_metrics = daily_metrics.take(5)\n",
        "print(\"\\nDaily Taxi Metrics Summary:\")\n",
        "print(\"=\" * 120)\n",
        "print(f\"{'Date':<12} {'Trips':<8} {'Revenue':<10} {'Avg Fare':<10} {'Total Miles':<12} {'Avg Distance':<12}\")\n",
        "print(\"-\" * 120)\n",
        "\n",
        "for metric in sample_metrics:\n",
        "    date = metric.get('pickup_date', 'N/A')\n",
        "    trip_count = metric.get('fare_amount_count', 0)\n",
        "    revenue = metric.get('total_amount_sum', 0)\n",
        "    avg_fare = metric.get('fare_amount_mean', 0)\n",
        "    total_miles = metric.get('trip_distance_sum', 0)\n",
        "    avg_distance = metric.get('trip_distance_mean', 0)\n",
        "    \n",
        "    print(f\"{str(date):<12} {trip_count:<8,} ${revenue:<9.0f} ${avg_fare:<9.2f} {total_miles:<11.1f}mi {avg_distance:<11.2f}mi\")\n",
        "\n",
        "print(\"-\" * 120)\n",
        "print(\"Note: This demonstrates Ray Data's native groupby aggregation capabilities\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8a3b2ed",
      "metadata": {},
      "source": [
        "### 4. **Cross-Dataset Joins and Enrichment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "404ee331",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform data enrichment using Ray Data operations\n",
        "def enrich_with_reviews(batch):\n",
        "    \"\"\"Enrich data with review sentiment analysis.\"\"\"\n",
        "    enriched_records = []\n",
        "    \n",
        "    for record in batch:\n",
        "        # Add sentiment analysis (simplified)\n",
        "        review_text = record.get('review_body', '')\n",
        "        \n",
        "        # Simple sentiment scoring based on keywords\n",
        "        positive_words = ['great', 'excellent', 'outstanding', 'love', 'perfect', 'wonderful']\n",
        "        negative_words = ['terrible', 'awful', 'hate', 'worst', 'horrible', 'disappointing']\n",
        "        \n",
        "        positive_count = sum(1 for word in positive_words if word in review_text.lower())\n",
        "        negative_count = sum(1 for word in negative_words if word in review_text.lower())\n",
        "        \n",
        "        # Calculate sentiment score\n",
        "        if positive_count > negative_count:\n",
        "            sentiment = 'positive'\n",
        "            sentiment_score = min(positive_count / (positive_count + negative_count + 1), 1.0)\n",
        "        elif negative_count > positive_count:\n",
        "            sentiment = 'negative'\n",
        "            sentiment_score = min(negative_count / (positive_count + negative_count + 1), 1.0)\n",
        "        else:\n",
        "            sentiment = 'neutral'\n",
        "            sentiment_score = 0.5\n",
        "        \n",
        "        enriched_record = {\n",
        "            **record,\n",
        "            'sentiment': sentiment,\n",
        "            'sentiment_score': sentiment_score,\n",
        "            'review_length': len(review_text),\n",
        "            'word_count': len(review_text.split()),\n",
        "            'enrichment_timestamp': pd.Timestamp.now().isoformat()\n",
        "        }\n",
        "        \n",
        "        enriched_records.append(enriched_record)\n",
        "    \n",
        "    return enriched_records\n",
        "\n",
        "# Apply enrichment using Ray Data native operations\n",
        "enriched_reviews = reviews_data.map_batches(\n",
        "    enrich_with_reviews,\n",
        "    batch_size=5000,\n",
        "    concurrency=6\n",
        ")\n",
        "\n",
        "print(f\"Enriched reviews: {enriched_reviews.count()} records\")\n",
        "\n",
        "# Display enrichment results with visual formatting\n",
        "sample_enriched = enriched_reviews.take(3)\n",
        "print(\"\\nEnriched Review Data Sample:\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "for i, review in enumerate(sample_enriched):\n",
        "    print(f\"\\nReview {i+1}:\")\n",
        "    print(f\"  Rating: {review.get('star_rating', 'N/A')} stars\")\n",
        "    print(f\"  Sentiment: {review.get('sentiment', 'N/A').upper()} (score: {review.get('sentiment_score', 0):.2f})\")\n",
        "    print(f\"  Text Length: {review.get('review_length', 0)} characters ({review.get('word_count', 0)} words)\")\n",
        "    print(f\"  Preview: {str(review.get('review_body', ''))[:80]}...\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Show sentiment distribution\n",
        "sentiments = [r.get('sentiment', 'unknown') for r in sample_enriched]\n",
        "sentiment_counts = pd.Series(sentiments).value_counts()\n",
        "print(f\"\\nSentiment Distribution (sample):\")\n",
        "for sentiment, count in sentiment_counts.items():\n",
        "    bar_length = int(count * 20 / len(sample_enriched))\n",
        "    bar = \"\u2588\" * bar_length + \"\u2591\" * (20 - bar_length)\n",
        "    print(f\"  {sentiment.capitalize():<10} {bar} {count}/{len(sample_enriched)}\")\n",
        "\n",
        "print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "451de728",
      "metadata": {},
      "source": [
        "### 5. **Optimized data loading and partitioning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bd3f724",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write results using Ray Data native operations with optimization\n",
        "import tempfile\n",
        "\n",
        "output_dir = tempfile.mkdtemp()\n",
        "print(f\"Output directory: {output_dir}\")\n",
        "\n",
        "# Write daily metrics with optimal partitioning\n",
        "daily_metrics.write_parquet(\n",
        "    f\"local://{output_dir}/daily_taxi_metrics\",\n",
        "    num_files=10,  # Control number of output files\n",
        "    compression=\"snappy\"\n",
        ")\n",
        "\n",
        "# Write enriched reviews with partitioning by category\n",
        "enriched_reviews.write_parquet(\n",
        "    f\"local://{output_dir}/enriched_reviews\",\n",
        "    num_files=20,\n",
        "    compression=\"gzip\"\n",
        ")\n",
        "\n",
        "# Write clean taxi data with date-based partitioning\n",
        "clean_taxi_data.write_parquet(\n",
        "    f\"local://{output_dir}/clean_taxi_data\",\n",
        "    num_files=50,\n",
        "    compression=\"snappy\"\n",
        ")\n",
        "\n",
        "print(f\"All datasets written to: {output_dir}\")\n",
        "\n",
        "# Display file output summary with visual formatting\n",
        "import os\n",
        "print(\"\\nETL Output Summary:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"{'Dataset':<30} {'Location':<35} {'Status':<15}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "datasets = [\n",
        "    (\"Daily Taxi Metrics\", f\"{output_dir}/daily_taxi_metrics\", \"Complete\"),\n",
        "    (\"Enriched Reviews\", f\"{output_dir}/enriched_reviews\", \"Complete\"), \n",
        "    (\"Clean Taxi Data\", f\"{output_dir}/clean_taxi_data\", \"Complete\")\n",
        "]\n",
        "\n",
        "for name, path, status in datasets:\n",
        "    print(f\"{name:<30} {path[-35:]:<35} {status:<15}\")\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print(\"All ETL outputs saved successfully!\")\n",
        "\n",
        "# Optional: Write to cloud storage for production (commented for demo)\n",
        "print(\"\\nProduction Storage Options:\")\n",
        "print(\"# daily_metrics.write_parquet('s3://your-bucket/etl-output/daily_metrics/')\")\n",
        "print(\"# enriched_reviews.write_parquet('s3://your-bucket/etl-output/enriched_reviews/')\")\n",
        "print(\"# clean_taxi_data.write_parquet('s3://your-bucket/etl-output/clean_taxi_data/')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95d75178",
      "metadata": {},
      "source": [
        "## Advanced ETL Patterns\n",
        "\n",
        "### **Memory-Efficient Processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03e95dc4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process large datasets with memory optimization\n",
        "def memory_optimized_transform(batch):\n",
        "    \"\"\"Transform data with memory optimization techniques.\"\"\"\n",
        "    # Process in smaller chunks to manage memory\n",
        "    chunk_size = 1000\n",
        "    transformed_records = []\n",
        "    \n",
        "    for i in range(0, len(batch), chunk_size):\n",
        "        chunk = batch[i:i + chunk_size]\n",
        "        \n",
        "        # Apply transformations to chunk\n",
        "        for record in chunk:\n",
        "            # Minimal memory footprint transformations\n",
        "            transformed_record = {\n",
        "                'id': record.get('id'),\n",
        "                'processed_value': record.get('value', 0) * 1.1,\n",
        "                'category': record.get('category', 'unknown').upper(),\n",
        "                'is_valid': record.get('value', 0) > 0\n",
        "            }\n",
        "            transformed_records.append(transformed_record)\n",
        "    \n",
        "    return transformed_records\n",
        "\n",
        "# Apply memory-optimized processing\n",
        "processed_data = taxi_data.map_batches(\n",
        "    memory_optimized_transform,\n",
        "    batch_size=5000,  # Smaller batches for memory efficiency\n",
        "    concurrency=10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "812b29c8",
      "metadata": {},
      "source": [
        "### **Distributed Deduplication**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18213c78",
      "metadata": {},
      "outputs": [],
      "source": [
        "# OPTIMIZED: Minimize pandas usage for deduplication\n",
        "def deduplicate_records(batch):\n",
        "    \"\"\"Remove duplicate records with minimal DataFrame conversion.\"\"\"\n",
        "    # Use native Python for deduplication when possible\n",
        "    seen_keys = set()\n",
        "    deduplicated_records = []\n",
        "    original_count = len(batch)\n",
        "    \n",
        "    for record in batch:\n",
        "        # Create composite key for deduplication\n",
        "        key = (\n",
        "            record.get('pickup_datetime', ''),\n",
        "            record.get('dropoff_datetime', ''),\n",
        "            record.get('fare_amount', 0)\n",
        "        )\n",
        "        \n",
        "        if key not in seen_keys:\n",
        "            seen_keys.add(key)\n",
        "            # Add deduplication metadata\n",
        "            enhanced_record = {\n",
        "                **record,\n",
        "                'deduplication_timestamp': pd.Timestamp.now().isoformat(),\n",
        "                'original_batch_size': original_count,\n",
        "                'deduplicated_batch_size': len(deduplicated_records) + 1\n",
        "            }\n",
        "            deduplicated_records.append(enhanced_record)\n",
        "    \n",
        "    return deduplicated_records\n",
        "\n",
        "# Apply deduplication with optimized batch processing\n",
        "deduplicated_data = clean_taxi_data.map_batches(\n",
        "    deduplicate_records,\n",
        "    batch_size=15000,  # Optimized batch size\n",
        "    concurrency=8      # Increased concurrency for efficiency\n",
        ")\n",
        "\n",
        "print(f\"Deduplicated data: {deduplicated_data.count()} records\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a776504d",
      "metadata": {},
      "source": [
        "## Performance optimization\n",
        "\n",
        "### **Block size and parallelism tuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47a87bb8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimize Ray Data configuration for large-scale ETL\n",
        "from ray.data.context import DataContext\n",
        "\n",
        "# Configure Ray Data for optimal ETL performance\n",
        "ctx = DataContext.get_current()\n",
        "ctx.target_max_block_size = 1024 * 1024 * 1024  # 1GB blocks\n",
        "ctx.enable_progress_bars = False\n",
        "\n",
        "# Read with optimized block configuration\n",
        "optimized_data = read_parquet(\n",
        "    \"s3://anonymous@nyc-tlc/trip_data/\",\n",
        "    parallelism=100,  # High parallelism for large datasets\n",
        "    columns=[\"pickup_datetime\", \"fare_amount\", \"trip_distance\"]\n",
        ")\n",
        "\n",
        "print(f\"Optimized data loading: {optimized_data.count()} records\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c978a71",
      "metadata": {},
      "source": [
        "### **Efficient column operations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f890fe8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use Ray Data native column operations\n",
        "def efficient_column_transforms(batch):\n",
        "    \"\"\"Apply efficient column-wise transformations.\"\"\"\n",
        "    import numpy as np\n",
        "    \n",
        "    transformed_batch = []\n",
        "    \n",
        "    for record in batch:\n",
        "        # Efficient numerical transformations\n",
        "        fare = record.get('fare_amount', 0)\n",
        "        distance = record.get('trip_distance', 0)\n",
        "        \n",
        "        # Calculate derived metrics efficiently\n",
        "        fare_per_mile = fare / distance if distance > 0 else 0\n",
        "        fare_tier = 'high' if fare > 20 else 'medium' if fare > 10 else 'low'\n",
        "        distance_tier = 'long' if distance > 10 else 'medium' if distance > 3 else 'short'\n",
        "        \n",
        "        transformed_record = {\n",
        "            **record,\n",
        "            'fare_per_mile': fare_per_mile,\n",
        "            'fare_tier': fare_tier,\n",
        "            'distance_tier': distance_tier,\n",
        "            'is_premium_trip': fare > 50 and distance > 10\n",
        "        }\n",
        "        \n",
        "        transformed_batch.append(transformed_record)\n",
        "    \n",
        "    return transformed_batch\n",
        "\n",
        "# Apply efficient transformations\n",
        "transformed_data = optimized_data.map_batches(\n",
        "    efficient_column_transforms,\n",
        "    batch_size=25000,\n",
        "    concurrency=8\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41928ee3",
      "metadata": {},
      "source": [
        "## Advanced features\n",
        "\n",
        "### **Distributed sorting and ranking**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91324d5e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use Ray Data native sorting for large datasets\n",
        "# Sort by fare amount for ranking analysis\n",
        "sorted_by_fare = transformed_data.sort(\"fare_amount\", descending=True)\n",
        "\n",
        "# Add ranking information\n",
        "def add_ranking_info(batch):\n",
        "    \"\"\"Add ranking information to records.\"\"\"\n",
        "    ranked_batch = []\n",
        "    \n",
        "    for i, record in enumerate(batch):\n",
        "        ranked_record = {\n",
        "            **record,\n",
        "            'fare_rank_in_batch': i + 1,\n",
        "            'is_top_10_percent': i < len(batch) * 0.1,\n",
        "            'ranking_timestamp': pd.Timestamp.now().isoformat()\n",
        "        }\n",
        "        ranked_batch.append(ranked_record)\n",
        "    \n",
        "    return ranked_batch\n",
        "\n",
        "# Apply ranking\n",
        "ranked_data = sorted_by_fare.map_batches(\n",
        "    add_ranking_info,\n",
        "    batch_size=10000,\n",
        "    concurrency=4\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53b2d13e",
      "metadata": {},
      "source": [
        "### **Complex business logic processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6df9caa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implement complex business rules using Ray Data operations\n",
        "def apply_business_rules(batch):\n",
        "    \"\"\"Apply complex business rules and calculations.\"\"\"\n",
        "    processed_batch = []\n",
        "    \n",
        "    for record in batch:\n",
        "        # Extract key metrics\n",
        "        fare = record.get('fare_amount', 0)\n",
        "        distance = record.get('trip_distance', 0)\n",
        "        passenger_count = record.get('passenger_count', 1)\n",
        "        \n",
        "        # Business rule calculations\n",
        "        efficiency_score = distance / fare if fare > 0 else 0\n",
        "        capacity_utilization = passenger_count / 4.0  # Assume 4-seat capacity\n",
        "        \n",
        "        # Trip categorization\n",
        "        if distance > 20:\n",
        "            trip_type = 'long_distance'\n",
        "        elif distance > 5:\n",
        "            trip_type = 'medium_distance'\n",
        "        else:\n",
        "            trip_type = 'short_distance'\n",
        "        \n",
        "        # Revenue calculations\n",
        "        base_revenue = fare * 0.8  # After commission\n",
        "        bonus_revenue = fare * 0.1 if fare > 30 else 0\n",
        "        total_revenue = base_revenue + bonus_revenue\n",
        "        \n",
        "        processed_record = {\n",
        "            **record,\n",
        "            'efficiency_score': efficiency_score,\n",
        "            'capacity_utilization': capacity_utilization,\n",
        "            'trip_type': trip_type,\n",
        "            'base_revenue': base_revenue,\n",
        "            'bonus_revenue': bonus_revenue,\n",
        "            'total_revenue': total_revenue,\n",
        "            'processing_timestamp': pd.Timestamp.now().isoformat()\n",
        "        }\n",
        "        \n",
        "        processed_batch.append(processed_record)\n",
        "    \n",
        "    return processed_batch\n",
        "\n",
        "# Apply business rules\n",
        "business_processed = ranked_data.map_batches(\n",
        "    apply_business_rules,\n",
        "    batch_size=15000,\n",
        "    concurrency=6\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be828fd4",
      "metadata": {},
      "source": [
        "## Production considerations\n",
        "\n",
        "### **Cluster configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "093e4709",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimal cluster setup for large-scale ETL\n",
        "cluster_config = {\n",
        "    \"head_node\": {\n",
        "        \"instance_type\": \"m5.4xlarge\",  # 16 vCPUs, 64GB RAM\n",
        "        \"cpu\": 16,\n",
        "        \"memory\": 64000\n",
        "    },\n",
        "    \"worker_nodes\": {\n",
        "        \"instance_type\": \"m5.8xlarge\",  # 32 vCPUs, 128GB RAM\n",
        "        \"min_workers\": 5,\n",
        "        \"max_workers\": 20,\n",
        "        \"cpu_per_worker\": 32,\n",
        "        \"memory_per_worker\": 128000\n",
        "    }\n",
        "}\n",
        "\n",
        "# Ray initialization for production ETL\n",
        "ray.init(\n",
        "    object_store_memory=50_000_000_000,  # 50GB object store\n",
        "    _memory=100_000_000_000,             # 100GB heap memory\n",
        "    log_to_driver=True,\n",
        "    enable_object_reconstruction=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "730a1df2",
      "metadata": {},
      "source": [
        "### **Resource monitoring**\n",
        "- Monitor memory usage and object store pressure\n",
        "- Track processing throughput and bottlenecks\n",
        "- Implement automatic scaling based on workload\n",
        "- Set up alerting for pipeline failures\n",
        "\n",
        "### **Data lineage and governance**\n",
        "- Track data transformations and dependencies\n",
        "- Maintain audit trails for compliance\n",
        "- Implement data quality monitoring\n",
        "- Ensure data security and access controls\n",
        "\n",
        "## Example workflows\n",
        "\n",
        "### **Daily ETL pipeline**\n",
        "1. Extract overnight data from operational systems\n",
        "2. Validate data quality and apply cleansing rules\n",
        "3. Perform complex transformations and enrichment\n",
        "4. Calculate business metrics and KPIs\n",
        "5. Load results to data warehouse with optimal partitioning\n",
        "\n",
        "### **Historical data migration**\n",
        "1. Extract historical data from legacy systems\n",
        "2. Transform data to new schema and formats\n",
        "3. Validate data integrity and completeness\n",
        "4. Load data to modern analytical platforms\n",
        "5. Verify migration success and performance\n",
        "\n",
        "### **Real-time analytics preparation**\n",
        "1. Process streaming data in micro-batches\n",
        "2. Apply real-time transformations and aggregations\n",
        "3. Prepare data for real-time dashboards\n",
        "4. Update analytical models and metrics\n",
        "5. Maintain data freshness and quality\n",
        "\n",
        "## Resource planning and configuration\n",
        "\n",
        "### **ETL processing considerations**\n",
        "\n",
        "| Operation Type | Resource Requirements | Ray Data Features | Cluster Configuration |\n",
        "|---------------|----------------------|-------------------|----------------------|\n",
        "| **Data Extraction** | I/O intensive | Parallel readers | Multiple worker nodes |\n",
        "| **Data Transformation** | CPU intensive | Distributed processing | High-CPU instances |\n",
        "| **Data Aggregation** | Memory intensive | In-memory operations | High-memory instances |\n",
        "| **Data Loading** | I/O intensive | Parallel writers | Multiple worker nodes |\n",
        "\n",
        "### **Cluster sizing guidelines**\n",
        "\n",
        "| Cluster Size | Memory Capacity | Processing Capability | Suitable Workloads |\n",
        "|-------------|-----------------|----------------------|-------------------|\n",
        "| **5 Nodes** | 32-64GB total | Moderate throughput | Development/Testing |\n",
        "| **10 Nodes** | 64-128GB total | High throughput | Production workloads |\n",
        "| **20+ Nodes** | 128GB+ total | Very high throughput | Large-scale processing |\n",
        "\n",
        "### **Resource utilization patterns**\n",
        "\n",
        "| Workload Type | CPU Requirements | Memory Requirements | Storage Requirements | Recommended Instance |\n",
        "|--------------|------------------|-------------------|---------------------|---------------------|\n",
        "| **Light ETL** | 2-4 cores | 4-8GB | Standard | m5.xlarge |\n",
        "| **Heavy Transformations** | 4-8 cores | 6-12GB | Standard | c5.2xlarge |\n",
        "| **Complex Joins** | 2-4 cores | 8-16GB | High-memory | r5.xlarge |\n",
        "| **ML Feature Engineering** | 4-8 cores | 12-24GB | Standard | c5.4xlarge |\n",
        "\n",
        "## Interactive ETL pipeline visualizations\n",
        "\n",
        "Let's create engaging visualizations to understand our ETL pipeline results and data insights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d5c5b1d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import visualization libraries for engaging data presentation\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set up appealing visual style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "def create_etl_summary(dataset, title=\"ETL Results\"):\n",
        "    \"\"\"Create concise ETL processing summary.\"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(f\"{title.upper()}\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Total records: {dataset.count():,}\")\n",
        "    print(f\"Schema: {dataset.schema()}\")\n",
        "    print(f\"Dataset size: {dataset.size_bytes() / (1024**2):.1f} MB\")\n",
        "\n",
        "def create_simple_etl_chart(dataset):\n",
        "    \"\"\"Create focused ETL results visualization.\"\"\"\n",
        "    # Convert sample data for visualization\n",
        "    sample_data = dataset.take(1000)  # Smaller sample for efficiency\n",
        "    df = pd.DataFrame(sample_data)\n",
        "    \n",
        "    # Create simple, focused visualization\n",
        "    if len(df) > 0 and 'trip_type' in df.columns:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        trip_counts = df['trip_type'].value_counts()\n",
        "        plt.bar(trip_counts.index, trip_counts.values, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
        "        plt.title('ETL Processing Results - Trip Type Distribution')\n",
        "        plt.xlabel('Trip Type')\n",
        "        plt.ylabel('Count')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        print(f\"ETL processing completed: {len(df):,} records analyzed\")\n",
        "    \n",
        "    # Simple trip type analysis\n",
        "    if 'trip_type' in df.columns:\n",
        "        trip_counts = df['trip_type'].value_counts()\n",
        "        print(f\"Trip type distribution: {trip_counts.to_dict()}\")\n",
        "    \n",
        "    # Simple revenue analysis\n",
        "    if 'total_revenue' in df.columns:\n",
        "        avg_revenue = df['total_revenue'].mean()\n",
        "        print(f\"Average revenue per trip: ${avg_revenue:.2f}\")\n",
        "    \n",
        "    # Basic efficiency metrics\n",
        "    if 'efficiency_score' in df.columns:\n",
        "        avg_efficiency = df['efficiency_score'].mean()\n",
        "        print(f\"Average efficiency score: {avg_efficiency:.2f}\")\n",
        "    \n",
        "    print(\"ETL processing visualization completed\")\n",
        "\n",
        "# Create focused ETL results summary\n",
        "create_etl_summary(business_processed, \"ETL Pipeline Results\")\n",
        "create_simple_etl_chart(business_processed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e888b03b",
      "metadata": {},
      "source": [
        "### ETL pipeline analysis using Ray Data native operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c3227c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze ETL results using Ray Data aggregations\n",
        "from ray.data.aggregate import Count, Mean, Sum, Max, Min\n",
        "\n",
        "# Business metrics analysis\n",
        "print(\"ETL Pipeline Analysis:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Group by trip type for analysis\n",
        "trip_analysis = business_processed.groupby(\"trip_type\").aggregate(\n",
        "    Count(),\n",
        "    Mean(\"total_revenue\"),\n",
        "    Mean(\"efficiency_score\")\n",
        ").rename_columns([\"trip_type\", \"trip_count\", \"avg_revenue\", \"avg_efficiency\"])\n",
        "\n",
        "print(\"Trip Type Analysis:\")\n",
        "trip_analysis.show()\n",
        "\n",
        "# Revenue analysis\n",
        "revenue_metrics = business_processed.aggregate(\n",
        "    Count(),\n",
        "    Sum(\"total_revenue\"),\n",
        "    Mean(\"total_revenue\"),\n",
        "    Max(\"total_revenue\"),\n",
        "    Min(\"total_revenue\")\n",
        ")\n",
        "\n",
        "print(f\"\\nRevenue Summary:\")\n",
        "for metric in revenue_metrics:\n",
        "    print(f\"  {metric}\")\n",
        "\n",
        "print(f\"\\nETL Pipeline Processing Complete!\")\n",
        "print(f\"Monitor detailed performance in Ray Dashboard: {ray.get_dashboard_url()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c55603b7",
      "metadata": {},
      "source": [
        "### **ETL pipeline status monitoring**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dd1448d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_pipeline_status(datasets_dict):\n",
        "    \"\"\"Display comprehensive pipeline status in a visual format.\"\"\"\n",
        "    \n",
        "    print(\"ETL Pipeline Status Monitor\")\n",
        "    print(\"=\" * 90)\n",
        "    print(f\"{'Pipeline Stage':<25} {'Dataset':<20} {'Records':<12} {'Status':<15} {'Notes':<20}\")\n",
        "    print(\"-\" * 90)\n",
        "    \n",
        "    stages = [\n",
        "        (\"Data Extraction\", \"Raw Taxi Data\", taxi_data.count(), \"Complete\", \"From S3 Parquet\"),\n",
        "        (\"Data Validation\", \"Validated Data\", clean_taxi_data.count(), \"Complete\", \"Business rules applied\"),\n",
        "        (\"Data Aggregation\", \"Daily Metrics\", daily_metrics.count(), \"Complete\", \"Grouped by date\"),\n",
        "        (\"Data Enrichment\", \"Enriched Reviews\", enriched_reviews.count(), \"Complete\", \"Sentiment analysis\"),\n",
        "        (\"Data Storage\", \"Final Output\", \"Multiple\", \"Complete\", \"Parquet format\")\n",
        "    ]\n",
        "    \n",
        "    for stage, dataset, records, status, notes in stages:\n",
        "        record_str = f\"{records:,}\" if isinstance(records, int) else records\n",
        "        status_symbol = \"[OK]\" if status == \"Complete\" else \"[WARN]\"\n",
        "        print(f\"{stage:<25} {dataset:<20} {record_str:<12} {status_symbol} {status:<14} {notes:<20}\")\n",
        "    \n",
        "    print(\"-\" * 90)\n",
        "    print(\"Pipeline Status: All stages completed successfully\")\n",
        "    \n",
        "    # Resource utilization summary\n",
        "    cluster_resources = ray.cluster_resources()\n",
        "    print(f\"\\nCluster Resource Summary:\")\n",
        "    print(f\"  Available CPUs: {cluster_resources.get('CPU', 0)}\")\n",
        "    print(f\"  Available Memory: {cluster_resources.get('memory', 0) / 1e9:.1f}GB\")\n",
        "    print(f\"  Available GPUs: {cluster_resources.get('GPU', 0)}\")\n",
        "    \n",
        "    return True\n",
        "\n",
        "# Display the pipeline status\n",
        "pipeline_status = display_pipeline_status({\n",
        "    \"taxi_data\": taxi_data,\n",
        "    \"clean_taxi_data\": clean_taxi_data,\n",
        "    \"daily_metrics\": daily_metrics,\n",
        "    \"enriched_reviews\": enriched_reviews\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4e8ae9b",
      "metadata": {},
      "source": [
        "### ETL pipeline flow diagram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1372ac53",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_etl_pipeline_diagram():\n",
        "    \"\"\"Create interactive ETL pipeline flow diagram.\"\"\"\n",
        "    print(\"Creating ETL pipeline flow diagram...\")\n",
        "    \n",
        "    # Create a network graph representing the ETL pipeline\n",
        "    G = nx.DiGraph()\n",
        "    \n",
        "    # Add nodes for different pipeline stages\n",
        "    pipeline_stages = {\n",
        "        'Data Sources': {'pos': (0, 2), 'color': 'lightblue', 'size': 3000},\n",
        "        'Extract': {'pos': (1, 2), 'color': 'lightgreen', 'size': 2500},\n",
        "        'Validate': {'pos': (2, 3), 'color': 'orange', 'size': 2000},\n",
        "        'Transform': {'pos': (2, 1), 'color': 'yellow', 'size': 2500},\n",
        "        'Aggregate': {'pos': (3, 2), 'color': 'lightcoral', 'size': 2000},\n",
        "        'Load': {'pos': (4, 2), 'color': 'lightpink', 'size': 2500},\n",
        "        'Data Warehouse': {'pos': (5, 2), 'color': 'lightgray', 'size': 3000}\n",
        "    }\n",
        "    \n",
        "    # Add nodes to graph\n",
        "    for stage, attrs in pipeline_stages.items():\n",
        "        G.add_node(stage, **attrs)\n",
        "    \n",
        "    # Add edges representing data flow\n",
        "    pipeline_edges = [\n",
        "        ('Data Sources', 'Extract'),\n",
        "        ('Extract', 'Validate'),\n",
        "        ('Extract', 'Transform'),\n",
        "        ('Validate', 'Aggregate'),\n",
        "        ('Transform', 'Aggregate'),\n",
        "        ('Aggregate', 'Load'),\n",
        "        ('Load', 'Data Warehouse')\n",
        "    ]\n",
        "    \n",
        "    G.add_edges_from(pipeline_edges)\n",
        "    \n",
        "    # Create visualization\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    pos = nx.get_node_attributes(G, 'pos')\n",
        "    colors = [pipeline_stages[node]['color'] for node in G.nodes()]\n",
        "    sizes = [pipeline_stages[node]['size'] for node in G.nodes()]\n",
        "    \n",
        "    # Draw the network\n",
        "    nx.draw(G, pos, with_labels=True, node_color=colors, node_size=sizes,\n",
        "            font_size=12, font_weight='bold', arrows=True, arrowsize=20,\n",
        "            edge_color='gray', linewidths=2, arrowstyle='->')\n",
        "    \n",
        "    plt.title('ETL Pipeline Flow Diagram', fontsize=16, fontweight='bold', pad=20)\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('etl_pipeline_diagram.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"ETL pipeline diagram saved as 'etl_pipeline_diagram.png'\")\n",
        "\n",
        "# Create pipeline diagram\n",
        "create_etl_pipeline_diagram()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26982871",
      "metadata": {},
      "source": [
        "### ETL performance dashboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71c6a9e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_etl_performance_dashboard():\n",
        "    \"\"\"Create comprehensive ETL performance monitoring dashboard.\"\"\"\n",
        "    print(\"Creating ETL performance dashboard...\")\n",
        "    \n",
        "    # Use Ray Dashboard for real performance monitoring\n",
        "    perf_df = pd.DataFrame()\n",
        "    \n",
        "    # Create comprehensive dashboard\n",
        "    fig = make_subplots(\n",
        "        rows=3, cols=2,\n",
        "        subplot_titles=('Records Processed Over Time', 'Processing Time Trends',\n",
        "                       'Resource Usage', 'Throughput Analysis',\n",
        "                       'Error Rate Monitoring', 'Performance Correlation'),\n",
        "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "               [{\"secondary_y\": True}, {\"secondary_y\": False}],\n",
        "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
        "    )\n",
        "    \n",
        "    # Example layout for production monitoring dashboards\n",
        "    \n",
        "    # Replace with real monitoring data from your environment\n",
        "    \n",
        "    pass\n",
        "    \n",
        "    # 4. Throughput analysis\n",
        "    fig.add_trace(\n",
        "        go.Bar(x=perf_df['timestamp'], y=perf_df['throughput'],\n",
        "               name='Throughput (records/sec)', marker_color='lightblue'),\n",
        "        row=2, col=2\n",
        "    )\n",
        "    \n",
        "    # 5. Error rate monitoring\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=perf_df['timestamp'], y=perf_df['error_rate'],\n",
        "                  mode='lines+markers', name='Error Rate (%)',\n",
        "                  line=dict(color='red', width=3),\n",
        "                  fill='tozeroy', fillcolor='rgba(255,0,0,0.1)'),\n",
        "        row=3, col=1\n",
        "    )\n",
        "    \n",
        "    # 6. Performance correlation heatmap\n",
        "    correlation_data = perf_df[['records_processed', 'processing_time', 'memory_usage', \n",
        "                               'cpu_usage', 'throughput']].corr()\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Heatmap(z=correlation_data.values,\n",
        "                  x=correlation_data.columns,\n",
        "                  y=correlation_data.index,\n",
        "                  colorscale='RdBu',\n",
        "                  zmid=0,\n",
        "                  text=correlation_data.round(2).values,\n",
        "                  texttemplate=\"%{text}\",\n",
        "                  showscale=True),\n",
        "        row=3, col=2\n",
        "    )\n",
        "    \n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        title_text=\"ETL Performance Monitoring Dashboard\",\n",
        "        height=1000,\n",
        "        showlegend=True\n",
        "    )\n",
        "    \n",
        "    # Update axes\n",
        "    fig.update_xaxes(title_text=\"Time\", row=1, col=1)\n",
        "    fig.update_yaxes(title_text=\"Records\", row=1, col=1)\n",
        "    fig.update_xaxes(title_text=\"Time\", row=1, col=2)\n",
        "    fig.update_yaxes(title_text=\"Minutes\", row=1, col=2)\n",
        "    fig.update_xaxes(title_text=\"Time\", row=2, col=1)\n",
        "    fig.update_yaxes(title_text=\"Memory %\", row=2, col=1)\n",
        "    fig.update_yaxes(title_text=\"CPU %\", row=2, col=1, secondary_y=True)\n",
        "    fig.update_xaxes(title_text=\"Time\", row=2, col=2)\n",
        "    fig.update_yaxes(title_text=\"Records/sec\", row=2, col=2)\n",
        "    fig.update_xaxes(title_text=\"Time\", row=3, col=1)\n",
        "    fig.update_yaxes(title_text=\"Error %\", row=3, col=1)\n",
        "    \n",
        "    # Save and show\n",
        "    print(\"Use Ray Dashboard for performance monitoring. This function shows example layout only.\")\n",
        "    \n",
        "    return fig\n",
        "\n",
        "# Create performance dashboard\n",
        "performance_dashboard = create_etl_performance_dashboard()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d474c113",
      "metadata": {},
      "source": [
        "### Data quality monitoring visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1029083e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_data_quality_dashboard():\n",
        "    \"\"\"Create data quality monitoring dashboard.\"\"\"\n",
        "    print(\"Creating data quality monitoring dashboard...\")\n",
        "    \n",
        "    # Example layout for data quality dashboards\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
        "    fig.suptitle('ETL Data Quality Monitoring Dashboard', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. Data Completeness by Source\n",
        "    ax1 = axes[0, 0]\n",
        "    sources = []\n",
        "    completeness = []\n",
        "    bars = ax1.bar(sources, completeness, color='green', alpha=0.7)\n",
        "    ax1.set_title('Data Completeness by Source', fontweight='bold')\n",
        "    ax1.set_ylabel('Completeness (%)')\n",
        "    ax1.axhline(y=95, color='red', linestyle='--', alpha=0.5, label='Target: 95%')\n",
        "    ax1.legend()\n",
        "    \n",
        "    # Add value labels\n",
        "    for bar, value in zip(bars, completeness):\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
        "                f'{value}%', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    # 2. Data Freshness Trends\n",
        "    ax2 = axes[0, 1]\n",
        "    ax2.plot([], [], 'b-o', linewidth=2, markersize=4)\n",
        "    ax2.fill_between(hours, freshness_delay, alpha=0.3)\n",
        "    ax2.set_title('Data Freshness (Delay in Hours)', fontweight='bold')\n",
        "    ax2.set_xlabel('Hour of Day')\n",
        "    ax2.set_ylabel('Delay (hours)')\n",
        "    ax2.axhline(y=1, color='red', linestyle='--', alpha=0.5, label='SLA: 1 hour')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Schema Validation Results\n",
        "    ax3 = axes[0, 2]\n",
        "    ax3.pie([], labels=[], autopct='%1.1f%%', colors=['green','orange','red'], startangle=90)\n",
        "    ax3.set_title('Schema Validation Results', fontweight='bold')\n",
        "    \n",
        "    # 4. Data Volume Trends\n",
        "    ax4 = axes[1, 0]\n",
        "    ax4.plot([], [], 'g-', linewidth=2)\n",
        "    ax4.set_title('Daily Data Volume Trends', fontweight='bold')\n",
        "    ax4.set_xlabel('Date')\n",
        "    ax4.set_ylabel('Volume (Millions of Records)')\n",
        "    ax4.tick_params(axis='x', rotation=45)\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 5. Data Type Distribution\n",
        "    ax5 = axes[1, 1]\n",
        "    bars = ax5.barh([], [], color='skyblue', alpha=0.7)\n",
        "    ax5.set_title('Data Type Distribution', fontweight='bold')\n",
        "    ax5.set_xlabel('Percentage of Columns')\n",
        "    \n",
        "    # Add value labels\n",
        "    for bar, value in zip(bars, type_counts):\n",
        "        width = bar.get_width()\n",
        "        ax5.text(width + 0.5, bar.get_y() + bar.get_height()/2.,\n",
        "                f'{value}%', ha='left', va='center', fontweight='bold')\n",
        "    \n",
        "    # 6. Duplicate Detection\n",
        "    ax6 = axes[1, 2]\n",
        "    bars = ax6.bar([], [], color='green', alpha=0.7)\n",
        "    ax6.set_title('Duplicate Detection Rates', fontweight='bold')\n",
        "    ax6.set_ylabel('Duplicate Rate (%)')\n",
        "    ax6.axhline(y=1, color='orange', linestyle='--', alpha=0.5, label='Warning: 1%')\n",
        "    ax6.axhline(y=3, color='red', linestyle='--', alpha=0.5, label='Critical: 3%')\n",
        "    ax6.legend()\n",
        "    \n",
        "    # 7. Processing Error Trends\n",
        "    ax7 = axes[2, 0]\n",
        "    ax7.bar([], [], color='red', alpha=0.6, width=0.8)\n",
        "    ax7.set_title('Processing Errors by Hour', fontweight='bold')\n",
        "    ax7.set_xlabel('Hour of Day')\n",
        "    ax7.set_ylabel('Error Count')\n",
        "    ax7.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 8. Data Quality Score Over Time\n",
        "    ax8 = axes[2, 1]\n",
        "    ax8.plot([], [], 'purple', linewidth=2, marker='o', markersize=3)\n",
        "    ax8.set_title('Overall Data Quality Score', fontweight='bold')\n",
        "    ax8.set_xlabel('Date')\n",
        "    ax8.set_ylabel('Quality Score (%)')\n",
        "    ax8.axhline(y=90, color='green', linestyle='--', alpha=0.5, label='Target: 90%')\n",
        "    ax8.legend()\n",
        "    ax8.tick_params(axis='x', rotation=45)\n",
        "    ax8.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 9. ETL Stage Performance\n",
        "    ax9 = axes[2, 2]\n",
        "    bars = ax9.bar([], [], color=['lightblue', 'lightgreen', 'lightcoral'], alpha=0.7)\n",
        "    ax9.set_title('ETL Stage Performance', fontweight='bold')\n",
        "    ax9.set_ylabel('Processing Time (minutes)')\n",
        "    \n",
        "    # Add value labels\n",
        "    for bar, value in zip(bars, avg_times):\n",
        "        height = bar.get_height()\n",
        "        ax9.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "                f'{value:.1f}m', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('etl_data_quality_dashboard.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"Use Ray Dashboard and your data platform's monitoring for real metrics.\")\n",
        "\n",
        "# Create data quality dashboard\n",
        "create_data_quality_dashboard()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "059a691e",
      "metadata": {},
      "source": [
        "### Real-time ETL monitoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac4f71d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_realtime_etl_monitor():\n",
        "    \"\"\"Create real-time ETL monitoring visualization.\"\"\"\n",
        "    print(\"Creating real-time ETL monitoring system...\")\n",
        "    \n",
        "    # Use Ray Dashboard and your observability stack for real-time monitoring\n",
        "    fig = go.Figure()\n",
        "    print(\"Use Ray Dashboard for real-time monitoring. This function shows example layout only.\")\n",
        "    \n",
        "    return fig\n",
        "\n",
        "# Create real-time monitor\n",
        "realtime_monitor = create_realtime_etl_monitor()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b4b30f7",
      "metadata": {},
      "source": [
        "### System resource monitoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d53f2604",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_system_resource_dashboard():\n",
        "    \"\"\"Create system resource monitoring dashboard.\"\"\"\n",
        "    print(\"Creating system resource monitoring dashboard...\")\n",
        "    \n",
        "    # Example layout for system resource dashboards\n",
        "    cpu_percent = []\n",
        "    class M: pass\n",
        "    memory = M(); memory.total = 0; memory.percent = 0\n",
        "    class D: pass\n",
        "    disk = D(); disk.total = 0; disk.used = 0\n",
        "    \n",
        "    # Create system resource visualization\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('System Resource Monitoring for ETL Pipeline', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. CPU Usage by Core\n",
        "    bars = ax1.bar([], [], color='green', alpha=0.7)\n",
        "    ax1.set_title('CPU Usage by Core', fontweight='bold')\n",
        "    ax1.set_ylabel('CPU Usage (%)')\n",
        "    ax1.axhline(y=80, color='red', linestyle='--', alpha=0.5, label='Critical: 80%')\n",
        "    ax1.axhline(y=60, color='orange', linestyle='--', alpha=0.5, label='Warning: 60%')\n",
        "    ax1.legend()\n",
        "    \n",
        "    # Add value labels\n",
        "    for bar, value in zip(bars, cpu_percent):\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "                f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    # 2. Memory Usage\n",
        "    ax2.pie([], labels=[], colors=['red','green','blue'], autopct='%1.1f%%', startangle=90)\n",
        "    ax2.set_title(f'Memory Usage (Total: {memory.total / (1024**3):.1f} GB)', fontweight='bold')\n",
        "    \n",
        "    # 3. Disk Usage\n",
        "    ax3.pie([], labels=[], colors=['red','green'], autopct='%1.1f%%', startangle=90)\n",
        "    ax3.set_title(f'Disk Usage (Total: {disk.total / (1024**3):.1f} GB)', fontweight='bold')\n",
        "    \n",
        "    # 4. Resource Trends (simulated)\n",
        "    ax4.plot([], [], 'b-o', label='CPU Usage (%)', linewidth=2, markersize=4)\n",
        "    ax4.plot([], [], 'r-s', label='Memory Usage (%)', linewidth=2, markersize=4)\n",
        "    ax4.set_title('24-Hour Resource Trends', fontweight='bold')\n",
        "    ax4.set_xlabel('Hour of Day')\n",
        "    ax4.set_ylabel('Usage (%)')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('system_resource_dashboard.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"Use Ray Dashboard and your infrastructure monitoring for system metrics.\")\n",
        "\n",
        "# Create system resource dashboard\n",
        "create_system_resource_dashboard()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f6be182",
      "metadata": {},
      "source": [
        "## Troubleshooting\n",
        "\n",
        "### **Common Issues**\n",
        "1. **Memory Pressure**: Reduce batch size or increase cluster memory\n",
        "2. **Slow Performance**: Optimize block size and parallelism settings\n",
        "3. **Data Skew**: Implement data redistribution strategies\n",
        "4. **Resource Contention**: Balance CPU and memory allocation\n",
        "\n",
        "### **Debug Mode**\n",
        "Enable detailed logging and performance monitoring:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e72c828",
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "# Enable Ray Data debugging\n",
        "from ray.data.context import DataContext\n",
        "ctx = DataContext.get_current()\n",
        "ctx.enable_progress_bars = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95590eb5",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "1. **Scale to Production**: Deploy to multi-node clusters with proper resource allocation\n",
        "2. **Add Data Sources**: Connect to your specific data systems and formats\n",
        "3. **Implement Monitoring**: Set up comprehensive pipeline monitoring and alerting\n",
        "4. **Optimize Performance**: Fine-tune based on your specific workload characteristics\n",
        "\n",
        "## Resources\n",
        "\n",
        "- [Ray Data Documentation](https://docs.ray.io/en/latest/data/index.html)\n",
        "- [Ray Data Performance Guide](https://docs.ray.io/en/latest/data/performance-tips.html)\n",
        "- [ETL Best Practices](https://docs.ray.io/en/latest/data/best-practices.html)\n",
        "- [Large-Scale Data Processing](https://docs.ray.io/en/latest/data/batch_inference.html)\n",
        "\n",
        "## Cleanup and Resource Management\n",
        "\n",
        "Always clean up Ray resources when done:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0125aa9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up Ray resources\n",
        "ray.shutdown()\n",
        "print(\"Ray cluster shutdown complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42c78f24",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "*This template demonstrates Ray Data's native capabilities for large-scale ETL processing. Focus on using Ray Data's built-in operations for optimal performance and scalability.*"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}