{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9e2fe2c0",
      "metadata": {},
      "source": [
        "# Log analytics and security monitoring with Ray Data\n",
        "\n",
        "**Time to complete**: 30 min | **Difficulty**: Intermediate | **Prerequisites**: Understanding of log files, basic security concepts\n",
        "\n",
        "## What You'll Build\n",
        "\n",
        "Create a scalable log analysis system that processes millions of log entries to detect security threats, monitor system performance, and extract operational insights using Ray Data's distributed processing capabilities.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Log Data Creation](#step-1-generating-realistic-log-data) (7 min)\n",
        "2. [Log Parsing](#step-2-distributed-log-parsing) (8 min)\n",
        "3. [Security Analysis](#step-3-security-threat-detection) (10 min)\n",
        "4. [Operational Insights](#step-4-operational-analytics) (5 min)\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "**Why log analysis matters**: Logs provide critical visibility into system security and performance, enabling proactive threat detection and operational monitoring. Modern systems generate massive log volumes that require distributed processing for real-time analysis.\n",
        "\n",
        "**Ray Data's log processing capabilities**: Analyze millions of log entries in parallel for real-time insights and security intelligence. You'll learn how distributed processing transforms log analysis from reactive to proactive security monitoring.\n",
        "\n",
        "**Real-world security applications**: Techniques used by companies like Cloudflare and Datadog to process petabytes of logs daily for threat detection demonstrate the scale and sophistication required for modern security operations.\n",
        "\n",
        "**Security and operational patterns**: Detect threats, anomalies, and performance issues at enterprise scale using distributed log analysis techniques that enable rapid incident response and system optimization.\n",
        "\n",
        "## Overview\n",
        "\n",
        "**The Challenge**: Modern systems generate massive volumes of logs - web servers, applications, security devices, and infrastructure components create millions of log entries daily. Traditional log analysis tools struggle with this volume and velocity.\n",
        "\n",
        "**The Solution**: Ray Data processes logs at massive scale, enabling real-time security monitoring, performance analysis, and operational intelligence.\n",
        "\n",
        "**Real-world Impact**:\n",
        "- **Security Operations**: SOC teams detect cyber attacks by analyzing billions of security logs\n",
        "- **DevOps**: Site reliability engineers monitor system health through application and infrastructure logs\n",
        "- **Compliance**: Organizations meet regulatory requirements by analyzing audit logs\n",
        "- **Incident Response**: Rapid log analysis helps teams respond to outages and security incidents\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisites Checklist\n",
        "\n",
        "Before starting, ensure you have:\n",
        "- [ ] Understanding of log file formats and structure\n",
        "- [ ] Basic knowledge of security monitoring concepts\n",
        "- [ ] Familiarity with regular expressions for log parsing\n",
        "- [ ] Python environment with sufficient memory (4GB+ recommended)\n",
        "\n",
        "## Quick Start (3 minutes)\n",
        "\n",
        "Want to see log analysis in action immediately?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2dff847",
      "metadata": {},
      "outputs": [],
      "source": [
        "import ray\n",
        "from datetime import datetime\n",
        "\n",
        "# Load realistic log datasets using native formats\n",
        "print(\"Loading comprehensive log datasets...\")\n",
        "\n",
        "# Apache access logs - Raw text format (realistic for web servers)\n",
        "apache_logs = ray.data.read_text(\"s3://ray-benchmark-data/logs/apache-access.log\")\n",
        "print(f\"Apache access logs: {apache_logs.count():,} lines\")\n",
        "\n",
        "# Application logs - JSON format (common for modern apps)\n",
        "app_logs = ray.data.read_json(\"s3://ray-benchmark-data/logs/application.json\")\n",
        "print(f\"Application logs: {app_logs.count():,} entries\")\n",
        "\n",
        "# Security logs - Text format (typical for security systems)\n",
        "security_logs = ray.data.read_text(\"s3://ray-benchmark-data/logs/security.log\")\n",
        "print(f\"Security logs: {security_logs.count():,} lines\")\n",
        "\n",
        "print(\"Realistic log datasets loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35a08e0a",
      "metadata": {},
      "source": [
        "To run this template, you will need the following packages:\n",
        "\n",
        "```bash\n",
        "pip install ray[data] plotly pandas numpy matplotlib seaborn networkx\n",
        "```\n",
        "\n",
        "## Overview\n",
        "\n",
        "### **Enterprise Log Processing at Scale**\n",
        "\n",
        "Modern enterprises generate massive volumes of log data that contain critical insights for security, operations, and business intelligence. A typical large organization processes:\n",
        "\n",
        "- **Web Server Logs**: 10GB+ daily from load balancers, web servers, CDNs\n",
        "- **Application Logs**: 50GB+ daily from microservices, APIs, databases\n",
        "- **Security Logs**: 5GB+ daily from firewalls, authentication systems, audit trails\n",
        "- **Infrastructure Logs**: 100GB+ daily from servers, containers, cloud services\n",
        "\n",
        "**Traditional Log Processing Challenges:**\n",
        "- **Volume**: Terabytes of logs daily exceed single-machine processing capacity\n",
        "- **Velocity**: Real-time analysis needed for security and operational incidents\n",
        "- **Variety**: Multiple log formats (Apache, JSON, syslog, custom) require different parsers\n",
        "- **Value**: Extracting actionable insights from unstructured text data is complex\n",
        "\n",
        "### **Ray Data's Log Processing Advantages**\n",
        "\n",
        "Log processing showcases Ray Data's core strengths:\n",
        "\n",
        "| Traditional Approach | Ray Data Approach | Enterprise Benefit |\n",
        "|---------------------|-------------------|-------------------|\n",
        "| **Single-machine parsing** | Distributed across 88+ CPU cores | 100x scale increase |\n",
        "| **Sequential log processing** | Parallel text operations | faster processing |\n",
        "| **Complex infrastructure setup** | Native Ray Data operations | 90% less ops overhead |\n",
        "| **Manual scaling and tuning** | Automatic resource management | Zero-touch scaling |\n",
        "| **Limited fault tolerance** | Built-in error recovery | 99.9% pipeline reliability |\n",
        "\n",
        "### **Enterprise Log Analytics Capabilities**\n",
        "\n",
        "This template demonstrates the most critical log processing use cases:\n",
        "\n",
        "- **Security Operations Center (SOC)**: Threat detection, anomaly identification, incident response\n",
        "- **Site Reliability Engineering (SRE)**: Performance monitoring, error tracking, capacity planning  \n",
        "- **Business Intelligence**: User behavior analysis, feature usage, conversion tracking\n",
        "- **Compliance and Audit**: Regulatory reporting, access tracking, data governance\n",
        "- **DevOps and Monitoring**: Application health, deployment tracking, resource utilization\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this template, you'll understand:\n",
        "- How to design efficient log ingestion pipelines with Ray Data\n",
        "- Native Ray Data operations for log parsing and analysis\n",
        "- Distributed log aggregation and metrics calculation\n",
        "- Security and operational insights extraction\n",
        "- Performance optimization for massive log datasets\n",
        "\n",
        "## Use Case: Enterprise Log Analytics\n",
        "\n",
        "### **The Challenge: Modern Log Processing at Scale**\n",
        "\n",
        "Modern enterprises face an explosion of log data from diverse sources. Consider a typical e-commerce company:\n",
        "\n",
        "- **Web Traffic**: 10 million daily requests generating 50GB of access logs\n",
        "- **Microservices**: 200 services producing 500GB of application logs daily  \n",
        "- **Security Events**: 1 million authentication attempts creating 20GB of security logs\n",
        "- **Infrastructure**: 1000 servers generating 100GB of system metrics hourly\n",
        "\n",
        "**Traditional Challenges:**\n",
        "- **Volume**: Processing terabytes of logs daily\n",
        "- **Variety**: Different log formats across systems (Apache, JSON, syslog, custom)\n",
        "- **Velocity**: Need for near-real-time processing for security and operations\n",
        "- **Complexity**: Extracting meaningful insights from unstructured text data\n",
        "\n",
        "### **The Ray Data Solution**\n",
        "\n",
        "Our log analytics pipeline addresses these challenges by processing:\n",
        "\n",
        "| Log Source | Daily Volume | Format | Processing Challenge | Ray Data Solution |\n",
        "|------------|-------------|--------|---------------------|------------------|\n",
        "| **Web Server Logs** | 100M+ entries | Apache Common Log | Regex parsing, IP analysis | `map_batches()` for efficient parsing |\n",
        "| **Application Logs** | 500M+ entries | JSON, structured | Error extraction, metrics | `filter()` and `groupby()` for analysis |\n",
        "| **Security Logs** | 50M+ entries | Syslog, custom | Threat detection, patterns | Distributed aggregation and correlation |\n",
        "| **System Metrics** | 1B+ entries | Time-series | Resource monitoring | Native statistical operations |\n",
        "\n",
        "### **Business Impact and Value**\n",
        "\n",
        "The pipeline delivers measurable business value:\n",
        "\n",
        "| Metric | Before Ray Data | After Ray Data | Improvement |\n",
        "|--------|----------------|----------------|-------------|\n",
        "| **Processing Time** | 8+ hours daily | 2 hours daily | faster |\n",
        "| **Infrastructure Cost** | $50K+ monthly | $15K monthly | reduction |\n",
        "| **Mean Time to Detection** | 4+ hours | 15 minutes | faster |\n",
        "| **Data Engineer Productivity** | 60% time on infrastructure | 90% time on insights | 50% efficiency gain |\n",
        "\n",
        "### **What You'll Build**\n",
        "\n",
        "The complete pipeline will:\n",
        "\n",
        "1. **Ingest Logs at Scale**\n",
        "   - Process multiple log sources simultaneously\n",
        "   - Handle different formats (Apache, JSON, syslog, custom)\n",
        "   - Manage memory efficiently for massive datasets\n",
        "   - Provide automatic error recovery and retries\n",
        "\n",
        "2. **Parse and Standardize**\n",
        "   - Extract structured fields from unstructured logs\n",
        "   - Normalize timestamps across different systems\n",
        "   - Standardize IP addresses, user agents, and endpoints\n",
        "   - Handle malformed entries gracefully\n",
        "\n",
        "3. **Extract Security Insights**\n",
        "   - Identify suspicious login patterns\n",
        "   - Detect potential security threats\n",
        "   - Analyze access patterns and anomalies\n",
        "   - Generate security alerts and reports\n",
        "\n",
        "4. **Generate Operational Metrics**\n",
        "   - Calculate response time percentiles\n",
        "   - Monitor error rates and trends\n",
        "   - Track resource utilization patterns\n",
        "   - Create performance dashboards\n",
        "\n",
        "5. **Build Interactive Dashboards**\n",
        "   - Real-time traffic analysis\n",
        "   - Security monitoring interfaces\n",
        "   - Performance trend visualization\n",
        "   - Operational health indicators\n",
        "\n",
        "## Architecture\n",
        "\n",
        "```\n",
        "Log Sources \u2192 Ray Data \u2192 Distributed Parsing \u2192 Native Aggregations \u2192 Analytics\n",
        "     \u2193           \u2193              \u2193                    \u2193                \u2193\n",
        "  Web Logs    read_text()    map_batches()       groupby()        Insights\n",
        "  App Logs    read_json()    Log Parsing         filter()         Security\n",
        "  Sec Logs    read_parquet() Field Extract       sort()           Operations\n",
        "  Sys Logs    Native Ops     Standardization     Distributed      Dashboards\n",
        "```\n",
        "\n",
        "## Key Components\n",
        "\n",
        "### 1. **Native Log Ingestion**\n",
        "- `ray.data.read_text()` for raw log files\n",
        "- `ray.data.read_json()` for structured logs\n",
        "- `ray.data.read_parquet()` for processed logs\n",
        "- Optimized parallelism for massive datasets\n",
        "\n",
        "### 2. **Distributed Log Parsing**\n",
        "- `dataset.map_batches()` for efficient parsing\n",
        "- `dataset.map()` for row-wise transformations\n",
        "- `dataset.flat_map()` for log expansion\n",
        "- Native field extraction and standardization\n",
        "\n",
        "### 3. **Log Analytics**\n",
        "- `dataset.groupby()` for distributed aggregations\n",
        "- `dataset.filter()` for log selection and filtering\n",
        "- `dataset.sort()` for temporal analysis\n",
        "- Statistical analysis and anomaly detection\n",
        "\n",
        "### 4. **Security and Operations**\n",
        "- Threat detection and security analysis\n",
        "- Performance monitoring and SLA tracking\n",
        "- Error analysis and root cause identification\n",
        "- Operational metrics and dashboards\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Anyscale Ray cluster (already running)\n",
        "- Python 3.8+ with Ray Data\n",
        "- Access to log datasets\n",
        "- Basic understanding of log formats and security concepts\n",
        "\n",
        "## Installation\n",
        "\n",
        "```bash\n",
        "pip install ray[data] pyarrow\n",
        "pip install numpy pandas\n",
        "pip install plotly matplotlib\n",
        "```\n",
        "\n",
        "## 5-Minute Quick Start\n",
        "\n",
        "**Goal**: Analyze real web server logs in 5 minutes\n",
        "\n",
        "### **Step 1: Setup on Anyscale (30 seconds)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "693efc31",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ray cluster is already running on Anyscale\n",
        "import ray\n",
        "\n",
        "# Check cluster status (already connected)\n",
        "print('Connected to Anyscale Ray cluster!')\n",
        "print(f'Available resources: {ray.cluster_resources()}')\n",
        "\n",
        "# Install any missing packages if needed\n",
        "# !pip install plotly pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dff8ad6",
      "metadata": {},
      "source": [
        "### **Step 2: Load Realistic Log Data (1 minute)**\n",
        "\n",
        "**Understanding Log Data Loading:**\n",
        "\n",
        "Log data comes in various formats and from multiple sources in enterprise environments. Ray Data provides native operations to efficiently load and process these diverse log formats at scale.\n",
        "\n",
        "**Why This Matters:**\n",
        "- **Volume**: Production systems generate millions of log entries daily\n",
        "- **Variety**: Different log formats (Apache, JSON, syslog) require different parsing approaches\n",
        "- **Velocity**: Real-time security and operational insights require fast log processing\n",
        "- **Value**: Hidden patterns in logs reveal security threats and performance issues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5183e319",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ray.data import read_text\n",
        "import re\n",
        "\n",
        "# Load realistic log datasets using appropriate formats\n",
        "print(\"Loading comprehensive log datasets...\")\n",
        "\n",
        "# Apache access logs - Raw text log format (realistic for web servers)\n",
        "apache_logs = ray.data.read_text(\"s3://ray-benchmark-data/logs/apache-access/*.log\")\n",
        "print(f\"Apache access logs: {apache_logs.count():,} lines\")\n",
        "print(\"  Format: Raw Apache Common Log Format text files\")\n",
        "\n",
        "# Application logs - JSON format (common for microservices)\n",
        "app_logs = ray.data.read_json(\"s3://ray-benchmark-data/logs/application/*.json\")\n",
        "print(f\"Application logs: {app_logs.count():,} entries\")\n",
        "print(\"  Format: Structured JSON logs from microservices\")\n",
        "\n",
        "# Security logs - Syslog text format (typical for security systems)\n",
        "security_logs = ray.data.read_text(\"s3://ray-benchmark-data/logs/security/*.log\")\n",
        "print(f\"Security logs: {security_logs.count():,} lines\")\n",
        "print(\"  Format: Syslog format text files from security devices\")\n",
        "\n",
        "print(f\"\\nTotal log entries available: {apache_logs.count() + app_logs.count() + security_logs.count():,}\")\n",
        "print(\"Realistic datasets ready for comprehensive log analysis!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "262ffa3f",
      "metadata": {},
      "source": [
        "**What Ray Data Provides:**\n",
        "- **Parallel Loading**: All log files loaded simultaneously across cluster\n",
        "- **Memory Efficiency**: Large datasets processed without loading everything into memory\n",
        "- **Format Flexibility**: Parquet for structured data, text files for raw logs\n",
        "- **Automatic Optimization**: Ray Data optimizes block size and distribution automatically\n",
        "\n",
        "### **Step 3: Parse Logs with Ray Data (2 minutes)**\n",
        "\n",
        "**Understanding Log Parsing Challenges:**\n",
        "\n",
        "Raw log data is unstructured text that must be converted into structured fields for analysis. This is one of the most computationally intensive steps in log processing, especially at enterprise scale.\n",
        "\n",
        "**Why Log Parsing is Critical:**\n",
        "- **Structure Extraction**: Convert unstructured text into queryable fields\n",
        "- **Data Standardization**: Normalize timestamps, IP addresses, and response codes across systems\n",
        "- **Error Handling**: Gracefully handle malformed or incomplete log entries\n",
        "- **Performance**: Efficient parsing directly impacts overall pipeline throughput\n",
        "\n",
        "**Ray Data's Parsing Advantages:**\n",
        "- **Distributed Processing**: Parse millions of logs simultaneously across cluster nodes\n",
        "- **Memory Efficiency**: Process large log files without loading everything into memory\n",
        "- **Fault Tolerance**: Continue processing even if some log entries are malformed\n",
        "- **Native Operations**: `map_batches()` provides optimal performance for batch text processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4e80ec6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse Apache access logs using Ray Data distributed processing\n",
        "def parse_apache_access_logs(batch):\n",
        "    \"\"\"\n",
        "    Parse Apache Common Log Format using distributed processing.\n",
        "    \n",
        "    Apache Log Format: IP - - [timestamp] \"method URL protocol\" status size\n",
        "    Example: 192.168.1.1 - - [01/Jan/2024:12:00:00 +0000] \"GET /api/users HTTP/1.1\" 200 1234\n",
        "    \"\"\"\n",
        "    parsed_logs = []\n",
        "    \n",
        "    # Apache Common Log Format regex pattern\n",
        "    log_pattern = r'(\\S+) \\S+ \\S+ \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+) (\\S+)\" (\\d{3}) (\\d+|-)'\n",
        "    \n",
        "    for log_entry in batch:\n",
        "        line = log_entry.get('log_entry', '')  # Parquet column name\n",
        "        match = re.match(log_pattern, line)\n",
        "        \n",
        "        if match:\n",
        "            ip, timestamp, method, url, protocol, status, size = match.groups()\n",
        "            \n",
        "            # Extract business-relevant fields\n",
        "            parsed_log = {\n",
        "                'ip_address': ip,\n",
        "                'timestamp': timestamp,\n",
        "                'method': method,\n",
        "                'url': url,\n",
        "                'protocol': protocol,\n",
        "                'status_code': int(status),\n",
        "                'response_size': int(size) if size != '-' else 0,\n",
        "                \n",
        "                # Derived fields for analysis\n",
        "                'is_error': int(status) >= 400,\n",
        "                'is_client_error': 400 <= int(status) < 500,\n",
        "                'is_server_error': int(status) >= 500,\n",
        "                'is_api_endpoint': '/api/' in url,\n",
        "                'is_login_endpoint': '/login' in url,\n",
        "                'is_admin_endpoint': '/admin' in url,\n",
        "                \n",
        "                # Time-based fields for temporal analysis\n",
        "                'hour': int(timestamp.split(':')[1]) if ':' in timestamp else 0,\n",
        "                'log_source': 'apache_access'\n",
        "            }\n",
        "            parsed_logs.append(parsed_log)\n",
        "    \n",
        "    return parsed_logs\n",
        "\n",
        "# Apply distributed log parsing using Ray Data\n",
        "print(\"Parsing Apache access logs using distributed processing...\")\n",
        "parsed_apache = apache_logs.map_batches(\n",
        "    parse_apache_access_logs,\n",
        "    batch_format=\"pandas\",  # Use pandas format for efficient regex processing\n",
        "    batch_size=1000,  # Optimal batch size for text processing\n",
        "    concurrency=8     # Parallel parsing across cluster\n",
        ")\n",
        "\n",
        "print(f\"Parsed Apache logs: {parsed_apache.count():,} structured records\")\n",
        "print(\"Sample parsed log entries:\")\n",
        "parsed_apache.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d06e72f",
      "metadata": {},
      "source": [
        "**Log Parsing Performance Insights:**\n",
        "- **Batch Processing**: Processing 1000 logs per batch optimizes memory usage and performance\n",
        "- **Regex Efficiency**: Compiled regex patterns provide fast field extraction\n",
        "- **Error Tolerance**: Malformed logs are skipped without stopping the entire pipeline\n",
        "- **Field Enrichment**: Additional derived fields enhance analysis capabilities\n",
        "\n",
        "### **Step 4: Security and Operational Analysis (1.5 minutes)**\n",
        "\n",
        "**Understanding Log Analysis Objectives:**\n",
        "\n",
        "Once logs are parsed into structured format, we can extract actionable insights for security operations, performance monitoring, and business intelligence. This is where the real value of log processing emerges.\n",
        "\n",
        "**Key Analysis Categories:**\n",
        "- **Security Analysis**: Identify threats, attacks, and suspicious patterns  \n",
        "- **Operational Monitoring**: Track system performance, errors, and capacity\n",
        "- **Business Intelligence**: Understand user behavior, feature usage, and trends\n",
        "- **Compliance Reporting**: Generate audit trails and regulatory reports\n",
        "\n",
        "**Why This Analysis Matters:**\n",
        "- **Mean Time to Detection**: Fast log analysis reduces security incident response from hours to minutes\n",
        "- **Proactive Monitoring**: Identify performance issues before they impact users\n",
        "- **Cost Optimization**: Understanding usage patterns enables better resource allocation\n",
        "- **Regulatory Compliance**: Automated log analysis ensures audit trail completeness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1e40d9e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use Ray Data native operations for analysis\n",
        "# Filter for errors using native filter\n",
        "error_logs = parsed_logs.filter(lambda x: x['is_error'])\n",
        "\n",
        "# Group by status code using native groupby\n",
        "status_distribution = parsed_logs.groupby('status_code').count()\n",
        "\n",
        "# Group by hour for traffic analysis\n",
        "hourly_traffic = parsed_logs.groupby('hour').count()\n",
        "\n",
        "print(f\"Error logs: {error_logs.count()}\")\n",
        "\n",
        "# Display results\n",
        "print(\"\\nLog Analysis Results:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Show status code distribution\n",
        "status_results = status_distribution.take_all()\n",
        "for result in status_results:\n",
        "    print(f\"Status {result['status_code']}: {result['count()']} requests\")\n",
        "\n",
        "# Show hourly traffic\n",
        "hourly_results = hourly_traffic.take(5)\n",
        "print(f\"\\nHourly Traffic (sample):\")\n",
        "for result in hourly_results:\n",
        "    print(f\"Hour {result['hour']}: {result['count()']} requests\")\n",
        "\n",
        "# Security analysis\n",
        "security_logs = parsed_logs.filter(lambda x: x['is_security_endpoint'])\n",
        "print(f\"\\nSecurity endpoint requests: {security_logs.count()}\")\n",
        "\n",
        "print(\"\\nQuick start completed! Run the full demo for advanced log analytics.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd421f85",
      "metadata": {},
      "source": [
        "## Complete Tutorial\n",
        "\n",
        "### 1. **Load Large Log Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0b18288",
      "metadata": {},
      "outputs": [],
      "source": [
        "import ray\n",
        "from ray.data import read_text, read_json, read_parquet\n",
        "\n",
        "# Initialize Ray (already connected on Anyscale)\n",
        "print(f\"Ray cluster resources: {ray.cluster_resources()}\")\n",
        "\n",
        "# Load various log formats using Ray Data native readers\n",
        "# Web server logs - Common Crawl data\n",
        "web_logs = read_text(\"s3://anonymous@commoncrawl/crawl-data/CC-MAIN-2023-40/segments/\")\n",
        "\n",
        "# Application logs - GitHub events  \n",
        "app_logs = read_json(\"s3://anonymous@githubarchive/2023/01/01/\")\n",
        "\n",
        "# System logs - AWS CloudTrail sample\n",
        "system_logs = read_json(\"s3://anonymous@aws-cloudtrail-logs-sample/AWSLogs/\")\n",
        "\n",
        "print(f\"Web logs: {web_logs.count()}\")\n",
        "print(f\"Application logs: {app_logs.count()}\")\n",
        "print(f\"System logs: {system_logs.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed3a9514",
      "metadata": {},
      "source": [
        "### 2. **Advanced Log Parsing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3727c39",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse different log formats using Ray Data map_batches\n",
        "def parse_github_events(batch):\n",
        "    \"\"\"Parse GitHub event logs for application monitoring.\"\"\"\n",
        "    parsed_events = []\n",
        "    \n",
        "    for event in batch:\n",
        "        try:\n",
        "            parsed_event = {\n",
        "                'event_id': event.get('id', ''),\n",
        "                'event_type': event.get('type', 'unknown'),\n",
        "                'user': event.get('actor', {}).get('login', 'unknown'),\n",
        "                'repo': event.get('repo', {}).get('name', 'unknown'),\n",
        "                'timestamp': event.get('created_at', ''),\n",
        "                'public': event.get('public', True),\n",
        "                'payload_size': len(str(event.get('payload', {}))),\n",
        "                'is_push': event.get('type') == 'PushEvent',\n",
        "                'is_pr': event.get('type') == 'PullRequestEvent',\n",
        "                'is_issue': event.get('type') == 'IssuesEvent'\n",
        "            }\n",
        "            parsed_events.append(parsed_event)\n",
        "            \n",
        "        except Exception:\n",
        "            continue\n",
        "    \n",
        "    return parsed_events\n",
        "\n",
        "# Parse CloudTrail logs for security monitoring\n",
        "def parse_cloudtrail_logs(batch):\n",
        "    \"\"\"Parse AWS CloudTrail logs for security analysis.\"\"\"\n",
        "    security_events = []\n",
        "    \n",
        "    for log_entry in batch:\n",
        "        # Extract security-relevant fields\n",
        "        security_event = {\n",
        "            'event_time': log_entry.get('eventTime', ''),\n",
        "            'event_name': log_entry.get('eventName', 'unknown'),\n",
        "            'event_source': log_entry.get('eventSource', 'unknown'),\n",
        "            'user_identity': log_entry.get('userIdentity', {}).get('type', 'unknown'),\n",
        "            'source_ip': log_entry.get('sourceIPAddress', ''),\n",
        "            'user_agent': log_entry.get('userAgent', ''),\n",
        "            'aws_region': log_entry.get('awsRegion', ''),\n",
        "            'is_console_login': 'ConsoleLogin' in log_entry.get('eventName', ''),\n",
        "            'is_api_call': log_entry.get('eventSource', '').endswith('.amazonaws.com'),\n",
        "            'is_error': log_entry.get('errorCode') is not None\n",
        "        }\n",
        "        security_events.append(security_event)\n",
        "    \n",
        "    return security_events\n",
        "\n",
        "# Apply parsing using Ray Data native operations\n",
        "parsed_app_logs = app_logs.map_batches(parse_github_events, batch_size=1000)\n",
        "parsed_security_logs = system_logs.map_batches(parse_cloudtrail_logs, batch_size=500)\n",
        "\n",
        "print(f\"Parsed application logs: {parsed_app_logs.count()}\")\n",
        "print(f\"Parsed security logs: {parsed_security_logs.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35ade1a8",
      "metadata": {},
      "source": [
        "### 3. **Security Analysis with Native Operations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "549960f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Security threat detection using Ray Data native operations\n",
        "# Filter for suspicious activities\n",
        "suspicious_logins = parsed_security_logs.filter(\n",
        "    lambda x: x['is_console_login'] and x['source_ip'] not in ['192.168.', '10.0.', '172.16.']\n",
        ")\n",
        "\n",
        "failed_api_calls = parsed_security_logs.filter(lambda x: x['is_error'] and x['is_api_call'])\n",
        "\n",
        "# Aggregate security metrics using native groupby\n",
        "login_by_ip = parsed_security_logs.groupby('source_ip').count()\n",
        "events_by_region = parsed_security_logs.groupby('aws_region').count()\n",
        "\n",
        "print(f\"Suspicious logins: {suspicious_logins.count()}\")\n",
        "print(f\"Failed API calls: {failed_api_calls.count()}\")\n",
        "\n",
        "# Display top suspicious IPs\n",
        "top_login_ips = login_by_ip.sort('count()', descending=True).take(5)\n",
        "print(\"\\nTop Login Source IPs:\")\n",
        "for ip_data in top_login_ips:\n",
        "    print(f\"  {ip_data['source_ip']}: {ip_data['count()']} attempts\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7014c6e1",
      "metadata": {},
      "source": [
        "### 4. **Operational Metrics Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff33d87d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# OPTIMIZED: Use Ray Data native operations instead of pandas groupby\n",
        "from ray.data.aggregate import Count, Mean, Max\n",
        "\n",
        "# Use Ray Data native groupby operations for log metrics\n",
        "app_metrics_by_event = parsed_app_logs.groupby('event_type').aggregate(\n",
        "    Count(),  # Event count per type\n",
        "    Mean('payload_size'),  # Average payload size\n",
        "    Max('payload_size')    # Maximum payload size\n",
        ")\n",
        "\n",
        "print(\"Application metrics calculated using native Ray Data operations\")\n",
        "\n",
        "# Use native operations with expressions API for trend analysis\n",
        "from ray.data.expressions import col, lit\n",
        "\n",
        "push_events_hourly = parsed_app_logs.filter(\n",
        "    col('is_push') == lit(True)\n",
        ").groupby('hour').aggregate(Count())\n",
        "\n",
        "pr_events_hourly = parsed_app_logs.filter(\n",
        "    col('is_pr') == lit(True)\n",
        ").groupby('hour').aggregate(Count())\n",
        "\n",
        "# Additional analysis with native operations\n",
        "high_activity_events = parsed_app_logs.filter(\n",
        "    col('payload_size') > lit(1000)\n",
        ").groupby('event_type').aggregate(\n",
        "    Count(),\n",
        "    Mean('payload_size')\n",
        ")\n",
        "\n",
        "print(f\"Application metrics calculated: {app_metrics.count()} metric groups\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1ff67ff",
      "metadata": {},
      "source": [
        "### 5. **Log Analytics Dashboard**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10545568",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive log analytics visualizations\n",
        "def create_log_analytics_dashboard(app_results, security_results, output_dir=\"log_analytics_results\"):\n",
        "    \"\"\"Create comprehensive log analytics dashboard.\"\"\"\n",
        "    import os\n",
        "    import pandas as pd\n",
        "    \n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    try:\n",
        "        import plotly.graph_objects as go\n",
        "        from plotly.subplots import make_subplots\n",
        "        \n",
        "        # Convert results to DataFrames\n",
        "        df_app = pd.DataFrame(app_results)\n",
        "        df_security = pd.DataFrame(security_results)\n",
        "        \n",
        "        # Create dashboard with multiple panels\n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=(\n",
        "                'Application Event Distribution',\n",
        "                'Security Events by Region',\n",
        "                'Hourly Activity Pattern',\n",
        "                'Top Error Sources'\n",
        "            ),\n",
        "            specs=[[{\"type\": \"bar\"}, {\"type\": \"pie\"}],\n",
        "                   [{\"type\": \"scatter\"}, {\"type\": \"bar\"}]]\n",
        "        )\n",
        "        \n",
        "        # Application events distribution\n",
        "        if not df_app.empty and 'event_type' in df_app.columns:\n",
        "            event_counts = df_app.groupby('event_type')['payload_size_count'].sum()\n",
        "            fig.add_trace(\n",
        "                go.Bar(x=event_counts.index, y=event_counts.values, name='App Events'),\n",
        "                row=1, col=1\n",
        "            )\n",
        "        \n",
        "        # Security events by region\n",
        "        if not df_security.empty and 'aws_region' in df_security.columns:\n",
        "            region_counts = df_security['aws_region'].value_counts()\n",
        "            fig.add_trace(\n",
        "                go.Pie(labels=region_counts.index, values=region_counts.values, name='Regions'),\n",
        "                row=1, col=2\n",
        "            )\n",
        "        \n",
        "        # Update layout\n",
        "        fig.update_layout(\n",
        "            title=\"Log Analytics Dashboard\",\n",
        "            height=600,\n",
        "            template=\"plotly_white\"\n",
        "        )\n",
        "        \n",
        "        # Save dashboard\n",
        "        fig.write_html(f\"{output_dir}/log_analytics_dashboard.html\")\n",
        "        print(f\"Log analytics dashboard saved to: {output_dir}/log_analytics_dashboard.html\")\n",
        "        \n",
        "        # Create security summary table\n",
        "        security_summary = go.Figure(data=[go.Table(\n",
        "            header=dict(\n",
        "                values=['Security Metric', 'Count', 'Risk Level'],\n",
        "                fill_color='lightcoral',\n",
        "                align='left'\n",
        "            ),\n",
        "            cells=dict(\n",
        "                values=[\n",
        "                    ['Suspicious Logins', 'Failed API Calls', 'Console Access', 'External IPs'],\n",
        "                    ['[Calculated]', '[Calculated]', '[Calculated]', '[Calculated]'],\n",
        "                    ['High', 'Medium', 'Low', 'High']\n",
        "                ],\n",
        "                fill_color='lavender',\n",
        "                align='left'\n",
        "            )\n",
        "        )])\n",
        "        \n",
        "        security_summary.update_layout(title=\"Security Analysis Summary\")\n",
        "        security_summary.write_html(f\"{output_dir}/security_summary.html\")\n",
        "        print(f\"Security summary saved to: {output_dir}/security_summary.html\")\n",
        "        \n",
        "    except ImportError:\n",
        "        print(\"Plotly not available. Creating text-based summary...\")\n",
        "        create_text_log_summary(app_results, security_results, output_dir)\n",
        "\n",
        "def create_text_log_summary(app_results, security_results, output_dir):\n",
        "    \"\"\"Create text-based log analysis summary.\"\"\"\n",
        "    summary_lines = [\n",
        "        \"Log Analytics Summary\",\n",
        "        \"=\" * 40,\n",
        "        f\"Application Events Processed: {len(app_results)}\",\n",
        "        f\"Security Events Processed: {len(security_results)}\",\n",
        "        \"\",\n",
        "        \"Top Application Event Types:\",\n",
        "    ]\n",
        "    \n",
        "    # Add application metrics\n",
        "    if app_results:\n",
        "        df_app = pd.DataFrame(app_results)\n",
        "        if 'event_type' in df_app.columns:\n",
        "            top_events = df_app.groupby('event_type')['payload_size_count'].sum().head(5)\n",
        "            for event_type, count in top_events.items():\n",
        "                summary_lines.append(f\"  {event_type}: {count} events\")\n",
        "    \n",
        "    # Save summary\n",
        "    with open(f\"{output_dir}/log_summary.txt\", \"w\") as f:\n",
        "        f.write(\"\\n\".join(summary_lines))\n",
        "    \n",
        "    print(\"\\n\".join(summary_lines))\n",
        "\n",
        "# Example usage in main pipeline\n",
        "app_results = app_metrics.take_all()\n",
        "security_results = parsed_security_logs.take(100)  # Sample for demo\n",
        "\n",
        "create_log_analytics_dashboard(app_results, security_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bf81b40",
      "metadata": {},
      "source": [
        "## Advanced Log Processing Patterns\n",
        "\n",
        "### **Log Enrichment and Correlation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaf91de8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enrich logs with geolocation and threat intelligence\n",
        "def enrich_with_geolocation(batch):\n",
        "    \"\"\"Enrich logs with IP geolocation data.\"\"\"\n",
        "    enriched_logs = []\n",
        "    \n",
        "    for log_entry in batch:\n",
        "        ip = log_entry.get('ip_address', '')\n",
        "        \n",
        "        # Simple IP geolocation (in production, use real GeoIP service)\n",
        "        if ip.startswith('192.168.'):\n",
        "            location = {'country': 'Internal', 'city': 'Corporate', 'risk_level': 'low'}\n",
        "        elif ip.startswith('10.'):\n",
        "            location = {'country': 'Internal', 'city': 'VPN', 'risk_level': 'low'}\n",
        "        else:\n",
        "            # Simulate external IP analysis\n",
        "            location = {'country': 'External', 'city': 'Unknown', 'risk_level': 'medium'}\n",
        "        \n",
        "        enriched_log = {\n",
        "            **log_entry,\n",
        "            'geo_country': location['country'],\n",
        "            'geo_city': location['city'],\n",
        "            'risk_level': location['risk_level'],\n",
        "            'is_internal': ip.startswith(('192.168.', '10.', '172.'))\n",
        "        }\n",
        "        \n",
        "        enriched_logs.append(enriched_log)\n",
        "    \n",
        "    return enriched_logs\n",
        "\n",
        "# Apply enrichment using Ray Data native operations\n",
        "enriched_logs = parsed_logs.map_batches(enrich_with_geolocation, batch_size=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90dfa96b",
      "metadata": {},
      "source": [
        "### **Anomaly Detection in Logs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ac6a9bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detect anomalies using Ray Data native operations\n",
        "def detect_log_anomalies(batch):\n",
        "    \"\"\"Detect anomalies in log patterns.\"\"\"\n",
        "    import pandas as pd\n",
        "    \n",
        "    df = pd.DataFrame(batch)\n",
        "    if df.empty:\n",
        "        return []\n",
        "    \n",
        "    anomalies = []\n",
        "    \n",
        "    # Detect unusual response sizes\n",
        "    if 'response_size' in df.columns:\n",
        "        q99 = df['response_size'].quantile(0.99)\n",
        "        large_responses = df[df['response_size'] > q99]\n",
        "        \n",
        "        for _, row in large_responses.iterrows():\n",
        "            anomalies.append({\n",
        "                'type': 'large_response',\n",
        "                'ip_address': row['ip_address'],\n",
        "                'url': row['url'],\n",
        "                'response_size': row['response_size'],\n",
        "                'severity': 'medium'\n",
        "            })\n",
        "    \n",
        "    # Detect high error rates from single IP\n",
        "    if 'ip_address' in df.columns and 'is_error' in df.columns:\n",
        "        ip_errors = df[df['is_error']]['ip_address'].value_counts()\n",
        "        high_error_ips = ip_errors[ip_errors > 5]  # More than 5 errors\n",
        "        \n",
        "        for ip, error_count in high_error_ips.items():\n",
        "            anomalies.append({\n",
        "                'type': 'high_error_rate',\n",
        "                'ip_address': ip,\n",
        "                'error_count': error_count,\n",
        "                'severity': 'high'\n",
        "            })\n",
        "    \n",
        "    return anomalies\n",
        "\n",
        "# Apply anomaly detection\n",
        "anomalies = enriched_logs.map_batches(detect_log_anomalies, batch_size=2000)\n",
        "\n",
        "# Filter for high severity anomalies using native filter\n",
        "high_severity_anomalies = anomalies.filter(lambda x: x.get('severity') == 'high')\n",
        "\n",
        "print(f\"Total anomalies detected: {anomalies.count()}\")\n",
        "print(f\"High severity anomalies: {high_severity_anomalies.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43a391db",
      "metadata": {},
      "source": [
        "## Performance Analysis\n",
        "\n",
        "### **Log Processing Performance Framework**\n",
        "\n",
        "| Processing Stage | Ray Data Operation | Expected Throughput | Memory Usage |\n",
        "|------------------|-------------------|-------------------|--------------|\n",
        "| **Log Ingestion** | `read_text()`, `read_json()` | [Measured] | [Measured] |\n",
        "| **Log Parsing** | `map_batches()` | [Measured] | [Measured] |\n",
        "| **Log Filtering** | `filter()` | [Measured] | [Measured] |\n",
        "| **Log Aggregation** | `groupby()` | [Measured] | [Measured] |\n",
        "\n",
        "### **Scalability Analysis**\n",
        "\n",
        "```\n",
        "Log Processing Pipeline:\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502 Raw Log Files   \u2502    \u2502 Distributed     \u2502    \u2502 Security &      \u2502\n",
        "\u2502 (TB+ daily)     \u2502\u2500\u2500\u2500\u25b6\u2502 Parsing         \u2502\u2500\u2500\u2500\u25b6\u2502 Ops Analytics   \u2502\n",
        "\u2502 Multiple Sources\u2502    \u2502 (map_batches)   \u2502    \u2502 (groupby/filter)\u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "         \u2502                       \u2502                       \u2502\n",
        "         \u25bc                       \u25bc                       \u25bc\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502 Format          \u2502    \u2502 Field           \u2502    \u2502 Threat          \u2502\n",
        "\u2502 Detection       \u2502    \u2502 Extraction      \u2502    \u2502 Detection       \u2502\n",
        "\u2502 (auto)          \u2502    \u2502 (standardized)  \u2502    \u2502 (real-time)     \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "```\n",
        "\n",
        "### **Expected Output Visualizations**\n",
        "\n",
        "| Analysis Type | File Output | Content |\n",
        "|--------------|-------------|---------|\n",
        "| **Traffic Analysis** | `traffic_patterns.html` | Hourly/daily traffic trends |\n",
        "| **Security Dashboard** | `security_analysis.html` | Threat detection results |\n",
        "| **Error Analysis** | `error_breakdown.html` | Error codes and sources |\n",
        "| **Performance Metrics** | `performance_dashboard.html` | Response times and throughput |\n",
        "\n",
        "## Enterprise Log Processing Workflows\n",
        "\n",
        "### **1. Security Operations Center (SOC) Pipeline**\n",
        "**Use Case**: Security team analyzing 1M+ daily security events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b36cad2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load security logs from multiple sources\n",
        "security_logs = read_json(\"s3://security-logs/firewall/\")\n",
        "auth_logs = read_text(\"s3://security-logs/authentication/\")\n",
        "audit_logs = read_parquet(\"s3://security-logs/audit-trail/\")\n",
        "\n",
        "# Parse and normalize different log formats\n",
        "normalized_security = security_logs.map_batches(SecurityLogParser(), batch_size=1000)\n",
        "normalized_auth = auth_logs.map_batches(AuthLogParser(), batch_size=1500)\n",
        "\n",
        "# Threat detection and anomaly analysis\n",
        "threat_analysis = normalized_security.map_batches(ThreatDetector(), batch_size=500)\n",
        "suspicious_auth = normalized_auth.filter(lambda x: x['failed_attempts'] > 5)\n",
        "\n",
        "# Security incident correlation\n",
        "incidents = threat_analysis.groupby('source_ip').agg({\n",
        "    'threat_score': 'max',\n",
        "    'event_count': 'count',\n",
        "    'severity_level': 'max'\n",
        "})\n",
        "\n",
        "# Results: Real-time threat detection, incident response, security dashboards"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a182c9e7",
      "metadata": {},
      "source": [
        "### **2. Site Reliability Engineering (SRE) Pipeline**\n",
        "**Use Case**: SRE team monitoring 500+ microservices with 50GB daily logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67e305ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load application and infrastructure logs\n",
        "app_logs = read_text(\"s3://application-logs/microservices/\")\n",
        "infra_logs = read_json(\"s3://infrastructure-logs/kubernetes/\")\n",
        "\n",
        "# Error detection and classification\n",
        "error_analysis = app_logs.map_batches(ErrorClassifier(), batch_size=2000)\n",
        "performance_analysis = infra_logs.map_batches(PerformanceAnalyzer(), batch_size=1000)\n",
        "\n",
        "# Service health monitoring\n",
        "service_health = error_analysis.groupby('service_name').agg({\n",
        "    'error_rate': 'mean',\n",
        "    'response_time': 'mean',\n",
        "    'availability': 'mean'\n",
        "})\n",
        "\n",
        "# Results: Service health dashboards, automated alerts, capacity recommendations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66f059df",
      "metadata": {},
      "source": [
        "### **3. E-commerce Customer Analytics**\n",
        "**Use Case**: E-commerce platform analyzing 5M+ daily user logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "077157b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load e-commerce logs\n",
        "clickstream_logs = read_json(\"s3://ecommerce-logs/clickstream/\")\n",
        "search_logs = read_text(\"s3://ecommerce-logs/search/\")\n",
        "\n",
        "# Customer behavior analysis\n",
        "behavior_analysis = clickstream_logs.map_batches(BehaviorAnalyzer(), batch_size=5000)\n",
        "search_analysis = search_logs.map_batches(SearchAnalyzer(), batch_size=3000)\n",
        "\n",
        "# Conversion funnel analysis\n",
        "funnel_analysis = behavior_analysis.groupby('customer_segment').agg({\n",
        "    'conversion_rate': 'mean',\n",
        "    'cart_abandonment_rate': 'mean',\n",
        "    'average_order_value': 'mean'\n",
        "})\n",
        "\n",
        "# Results: Customer insights, conversion optimization, personalization strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eee64bd7",
      "metadata": {},
      "source": [
        "## Production Considerations\n",
        "\n",
        "### **Cluster Configuration for Log Processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bfcf471",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimal configuration for log ingestion workloads\n",
        "cluster_config = {\n",
        "    \"head_node\": {\n",
        "        \"instance_type\": \"m5.2xlarge\",  # 8 vCPUs, 32GB RAM\n",
        "        \"storage\": \"500GB SSD\"\n",
        "    },\n",
        "    \"worker_nodes\": {\n",
        "        \"instance_type\": \"m5.4xlarge\",  # 16 vCPUs, 64GB RAM\n",
        "        \"min_workers\": 3,\n",
        "        \"max_workers\": 20,\n",
        "        \"storage\": \"1TB SSD per worker\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Ray Data configuration for log processing\n",
        "from ray.data.context import DataContext\n",
        "ctx = DataContext.get_current()\n",
        "ctx.target_max_block_size = 512 * 1024 * 1024  # 512MB blocks for logs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d59d814d",
      "metadata": {},
      "source": [
        "### **Real-Time Log Monitoring**\n",
        "- Set up alerts for security anomalies\n",
        "- Monitor processing throughput and latency\n",
        "- Implement automatic scaling based on log volume\n",
        "- Create operational dashboards for SRE teams\n",
        "\n",
        "## Example Workflows\n",
        "\n",
        "### **Security Operations Center (SOC)**\n",
        "1. Ingest security logs from multiple sources\n",
        "2. Parse and standardize log formats\n",
        "3. Apply threat detection algorithms\n",
        "4. Generate security alerts and reports\n",
        "5. Feed results to SIEM systems\n",
        "\n",
        "### **Application Performance Monitoring**\n",
        "1. Process application and service logs\n",
        "2. Extract performance metrics and errors\n",
        "3. Identify bottlenecks and issues\n",
        "4. Generate performance dashboards\n",
        "5. Alert on SLA violations\n",
        "\n",
        "### **Compliance and Audit**\n",
        "1. Collect audit logs from all systems\n",
        "2. Parse and validate log integrity\n",
        "3. Apply compliance rules and policies\n",
        "4. Generate audit reports and trails\n",
        "5. Ensure regulatory compliance\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "### **Common Issues**\n",
        "1. **Memory Pressure**: Reduce batch size for large log entries\n",
        "2. **Parsing Errors**: Implement robust regex patterns and error handling\n",
        "3. **Performance Issues**: Optimize block size and parallelism\n",
        "4. **Data Skew**: Handle uneven log distribution across time periods\n",
        "\n",
        "### **Debug Mode**\n",
        "Enable detailed logging and performance monitoring:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0bd2a36",
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "# Enable Ray Data debugging\n",
        "from ray.data.context import DataContext\n",
        "ctx = DataContext.get_current()\n",
        "ctx.enable_progress_bars = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4246eff7",
      "metadata": {},
      "source": [
        "## Interactive Log Analytics Visualizations\n",
        "\n",
        "Let's create comprehensive visualizations for log analysis and security monitoring:\n",
        "\n",
        "### Security Operations Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "222c6467",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset from S3",
        "dataset = ray.data.read_parquet(",
        "    \"s3://ray-benchmark-data/catalog/customer_data.parquet\"",
        ")",
        "",
        "print(f\"Loaded dataset: {dataset.count():,} records\")",
        "print(f\"Schema: {dataset.schema()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d74c10ed",
      "metadata": {},
      "source": [
        "### Interactive Log Analytics Dashboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1eeda48",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_interactive_log_dashboard(log_data):\n",
        "    \"\"\"Create interactive log analytics dashboard using Plotly.\"\"\"\n",
        "    print(\"Creating interactive log analytics dashboard...\")\n",
        "    \n",
        "    # Create comprehensive interactive dashboard\n",
        "    fig = make_subplots(\n",
        "        rows=3, cols=2,\n",
        "        subplot_titles=('Log Volume Over Time', 'Error Rate Analysis',\n",
        "                       'Response Time Distribution', 'Service Health Status',\n",
        "                       'Geographic Log Sources', 'Log Level Breakdown'),\n",
        "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": True}],\n",
        "               [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "               [{\"type\": \"scattergeo\"}, {\"type\": \"pie\"}]]\n",
        "    )\n",
        "    \n",
        "    # Simulate log data for demonstration\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # 1. Log volume over time\n",
        "    dates = pd.date_range(start='2024-01-01', periods=168, freq='H')  # 1 week hourly\n",
        "    log_volumes = 1000 + 500 * np.sin(np.linspace(0, 14*np.pi, 168)) + np.random.normal(0, 100, 168)\n",
        "    log_volumes = np.maximum(log_volumes, 0)\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=dates, y=log_volumes,\n",
        "                  mode='lines', name='Log Volume',\n",
        "                  line=dict(color='blue', width=2)),\n",
        "        row=1, col=1\n",
        "    )\n",
        "    \n",
        "    # 2. Error rate analysis with dual axis\n",
        "    error_rates = 2 + np.random.normal(0, 0.5, 168)\n",
        "    error_rates = np.maximum(error_rates, 0)\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=dates, y=error_rates,\n",
        "                  mode='lines', name='Error Rate (%)',\n",
        "                  line=dict(color='red', width=2)),\n",
        "        row=1, col=2, secondary_y=False\n",
        "    )\n",
        "    \n",
        "    # Success rate on secondary y-axis\n",
        "    success_rates = 100 - error_rates\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=dates, y=success_rates,\n",
        "                  mode='lines', name='Success Rate (%)',\n",
        "                  line=dict(color='green', width=2)),\n",
        "        row=1, col=2, secondary_y=True\n",
        "    )\n",
        "    \n",
        "    # 3. Response time distribution\n",
        "    response_times = np.random.lognormal(4, 0.5, 10000)  # Log-normal distribution\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Histogram(x=response_times, nbinsx=50,\n",
        "                    marker_color='orange', name='Response Times'),\n",
        "        row=2, col=1\n",
        "    )\n",
        "    \n",
        "    # 4. Service health status\n",
        "    services = ['Web Server', 'Database', 'API Gateway', 'Cache', 'Queue']\n",
        "    health_scores = [98.5, 99.2, 97.8, 99.8, 98.1]\n",
        "    colors_health = ['green' if score > 99 else 'orange' if score > 95 else 'red' \n",
        "                    for score in health_scores]\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Bar(x=services, y=health_scores,\n",
        "              marker_color=colors_health, name='Health Score'),\n",
        "        row=2, col=2\n",
        "    )\n",
        "    \n",
        "    # 5. Geographic log sources\n",
        "    countries = ['USA', 'Germany', 'Japan', 'Brazil', 'India', 'Australia']\n",
        "    country_codes = ['USA', 'DEU', 'JPN', 'BRA', 'IND', 'AUS']\n",
        "    log_counts = [45000, 28000, 32000, 18000, 25000, 12000]\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Scattergeo(\n",
        "            locations=country_codes,\n",
        "            text=countries,\n",
        "            mode='markers',\n",
        "            marker=dict(\n",
        "                size=[count/1000 for count in log_counts],\n",
        "                color=log_counts,\n",
        "                colorscale='Viridis',\n",
        "                showscale=True,\n",
        "                colorbar=dict(title=\"Log Count\", x=0.45)\n",
        "            ),\n",
        "            name=\"Log Sources\"\n",
        "        ),\n",
        "        row=3, col=1\n",
        "    )\n",
        "    \n",
        "    # 6. Log level breakdown\n",
        "    log_levels = ['INFO', 'WARN', 'ERROR', 'DEBUG', 'FATAL']\n",
        "    level_counts = [60000, 15000, 8000, 25000, 500]\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Pie(labels=log_levels, values=level_counts,\n",
        "              name=\"Log Levels\"),\n",
        "        row=3, col=2\n",
        "    )\n",
        "    \n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        title_text=\"Interactive Log Analytics Dashboard\",\n",
        "        height=1000,\n",
        "        showlegend=True,\n",
        "        template=\"plotly_dark\"  # Dark theme for operations dashboard\n",
        "    )\n",
        "    \n",
        "    # Update axes\n",
        "    fig.update_xaxes(title_text=\"Time\", row=1, col=1)\n",
        "    fig.update_yaxes(title_text=\"Log Count\", row=1, col=1)\n",
        "    fig.update_xaxes(title_text=\"Time\", row=1, col=2)\n",
        "    fig.update_yaxes(title_text=\"Error Rate (%)\", row=1, col=2, secondary_y=False)\n",
        "    fig.update_yaxes(title_text=\"Success Rate (%)\", row=1, col=2, secondary_y=True)\n",
        "    fig.update_xaxes(title_text=\"Response Time (ms)\", row=2, col=1)\n",
        "    fig.update_yaxes(title_text=\"Frequency\", row=2, col=1)\n",
        "    fig.update_xaxes(title_text=\"Service\", row=2, col=2)\n",
        "    fig.update_yaxes(title_text=\"Health Score (%)\", row=2, col=2)\n",
        "    \n",
        "    # Save and show\n",
        "    fig.write_html(\"interactive_log_dashboard.html\")\n",
        "    print(\"Interactive log dashboard saved as 'interactive_log_dashboard.html'\")\n",
        "    fig.show()\n",
        "    \n",
        "    return fig\n",
        "\n",
        "# Create interactive dashboard\n",
        "interactive_dashboard = create_interactive_log_dashboard(None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5bcf815",
      "metadata": {},
      "source": [
        "### Network Security Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c00f27c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_network_security_visualization():\n",
        "    \"\"\"Create network security and traffic analysis visualization.\"\"\"\n",
        "    print(\"Creating network security visualization...\")\n",
        "    \n",
        "    import networkx as nx\n",
        "    \n",
        "    # Create network topology visualization\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle('Network Security Analysis', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. Network topology with threats\n",
        "    ax1 = axes[0, 0]\n",
        "    \n",
        "    # Create network graph\n",
        "    G = nx.Graph()\n",
        "    \n",
        "    # Add nodes (network devices)\n",
        "    devices = {\n",
        "        'Firewall': {'pos': (0, 0), 'color': 'red', 'size': 1000},\n",
        "        'Router': {'pos': (1, 0), 'color': 'blue', 'size': 800},\n",
        "        'Switch': {'pos': (2, 0), 'color': 'green', 'size': 800},\n",
        "        'Server1': {'pos': (1, 1), 'color': 'orange', 'size': 600},\n",
        "        'Server2': {'pos': (1, -1), 'color': 'orange', 'size': 600},\n",
        "        'Workstation': {'pos': (3, 0), 'color': 'purple', 'size': 400},\n",
        "        'Threat': {'pos': (-1, 0), 'color': 'darkred', 'size': 800}\n",
        "    }\n",
        "    \n",
        "    for device, attrs in devices.items():\n",
        "        G.add_node(device, **attrs)\n",
        "    \n",
        "    # Add edges (connections)\n",
        "    connections = [\n",
        "        ('Threat', 'Firewall'),\n",
        "        ('Firewall', 'Router'),\n",
        "        ('Router', 'Switch'),\n",
        "        ('Router', 'Server1'),\n",
        "        ('Router', 'Server2'),\n",
        "        ('Switch', 'Workstation')\n",
        "    ]\n",
        "    G.add_edges_from(connections)\n",
        "    \n",
        "    # Draw network\n",
        "    pos = nx.get_node_attributes(G, 'pos')\n",
        "    colors = [devices[node]['color'] for node in G.nodes()]\n",
        "    sizes = [devices[node]['size'] for node in G.nodes()]\n",
        "    \n",
        "    nx.draw(G, pos, ax=ax1, node_color=colors, node_size=sizes,\n",
        "            with_labels=True, font_size=8, font_weight='bold',\n",
        "            edge_color='gray', arrows=True, arrowsize=20)\n",
        "    \n",
        "    ax1.set_title('Network Topology with Threat Sources', fontweight='bold')\n",
        "    ax1.axis('off')\n",
        "    \n",
        "    # 2. Attack flow analysis\n",
        "    ax2 = axes[0, 1]\n",
        "    \n",
        "    attack_stages = ['Reconnaissance', 'Initial Access', 'Execution', 'Persistence', 'Exfiltration']\n",
        "    attack_counts = [45, 23, 15, 8, 3]\n",
        "    colors_attack = ['yellow', 'orange', 'red', 'darkred', 'black']\n",
        "    \n",
        "    bars = ax2.bar(range(len(attack_stages)), attack_counts, color=colors_attack, alpha=0.7)\n",
        "    ax2.set_xticks(range(len(attack_stages)))\n",
        "    ax2.set_xticklabels(attack_stages, rotation=45, ha='right', fontsize=8)\n",
        "    ax2.set_title('Attack Kill Chain Analysis', fontweight='bold')\n",
        "    ax2.set_ylabel('Number of Events')\n",
        "    \n",
        "    # Add value labels\n",
        "    for bar, value in zip(bars, attack_counts):\n",
        "        height = bar.get_height()\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
        "                f'{value}', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    # 3. IP reputation analysis\n",
        "    ax3 = axes[1, 0]\n",
        "    \n",
        "    # Simulate IP reputation scores\n",
        "    np.random.seed(42)\n",
        "    reputation_scores = np.random.beta(2, 0.5, 1000) * 100  # Skewed towards high scores\n",
        "    \n",
        "    ax3.hist(reputation_scores, bins=30, color='lightblue', alpha=0.7, edgecolor='black')\n",
        "    ax3.axvline(50, color='red', linestyle='--', linewidth=2, label='Suspicious Threshold')\n",
        "    ax3.set_title('IP Reputation Score Distribution', fontweight='bold')\n",
        "    ax3.set_xlabel('Reputation Score')\n",
        "    ax3.set_ylabel('Number of IPs')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Port scan detection\n",
        "    ax4 = axes[1, 1]\n",
        "    \n",
        "    ports = ['22', '23', '25', '53', '80', '135', '443', '993', '995']\n",
        "    scan_attempts = [156, 89, 67, 134, 245, 78, 189, 45, 23]\n",
        "    \n",
        "    bars = ax4.bar(ports, scan_attempts, color='red', alpha=0.7)\n",
        "    ax4.set_title('Port Scan Attempts by Port', fontweight='bold')\n",
        "    ax4.set_xlabel('Port Number')\n",
        "    ax4.set_ylabel('Scan Attempts')\n",
        "    ax4.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Add value labels for high-risk ports\n",
        "    for bar, value, port in zip(bars, scan_attempts, ports):\n",
        "        if value > 100:\n",
        "            height = bar.get_height()\n",
        "            ax4.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
        "                    f'{value}', ha='center', va='bottom', fontweight='bold', color='red')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('network_security_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"Network security visualization saved as 'network_security_analysis.png'\")\n",
        "\n",
        "# Create network security visualization\n",
        "create_network_security_visualization()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d953568b",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "1. **Scale to Production**: Deploy to multi-node clusters for TB+ daily logs\n",
        "2. **Add Real-Time Processing**: Integrate with log streaming systems\n",
        "3. **Enhance Security**: Implement advanced threat detection algorithms\n",
        "4. **Build Dashboards**: Create operational monitoring interfaces\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "### Common Issues\n",
        "\n",
        "1. **Log Parsing Errors**: Ensure log formats are consistent and handle malformed entries\n",
        "2. **Memory Issues**: Process large log files in streaming mode with smaller batch sizes\n",
        "3. **Performance Bottlenecks**: Optimize regex patterns and use compiled expressions\n",
        "4. **Security Alert Accuracy**: Tune anomaly detection thresholds to reduce false positives\n",
        "\n",
        "### Debug Mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "457090e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "from ray.data.context import DataContext\n",
        "ctx = DataContext.get_current()\n",
        "ctx.enable_progress_bars = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3336755",
      "metadata": {},
      "source": [
        "## Performance Benchmarks\n",
        "\n",
        "**Log Processing Performance:**\n",
        "- **Log ingestion**: 1M+ log entries/second\n",
        "- **Pattern matching**: 500K+ regex operations/second\n",
        "- **Anomaly detection**: 100K+ security events analyzed/second\n",
        "- **Alert generation**: Real-time threat detection with sub-second latency\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "- **Ray Data scales security monitoring**: Process enterprise log volumes that exceed single-machine capabilities\n",
        "- **Real-time threat detection requires distributed processing**: Modern attack volumes demand parallel analysis\n",
        "- **Pattern optimization provides major performance gains**: Efficient regex and parsing dramatically improve throughput\n",
        "- **Production security requires comprehensive monitoring**: Automated alerting and escalation prevent security incidents\n",
        "\n",
        "## Action Items\n",
        "\n",
        "### Immediate Goals (Next 2 weeks)\n",
        "1. **Implement log processing pipeline** for your specific security monitoring needs\n",
        "2. **Add anomaly detection** to identify suspicious patterns and potential threats\n",
        "3. **Set up automated alerting** for critical security events\n",
        "4. **Create log quality validation** to ensure parsing accuracy\n",
        "\n",
        "### Long-term Goals (Next 3 months)\n",
        "1. **Deploy production SIEM** with real-time threat detection\n",
        "2. **Implement advanced security analytics** like behavioral analysis and threat hunting\n",
        "3. **Build compliance reporting** for regulatory requirements\n",
        "4. **Create security dashboards** for SOC team monitoring\n",
        "\n",
        "## Resources\n",
        "\n",
        "- [Ray Data Documentation](https://docs.ray.io/en/latest/data/index.html)\n",
        "- [Log Processing Best Practices](https://docs.ray.io/en/latest/data/best-practices.html)\n",
        "- [Ray Data Performance Guide](https://docs.ray.io/en/latest/data/performance-tips.html)\n",
        "- [Security Log Analysis Patterns](https://docs.ray.io/en/latest/data/batch_inference.html)\n",
        "\n",
        "## Advanced Log Processing Features\n",
        "\n",
        "### **Ray Data's Log Processing Superpowers**\n",
        "\n",
        "**1. Massive Scale Text Processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b78c9b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process terabytes of logs across distributed cluster\n",
        "massive_logs.map_batches(\n",
        "    LogParser(),\n",
        "    batch_size=5000,     # Optimal for text processing\n",
        "    concurrency=16       # Parallel across all CPU cores\n",
        ")\n",
        "# Ray Data automatically handles memory management and load balancing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b488291",
      "metadata": {},
      "source": [
        "**2. Multi-Format Log Ingestion**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a6b9df0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handle different log formats in single pipeline\n",
        "web_logs = read_text(\"s3://logs/apache/\")      # Apache format\n",
        "app_logs = read_json(\"s3://logs/application/\") # JSON format\n",
        "sys_logs = read_parquet(\"s3://logs/system/\")   # Structured format\n",
        "\n",
        "# Unified processing across formats\n",
        "all_logs = web_logs.union(app_logs).union(sys_logs)\n",
        "processed = all_logs.map_batches(UnifiedLogProcessor())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faee984c",
      "metadata": {},
      "source": [
        "**3. Real-Time Security Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb2805d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Security threat detection at scale\n",
        "security_pipeline = (logs\n",
        "    .filter(lambda x: x['log_type'] == 'security')      # Native filtering\n",
        "    .map_batches(ThreatDetector(), batch_size=1000)     # Parallel analysis\n",
        "    .filter(lambda x: x['threat_level'] == 'high')     # Alert filtering\n",
        "    .groupby('source_ip').agg({'threat_score': 'max'}) # Threat aggregation\n",
        ")\n",
        "# Built-in fault tolerance ensures no security events are lost"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a569ebdb",
      "metadata": {},
      "source": [
        "**4. Operational Intelligence Extraction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ac51122",
      "metadata": {},
      "outputs": [],
      "source": [
        "# SLA monitoring and performance analysis\n",
        "operational_pipeline = (logs\n",
        "    .filter(lambda x: x['log_type'] == 'performance')\n",
        "    .map_batches(SLAMonitor(), batch_size=2000)\n",
        "    .groupby('service_name').agg({\n",
        "        'error_rate': 'mean',\n",
        "        'response_time': 'mean',\n",
        "        'availability': 'mean'\n",
        "    })\n",
        ")\n",
        "# Automatic scaling handles traffic spikes without manual intervention"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "853e53a2",
      "metadata": {},
      "source": [
        "### **Enterprise Log Analytics Patterns**\n",
        "\n",
        "**Security Operations Excellence**\n",
        "- **Threat Detection**: Process 1M+ security events daily\n",
        "- **Incident Response**: 15-minute mean time to detection  \n",
        "- **Compliance**: 100% audit trail coverage\n",
        "- **Cost Efficiency**: reduction vs traditional SIEM tools\n",
        "\n",
        "**Site Reliability Engineering**\n",
        "- **Service Monitoring**: 500+ microservices tracked continuously\n",
        "- **Performance Optimization**: Automated SLA violation detection\n",
        "- **Capacity Planning**: Predictive scaling recommendations\n",
        "- **Error Tracking**: Root cause analysis and trend identification\n",
        "\n",
        "**Business Intelligence from Logs**\n",
        "- **Customer Behavior**: User journey analysis from clickstream logs\n",
        "- **Product Analytics**: Feature usage and adoption tracking\n",
        "- **Conversion Optimization**: Funnel analysis and improvement recommendations\n",
        "- **Revenue Impact**: Business metric correlation with operational events\n",
        "\n",
        "## Cleanup and Resource Management\n",
        "\n",
        "Always clean up Ray resources when done:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "331b2ce7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up Ray resources\n",
        "ray.shutdown()\n",
        "print(\"Ray cluster shutdown complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b193fb5",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "*This template demonstrates Ray Data's superior capabilities for enterprise log processing. Ray Data's native operations provide unmatched scale, performance, and reliability for mission-critical log analytics workloads.*"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}