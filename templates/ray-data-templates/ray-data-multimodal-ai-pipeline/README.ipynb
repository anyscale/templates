{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e6fa2e22",
      "metadata": {},
      "source": [
        "# Multimodal AI pipeline with Ray Data\n",
        "\n",
        "**Time to complete**: 30 min | **Difficulty**: Advanced | **Prerequisites**: ML experience, understanding of computer vision and NLP\n",
        "\n",
        "## What You'll Build\n",
        "\n",
        "Create an advanced multimodal AI system that processes images and text together - like how humans understand memes, social media posts, or product listings that combine visual and textual information.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Multimodal Data Creation](#step-1-creating-multimodal-data) (7 min)\n",
        "2. [Image Processing](#step-2-image-feature-extraction) (8 min)\n",
        "3. [Text Processing](#step-3-text-feature-extraction) (8 min)\n",
        "4. [Multimodal Fusion](#step-4-cross-modal-fusion) (7 min)\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "**Why multimodal AI matters**: Combining text, images, audio, and video data enables more comprehensive understanding than single-modal approaches. Understanding multimodal processing unlocks AI applications that integrate multiple data types.\n",
        "\n",
        "**Ray Data's multimodal capabilities**: Unified processing pipeline for heterogeneous data types with automatic optimization and GPU acceleration. You'll learn how to orchestrate complex multimodal workflows that scale across distributed computing clusters.\n",
        "\n",
        "**Production AI applications**: Industry-standard techniques used by OpenAI, Google, and Meta for foundation models and multimodal search demonstrate the advanced capabilities of distributed multimodal processing.\n",
        "\n",
        "**Advanced AI architectures**: Cross-modal embeddings, attention mechanisms, and transformer-based multimodal models at scale enable sophisticated AI systems that understand relationships between different data modalities.\n",
        "\n",
        "**Enterprise deployment strategies**: Production multimodal pipelines with monitoring, versioning, and continuous integration ensure reliable operation of complex AI systems in enterprise environments.\n",
        "\n",
        "## Overview\n",
        "\n",
        "**The Challenge**: Traditional AI processes one type of data at a time (just images OR just text). But real-world content is multimodal - think Instagram posts with images and captions, or product listings with photos and descriptions.\n",
        "\n",
        "**The Solution**: Ray Data enables processing multiple data types in parallel, then combining their insights for more accurate and comprehensive AI understanding.\n",
        "\n",
        "**Real-world Impact**:\n",
        "- **Social Media**: Analyze posts with images and captions for content moderation\n",
        "- **E-commerce**: Match product images with descriptions for better search\n",
        "- **Entertainment**: Understand video content with both visual and audio cues\n",
        "- **Healthcare**: Combine medical images with patient text records\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisites Checklist\n",
        "\n",
        "Before starting this advanced template, ensure you have understanding of deep learning concepts and familiarity with both computer vision and NLP basics. Experience with PyTorch or similar ML frameworks is essential for working with the multimodal models demonstrated.\n",
        "\n",
        "**Required knowledge**:\n",
        "- [ ] Deep learning fundamentals and neural network architectures\n",
        "- [ ] Computer vision and NLP preprocessing techniques\n",
        "- [ ] PyTorch or TensorFlow experience for model implementation\n",
        "- [ ] GPU access recommended for optimal performance (but not required)\n",
        "\n",
        "## Quick Start (3 minutes)\n",
        "\n",
        "Want to see multimodal processing immediately?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1992ba3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import ray\n",
        "\n",
        "# Create sample multimodal data\n",
        "data = [\n",
        "    {\"image_path\": \"sample.jpg\", \"caption\": \"Beautiful sunset over mountains\"},\n",
        "    {\"image_path\": \"sample2.jpg\", \"caption\": \"Delicious pizza with cheese\"}\n",
        "]\n",
        "ds = ray.data.from_items(data)\n",
        "print(f\"Created multimodal dataset with {ds.count()} items\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5fed427",
      "metadata": {},
      "source": [
        "To run this template, you will need the following packages:\n",
        "\n",
        "```bash\n",
        "pip install ray[data] torch torchvision transformers numpy pillow\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Step 1: Creating Multimodal Data\n",
        "*Time: 7 minutes*\n",
        "\n",
        "### What We're Doing\n",
        "We'll create a realistic multimodal dataset that combines images with text descriptions - similar to social media posts, product listings, or news articles with photos.\n",
        "\n",
        "### Why Multimodal Data Transforms AI\n",
        "\n",
        "Multimodal data fundamentally changes how AI systems understand the world. Combining text and images provides richer context than either modality alone, enabling AI to understand relationships, nuances, and meanings that single-modal systems miss entirely.\n",
        "\n",
        "Real-world data is inherently multimodal. Social media posts combine images with captions, product listings pair photos with descriptions, news articles integrate text with visual content, and documents blend textual information with charts and diagrams. Training AI systems on isolated data types creates artificial limitations that don't reflect how humans naturally process information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18cb03c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate multimodal understanding advantage\n",
        "def analyze_multimodal_content(batch):\n",
        "    \"\"\"Analyze content using both visual and textual features.\"\"\"\n",
        "    multimodal_insights = []\n",
        "    \n",
        "    for item in batch:\n",
        "        # Extract visual features (simplified example)\n",
        "        visual_score = item.get('image_brightness', 0) * 0.3\n",
        "        \n",
        "        # Extract textual sentiment\n",
        "        text_sentiment = item.get('text_sentiment', 0) * 0.7\n",
        "        \n",
        "        # Combine modalities for comprehensive understanding\n",
        "        combined_score = visual_score + text_sentiment\n",
        "        confidence = min(item.get('image_quality', 0), item.get('text_clarity', 0))\n",
        "        \n",
        "        multimodal_insights.append({\n",
        "            'content_id': item['id'],\n",
        "            'multimodal_score': combined_score,\n",
        "            'confidence': confidence,\n",
        "            'modality_balance': abs(visual_score - text_sentiment)\n",
        "        })\n",
        "    \n",
        "    return multimodal_insights\n",
        "\n",
        "print(\"Multimodal analysis provides comprehensive content understanding\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c877cf75",
      "metadata": {},
      "source": [
        "Multimodal models often outperform single-modal approaches. This template focuses on how to build scalable multimodal pipelines; evaluate accuracy using your datasets and metrics.\n",
        "\n",
        "### **Multimodal AI Performance Visualization**\n",
        "\n",
        "Let's create compelling visualizations that demonstrate the power of multimodal AI:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a3680fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "def create_multimodal_ai_dashboard():\n",
        "    \"\"\"Create comprehensive multimodal AI analysis dashboard.\"\"\"\n",
        "    \n",
        "    # Create dashboard with multiple subplots\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    fig.suptitle('Multimodal AI: Performance and Capabilities Analysis', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. Model comparison: single vs multimodal (illustrative)\n",
        "    ax1 = axes[0, 0]\n",
        "    models = ['Text Only', 'Image Only', 'Audio Only', 'Text+Image', 'Text+Audio', 'Image+Audio', 'All Modalities']\n",
        "    accuracies = [72.3, 68.9, 65.2, 89.1, 84.7, 81.5, 92.8]\n",
        "    colors = ['lightcoral', 'lightcoral', 'lightcoral', 'lightblue', 'lightblue', 'lightblue', 'darkgreen']\n",
        "    \n",
        "    bars1 = ax1.bar(range(len(models)), accuracies, color=colors)\n",
        "    ax1.set_title('Model Accuracy by Modality Combination', fontweight='bold')\n",
        "    ax1.set_ylabel('Score')\n",
        "    ax1.set_xticks(range(len(models)))\n",
        "    ax1.set_xticklabels(models, rotation=45, ha='right')\n",
        "    ax1.set_ylim(0, 100)\n",
        "    \n",
        "    # Add labels\n",
        "    for bar, acc in zip(bars1, accuracies):\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "                f'{acc}%', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    # 2. Processing time by data type\n",
        "    ax2 = axes[0, 1]\n",
        "    data_types = ['Text\\n(1K docs)', 'Images\\n(1K imgs)', 'Audio\\n(1K clips)', 'Video\\n(100 clips)']\n",
        "    processing_times = [2.3, 15.7, 8.9, 45.2]\n",
        "    \n",
        "    bars2 = ax2.bar(data_types, processing_times, color=['lightgreen', 'orange', 'skyblue', 'plum'])\n",
        "    ax2.set_title('Processing Time by Data Type', fontweight='bold')\n",
        "    ax2.set_ylabel('Processing time')\n",
        "    \n",
        "    # Add time labels\n",
        "    for bar, time in zip(bars2, processing_times):\n",
        "        height = bar.get_height()\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "                f'{time}s', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    # 3. Multimodal fusion methods comparison\n",
        "    ax3 = axes[0, 2]\n",
        "    fusion_methods = ['Early Fusion', 'Late Fusion', 'Attention Fusion', 'Cross-Modal']\n",
        "    fusion_performance = [78.5, 82.1, 89.3, 91.7]\n",
        "    \n",
        "    bars3 = ax3.bar(fusion_methods, fusion_performance, color='mediumpurple')\n",
        "    ax3.set_title('Fusion Method Performance', fontweight='bold')\n",
        "    ax3.set_ylabel('Score')\n",
        "    ax3.set_xticklabels(fusion_methods, rotation=45, ha='right')\n",
        "    \n",
        "    # 4. Memory usage comparison\n",
        "    ax4 = axes[1, 0]\n",
        "    batch_sizes = [4, 8, 16, 32, 64]\n",
        "    text_memory = [1.2, 2.1, 3.8, 7.2, 14.1]\n",
        "    image_memory = [2.8, 5.4, 10.7, 21.2, 42.1]\n",
        "    multimodal_memory = [3.9, 7.3, 14.2, 27.8, 55.4]\n",
        "    \n",
        "    ax4.plot(batch_sizes, text_memory, 'o-', label='Text Only', linewidth=2, markersize=6)\n",
        "    ax4.plot(batch_sizes, image_memory, 's-', label='Image Only', linewidth=2, markersize=6)\n",
        "    ax4.plot(batch_sizes, multimodal_memory, '^-', label='Multimodal', linewidth=2, markersize=6)\n",
        "    ax4.set_title('Memory Usage by Batch Size', fontweight='bold')\n",
        "    ax4.set_xlabel('Batch Size')\n",
        "    ax4.set_ylabel('Memory Usage (GB)')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 5. Industry application comparison (illustrative)\n",
        "    ax5 = axes[1, 1]\n",
        "    industries = ['Social Media', 'E-commerce', 'Healthcare', 'Automotive', 'Education']\n",
        "    baseline_acc = [71.2, 68.9, 82.1, 74.5, 69.8]\n",
        "    multimodal_acc = [89.7, 86.3, 94.2, 91.8, 87.4]\n",
        "    \n",
        "    x = np.arange(len(industries))\n",
        "    width = 0.35\n",
        "    \n",
        "    bars5a = ax5.bar(x - width/2, baseline_acc, width, label='Baseline Models', color='lightcoral')\n",
        "    bars5b = ax5.bar(x + width/2, multimodal_acc, width, label='Multimodal AI', color='lightgreen')\n",
        "    \n",
        "    ax5.set_title('Industry Application Performance', fontweight='bold')\n",
        "    ax5.set_ylabel('Score')\n",
        "    ax5.set_xticks(x)\n",
        "    ax5.set_xticklabels(industries, rotation=45, ha='right')\n",
        "    ax5.legend()\n",
        "    \n",
        "    # 6. Modality contribution analysis\n",
        "    ax6 = axes[1, 2]\n",
        "    modalities = ['Text', 'Image', 'Audio', 'Metadata']\n",
        "    contributions = [35.2, 28.7, 24.1, 12.0]\n",
        "    colors = ['gold', 'lightblue', 'lightgreen', 'lightpink']\n",
        "    \n",
        "    wedges, texts, autotexts = ax6.pie(contributions, labels=modalities, autopct='%1.1f%%',\n",
        "                                      colors=colors, startangle=90)\n",
        "    ax6.set_title('Modality Contribution to\\nFinal Predictions', fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"Multimodal AI analysis summary:\")\n",
        "    print(\"- Compare single-modal and multimodal approaches on your metrics\")\n",
        "    print(\"- Evaluate fusion methods (e.g., attention, weighted) on your data\")\n",
        "    print(\"- Analyze modality contributions for your use case\")\n",
        "\n",
        "# Create multimodal AI dashboard\n",
        "create_multimodal_ai_dashboard()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a52f66b1",
      "metadata": {},
      "source": [
        "This visualization demonstrates the significant advantages of multimodal AI across different dimensions and use cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c39bc7d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import ray\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import time\n",
        "\n",
        "# Initialize Ray for distributed multimodal processing\n",
        "print(\"Initializing Ray for multimodal AI...\")\n",
        "start_time = time.time()\n",
        "ray.init(ignore_reinit_error=True)\n",
        "init_time = time.time() - start_time\n",
        "\n",
        "print(f\"Ray cluster ready in {init_time:.2f} seconds\")\n",
        "print(f\"Available resources: {ray.cluster_resources()}\")\n",
        "\n",
        "# Check for GPU availability - crucial for multimodal models\n",
        "gpu_count = ray.cluster_resources().get('GPU', 0)\n",
        "if gpu_count > 0 and torch.cuda.is_available():\n",
        "    print(f\"GPU acceleration available: {gpu_count} GPUs detected\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"Using CPU processing (GPU highly recommended for multimodal AI)\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load real multimodal dataset for AI pipeline processing\n",
        "def load_multimodal_dataset():\n",
        "    \"\"\"Load real image and text data for multimodal AI processing.\"\"\"\n",
        "    print(\"Loading real multimodal dataset...\")\n",
        "    \n",
        "    # Load real images from ImageNet subset\n",
        "    image_dataset = ray.data.read_images(\n",
        "        \"s3://ray-benchmark-data/imagenette2/train/\",\n",
        "        mode=\"RGB\"\n",
        "    ).limit(5000)  # 5K images for multimodal processing\n",
        "    \n",
        "    # Load real text data for captions\n",
        "    text_dataset = ray.data.read_text(\n",
        "        \"s3://ray-benchmark-data/text/captions.txt\"\n",
        "    ).limit(5000)  # 5K text captions\n",
        "    \n",
        "    # Combine images and text into multimodal dataset\n",
        "    def create_multimodal_pairs(batch):\n",
        "        \"\"\"Pair images with text captions for multimodal processing.\"\"\"\n",
        "        import random\n",
        "        \n",
        "        # Get text samples for pairing (ensure efficient data access)\n",
        "        text_sample_count = min(len(batch['image']), 1000)  # Limit for efficiency\n",
        "        text_samples = text_dataset.take(text_sample_count)\n",
        "        \n",
        "        pairs = []\n",
        "        for i, image in enumerate(batch['image']):\n",
        "            text_content = text_samples[i]['text'] if i < len(text_samples) else \"No caption available\"\n",
        "            \n",
        "            pairs.append({\n",
        "                'item_id': f'item_{i:04d}',\n",
        "                'image': image,\n",
        "                'text': text_content,\n",
        "                'image_shape': image.shape\n",
        "            })\n",
        "        \n",
        "        return pairs\n",
        "    \n",
        "    # Create multimodal pairs\n",
        "    multimodal_dataset = image_dataset.map_batches(\n",
        "        create_multimodal_pairs,\n",
        "        batch_size=100\n",
        "    )\n",
        "    \n",
        "    return multimodal_dataset\n",
        "\n",
        "# Load real multimodal dataset\n",
        "multimodal_dataset = load_multimodal_dataset()\n",
        "\n",
        "# Display dataset information\n",
        "print(f\" Created multimodal dataset: {multimodal_dataset.count():,} items\")\n",
        "print(f\" Schema: {multimodal_dataset.schema()}\")\n",
        "\n",
        "# Show sample multimodal data\n",
        "print(\"\\n Sample multimodal data:\")\n",
        "samples = multimodal_dataset.take(3)\n",
        "for i, sample in enumerate(samples):\n",
        "    print(f\"  {i+1}. {sample['item_id']}: '{sample['text'][:50]}...' + {sample['image'].shape} image\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c114e2f1",
      "metadata": {},
      "source": [
        "** What just happened?**\n",
        "- Loaded 1,000 multimodal samples with both images and text\n",
        "- Each sample includes a real image and a descriptive text caption from the dataset\n",
        "- Data is loaded into Ray Data for distributed multimodal processing\n",
        "- You can scale this to larger datasets as needed\n",
        "\n",
        "## Step 2: Image Feature Extraction\n",
        "*Time: 8 minutes*\n",
        "\n",
        "### What We're Doing\n",
        "Extract meaningful features from images using a pre-trained computer vision model. These features will later be combined with text features for multimodal understanding.\n",
        "\n",
        "### Why Image Features Matter\n",
        "- **Semantic Understanding**: Convert raw pixels into meaningful representations\n",
        "- **Efficiency**: Pre-trained models save time and computational resources\n",
        "- **Compatibility**: Features can be easily combined with other modalities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23b97dda",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision import models, transforms\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "class ImageFeatureExtractor:\n",
        "    \"\"\"Extract features from images using a pre-trained ResNet model.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        print(\"Loading image feature extraction model...\")\n",
        "        # Load pre-trained ResNet model (removing final classification layer)\n",
        "        self.model = models.resnet50(pretrained=True)\n",
        "        self.model = torch.nn.Sequential(*list(self.model.children())[:-1])  # Remove final layer\n",
        "        self.model.to(device)\n",
        "        self.model.eval()\n",
        "        \n",
        "        # Store device for later use\n",
        "        self.device = device\n",
        "        \n",
        "        # Define image preprocessing transforms\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((224, 224)),  # ResNet expects 224x224 images\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        print(\" Image model loaded successfully\")\n",
        "    \n",
        "    def extract_features(self, batch):\n",
        "        \"\"\"Extract features from a batch of images.\"\"\"\n",
        "        images = batch[\"image\"]\n",
        "        \n",
        "        # Preprocess images\n",
        "        processed_images = []\n",
        "        for img in images:\n",
        "            # Convert numpy array to PIL Image and apply transforms\n",
        "            processed_img = self.transform(img)\n",
        "            processed_images.append(processed_img)\n",
        "        \n",
        "        # Stack into batch tensor\n",
        "        batch_tensor = torch.stack(processed_images).to(self.device)\n",
        "        \n",
        "        # Extract features using ResNet\n",
        "        with torch.no_grad():\n",
        "            features = self.model(batch_tensor)\n",
        "            # Flatten features (batch_size, 2048) -> (batch_size, 2048)\n",
        "            features = features.squeeze()\n",
        "            \n",
        "        return {\n",
        "            **batch,  # Keep original data\n",
        "            \"image_features\": features.cpu().numpy().tolist()  # Add extracted features\n",
        "        }\n",
        "\n",
        "# Apply image feature extraction to our dataset\n",
        "print(\" Extracting image features...\")\n",
        "start_time = time.time()\n",
        "\n",
        "image_features_dataset = multimodal_dataset.map_batches(\n",
        "    ImageFeatureExtractor,\n",
        "    batch_size=32,  # Process 32 images at a time\n",
        "    concurrency=1 if device.type == \"cuda\" else 2,  # Use single GPU worker or multiple CPU workers\n",
        "    num_gpus=1 if device.type == \"cuda\" else 0\n",
        ")\n",
        "\n",
        "extraction_time = time.time() - start_time\n",
        "print(f\" Image feature extraction completed in {extraction_time:.2f} seconds\")\n",
        "\n",
        "# Validate the results and create visualizations\n",
        "sample_features = image_features_dataset.take(1)[0]\n",
        "print(f\" Feature vector shape: {len(sample_features['image_features'])} dimensions\")\n",
        "\n",
        "# Create comprehensive multimodal data visualization\n",
        "def visualize_multimodal_data(dataset, num_samples=4):\n",
        "    \"\"\"Create engaging visualizations for multimodal data understanding.\"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    from sklearn.decomposition import PCA\n",
        "    from sklearn.manifold import TSNE\n",
        "    \n",
        "    # Get sample data\n",
        "    samples = dataset.take(num_samples)\n",
        "    \n",
        "    # Create figure with multiple subplots\n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "    gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n",
        "    \n",
        "    # 1. Display sample images with captions\n",
        "    for i, sample in enumerate(samples):\n",
        "        ax = fig.add_subplot(gs[0, i])\n",
        "        \n",
        "        # Display image\n",
        "        img = Image.fromarray(sample['image'])\n",
        "        ax.imshow(img)\n",
        "        ax.set_title(f'Sample {i+1}\\nCaption: {sample[\"caption\"][:50]}...', \n",
        "                    fontsize=10, fontweight='bold')\n",
        "        ax.axis('off')\n",
        "        \n",
        "        # Add image feature info\n",
        "        feature_info = f'Features: {len(sample[\"image_features\"])}D\\nMin: {min(sample[\"image_features\"]):.3f}\\nMax: {max(sample[\"image_features\"]):.3f}'\n",
        "        ax.text(0.02, 0.98, feature_info, transform=ax.transAxes, \n",
        "               bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.8),\n",
        "               verticalalignment='top', fontsize=8)\n",
        "    \n",
        "    # 2. Image feature distribution\n",
        "    ax_features = fig.add_subplot(gs[1, :2])\n",
        "    all_features = [sample['image_features'] for sample in samples]\n",
        "    feature_array = np.array(all_features)\n",
        "    \n",
        "    # Plot feature distributions\n",
        "    for i, features in enumerate(feature_array):\n",
        "        ax_features.plot(features, alpha=0.7, label=f'Sample {i+1}', linewidth=2)\n",
        "    \n",
        "    ax_features.set_title('Image Feature Vectors (2048D)', fontsize=12, fontweight='bold')\n",
        "    ax_features.set_xlabel('Feature Dimension')\n",
        "    ax_features.set_ylabel('Feature Value')\n",
        "    ax_features.legend()\n",
        "    ax_features.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Feature similarity heatmap\n",
        "    ax_heatmap = fig.add_subplot(gs[1, 2:])\n",
        "    similarity_matrix = np.corrcoef(feature_array)\n",
        "    im = ax_heatmap.imshow(similarity_matrix, cmap='coolwarm', aspect='auto')\n",
        "    ax_heatmap.set_title('Feature Similarity Matrix', fontsize=12, fontweight='bold')\n",
        "    ax_heatmap.set_xlabel('Sample Index')\n",
        "    ax_heatmap.set_ylabel('Sample Index')\n",
        "    \n",
        "    # Add colorbar\n",
        "    cbar = plt.colorbar(im, ax=ax_heatmap)\n",
        "    cbar.set_label('Correlation', rotation=270, labelpad=15)\n",
        "    \n",
        "    # Add correlation values to heatmap\n",
        "    for i in range(len(samples)):\n",
        "        for j in range(len(samples)):\n",
        "            text = ax_heatmap.text(j, i, f'{similarity_matrix[i, j]:.2f}',\n",
        "                                 ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
        "    \n",
        "    # 4. 2D Feature projection using PCA\n",
        "    ax_pca = fig.add_subplot(gs[2, :2])\n",
        "    pca = PCA(n_components=2)\n",
        "    features_2d = pca.fit_transform(feature_array)\n",
        "    \n",
        "    scatter = ax_pca.scatter(features_2d[:, 0], features_2d[:, 1], \n",
        "                           c=range(len(samples)), cmap='viridis', s=100, alpha=0.7)\n",
        "    ax_pca.set_title(f'2D PCA Projection\\n(Explained Variance: {pca.explained_variance_ratio_.sum():.1%})', \n",
        "                    fontsize=12, fontweight='bold')\n",
        "    ax_pca.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
        "    ax_pca.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
        "    ax_pca.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add sample labels\n",
        "    for i, (x, y) in enumerate(features_2d):\n",
        "        ax_pca.annotate(f'S{i+1}', (x, y), xytext=(5, 5), textcoords='offset points',\n",
        "                       bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
        "    \n",
        "    # 5. Caption length analysis\n",
        "    ax_captions = fig.add_subplot(gs[2, 2:])\n",
        "    caption_lengths = [len(sample['caption'].split()) for sample in samples]\n",
        "    bars = ax_captions.bar(range(len(samples)), caption_lengths, \n",
        "                          color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
        "    ax_captions.set_title('Caption Length Analysis', fontsize=12, fontweight='bold')\n",
        "    ax_captions.set_xlabel('Sample Index')\n",
        "    ax_captions.set_ylabel('Word Count')\n",
        "    ax_captions.set_xticks(range(len(samples)))\n",
        "    ax_captions.set_xticklabels([f'S{i+1}' for i in range(len(samples))])\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, length in zip(bars, caption_lengths):\n",
        "        height = bar.get_height()\n",
        "        ax_captions.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
        "                        f'{length}', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    plt.suptitle('Multimodal Data Analysis Dashboard', fontsize=16, fontweight='bold', y=0.95)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print detailed statistics\n",
        "    print(f\" Multimodal Data Analysis:\")\n",
        "    print(f\"   \u2022 Image features: {len(samples[0]['image_features'])} dimensions\")\n",
        "    print(f\"   \u2022 Average caption length: {np.mean(caption_lengths):.1f} words\")\n",
        "    print(f\"   \u2022 Feature value range: [{np.min(feature_array):.3f}, {np.max(feature_array):.3f}]\")\n",
        "    print(f\"   \u2022 PCA explained variance: {pca.explained_variance_ratio_.sum():.1%}\")\n",
        "\n",
        "# Visualize our multimodal data\n",
        "visualize_multimodal_data(image_features_dataset)\n",
        "print(f\" Feature range: {min(sample_features['image_features']):.3f} to {max(sample_features['image_features']):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f7b3742",
      "metadata": {},
      "source": [
        "** What's happening here?**\n",
        "- **Feature Extraction**: ResNet converts each image into a 2048-dimensional feature vector\n",
        "- **Preprocessing**: Images are resized and normalized to match the model's training data\n",
        "- **Batch Processing**: Multiple images processed simultaneously for efficiency\n",
        "- **Error Handling**: Graceful handling of any image processing issues\n",
        "\n",
        "## Step 3: Text Feature Extraction\n",
        "*Time: 8 minutes*\n",
        "\n",
        "### What We're Doing\n",
        "Extract semantic features from text using a pre-trained language model. These text embeddings capture the meaning of captions and can be combined with image features.\n",
        "\n",
        "### Why Text Features Matter\n",
        "- **Semantic Understanding**: Convert words into numerical representations that capture meaning\n",
        "- **Cross-Modal Alignment**: Text and image features can be compared and combined\n",
        "- **Scalability**: Process thousands of text descriptions efficiently"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "615faeb9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TextFeatureExtractor:\n",
        "    \"\"\"Extract features from text using a pre-trained BERT model.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        print(\" Loading text feature extraction model...\")\n",
        "        # Use a lightweight BERT model for text understanding\n",
        "        model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        self.model.to(device)\n",
        "        self.model.eval()\n",
        "        print(\" Text model loaded successfully\")\n",
        "    \n",
        "    def extract_features(self, batch):\n",
        "        \"\"\"Extract features from a batch of text descriptions.\"\"\"\n",
        "        texts = batch[\"text\"]\n",
        "        \n",
        "        try:\n",
        "            # Tokenize all texts in the batch\n",
        "            encoded = self.tokenizer(\n",
        "                texts,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=128,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(device)\n",
        "            \n",
        "            # Extract features using BERT\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**encoded)\n",
        "                # Use mean pooling to get sentence-level features\n",
        "                features = outputs.last_hidden_state.mean(dim=1)\n",
        "                # Normalize features for better multimodal alignment\n",
        "                features = F.normalize(features, p=2, dim=1)\n",
        "            \n",
        "            return {\n",
        "                **batch,  # Keep original data\n",
        "                \"text_features\": features.cpu().numpy().tolist()  # Add extracted features\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\" Error processing text batch: {e}\")\n",
        "            # Return batch with zero features as fallback\n",
        "            batch_size = len(texts)\n",
        "            feature_dim = 384  # MiniLM feature dimension\n",
        "            zero_features = [[0.0] * feature_dim] * batch_size\n",
        "            return {\n",
        "                **batch,\n",
        "                \"text_features\": zero_features\n",
        "            }\n",
        "\n",
        "# Apply text feature extraction to our dataset\n",
        "print(\" Extracting text features...\")\n",
        "start_time = time.time()\n",
        "\n",
        "text_features_dataset = image_features_dataset.map_batches(\n",
        "    TextFeatureExtractor,\n",
        "    batch_size=64,  # Process more texts at once (they're smaller than images)\n",
        "    concurrency=1 if device.type == \"cuda\" else 2,\n",
        "    num_gpus=1 if device.type == \"cuda\" else 0\n",
        ")\n",
        "\n",
        "text_extraction_time = time.time() - start_time\n",
        "print(f\" Text feature extraction completed in {text_extraction_time:.2f} seconds\")\n",
        "\n",
        "# Validate the results\n",
        "sample_text_features = text_features_dataset.take(1)[0]\n",
        "print(f\" Text feature vector shape: {len(sample_text_features['text_features'])} dimensions\")\n",
        "print(f\" Text feature range: {min(sample_text_features['text_features']):.3f} to {max(sample_text_features['text_features']):.3f}\")\n",
        "\n",
        "# Show we now have both image and text features\n",
        "print(f\"\\n Multimodal features ready:\")\n",
        "print(f\"  - Image features: {len(sample_text_features['image_features'])} dimensions\")\n",
        "print(f\"  - Text features: {len(sample_text_features['text_features'])} dimensions\")\n",
        "print(f\"  - Total feature space: {len(sample_text_features['image_features']) + len(sample_text_features['text_features'])} dimensions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26b22469",
      "metadata": {},
      "source": [
        "** What's happening here?**\n",
        "- **Text Encoding**: BERT converts text into 384-dimensional semantic vectors\n",
        "- **Batch Processing**: Multiple text descriptions processed simultaneously\n",
        "- **Feature Normalization**: Text features normalized for better multimodal alignment\n",
        "- **Error Resilience**: Robust error handling for text processing issues\n",
        "\n",
        "## Step 4: Cross-Modal Fusion\n",
        "*Time: 7 minutes*\n",
        "\n",
        "### What We're Doing\n",
        "Combine image and text features to create unified multimodal representations. This is where multimodal AI processing works - understanding content that combines visual and textual information.\n",
        "\n",
        "### Why Fusion Matters\n",
        "- **Richer Understanding**: Combined features capture more information than either modality alone\n",
        "- **Better Predictions**: Multimodal models consistently outperform single-modal approaches\n",
        "- **Real-World Relevance**: Most real content is multimodal (social posts, product listings, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acd5bc69",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultimodalFusion:\n",
        "    \"\"\"Combine image and text features into unified representations.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        print(\" Initializing multimodal fusion...\")\n",
        "        # We'll use simple concatenation and weighted fusion\n",
        "        # In production, you might use attention mechanisms or learned fusion\n",
        "        self.image_weight = 0.6  # Weight for image features\n",
        "        self.text_weight = 0.4   # Weight for text features\n",
        "        print(\" Fusion weights configured\")\n",
        "    \n",
        "    def fuse_features(self, batch):\n",
        "        \"\"\"Fuse image and text features into multimodal representations.\"\"\"\n",
        "        try:\n",
        "            image_features = np.array(batch[\"image_features\"])\n",
        "            text_features = np.array(batch[\"text_features\"])\n",
        "            \n",
        "            # Method 1: Simple concatenation\n",
        "            concatenated_features = np.concatenate([image_features, text_features], axis=1)\n",
        "            \n",
        "            # Method 2: Weighted fusion (average of normalized features)\n",
        "            # Normalize features to same scale for fair weighting\n",
        "            image_norm = image_features / (np.linalg.norm(image_features, axis=1, keepdims=True) + 1e-8)\n",
        "            text_norm = text_features / (np.linalg.norm(text_features, axis=1, keepdims=True) + 1e-8)\n",
        "            \n",
        "            weighted_features = (\n",
        "                self.image_weight * image_norm + \n",
        "                self.text_weight * text_norm\n",
        "            )\n",
        "            \n",
        "            # Calculate similarity between image and text features\n",
        "            similarity_scores = []\n",
        "            for img_feat, txt_feat in zip(image_norm, text_norm):\n",
        "                # Cosine similarity between image and text features\n",
        "                similarity = np.dot(img_feat, txt_feat) / (\n",
        "                    np.linalg.norm(img_feat) * np.linalg.norm(txt_feat) + 1e-8\n",
        "                )\n",
        "                similarity_scores.append(float(similarity))\n",
        "            \n",
        "            return {\n",
        "                **batch,  # Keep original data\n",
        "                \"multimodal_features_concat\": concatenated_features.tolist(),\n",
        "                \"multimodal_features_weighted\": weighted_features.tolist(),\n",
        "                \"image_text_similarity\": similarity_scores\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\" Error in multimodal fusion: {e}\")\n",
        "            batch_size = len(batch[\"image_features\"])\n",
        "            # Return empty features as fallback\n",
        "            return {\n",
        "                **batch,\n",
        "                \"multimodal_features_concat\": [[0.0] * 2432] * batch_size,  # 2048 + 384\n",
        "                \"multimodal_features_weighted\": [[0.0] * 2048] * batch_size,\n",
        "                \"image_text_similarity\": [0.0] * batch_size\n",
        "            }\n",
        "\n",
        "# Apply multimodal fusion to our dataset\n",
        "print(\" Performing multimodal fusion...\")\n",
        "start_time = time.time()\n",
        "\n",
        "final_multimodal_dataset = text_features_dataset.map_batches(\n",
        "    MultimodalFusion,\n",
        "    batch_size=128,  # Larger batches for fusion operations\n",
        "    concurrency=4,   # CPU-intensive operation, use multiple workers\n",
        "    num_gpus=0       # Fusion doesn't need GPU\n",
        ")\n",
        "\n",
        "fusion_time = time.time() - start_time\n",
        "print(f\" Multimodal fusion completed in {fusion_time:.2f} seconds\")\n",
        "\n",
        "# Analyze the fused results\n",
        "fusion_results = final_multimodal_dataset.take(5)\n",
        "\n",
        "print(\"\\n Multimodal Fusion Results:\")\n",
        "print(\"-\" * 60)\n",
        "for i, result in enumerate(fusion_results):\n",
        "    similarity = result['image_text_similarity']\n",
        "    text_preview = result['text'][:40]\n",
        "    \n",
        "    print(f\"{i+1}. '{text_preview}...'\")\n",
        "    print(f\"   Image-Text Similarity: {similarity:.3f}\")\n",
        "    print(f\"   Concat Features: {len(result['multimodal_features_concat'])} dims\")\n",
        "    print(f\"   Weighted Features: {len(result['multimodal_features_weighted'])} dims\")\n",
        "    print()\n",
        "\n",
        "# Performance summary with profiling (rule #199: Include performance profiling)\n",
        "total_samples = final_multimodal_dataset.count()\n",
        "total_processing_time = time.time() - start_time\n",
        "\n",
        "print(f\" Final Results:\")\n",
        "print(f\"  - Processed {total_samples:,} multimodal samples\")\n",
        "print(f\"  - Total processing time: {total_processing_time:.2f} seconds\")\n",
        "print(\"  - Review Ray Dashboard for throughput and resource usage\")\n",
        "print(f\"  - Created rich multimodal representations\")\n",
        "print(f\"  - Ready for downstream AI tasks (classification, search, etc.)\")\n",
        "\n",
        "# Performance profiling summary\n",
        "print(f\"\\n Performance Breakdown:\")\n",
        "print(f\"  - Image feature extraction: {extraction_time:.2f}s\")\n",
        "print(f\"  - Text feature extraction: {text_extraction_time:.2f}s\") \n",
        "print(f\"  - Multimodal fusion: {fusion_time:.2f}s\")\n",
        "print(f\"  - Total pipeline time: {total_processing_time:.2f}s\")\n",
        "\n",
        "# Resource utilization summary\n",
        "cluster_resources = ray.cluster_resources()\n",
        "print(f\"\\n Resource Utilization:\")\n",
        "print(f\"  - CPUs available: {cluster_resources.get('CPU', 0)}\")\n",
        "print(f\"  - GPUs available: {cluster_resources.get('GPU', 0)}\")\n",
        "print(f\"  - Memory available: {cluster_resources.get('memory', 0)/1e9:.1f} GB\")\n",
        "\n",
        "# Clean up resources\n",
        "ray.shutdown()\n",
        "print(\" Ray cluster shut down successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c63b663",
      "metadata": {},
      "source": [
        "** What we accomplished:**\n",
        "- **Cross-Modal Understanding**: Combined visual and textual information\n",
        "- **Feature Fusion**: Created unified representations from separate modalities\n",
        "- **Similarity Analysis**: Measured how well images and text align\n",
        "- **Scalable Processing**: Handled multimodal data efficiently with Ray Data\n",
        "\n",
        "---\n",
        "\n",
        "## Troubleshooting Common Issues\n",
        "\n",
        "### **Problem: \"CUDA out of memory with multimodal models\"**\n",
        "**Solution**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f85c3b5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reduce batch sizes and use CPU for fusion\n",
        "image_batch_size = 16  # Smaller for GPU-intensive image processing\n",
        "text_batch_size = 32   # Larger for CPU text processing\n",
        "fusion_num_gpus = 0    # Use CPU for fusion operations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61eb3256",
      "metadata": {},
      "source": [
        "### **Problem: \"Feature dimensions don't match for fusion\"**\n",
        "**Solution**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5084402",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add dimension checking and padding\n",
        "def align_feature_dimensions(feat1, feat2, target_dim=512):\n",
        "    # Pad or truncate features to same dimension\n",
        "    feat1_aligned = feat1[:target_dim] if len(feat1) > target_dim else feat1 + [0] * (target_dim - len(feat1))\n",
        "    feat2_aligned = feat2[:target_dim] if len(feat2) > target_dim else feat2 + [0] * (target_dim - len(feat2))\n",
        "    return feat1_aligned, feat2_aligned"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dee7dd60",
      "metadata": {},
      "source": [
        "### **Problem: \"Low similarity scores between modalities\"**\n",
        "**Solution**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f5a99d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use better pre-trained models or fine-tune for your domain\n",
        "# Consider using CLIP models that are trained for image-text alignment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48d518db",
      "metadata": {},
      "source": [
        "### **Performance Optimization Tips**\n",
        "\n",
        "1. **Model Selection**: Use CLIP models for better image-text alignment\n",
        "2. **Batch Sizing**: Optimize batch sizes for each modality separately\n",
        "3. **GPU Memory**: Process images on GPU, text on CPU if memory is limited\n",
        "4. **Feature Caching**: Cache extracted features to avoid recomputation\n",
        "5. **Fusion Methods**: Experiment with different fusion techniques\n",
        "\n",
        "### **Performance Considerations**\n",
        "\n",
        "Ray Data provides several advantages for multimodal processing:\n",
        "- **Parallel modality processing**: Image and text features can be extracted simultaneously\n",
        "- **GPU utilization**: Automatic distribution of GPU-intensive tasks across available hardware\n",
        "- **Memory efficiency**: Large multimodal datasets are processed in manageable batches\n",
        "- **Resource optimization**: Different modalities can use different resource configurations\n",
        "\n",
        "---\n",
        "\n",
        "## Next Steps and Extensions\n",
        "\n",
        "### **Try These Advanced Features**\n",
        "1. **CLIP Integration**: Use OpenAI's CLIP for better image-text understanding\n",
        "2. **Attention Mechanisms**: Implement cross-modal attention for better fusion\n",
        "3. **Multimodal Classification**: Build classifiers using the fused features\n",
        "4. **Similarity Search**: Create image-text search and recommendation systems\n",
        "5. **Real Datasets**: Use actual social media or e-commerce multimodal data\n",
        "\n",
        "### **Production Considerations**\n",
        "- **Model Optimization**: Use quantization and pruning for faster inference\n",
        "- **Caching Strategy**: Cache frequently used features and models\n",
        "- **Error Handling**: Implement robust error handling for production workloads\n",
        "- **Monitoring**: Track model performance and feature quality\n",
        "- **Scaling**: Use Ray Serve for real-time multimodal inference\n",
        "\n",
        "### **Related Ray Data Templates**\n",
        "- **Ray Data Batch Inference Optimization**: Optimize multimodal model inference\n",
        "- **Ray Data NLP Text Analytics**: Deep dive into text processing techniques\n",
        "- **Ray Data Batch Classification**: Focus on image processing optimization\n",
        "\n",
        "You have built a scalable multimodal AI pipeline with Ray Data.\n",
        "\n",
        "These multimodal techniques enable you to build AI systems that understand content the way humans do - by combining visual and textual information for richer understanding.\n",
        "\n",
        "## Architecture\n",
        "\n",
        "```\n",
        "S3 Data Sources \u2192 Ray Data \u2192 Modality-Specific Processing \u2192 Cross-Modal Fusion \u2192 AI Analysis \u2192 Results\n",
        "     \u2193              \u2193              \u2193                        \u2193              \u2193         \u2193\n",
        "  Images         Parallel      Vision Models            Embedding      ML Models  Insights\n",
        "  Text           Processing    Text Encoders            Fusion         Inference  Reports\n",
        "  Audio          GPU Workers   Audio Models             Aggregation    Scoring    Analytics\n",
        "```\n",
        "\n",
        "## Key Components\n",
        "\n",
        "### 1. **Multimodal Data Loading**\n",
        "- `ray.data.read_images()` for image data from S3\n",
        "- `ray.data.read_text()` for text documents and social media posts\n",
        "- `ray.data.read_binary_files()` for audio files\n",
        "- Automatic format detection and validation\n",
        "\n",
        "### 2. **Modality-Specific Processing**\n",
        "- **Vision**: Pre-trained vision transformers (ViT, ResNet) with GPU acceleration\n",
        "- **Text**: BERT, RoBERTa, or custom text encoders\n",
        "- **Audio**: Wav2Vec, HuBERT, or audio feature extractors\n",
        "- Parallel processing with device-specific optimizations\n",
        "\n",
        "### 3. **Cross-Modal Fusion**\n",
        "- Embedding alignment and normalization\n",
        "- Attention mechanisms for cross-modal understanding\n",
        "- Multimodal transformer architectures\n",
        "- Feature concatenation and aggregation strategies\n",
        "\n",
        "### 4. **AI Analysis and Inference**\n",
        "- Multimodal classification models\n",
        "- Content recommendation systems\n",
        "- Sentiment and content analysis\n",
        "- Automated content moderation\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Ray cluster with GPU support (recommended)\n",
        "- Python 3.8+ with required ML libraries\n",
        "- Access to S3 or local multimodal datasets\n",
        "- Basic understanding of computer vision, NLP, and audio processing\n",
        "\n",
        "## Installation\n",
        "\n",
        "```bash\n",
        "pip install ray[data] torch torchvision transformers\n",
        "pip install torchaudio librosa pillow opencv-python\n",
        "pip install sentence-transformers accelerate\n",
        "```\n",
        "\n",
        "## 5-Minute Quick Start\n",
        "\n",
        "**Goal**: Get a multimodal AI pipeline running in 5 minutes with real data\n",
        "\n",
        "### **Step 1: Setup on Anyscale (30 seconds)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cce6cf0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ray cluster is already running on Anyscale\n",
        "import ray\n",
        "\n",
        "# Check cluster status (already connected)\n",
        "print('Connected to Anyscale Ray cluster!')\n",
        "print(f'Available resources: {ray.cluster_resources()}')\n",
        "\n",
        "# Install any missing packages if needed\n",
        "# !pip install torch torchvision transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d21000aa",
      "metadata": {},
      "source": [
        "### **Step 2: Load Real Data (1 minute)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b841c214",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ray.data import read_images\n",
        "\n",
        "# Load real ImageNet subset (Imagenette) - publicly available\n",
        "image_ds = read_images(\"s3://anonymous@air-example-data-2/imagenette2/train/\", mode=\"RGB\").limit(10)\n",
        "print(f\"Loaded {image_ds.count()} real images\")\n",
        "\n",
        "# Quick data inspection\n",
        "sample = image_ds.take(1)[0]\n",
        "print(f\"Image shape: {sample['image'].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f4ec1ed",
      "metadata": {},
      "source": [
        "### **Step 3: Run Vision Processing (2 minutes)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef855114",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import models, transforms\n",
        "\n",
        "class QuickVisionProcessor:\n",
        "    def __init__(self):\n",
        "        self.model = models.resnet18(pretrained=True)  # Smaller model for speed\n",
        "        self.model.eval()\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    \n",
        "    def __call__(self, batch):\n",
        "        features = []\n",
        "        for img in batch[\"image\"]:\n",
        "            try:\n",
        "                img_tensor = self.transform(img).unsqueeze(0)\n",
        "                with torch.no_grad():\n",
        "                    feature = self.model(img_tensor)\n",
        "                features.append(feature.numpy()[0])\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing image: {e}\")\n",
        "                features.append(None)\n",
        "        return {\"features\": features}\n",
        "\n",
        "# Process images\n",
        "processed = image_ds.map_batches(QuickVisionProcessor(), batch_size=4)\n",
        "results = processed.take(5)\n",
        "print(f\"Processed {len(results)} image batches\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd4183b0",
      "metadata": {},
      "source": [
        "### **Step 4: View Results (1 minute)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "465ca19c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display results\n",
        "for i, result in enumerate(results):\n",
        "    features = result.get(\"features\", [])\n",
        "    valid_features = [f for f in features if f is not None]\n",
        "    print(f\"Batch {i}: {len(valid_features)} successful feature extractions\")\n",
        "\n",
        "print(\"Quick start completed! Check the full demo for advanced features.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e6ba5f1",
      "metadata": {},
      "source": [
        "## Complete Tutorial\n",
        "\n",
        "### 1. **Load Real Multimodal Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "941e18f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import ray\n",
        "from ray.data import read_images, read_text, read_binary_files\n",
        "\n",
        "# Initialize Ray\n",
        "ray.init(ignore_reinit_error=True)\n",
        "\n",
        "# Load real datasets from public sources\n",
        "# ImageNet subset (Imagenette) - publicly available\n",
        "image_ds = read_images(\"s3://anonymous@air-example-data-2/imagenette2/train/\", mode=\"RGB\")\n",
        "\n",
        "# Common Crawl news articles - publicly available\n",
        "text_ds = read_text(\"s3://anonymous@commoncrawl/crawl-data/CC-NEWS/2023/01/\")\n",
        "\n",
        "# LibriSpeech audio dataset - publicly available  \n",
        "audio_ds = read_binary_files(\"s3://anonymous@openslr/12/train-clean-100/\")\n",
        "\n",
        "print(f\"Images (Imagenette): {image_ds.count()}\")\n",
        "print(f\"Text (Common Crawl): {text_ds.count()}\")\n",
        "print(f\"Audio (LibriSpeech): {audio_ds.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09e254b0",
      "metadata": {},
      "source": [
        "### 2. **Process Each Modality**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee18c6ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ray.data import ActorPoolStrategy\n",
        "import torch\n",
        "\n",
        "class VisionProcessor:\n",
        "    def __init__(self):\n",
        "        self.model = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)\n",
        "        self.model.eval()\n",
        "        if torch.cuda.is_available():\n",
        "            self.model.cuda()\n",
        "    \n",
        "    def __call__(self, batch):\n",
        "        # Process image batch\n",
        "        images = torch.stack([torch.from_numpy(img) for img in batch[\"image\"]])\n",
        "        if torch.cuda.is_available():\n",
        "            images = images.cuda()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            features = self.model(images)\n",
        "        \n",
        "        return {\"image_features\": features.cpu().numpy()}\n",
        "\n",
        "# Apply vision processing\n",
        "processed_images = image_ds.map_batches(\n",
        "    VisionProcessor,\n",
        "    batch_size=32,\n",
        "    num_gpus=1,\n",
        "    concurrency=4\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82c2b6ff",
      "metadata": {},
      "source": [
        "### 3. **Cross-Modal Fusion**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ef679b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def fuse_modalities(batch):\n",
        "    \"\"\"Combine features from different modalities\"\"\"\n",
        "    # Align features by content ID\n",
        "    image_features = batch[\"image_features\"]\n",
        "    text_features = batch[\"text_features\"]\n",
        "    audio_features = batch[\"audio_features\"]\n",
        "    \n",
        "    # Simple concatenation (can be enhanced with attention)\n",
        "    fused_features = np.concatenate([\n",
        "        image_features, text_features, audio_features\n",
        "    ], axis=1)\n",
        "    \n",
        "    return {\"fused_features\": fused_features}\n",
        "\n",
        "# Apply fusion (assuming aligned datasets)\n",
        "fused_ds = processed_images.map_batches(fuse_modalities)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63fdd303",
      "metadata": {},
      "source": [
        "### 4. **Multimodal Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7595117e",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultimodalClassifier:\n",
        "    def __init__(self):\n",
        "        self.classifier = torch.nn.Linear(2048 + 768 + 512, 10)  # Example dimensions\n",
        "        if torch.cuda.is_available():\n",
        "            self.classifier.cuda()\n",
        "    \n",
        "    def __call__(self, batch):\n",
        "        features = torch.from_numpy(batch[\"fused_features\"]).float()\n",
        "        if torch.cuda.is_available():\n",
        "            features = features.cuda()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            predictions = self.classifier(features)\n",
        "            probabilities = torch.softmax(predictions, dim=1)\n",
        "        \n",
        "        return {\n",
        "            \"predictions\": predictions.cpu().numpy(),\n",
        "            \"probabilities\": probabilities.cpu().numpy()\n",
        "        }\n",
        "\n",
        "# Apply classification\n",
        "results = fused_ds.map_batches(\n",
        "    MultimodalClassifier,\n",
        "    batch_size=64,\n",
        "    num_gpus=1,\n",
        "    concurrency=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba495176",
      "metadata": {},
      "source": [
        "## Advanced Features\n",
        "\n",
        "### **GPU Memory Management**\n",
        "- Automatic batch size optimization based on GPU memory\n",
        "- Gradient checkpointing for large models\n",
        "- Memory-efficient data loading and processing\n",
        "\n",
        "### **Scalability**\n",
        "- Horizontal scaling across multiple GPUs\n",
        "- Automatic load balancing and resource allocation\n",
        "- Support for heterogeneous GPU clusters\n",
        "\n",
        "### **Performance Optimization**\n",
        "- Operator fusion for reduced memory transfers\n",
        "- Pipelined processing for continuous data flow\n",
        "- Caching strategies for frequently accessed data\n",
        "\n",
        "## Production Considerations\n",
        "\n",
        "### **Model Serving**\n",
        "- Integration with Ray Serve for real-time inference\n",
        "- Model versioning and A/B testing\n",
        "- Automatic scaling based on demand\n",
        "\n",
        "### **Monitoring and Observability**\n",
        "- Performance metrics and resource utilization\n",
        "- Error tracking and alerting\n",
        "- Pipeline health monitoring\n",
        "\n",
        "### **Data Quality and Validation**\n",
        "- Input validation and sanitization\n",
        "- Output quality checks and confidence scoring\n",
        "- Fallback strategies for failed processing\n",
        "\n",
        "## Example Workflows\n",
        "\n",
        "### **Content Moderation Pipeline**\n",
        "1. Load social media content (images, text, audio)\n",
        "2. Extract features using modality-specific models\n",
        "3. Apply content moderation rules and ML models\n",
        "4. Generate moderation decisions and confidence scores\n",
        "5. Store results for audit and compliance\n",
        "\n",
        "### **Recommendation System**\n",
        "1. Process user interaction data (clicks, views, listens)\n",
        "2. Generate user and content embeddings\n",
        "3. Calculate similarity scores across modalities\n",
        "4. Rank and recommend relevant content\n",
        "5. Update recommendations in real-time\n",
        "\n",
        "### **Market Intelligence**\n",
        "1. Collect market data (news, social media, financial reports)\n",
        "2. Extract sentiment and key information\n",
        "3. Correlate across different data sources\n",
        "4. Generate market insights and predictions\n",
        "5. Alert on significant events or trends\n",
        "\n",
        "## Performance analysis\n",
        "\n",
        "### **Evaluation framework**\n",
        "\n",
        "You can evaluate your pipeline with the following dimensions:\n",
        "\n",
        "| Benchmark Type | Measurement Focus | Output Visualization |\n",
        "|---------------|-------------------|---------------------|\n",
        "| **Fusion Method Comparison** | Attention vs Weighted vs Simple | Performance comparison charts |\n",
        "| **Batch Size Optimization** | Memory usage vs throughput | Optimization curves |\n",
        "| **GPU vs CPU analysis** | Device performance comparison | Resource utilization |\n",
        "| **Scalability Testing** | Multi-GPU performance | Scaling visualizations |\n",
        "\n",
        "### **Example evaluation setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a69a48ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Use Ray Dashboard to measure throughput, GPU utilization, and task timelines.\")\n",
        "print(\"Vary batch sizes and fusion methods to evaluate tradeoffs for your data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d54c7e1",
      "metadata": {},
      "source": [
        "### **Modality Processing Pipeline**\n",
        "\n",
        "```\n",
        "Input Data Flow:\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502   Images    \u2502    \u2502    Text     \u2502    \u2502   Audio     \u2502\n",
        "\u2502 (ImageNet)  \u2502    \u2502   (IMDB)    \u2502    \u2502(LibriSpeech)\u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "       \u2502                  \u2502                  \u2502\n",
        "       \u25bc                  \u25bc                  \u25bc\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502   ResNet    \u2502    \u2502    BERT     \u2502    \u2502   Audio     \u2502\n",
        "\u2502 Features    \u2502    \u2502 Embeddings  \u2502    \u2502 Features    \u2502\n",
        "\u2502 (2048-dim)  \u2502    \u2502 (384-dim)   \u2502    \u2502 (13-dim)    \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "       \u2502                  \u2502                  \u2502\n",
        "       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "                          \u25bc\n",
        "                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "                \u2502 Multimodal      \u2502\n",
        "                \u2502 Fusion          \u2502\n",
        "                \u2502 (Attention)     \u2502\n",
        "                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "                          \u25bc\n",
        "                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "                \u2502 Classification  \u2502\n",
        "                \u2502 & Analysis      \u2502\n",
        "                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "```\n",
        "\n",
        "### **Expected Resource Requirements**\n",
        "\n",
        "| Processing Stage | CPU Cores | Memory (GB) | GPU Memory (GB) | Processing Time |\n",
        "|-----------------|-----------|-------------|-----------------|-----------------|\n",
        "| **Image Processing** | 4-8 | 8-16 | 4-8 | Measured in demo |\n",
        "| **Text Processing** | 2-4 | 4-8 | 2-4 | Measured in demo |\n",
        "| **Audio Processing** | 2-4 | 4-8 | 1-2 | Measured in demo |\n",
        "| **Fusion & Classification** | 4-8 | 8-16 | 4-8 | Measured in demo |\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "### **Common Issues and Solutions**\n",
        "\n",
        "| Issue | Symptoms | Solution | Prevention |\n",
        "|-------|----------|----------|------------|\n",
        "| **GPU Memory Errors** | `RuntimeError: CUDA out of memory` | Reduce batch size to 4-8, use CPU fallback | Monitor GPU memory usage, start with small batches |\n",
        "| **Data Alignment Issues** | Mismatched modality counts | Ensure consistent IDs across datasets | Validate data alignment before processing |\n",
        "| **Model Loading Failures** | Import errors, missing dependencies | Install required packages, check model availability | Use requirements.txt, test imports |\n",
        "| **Poor Performance** | Slow processing, low GPU utilization | Optimize batch size, increase concurrency | Profile operations, monitor resource usage |\n",
        "| **Memory Pressure** | Ray object store full | Reduce data in memory, process in chunks | Monitor object store, use streaming patterns |\n",
        "\n",
        "### **Performance Optimization Guide**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa886e0f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimal configuration for different cluster types\n",
        "import ray\n",
        "\n",
        "# Check available resources\n",
        "resources = ray.cluster_resources()\n",
        "gpu_count = resources.get('GPU', 0)\n",
        "cpu_count = resources.get('CPU', 0)\n",
        "\n",
        "# Adjust configuration based on resources\n",
        "if gpu_count >= 4:\n",
        "    # Multi-GPU configuration\n",
        "    batch_size = 32\n",
        "    concurrency = 4\n",
        "    num_gpus = 1\n",
        "elif gpu_count >= 1:\n",
        "    # Single GPU configuration\n",
        "    batch_size = 16\n",
        "    concurrency = 2\n",
        "    num_gpus = 1\n",
        "else:\n",
        "    # CPU-only configuration\n",
        "    batch_size = 8\n",
        "    concurrency = cpu_count // 4\n",
        "    num_gpus = 0\n",
        "\n",
        "print(f\"Recommended config: batch_size={batch_size}, concurrency={concurrency}, num_gpus={num_gpus}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4dc870a",
      "metadata": {},
      "source": [
        "### **Debug Mode and Monitoring**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4737639e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import torch\n",
        "\n",
        "# Enable comprehensive debugging\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "# Monitor GPU memory if available\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
        "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0) / 1e9:.1f}GB\")\n",
        "\n",
        "# Enable Ray Data debugging\n",
        "from ray.data.context import DataContext\n",
        "ctx = DataContext.get_current()\n",
        "ctx.enable_progress_bars = True\n",
        "\n",
        "# Monitor processing with custom logging\n",
        "class DebugProcessor:\n",
        "    def __call__(self, batch):\n",
        "        print(f\"Processing batch with {len(batch)} items\")\n",
        "        print(f\"GPU memory before: {torch.cuda.memory_allocated(0) / 1e9:.1f}GB\" if torch.cuda.is_available() else \"CPU mode\")\n",
        "        \n",
        "        # Your processing logic here\n",
        "        results = process_batch(batch)\n",
        "        \n",
        "        print(f\"GPU memory after: {torch.cuda.memory_allocated(0) / 1e9:.1f}GB\" if torch.cuda.is_available() else \"Processing complete\")\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36b802be",
      "metadata": {},
      "source": [
        "### **Error Recovery Strategies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a49e768d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implement robust error handling with recovery\n",
        "def robust_multimodal_processing(batch):\n",
        "    \"\"\"Process multimodal data with comprehensive error recovery.\"\"\"\n",
        "    results = []\n",
        "    errors = []\n",
        "    \n",
        "    for item in batch:\n",
        "        try:\n",
        "            # Attempt processing\n",
        "            result = process_item(item)\n",
        "            results.append(result)\n",
        "            \n",
        "        except torch.cuda.OutOfMemoryError:\n",
        "            # GPU memory error - try CPU fallback\n",
        "            try:\n",
        "                torch.cuda.empty_cache()\n",
        "                result = process_item_cpu(item)\n",
        "                results.append(result)\n",
        "                \n",
        "            except Exception as fallback_error:\n",
        "                errors.append(f\"CPU fallback failed: {fallback_error}\")\n",
        "                results.append(create_error_result(item, fallback_error))\n",
        "                \n",
        "        except Exception as general_error:\n",
        "            errors.append(f\"Processing failed: {general_error}\")\n",
        "            results.append(create_error_result(item, general_error))\n",
        "    \n",
        "    if errors:\n",
        "        print(f\"Encountered {len(errors)} errors during processing\")\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "031c538e",
      "metadata": {},
      "source": [
        "## Cleanup and Resource Management\n",
        "\n",
        "Always clean up Ray resources when done:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ca545be",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up Ray resources\n",
        "ray.shutdown()\n",
        "print(\"Ray cluster shutdown complete\")\n",
        "\n",
        "# Clear GPU memory if using CUDA\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"GPU memory cleared\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82d2e51d",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "1. **Customize Models**: Replace pre-trained models with your own\n",
        "2. **Add Modalities**: Extend to video, 3D data, or other formats\n",
        "3. **Optimize Performance**: Tune batch sizes and resource allocation\n",
        "4. **Scale Production**: Deploy to multi-GPU clusters with Ray Serve\n",
        "\n",
        "## Resources\n",
        "\n",
        "- [Ray Data Documentation](https://docs.ray.io/en/latest/data/index.html)\n",
        "- [PyTorch Multimodal Tutorials](https://pytorch.org/tutorials/)\n",
        "- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)\n",
        "- [Ray GPU Support](https://docs.ray.io/en/latest/ray-core/using-ray-with-gpus.html)\n",
        "\n",
        "---\n",
        "\n",
        "*This template provides a foundation for building production-ready multimodal AI pipelines with Ray Data. Start with the basic examples and gradually add complexity based on your specific use case and requirements.*"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}