{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6daa1457",
   "metadata": {},
   "source": [
    "# NLP text analytics with Ray Data\n",
    "\n",
    "**Time to complete**: 30 min | **Difficulty**: Intermediate | **Prerequisites**: Basic Python, familiarity with text processing\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "Create a scalable text processing pipeline that analyzes thousands of text documents in parallel. You'll learn sentiment analysis, text classification, and how to process large text datasets efficiently.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Text Data Loading](#step-1-loading-text-data) (5 min)\n",
    "2. [Text Preprocessing](#step-2-text-preprocessing-at-scale) (8 min)\n",
    "3. [Sentiment Analysis](#step-3-distributed-sentiment-analysis) (10 min)\n",
    "4. [Results and Insights](#step-4-analyzing-results) (7 min)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "**Why text processing matters**: Memory and computation challenges with large text datasets require distributed processing solutions. Understanding scalable NLP enables analysis of massive text corpora that traditional tools cannot handle.\n",
    "\n",
    "**Ray Data's text capabilities**: Distribute NLP tasks across multiple workers for scalable text analytics. You'll learn how to transform text processing from single-machine limitations to distributed scale.\n",
    "\n",
    "**Real-world text applications**: Techniques used by companies to process millions of reviews, comments, and documents demonstrate the practical value of distributed NLP for business intelligence.\n",
    "\n",
    "**Production NLP strategies**: Scale text processing workflows for enterprise applications enabling real-time text analytics and automated content analysis at massive scale.\n",
    "\n",
    "## Overview\n",
    "\n",
    "**The Challenge**: Processing large text datasets (reviews, social media, documents) with traditional tools is slow and often runs out of memory.\n",
    "\n",
    "**The Solution**: Ray Data distributes text processing across multiple cores, making it possible to analyze millions of documents quickly.\n",
    "\n",
    "**Real-world impact**:\n",
    "- **E-commerce**: Analyze product reviews for insights\n",
    "- **Social media**: Process posts for sentiment trends\n",
    "- **News**: Classify and analyze large volumes of articles\n",
    "- **Customer support**: Automatically categorize and route support tickets\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites Checklist\n",
    "\n",
    "Before starting, ensure you have:\n",
    "- [ ] Basic understanding of text processing concepts\n",
    "- [ ] Familiarity with sentiment analysis\n",
    "- [ ] Python environment with sufficient memory (4GB+ recommended)\n",
    "- [ ] Understanding of machine learning basics\n",
    "\n",
    "## Quick Start (3 minutes)\n",
    "\n",
    "Want to see text processing in action immediately?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a0ce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "# Create sample text data\n",
    "texts = [\"I love this product!\", \"This is terrible\", \"Pretty good overall\"]\n",
    "ds = ray.data.from_items([{\"text\": t} for t in texts * 1000])\n",
    "print(f\" Created dataset with {ds.count()} text samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796f84a7",
   "metadata": {},
   "source": [
    "To run this template, you will need the following packages:\n",
    "\n",
    "```bash\n",
    "pip install ray[data] transformers torch nltk wordcloud matplotlib seaborn plotly textstat\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Loading Text Data\n",
    "*Time: 5 minutes*\n",
    "\n",
    "### What We're Doing\n",
    "We'll create a realistic text dataset similar to product reviews or social media posts. This gives us something meaningful to analyze without requiring huge downloads.\n",
    "\n",
    "### Why This Approach Transforms Text Processing\n",
    "\n",
    "Working with realistic data fundamentally changes how you understand text analytics. When you learn with data that resembles real-world text patterns, the techniques naturally scale from thousands to millions of documents without architectural changes. This approach ensures that insights gained during development translate directly to production environments.\n",
    "\n",
    "Memory efficiency becomes critical when processing large text datasets. Traditional text processing methods often require loading entire datasets into memory, creating bottlenecks that prevent scaling to enterprise data volumes. Ray Data's distributed approach enables processing unlimited text volumes without memory constraints, allowing organizations to analyze their complete text archives rather than samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01020794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate scalable text processing efficiency\n",
    "def measure_text_processing_efficiency():\n",
    "    \"\"\"Show how Ray Data handles increasing text volumes.\"\"\"\n",
    "    \n",
    "    # Start with smaller dataset for comparison\n",
    "    small_texts = [\"Sample text\"] * 1000\n",
    "    small_dataset = ray.data.from_items([{\"text\": t} for t in small_texts])\n",
    "    \n",
    "    # Scale to larger dataset \n",
    "    large_texts = [\"Sample text\"] * 100000\n",
    "    large_dataset = ray.data.from_items([{\"text\": t} for t in large_texts])\n",
    "    \n",
    "    print(f\"Small dataset: {small_dataset.count():,} texts\")\n",
    "    print(f\"Large dataset: {large_dataset.count():,} texts\")\n",
    "    print(\"Memory usage remains constant - Ray Data streams processing\")\n",
    "    \n",
    "    return large_dataset\n",
    "\n",
    "# Demonstrate memory-efficient text processing\n",
    "efficient_dataset = measure_text_processing_efficiency()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3106ad0e",
   "metadata": {},
   "source": [
    "This scalable foundation enables sophisticated text analytics that work consistently across different data volumes and organizational requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5341ca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from wordcloud import WordCloud\n",
    "import textstat\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Initialize Ray for distributed processing\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "def load_real_text_data():\n",
    "    \"\"\"Load real Amazon product reviews for text analytics.\"\"\"\n",
    "    print(\"Loading real Amazon product review dataset...\")\n",
    "    \n",
    "    # Load real Amazon product reviews from public dataset\n",
    "    try:\n",
    "        # Load Amazon reviews parquet data\n",
    "        text_dataset = ray.data.read_parquet(\n",
    "            \"s3://amazon-reviews-pds/parquet/product_category=Books/\",\n",
    "            columns=[\"review_body\", \"star_rating\", \"product_title\", \"verified_purchase\"]\n",
    "        ).limit(10000)  # Limit to 10K reviews for processing efficiency\n",
    "        \n",
    "    print(f\"Loaded {text_dataset.count():,} Amazon book reviews\")\n",
    "        \n",
    "        # BEST PRACTICE: Use Ray Data native operations for data transformation\n",
    "        from ray.data.expressions import col, lit\n",
    "        \n",
    "        # Add sentiment mapping using native column operations\n",
    "        def map_sentiment(batch):\n",
    "            \"\"\"Map star ratings to sentiment labels efficiently.\"\"\"\n",
    "            transformed = []\n",
    "            for review in batch:\n",
    "                # Map star rating to sentiment\n",
    "                rating = review.get('star_rating', 3)\n",
    "                if rating >= 4:\n",
    "                    sentiment = 'positive'\n",
    "                elif rating <= 2:\n",
    "                    sentiment = 'negative'\n",
    "                else:\n",
    "                    sentiment = 'neutral'\n",
    "                \n",
    "                # Calculate text length without pandas overhead\n",
    "                text_length = len(str(review.get('review_body', '')))\n",
    "                \n",
    "                transformed.append({\n",
    "                    'text': review.get('review_body', ''),\n",
    "                    'sentiment': sentiment,\n",
    "                    'star_rating': rating,\n",
    "                    'product_title': review.get('product_title', ''),\n",
    "                    'verified_purchase': review.get('verified_purchase', False),\n",
    "                    'length': text_length\n",
    "                })\n",
    "            \n",
    "            return transformed\n",
    "        \n",
    "        # Apply transformation with optimized batch processing\n",
    "        text_dataset = text_dataset.map_batches(\n",
    "            map_sentiment,\n",
    "            batch_size=2000,  # Larger batch size for efficiency\n",
    "            concurrency=4     # Parallel processing\n",
    "        )\n",
    "        \n",
    "        # Use native column operations for additional features\n",
    "        text_dataset = text_dataset.add_column(\n",
    "            \"is_long_review\", \n",
    "            col(\"length\") > lit(500)\n",
    "        ).add_column(\n",
    "            \"is_positive\",\n",
    "            col(\"star_rating\") >= lit(4)\n",
    "        )\n",
    "        \n",
    "        return text_dataset\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load Amazon reviews: {e}\")\n",
    "\n",
    "# Load real text dataset\n",
    "text_dataset = load_real_text_data()\n",
    "\n",
    "# Display basic information about our dataset\n",
    "print(f\"Loaded dataset with {text_dataset.count():,} text samples\")\n",
    "print(f\"Schema: {text_dataset.schema()}\")\n",
    "\n",
    "# Show a few sample texts\n",
    "print(\"\\nSample texts:\")\n",
    "samples = text_dataset.take(3)\n",
    "for i, sample in enumerate(samples):\n",
    "    text_preview = sample['text'][:100] + \"...\" if len(sample['text']) > 100 else sample['text']\n",
    "    print(f\"{i+1}. {text_preview} (sentiment: {sample.get('sentiment', 'unknown')})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7e51bb",
   "metadata": {},
   "source": [
    "### Interactive NLP Text Analytics Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06ae8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an engaging NLP text analytics visualization dashboard\n",
    "def create_nlp_dashboard(dataset, sample_size=1000):\n",
    "    \"\"\"Generate a comprehensive NLP text analytics dashboard.\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from collections import Counter\n",
    "    import re\n",
    "    from wordcloud import WordCloud\n",
    "    \n",
    "    # Sample data for analysis\n",
    "    sample_data = dataset.take(sample_size)\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    \n",
    "    # Create comprehensive dashboard\n",
    "    fig = plt.figure(figsize=(24, 16))\n",
    "    gs = fig.add_gridspec(4, 5, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. Sentiment Distribution\n",
    "    ax_sentiment = fig.add_subplot(gs[0, :2])\n",
    "    sentiment_counts = df['true_sentiment'].value_counts()\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "    \n",
    "    wedges, texts, autotexts = ax_sentiment.pie(sentiment_counts.values, \n",
    "                                               labels=sentiment_counts.index, \n",
    "                                               autopct='%1.1f%%', colors=colors, \n",
    "                                               startangle=90)\n",
    "    ax_sentiment.set_title('Sentiment Distribution', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 2. Text Length Analysis\n",
    "    ax_length = fig.add_subplot(gs[0, 2:])\n",
    "    df['length'] = df['text'].str.len()\n",
    "    ax_length.hist(df['length'], bins=30, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "    ax_length.axvline(df['length'].mean(), color='red', linestyle='--', linewidth=2,\n",
    "                     label=f'Mean: {df[\"length\"].mean():.1f} chars')\n",
    "    ax_length.set_title('Text Length Distribution', fontsize=12, fontweight='bold')\n",
    "    ax_length.set_xlabel('Character Count')\n",
    "    ax_length.set_ylabel('Frequency')\n",
    "    ax_length.legend()\n",
    "    ax_length.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Simplified text analysis\n",
    "    df['word_count'] = df['text'].str.split().str.len()\n",
    "    \n",
    "    print(\"\\nText Analytics Summary:\")\n",
    "    print(f\"Total reviews: {len(df):,}\")\n",
    "    print(f\"Average text length: {df['length'].mean():.1f} characters\")\n",
    "    print(f\"Average word count: {df['word_count'].mean():.1f} words\")\n",
    "    \n",
    "    # Sentiment distribution\n",
    "    sentiment_dist = df['true_sentiment'].value_counts()\n",
    "    print(f\"\\nSentiment distribution:\")\n",
    "    for sentiment, count in sentiment_dist.items():\n",
    "        print(f\"  {sentiment}: {count:,} ({count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # 4. Sentiment vs Length Analysis\n",
    "    ax_sent_length = fig.add_subplot(gs[1, 2:])\n",
    "    for sentiment in df['true_sentiment'].unique():\n",
    "        sentiment_data = df[df['true_sentiment'] == sentiment]\n",
    "        ax_sent_length.scatter(sentiment_data['length'], sentiment_data['word_count'], \n",
    "                              label=sentiment, alpha=0.6, s=30)\n",
    "    ax_sent_length.set_title('Sentiment vs Text Characteristics', fontsize=12, fontweight='bold')\n",
    "    ax_sent_length.set_xlabel('Character Count')\n",
    "    ax_sent_length.set_ylabel('Word Count')\n",
    "    ax_sent_length.legend()\n",
    "    ax_sent_length.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Most Common Words\n",
    "    ax_words_common = fig.add_subplot(gs[2, :2])\n",
    "    all_text = ' '.join(df['text'].str.lower())\n",
    "    # Remove common stop words and punctuation\n",
    "    words = re.findall(r'\\b[a-z]{3,}\\b', all_text)\n",
    "    word_counts = Counter(words)\n",
    "    common_words = dict(word_counts.most_common(15))\n",
    "    \n",
    "    words_list = list(common_words.keys())\n",
    "    counts_list = list(common_words.values())\n",
    "    \n",
    "    bars = ax_words_common.barh(range(len(words_list)), counts_list, \n",
    "                               color=plt.cm.viridis(np.linspace(0, 1, len(words_list))))\n",
    "    ax_words_common.set_yticks(range(len(words_list)))\n",
    "    ax_words_common.set_yticklabels(words_list)\n",
    "    ax_words_common.set_title('Most Common Words', fontsize=12, fontweight='bold')\n",
    "    ax_words_common.set_xlabel('Frequency')\n",
    "    \n",
    "    # Add count labels\n",
    "    for i, (bar, count) in enumerate(zip(bars, counts_list)):\n",
    "        width = bar.get_width()\n",
    "        ax_words_common.text(width + width*0.01, bar.get_y() + bar.get_height()/2,\n",
    "                            f'{count}', ha='left', va='center', fontsize=9)\n",
    "    \n",
    "    # 6. Sentiment by Word Count\n",
    "    ax_sent_words = fig.add_subplot(gs[2, 2:])\n",
    "    sentiment_word_avg = df.groupby('true_sentiment')['word_count'].mean()\n",
    "    bars = ax_sent_words.bar(range(len(sentiment_word_avg)), sentiment_word_avg.values,\n",
    "                            color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "    ax_sent_words.set_title('Average Word Count by Sentiment', fontsize=12, fontweight='bold')\n",
    "    ax_sent_words.set_ylabel('Average Word Count')\n",
    "    ax_sent_words.set_xticks(range(len(sentiment_word_avg)))\n",
    "    ax_sent_words.set_xticklabels(sentiment_word_avg.index)\n",
    "    \n",
    "    # Add average labels\n",
    "    for bar, avg in zip(bars, sentiment_word_avg.values):\n",
    "        height = bar.get_height()\n",
    "        ax_sent_words.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                          f'{avg:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 7. Text Statistics Summary\n",
    "    ax_stats = fig.add_subplot(gs[3, :2])\n",
    "    ax_stats.axis('off')\n",
    "    \n",
    "    # Calculate text statistics\n",
    "    total_reviews = len(df)\n",
    "    avg_length = df['length'].mean()\n",
    "    avg_words = df['word_count'].mean()\n",
    "    unique_words = len(set(' '.join(df['text']).lower().split()))\n",
    "    sentiment_dist = df['true_sentiment'].value_counts()\n",
    "    \n",
    "    stats_text = \"Text Analytics Summary\\n\" + \"=\"*50 + \"\\n\"\n",
    "    stats_text += f\"Total Reviews: {total_reviews:,}\\n\"\n",
    "    stats_text += f\"Average Length: {avg_length:.1f} characters\\n\"\n",
    "    stats_text += f\"Average Words: {avg_words:.1f} words\\n\"\n",
    "    stats_text += f\"Unique Words: {unique_words:,}\\n\"\n",
    "    stats_text += f\"Vocabulary Density: {unique_words/len(' '.join(df['text']).split()):.3f}\\n\"\n",
    "    stats_text += f\"Positive: {sentiment_dist.get('positive', 0):,} ({sentiment_dist.get('positive', 0)/total_reviews*100:.1f}%)\\n\"\n",
    "    stats_text += f\"Negative: {sentiment_dist.get('negative', 0):,} ({sentiment_dist.get('negative', 0)/total_reviews*100:.1f}%)\\n\"\n",
    "    stats_text += f\"Neutral: {sentiment_dist.get('neutral', 0):,} ({sentiment_dist.get('neutral', 0)/total_reviews*100:.1f}%)\\n\"\n",
    "    \n",
    "    ax_stats.text(0.05, 0.95, stats_text, transform=ax_stats.transAxes, \n",
    "                 fontsize=11, verticalalignment='top', fontfamily='monospace',\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightyellow\", alpha=0.8))\n",
    "    \n",
    "    # 8. Sample Reviews Table\n",
    "    ax_table = fig.add_subplot(gs[3, 2:])\n",
    "    ax_table.axis('off')\n",
    "    \n",
    "    # Create sample reviews table\n",
    "    sample_df = df.head(6)[['review_id', 'true_sentiment', 'text', 'word_count']].copy()\n",
    "    sample_df['text'] = sample_df['text'].str[:40] + '...'  # Truncate long texts\n",
    "    \n",
    "    table_text = \"Sample Reviews\\n\" + \"=\"*80 + \"\\n\"\n",
    "    table_text += f\"{'ID':<12} {'Sentiment':<10} {'Text':<30} {'Words':<6}\\n\"\n",
    "    table_text += \"-\"*80 + \"\\n\"\n",
    "    \n",
    "    for _, row in sample_df.iterrows():\n",
    "        table_text += f\"{row['review_id']:<12} {row['true_sentiment']:<10} {row['text']:<30} {row['word_count']:<6}\\n\"\n",
    "    \n",
    "    ax_table.text(0.05, 0.95, table_text, transform=ax_table.transAxes, \n",
    "                 fontsize=9, verticalalignment='top', fontfamily='monospace',\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\", alpha=0.8))\n",
    "    \n",
    "    plt.suptitle('NLP Text Analytics Dashboard', fontsize=18, fontweight='bold', y=0.95)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print NLP insights\n",
    "    print(\"NLP Text Analytics Insights:\")\n",
    "    print(f\"- Text diversity: {unique_words:,} unique words across {total_reviews:,} reviews\")\n",
    "    print(f\"- Average complexity: {avg_words:.1f} words per review\")\n",
    "    print(f\"- Sentiment balance: {sentiment_dist.to_dict()}\")\n",
    "    print(f\"- Vocabulary density: {unique_words/len(' '.join(df['text']).split()):.3f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate the NLP dashboard\n",
    "nlp_df = create_nlp_dashboard(text_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02050bf",
   "metadata": {},
   "source": [
    "**Why This Dashboard Matters:**\n",
    "- **Text Understanding**: Visualize text characteristics and patterns across different sentiments\n",
    "- **Quality Assessment**: Analyze text length, word count, and vocabulary diversity\n",
    "- **Sentiment Analysis**: Understand sentiment distribution and text characteristics\n",
    "- **Pattern Recognition**: Identify common words and linguistic patterns in the dataset\n",
    "\n",
    "** What just happened?**\n",
    "- Created 1,000 realistic text samples (reviews)\n",
    "- Each sample has text content and known sentiment\n",
    "- Data is loaded into Ray Data for distributed processing\n",
    "- We can easily scale this to millions of real reviews\n",
    "\n",
    "## Use Case: Enterprise Content Intelligence Platform\n",
    "\n",
    "### **Real-World Business Scenario**\n",
    "\n",
    "A large e-commerce company receives 100,000+ pieces of text content daily from multiple sources and needs to extract actionable business insights at scale. Traditional NLP tools can't handle this volume efficiently.\n",
    "\n",
    "**Content Sources and Volumes:**\n",
    "- **Customer Reviews**: 50,000+ daily product and service reviews\n",
    "- **Support Tickets**: 15,000+ daily customer service interactions  \n",
    "- **Social Media**: 25,000+ daily mentions, posts, and comments\n",
    "- **Internal Documents**: 10,000+ daily emails, reports, and documentation\n",
    "\n",
    "**Business Challenges:**\n",
    "- **Manual Processing**: Takes 40+ hours daily with human analysts\n",
    "- **Inconsistent Analysis**: Different analysts provide varying insights\n",
    "- **Delayed Response**: 24-48 hour delay for sentiment analysis and issue identification\n",
    "- **Limited Scale**: Can only process 10% of available content\n",
    "- **High Cost**: Expensive external NLP service dependencies\n",
    "\n",
    "### **Ray Data Solution Benefits**\n",
    "\n",
    "The comprehensive NLP pipeline delivers:\n",
    "\n",
    "| Business Aspect | Traditional Approach | Ray Data Solution | Business Impact |\n",
    "|----------------|----------------------|-------------------|-----------------|\n",
    "| **Processing Speed** | Manual analysis required | Automated distributed processing | Faster insights |\n",
    "| **Content Coverage** | Sample-based analysis | Full-dataset processing capability | Complete coverage |\n",
    "| **Consistency** | Variable analyst results | Standardized ML processing | Reliable outcomes |\n",
    "| **Scalability** | Limited to analyst capacity | Distributed across workers | Horizontal scaling |\n",
    "| **Cost Structure** | External service dependencies | Infrastructure automation | Operational efficiency |\n",
    "| **Response Time** | Significant processing delays | Near real-time processing | Faster decision making |\n",
    "\n",
    "### **Enterprise NLP Pipeline Capabilities**\n",
    "\n",
    "The pipeline provides comprehensive text intelligence:\n",
    "\n",
    "1. **Content Classification and Routing**\n",
    "   - Automatically categorize incoming content by type and urgency\n",
    "   - Route high-priority issues to appropriate teams\n",
    "   - Identify trending topics and emerging issues\n",
    "\n",
    "2. **Customer Experience Analytics**\n",
    "   - Real-time sentiment monitoring across all channels\n",
    "   - Product satisfaction scoring and trend analysis\n",
    "   - Customer pain point identification and escalation\n",
    "\n",
    "3. **Competitive Intelligence**\n",
    "   - Brand mention analysis and competitive comparison\n",
    "   - Market sentiment tracking and trend identification\n",
    "   - Product feature feedback and improvement suggestions\n",
    "\n",
    "4. **Operational Efficiency**\n",
    "   - Automated content summarization for executive reports\n",
    "   - Key entity extraction for CRM enrichment\n",
    "   - Multi-language content processing for global operations\n",
    "\n",
    "## Architecture\n",
    "\n",
    "### **Ray Data NLP Processing Architecture**\n",
    "\n",
    "```\n",
    "Enterprise Text Sources (100K+ daily)\n",
    "├── Customer Reviews (50K)\n",
    "├── Support Tickets (15K) \n",
    "├── Social Media (25K)\n",
    "└── Documents (10K)\n",
    "         │\n",
    "         ▼\n",
    "┌─────────────────────────────────────────┐\n",
    "│           Ray Data Ingestion            │\n",
    "│  • read_text() • read_parquet()        │\n",
    "│  • from_huggingface() • read_json()    │\n",
    "│  • Distributed loading across cluster  │\n",
    "└─────────────────┬───────────────────────┘\n",
    "                  │\n",
    "                  ▼\n",
    "┌─────────────────────────────────────────┐\n",
    "│     Distributed Text Processing         │\n",
    "│  • map_batches() for vectorized ops    │\n",
    "│  • Parallel preprocessing across nodes │\n",
    "│  • Memory-efficient text cleaning      │\n",
    "│  • Automatic load balancing           │\n",
    "└─────────────────┬───────────────────────┘\n",
    "                  │\n",
    "                  ▼\n",
    "┌─────────────────────────────────────────┐\n",
    "│        Multi-Model NLP Analysis         │\n",
    "│  • BERT embeddings (GPU accelerated)   │\n",
    "│  • Sentiment analysis (transformer)    │\n",
    "│  • Topic modeling (LDA + clustering)   │\n",
    "│  • Named entity recognition (spaCy)    │\n",
    "│  • Text summarization (BART)          │\n",
    "│  • Language detection (multilingual)   │\n",
    "└─────────────────┬───────────────────────┘\n",
    "                  │\n",
    "                  ▼\n",
    "┌─────────────────────────────────────────┐\n",
    "│      Ray Data LLM Integration          │ (Optional)\n",
    "│  • Production LLM inference           │\n",
    "│  • Batch processing optimization      │\n",
    "│  • Structured prompt engineering      │\n",
    "│  • GPU resource management           │\n",
    "└─────────────────┬───────────────────────┘\n",
    "                  │\n",
    "                  ▼\n",
    "┌─────────────────────────────────────────┐\n",
    "│      Business Intelligence Layer        │\n",
    "│  • Aggregated insights and metrics     │\n",
    "│  • Interactive dashboards             │\n",
    "│  • Real-time alerts and notifications │\n",
    "│  • Executive reporting and analytics  │\n",
    "└─────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### **Ray Data Advantages for NLP**\n",
    "\n",
    "| Traditional NLP Approach | Ray Data NLP Approach | Business Impact |\n",
    "|---------------------------|----------------------|-----------------|\n",
    "| **Single-machine processing** | Distributed across 88+ CPU cores | 50x scale increase |\n",
    "| **Sequential model inference** | Parallel GPU acceleration | faster processing |\n",
    "| **Manual pipeline orchestration** | Native Ray Data operations | 80% less infrastructure code |\n",
    "| **Complex resource management** | Automatic scaling and load balancing | Zero ops overhead |\n",
    "| **Limited fault tolerance** | Built-in error recovery and retries | 99.9% pipeline reliability |\n",
    "\n",
    "## Key Components\n",
    "\n",
    "### 1. **Text Data Loading**\n",
    "- `ray.data.read_text()` for text files\n",
    "- `ray.data.read_parquet()` for structured text data\n",
    "- Custom readers for specific text formats\n",
    "- Text data validation and schema management\n",
    "\n",
    "### 2. **Text Preprocessing**\n",
    "- Text cleaning and normalization\n",
    "- Tokenization and stemming\n",
    "- Stop word removal and lemmatization\n",
    "- Language detection and encoding handling\n",
    "\n",
    "### 3. **NLP Model Integration**\n",
    "- Pre-trained language models (BERT, RoBERTa, GPT)\n",
    "- Custom model training and fine-tuning\n",
    "- Embedding generation and similarity analysis\n",
    "- Multi-language support and localization\n",
    "\n",
    "### 4. **Text Analytics**\n",
    "- Sentiment analysis and emotion detection\n",
    "- Topic modeling and clustering\n",
    "- Named entity recognition\n",
    "- Text classification and categorization\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Ray cluster with GPU support (recommended)\n",
    "- Python 3.8+ with NLP libraries\n",
    "- Access to text datasets\n",
    "- Basic understanding of NLP concepts and techniques\n",
    "\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "pip install ray[data] transformers torch\n",
    "pip install nltk spacy textblob\n",
    "pip install sentence-transformers scikit-learn\n",
    "pip install pandas numpy pyarrow\n",
    "```\n",
    "\n",
    "## 5-Minute Quick Start\n",
    "\n",
    "**Goal**: Analyze sentiment of real text data in 5 minutes\n",
    "\n",
    "### **Step 1: Setup on Anyscale (30 seconds)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0f882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ray cluster is already running on Anyscale\n",
    "import ray\n",
    "\n",
    "# Check cluster status (already connected)\n",
    "print('Connected to Anyscale Ray cluster!')\n",
    "print(f'Available resources: {ray.cluster_resources()}')\n",
    "\n",
    "# Install any missing packages if needed\n",
    "# !pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8e4bcb",
   "metadata": {},
   "source": [
    "### **Step 2: Load Real Text Data (1 minute)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c2b8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "# Create sample real movie reviews for quick demo\n",
    "real_reviews = [\n",
    "    \"This movie was absolutely fantastic! Great acting and plot.\",\n",
    "    \"Terrible film. Waste of time and money. Very disappointed.\",\n",
    "    \"Amazing cinematography and outstanding performances throughout.\",\n",
    "    \"The movie was okay, nothing special but entertaining enough.\",\n",
    "    \"Brilliant storytelling and incredible attention to detail.\"\n",
    "]\n",
    "\n",
    "# Convert to Ray dataset\n",
    "text_ds = ray.data.from_items([{\"text\": review, \"id\": i} for i, review in enumerate(real_reviews)])\n",
    "print(f\"Loaded {text_ds.count()} real movie reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3318954",
   "metadata": {},
   "source": [
    "### **Step 3: Run Sentiment Analysis (2 minutes)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e346c634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "class QuickSentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.sentiment_pipeline = pipeline(\"sentiment-analysis\", device=-1)  # CPU for speed\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        results = []\n",
    "        for item in batch:\n",
    "            try:\n",
    "                text = item[\"text\"]\n",
    "                sentiment = self.sentiment_pipeline(text[:512])[0]\n",
    "                results.append({\n",
    "                    **item,\n",
    "                    \"sentiment\": sentiment[\"label\"],\n",
    "                    \"confidence\": sentiment[\"score\"]\n",
    "                })\n",
    "            except Exception as e:\n",
    "                results.append({**item, \"error\": str(e)})\n",
    "        return results\n",
    "\n",
    "# Analyze sentiment\n",
    "sentiment_results = text_ds.map_batches(QuickSentimentAnalyzer(), batch_size=5)\n",
    "final_results = sentiment_results.take_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d65f639",
   "metadata": {},
   "source": [
    "### **Step 4: View Results (1 minute)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c06f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sentiment analysis results\n",
    "print(\"\\nSentiment Analysis Results:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for result in final_results:\n",
    "    text = result[\"text\"][:50] + \"...\" if len(result[\"text\"]) > 50 else result[\"text\"]\n",
    "    sentiment = result.get(\"sentiment\", \"ERROR\")\n",
    "    confidence = result.get(\"confidence\", 0)\n",
    "    \n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {sentiment} (confidence: {confidence:.2f})\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c72975a",
   "metadata": {},
   "source": [
    "## Interactive Text Analytics Visualizations\n",
    "\n",
    "Let's create stunning visualizations to analyze our text data:\n",
    "\n",
    "### Word Clouds and Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f1e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_visualizations(dataset):\n",
    "    \"\"\"Create comprehensive text analytics visualizations.\"\"\"\n",
    "    print(\"Creating text analytics visualizations...\")\n",
    "    \n",
    "    # Convert to pandas for visualization\n",
    "    text_df = dataset.to_pandas()\n",
    "    \n",
    "    # Get sentiment results\n",
    "    sentiment_df = pd.DataFrame(final_results)\n",
    "    \n",
    "    # Create comprehensive dashboard\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
    "    fig.suptitle('Text Analytics Dashboard', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Sentiment Distribution\n",
    "    ax1 = axes[0, 0]\n",
    "    if 'sentiment' in sentiment_df.columns:\n",
    "        sentiment_counts = sentiment_df['sentiment'].value_counts()\n",
    "        colors = ['green' if s == 'positive' else 'red' if s == 'negative' else 'gray' \n",
    "                 for s in sentiment_counts.index]\n",
    "        bars = ax1.bar(sentiment_counts.index, sentiment_counts.values, color=colors, alpha=0.7)\n",
    "        ax1.set_title('Sentiment Distribution', fontweight='bold')\n",
    "        ax1.set_ylabel('Number of Texts')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                    f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Text Length Distribution\n",
    "    ax2 = axes[0, 1]\n",
    "    text_lengths = text_df['length'].values\n",
    "    ax2.hist(text_lengths, bins=30, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "    ax2.set_title('Text Length Distribution', fontweight='bold')\n",
    "    ax2.set_xlabel('Character Count')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.axvline(np.mean(text_lengths), color='red', linestyle='--', \n",
    "               label=f'Mean: {np.mean(text_lengths):.1f}')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # 3. Word Cloud for Positive Sentiment\n",
    "    ax3 = axes[0, 2]\n",
    "    if 'sentiment' in sentiment_df.columns:\n",
    "        positive_texts = sentiment_df[sentiment_df['sentiment'] == 'positive']['text'].tolist()\n",
    "        if positive_texts:\n",
    "            positive_text = ' '.join(positive_texts)\n",
    "            wordcloud_pos = WordCloud(width=400, height=300, background_color='white',\n",
    "                                    colormap='Greens').generate(positive_text)\n",
    "            ax3.imshow(wordcloud_pos, interpolation='bilinear')\n",
    "            ax3.set_title('Positive Sentiment Word Cloud', fontweight='bold')\n",
    "            ax3.axis('off')\n",
    "    \n",
    "    # 4. Word Cloud for Negative Sentiment\n",
    "    ax4 = axes[1, 0]\n",
    "    if 'sentiment' in sentiment_df.columns:\n",
    "        negative_texts = sentiment_df[sentiment_df['sentiment'] == 'negative']['text'].tolist()\n",
    "        if negative_texts:\n",
    "            negative_text = ' '.join(negative_texts)\n",
    "            wordcloud_neg = WordCloud(width=400, height=300, background_color='white',\n",
    "                                    colormap='Reds').generate(negative_text)\n",
    "            ax4.imshow(wordcloud_neg, interpolation='bilinear')\n",
    "            ax4.set_title('Negative Sentiment Word Cloud', fontweight='bold')\n",
    "            ax4.axis('off')\n",
    "    \n",
    "    # 5. Most Common Words\n",
    "    ax5 = axes[1, 1]\n",
    "    all_text = ' '.join(text_df['text'].tolist())\n",
    "    # Simple word extraction (remove punctuation and convert to lowercase)\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', all_text.lower())\n",
    "    # Filter out common stop words\n",
    "    stop_words = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them', 'my', 'your', 'his', 'her', 'its', 'our', 'their'}\n",
    "    filtered_words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    if filtered_words:\n",
    "        word_counts = Counter(filtered_words).most_common(10)\n",
    "        words_list, counts_list = zip(*word_counts)\n",
    "        \n",
    "        bars = ax5.barh(range(len(words_list)), counts_list, color='lightcoral')\n",
    "        ax5.set_yticks(range(len(words_list)))\n",
    "        ax5.set_yticklabels(words_list)\n",
    "        ax5.set_title('Top 10 Most Common Words', fontweight='bold')\n",
    "        ax5.set_xlabel('Frequency')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, bar in enumerate(bars):\n",
    "            width = bar.get_width()\n",
    "            ax5.text(width + 0.5, bar.get_y() + bar.get_height()/2.,\n",
    "                    f'{int(width)}', ha='left', va='center', fontweight='bold')\n",
    "    \n",
    "    # 6. Text Complexity Analysis\n",
    "    ax6 = axes[1, 2]\n",
    "    if text_df['text'].notna().any():\n",
    "        # Calculate readability scores for a sample of texts\n",
    "        sample_texts = text_df['text'].dropna().head(100).tolist()\n",
    "        readability_scores = []\n",
    "        \n",
    "        for text in sample_texts:\n",
    "            try:\n",
    "                # Flesch Reading Ease Score (higher = easier to read)\n",
    "                score = textstat.flesch_reading_ease(text)\n",
    "                readability_scores.append(score)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if readability_scores:\n",
    "            ax6.hist(readability_scores, bins=20, color='lightgreen', alpha=0.7, edgecolor='black')\n",
    "            ax6.set_title('Text Readability Distribution', fontweight='bold')\n",
    "            ax6.set_xlabel('Flesch Reading Ease Score')\n",
    "            ax6.set_ylabel('Frequency')\n",
    "            ax6.axvline(np.mean(readability_scores), color='red', linestyle='--',\n",
    "                       label=f'Mean: {np.mean(readability_scores):.1f}')\n",
    "            ax6.legend()\n",
    "    \n",
    "    # 7. Sentiment by Text Length\n",
    "    ax7 = axes[2, 0]\n",
    "    if 'sentiment' in sentiment_df.columns and 'length' in text_df.columns:\n",
    "        # Merge sentiment with original text data\n",
    "        merged_df = pd.merge(sentiment_df, text_df, left_on='text', right_on='text', how='inner')\n",
    "        \n",
    "        sentiment_colors = {'positive': 'green', 'negative': 'red', 'neutral': 'gray'}\n",
    "        for sentiment in merged_df['sentiment'].unique():\n",
    "            sentiment_data = merged_df[merged_df['sentiment'] == sentiment]\n",
    "            ax7.scatter(sentiment_data['length'], [sentiment]*len(sentiment_data), \n",
    "                       c=sentiment_colors.get(sentiment, 'blue'), alpha=0.6, \n",
    "                       label=sentiment, s=30)\n",
    "        \n",
    "        ax7.set_title('Sentiment vs Text Length', fontweight='bold')\n",
    "        ax7.set_xlabel('Text Length (characters)')\n",
    "        ax7.set_ylabel('Sentiment')\n",
    "        ax7.legend()\n",
    "    \n",
    "    # 8. Character Distribution\n",
    "    ax8 = axes[2, 1]\n",
    "    char_counts = {}\n",
    "    for text in text_df['text'].head(100):  # Sample for performance\n",
    "        for char in text.lower():\n",
    "            if char.isalpha():\n",
    "                char_counts[char] = char_counts.get(char, 0) + 1\n",
    "    \n",
    "    if char_counts:\n",
    "        sorted_chars = sorted(char_counts.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "        chars, counts = zip(*sorted_chars)\n",
    "        \n",
    "        bars = ax8.bar(chars, counts, color='lightblue', alpha=0.7)\n",
    "        ax8.set_title('Character Frequency Distribution', fontweight='bold')\n",
    "        ax8.set_xlabel('Characters')\n",
    "        ax8.set_ylabel('Frequency')\n",
    "        ax8.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 9. Sentiment Confidence (if available)\n",
    "    ax9 = axes[2, 2]\n",
    "    if 'confidence' in sentiment_df.columns:\n",
    "        confidence_scores = sentiment_df['confidence'].dropna()\n",
    "        ax9.hist(confidence_scores, bins=20, color='orange', alpha=0.7, edgecolor='black')\n",
    "        ax9.set_title('Sentiment Confidence Distribution', fontweight='bold')\n",
    "        ax9.set_xlabel('Confidence Score')\n",
    "        ax9.set_ylabel('Frequency')\n",
    "    else:\n",
    "        # Show text categories distribution instead\n",
    "        if 'true_sentiment' in text_df.columns:\n",
    "            category_counts = text_df['true_sentiment'].value_counts()\n",
    "            ax9.pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%',\n",
    "                   colors=['lightgreen', 'lightcoral', 'lightgray'])\n",
    "            ax9.set_title('True Sentiment Distribution', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('text_analytics_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Text analytics dashboard saved as 'text_analytics_dashboard.png'\")\n",
    "\n",
    "# Create text visualizations\n",
    "create_text_visualizations(text_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada6bc4c",
   "metadata": {},
   "source": [
    "### Interactive Text Analytics Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f952c0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_text_dashboard(dataset, sentiment_results):\n",
    "    \"\"\"Create engaging interactive text analytics dashboard using Plotly.\"\"\"\n",
    "    print(\"Creating interactive text analytics dashboard...\")\n",
    "    \n",
    "    # Convert data for visualization\n",
    "    text_df = dataset.to_pandas()\n",
    "    sentiment_df = pd.DataFrame(sentiment_results)\n",
    "    \n",
    "    # Create comprehensive interactive dashboard\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=3,\n",
    "        subplot_titles=('Sentiment Distribution', 'Text Length Analysis', 'Confidence Scores',\n",
    "                       'Word Count Distribution', 'Sentiment vs Length', 'Top Keywords'),\n",
    "        specs=[[{\"type\": \"bar\"}, {\"type\": \"histogram\"}, {\"type\": \"histogram\"}],\n",
    "               [{\"type\": \"histogram\"}, {\"type\": \"scatter\"}, {\"type\": \"bar\"}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Sentiment Distribution (Business Insights)\n",
    "    if 'sentiment' in sentiment_df.columns:\n",
    "        sentiment_counts = sentiment_df['sentiment'].value_counts()\n",
    "        colors = ['#00CC96' if s == 'POSITIVE' else '#EF553B' if s == 'NEGATIVE' else '#636EFA' \n",
    "                 for s in sentiment_counts.index]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=sentiment_counts.index, y=sentiment_counts.values,\n",
    "                  marker_color=colors, name=\"Sentiment\"),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # 2. Text Length Distribution (Data Characteristics)\n",
    "    text_df['length'] = text_df['text'].str.len()\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=text_df['length'], nbinsx=30, marker_color='skyblue',\n",
    "                    name=\"Text Length\"),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Confidence Score Distribution (Model Performance)\n",
    "    if 'confidence' in sentiment_df.columns:\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=sentiment_df['confidence'], nbinsx=20, marker_color='lightgreen',\n",
    "                        name=\"Confidence\"),\n",
    "            row=1, col=3\n",
    "        )\n",
    "    \n",
    "    # 4. Word Count Analysis\n",
    "    text_df['word_count'] = text_df['text'].str.split().str.len()\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=text_df['word_count'], nbinsx=25, marker_color='orange',\n",
    "                    name=\"Word Count\"),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 5. Sentiment vs Text Length Relationship\n",
    "    if 'sentiment' in sentiment_df.columns:\n",
    "        merged_df = pd.merge(sentiment_df, text_df, on='text', how='inner')\n",
    "        for sentiment in merged_df['sentiment'].unique():\n",
    "            sentiment_data = merged_df[merged_df['sentiment'] == sentiment]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=sentiment_data['length'], y=sentiment_data['word_count'],\n",
    "                          mode='markers', name=sentiment, opacity=0.6),\n",
    "                row=2, col=2\n",
    "            )\n",
    "    \n",
    "    # 6. Top Keywords Analysis\n",
    "    from collections import Counter\n",
    "    import re\n",
    "    \n",
    "    all_text = ' '.join(text_df['text'].str.lower())\n",
    "    words = re.findall(r'\\b[a-zA-Z]{4,}\\b', all_text)  # Words 4+ chars\n",
    "    stop_words = {'this', 'that', 'with', 'have', 'will', 'from', 'they', 'been', 'said', 'each', 'which', 'their'}\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    if filtered_words:\n",
    "        word_counts = Counter(filtered_words).most_common(10)\n",
    "        words_list, counts_list = zip(*word_counts)\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=list(words_list), y=list(counts_list),\n",
    "                  marker_color='lightcoral', name=\"Top Keywords\"),\n",
    "            row=2, col=3\n",
    "        )\n",
    "    \n",
    "    # Update layout for professional appearance\n",
    "    fig.update_layout(\n",
    "        title_text=\"Text Analytics Dashboard - NLP Insights\",\n",
    "        height=800,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Show interactive dashboard\n",
    "    fig.show()\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"Interactive text analytics dashboard created!\")\n",
    "    print(\"Dashboard shows sentiment patterns, text characteristics, and key insights\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create interactive text analytics dashboard\n",
    "text_dashboard = create_interactive_text_dashboard(text_dataset, final_results)\n",
    "    \n",
    "    # 1. Sentiment Distribution\n",
    "    if 'sentiment' in sentiment_df.columns:\n",
    "        sentiment_counts = sentiment_df['sentiment'].value_counts()\n",
    "        colors = ['green' if s == 'positive' else 'red' if s == 'negative' else 'orange' \n",
    "                 for s in sentiment_counts.index]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=sentiment_counts.index, y=sentiment_counts.values,\n",
    "                  marker_color=colors, name=\"Sentiment\"),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # 2. Text Length Distribution\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=text_df['length'], nbinsx=30, marker_color='skyblue', \n",
    "                    name=\"Text Length\"),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Top Words Frequency\n",
    "    all_text = ' '.join(text_df['text'].tolist())\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', all_text.lower())\n",
    "    stop_words = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were'}\n",
    "    filtered_words = [word for word in words if word not in stop_words and len(word) > 3]\n",
    "    \n",
    "    if filtered_words:\n",
    "        word_counts = Counter(filtered_words).most_common(10)\n",
    "        words_list, counts_list = zip(*word_counts)\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=list(words_list), y=list(counts_list), \n",
    "                  marker_color='lightcoral', name=\"Word Frequency\"),\n",
    "            row=1, col=3\n",
    "        )\n",
    "    \n",
    "    # 4. Sentiment vs Text Length Scatter\n",
    "    if 'sentiment' in sentiment_df.columns:\n",
    "        merged_df = pd.merge(sentiment_df, text_df, left_on='text', right_on='text', how='inner')\n",
    "        \n",
    "        for sentiment in merged_df['sentiment'].unique():\n",
    "            sentiment_data = merged_df[merged_df['sentiment'] == sentiment]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=sentiment_data['length'], \n",
    "                          y=[sentiment]*len(sentiment_data),\n",
    "                          mode='markers', name=sentiment,\n",
    "                          marker=dict(size=8, opacity=0.6)),\n",
    "                row=2, col=1\n",
    "            )\n",
    "    \n",
    "    # 5. Readability Scores\n",
    "    sample_texts = text_df['text'].dropna().head(50).tolist()\n",
    "    readability_scores = []\n",
    "    \n",
    "    for text in sample_texts:\n",
    "        try:\n",
    "            score = textstat.flesch_reading_ease(text)\n",
    "            readability_scores.append(score)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if readability_scores:\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=readability_scores, nbinsx=15, marker_color='lightgreen',\n",
    "                        name=\"Readability\"),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # 6. Text Categories Pie Chart\n",
    "    if 'true_sentiment' in text_df.columns:\n",
    "        category_counts = text_df['true_sentiment'].value_counts()\n",
    "        fig.add_trace(\n",
    "            go.Pie(labels=category_counts.index, values=category_counts.values,\n",
    "                  name=\"Categories\"),\n",
    "            row=2, col=3\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title_text=\"Interactive Text Analytics Dashboard\",\n",
    "        height=800,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Update axes\n",
    "    fig.update_xaxes(title_text=\"Sentiment\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Text Length\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Frequency\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Words\", row=1, col=3)\n",
    "    fig.update_yaxes(title_text=\"Frequency\", row=1, col=3)\n",
    "    fig.update_xaxes(title_text=\"Text Length\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Sentiment\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Readability Score\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Frequency\", row=2, col=2)\n",
    "    \n",
    "    # Save and show\n",
    "    fig.write_html(\"interactive_text_dashboard.html\")\n",
    "    print(\"Interactive text dashboard saved as 'interactive_text_dashboard.html'\")\n",
    "    fig.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create interactive dashboard\n",
    "interactive_dashboard = create_interactive_text_dashboard(text_dataset)\n",
    "\n",
    "print(\"Quick start completed! Run the full demo for advanced NLP features.\")\n",
    "\n",
    "# Expected Output:\n",
    "# Sentiment Analysis Results:\n",
    "# --------------------------------------------------\n",
    "# Text: This movie was absolutely fantastic! Great acting...\n",
    "# Sentiment: POSITIVE (confidence: 0.95)\n",
    "# ------------------------------\n",
    "# Text: Terrible film. Waste of time and money. Very dis...\n",
    "# Sentiment: NEGATIVE (confidence: 0.92)\n",
    "# ------------------------------\n",
    "# Text: Amazing cinematography and outstanding performanc...\n",
    "# Sentiment: POSITIVE (confidence: 0.88)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe05aef",
   "metadata": {},
   "source": [
    "## Complete Tutorial\n",
    "\n",
    "### 1. **Load Real Text Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d25424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.data import read_text, read_parquet, from_huggingface\n",
    "\n",
    "# Ray cluster is already running on Anyscale\n",
    "print(f'Ray cluster resources: {ray.cluster_resources()}')\n",
    "\n",
    "# Load real text datasets using Ray Data native APIs\n",
    "# Use Ray Data's native Hugging Face integration\n",
    "imdb_reviews = from_huggingface(\"imdb\", split=\"train[:1000]\")\n",
    "print(f\"IMDB Reviews: {imdb_reviews.count()}\")\n",
    "\n",
    "# Load Amazon reviews using native read_parquet\n",
    "amazon_reviews = read_parquet(\n",
    "    \"s3://amazon-reviews-pds/parquet/product_category=Books/\",\n",
    "    columns=[\"review_body\", \"star_rating\"]\n",
    ").limit(500)\n",
    "print(f\"Amazon Reviews: {amazon_reviews.count()}\")\n",
    "\n",
    "# Inspect sample data\n",
    "sample_review = imdb_reviews.take(1)[0]\n",
    "print(f\"Sample review: {sample_review['text'][:100]}...\")\n",
    "print(f\"Sample label: {sample_review['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd46c9b",
   "metadata": {},
   "source": [
    "### 2. **Text Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837270cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"Preprocess text data for NLP analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Download required NLTK data\n",
    "        try:\n",
    "            nltk.download('punkt', quiet=True)\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            nltk.download('wordnet', quiet=True)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Preprocess a batch of text documents.\"\"\"\n",
    "        processed_texts = []\n",
    "        \n",
    "        for text in batch[\"text\"]:\n",
    "            try:\n",
    "                # Clean and normalize text\n",
    "                cleaned_text = self._clean_text(text)\n",
    "                \n",
    "                # Tokenize\n",
    "                tokens = self._tokenize(cleaned_text)\n",
    "                \n",
    "                # Remove stop words and lemmatize\n",
    "                processed_tokens = self._process_tokens(tokens)\n",
    "                \n",
    "                # Join tokens back into text\n",
    "                processed_text = \" \".join(processed_tokens)\n",
    "                \n",
    "                processed_texts.append({\n",
    "                    \"original_text\": text,\n",
    "                    \"processed_text\": processed_text,\n",
    "                    \"token_count\": len(processed_tokens),\n",
    "                    \"cleaned_length\": len(processed_text)\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error preprocessing text: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return {\"processed_texts\": processed_texts}\n",
    "    \n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"Clean and normalize text.\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters and extra whitespace\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Remove numbers (optional)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def _tokenize(self, text):\n",
    "        \"\"\"Tokenize text into words.\"\"\"\n",
    "        return nltk.word_tokenize(text)\n",
    "    \n",
    "    def _process_tokens(self, tokens):\n",
    "        \"\"\"Remove stop words and lemmatize tokens.\"\"\"\n",
    "        processed = []\n",
    "        for token in tokens:\n",
    "            if token not in self.stop_words and len(token) > 2:\n",
    "                lemmatized = self.lemmatizer.lemmatize(token)\n",
    "                processed.append(lemmatized)\n",
    "        return processed\n",
    "\n",
    "# Apply text preprocessing\n",
    "processed_texts = reviews_ds.map_batches(\n",
    "    TextPreprocessor(),\n",
    "    batch_size=100,\n",
    "    concurrency=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20bf756",
   "metadata": {},
   "source": [
    "### 3. **Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19f65ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    \"\"\"Perform sentiment analysis using pre-trained models.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Load sentiment analysis pipeline\n",
    "        self.sentiment_pipeline = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "            device=0 if torch.cuda.is_available() else -1\n",
    "        )\n",
    "        \n",
    "        # Load emotion detection pipeline\n",
    "        self.emotion_pipeline = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
    "            device=0 if torch.cuda.is_available() else -1\n",
    "        )\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Analyze sentiment and emotions in text batch.\"\"\"\n",
    "        sentiment_results = []\n",
    "        \n",
    "        for item in batch[\"processed_texts\"]:\n",
    "            try:\n",
    "                text = item[\"processed_text\"]\n",
    "                \n",
    "                # Perform sentiment analysis\n",
    "                sentiment_result = self.sentiment_pipeline(text[:512])[0]\n",
    "                \n",
    "                # Perform emotion detection\n",
    "                emotion_result = self.emotion_pipeline(text[:512])[0]\n",
    "                \n",
    "                # Combine results\n",
    "                result = {\n",
    "                    \"id\": item.get(\"id\"),\n",
    "                    \"original_text\": item[\"original_text\"],\n",
    "                    \"processed_text\": text,\n",
    "                    \"sentiment\": sentiment_result[\"label\"],\n",
    "                    \"sentiment_score\": sentiment_result[\"score\"],\n",
    "                    \"emotion\": emotion_result[\"label\"],\n",
    "                    \"emotion_score\": emotion_result[\"score\"],\n",
    "                    \"token_count\": item[\"token_count\"]\n",
    "                }\n",
    "                \n",
    "                sentiment_results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in sentiment analysis: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return {\"sentiment_results\": sentiment_results}\n",
    "\n",
    "# Apply sentiment analysis\n",
    "sentiment_analysis = processed_texts.map_batches(\n",
    "    SentimentAnalyzer(),\n",
    "    batch_size=32,\n",
    "    num_gpus=1 if ray.cluster_resources().get(\"GPU\", 0) > 0 else 0,\n",
    "    concurrency=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec6f211",
   "metadata": {},
   "source": [
    "### 4. **Topic Modeling and Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f4879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "class TopicModeler:\n",
    "    \"\"\"Perform topic modeling and text clustering.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_topics=10, num_clusters=5):\n",
    "        self.num_topics = num_topics\n",
    "        self.num_clusters = num_clusters\n",
    "        \n",
    "        # Initialize TF-IDF vectorizer\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=1000,\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 2)\n",
    "        )\n",
    "        \n",
    "        # Initialize topic model\n",
    "        self.lda_model = LatentDirichletAllocation(\n",
    "            n_components=num_topics,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Initialize clustering model\n",
    "        self.kmeans_model = KMeans(\n",
    "            n_clusters=num_clusters,\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Perform topic modeling and clustering on text batch.\"\"\"\n",
    "        try:\n",
    "            texts = [item[\"processed_text\"] for item in batch[\"sentiment_results\"]]\n",
    "            \n",
    "            if not texts:\n",
    "                return {\"topic_results\": []}\n",
    "            \n",
    "            # Vectorize texts\n",
    "            tfidf_matrix = self.vectorizer.fit_transform(texts)\n",
    "            \n",
    "            # Topic modeling\n",
    "            topic_distributions = self.lda_model.fit_transform(tfidf_matrix)\n",
    "            \n",
    "            # Clustering\n",
    "            cluster_labels = self.kmeans_model.fit_predict(tfidf_matrix)\n",
    "            \n",
    "            # Get feature names for topic interpretation\n",
    "            feature_names = self.vectorizer.get_feature_names_out()\n",
    "            \n",
    "            # Extract top words for each topic\n",
    "            top_words_per_topic = []\n",
    "            for topic_idx, topic in enumerate(self.lda_model.components_):\n",
    "                top_words_idx = topic.argsort()[-10:][::-1]\n",
    "                top_words = [feature_names[i] for i in top_words_idx]\n",
    "                top_words_per_topic.append(top_words)\n",
    "            \n",
    "            # Combine results\n",
    "            topic_results = []\n",
    "            for i, item in enumerate(batch[\"sentiment_results\"]):\n",
    "                result = {\n",
    "                    **item,\n",
    "                    \"topic_distribution\": topic_distributions[i].tolist(),\n",
    "                    \"dominant_topic\": int(np.argmax(topic_distributions[i])),\n",
    "                    \"topic_confidence\": float(np.max(topic_distributions[i])),\n",
    "                    \"cluster_label\": int(cluster_labels[i]),\n",
    "                    \"top_topic_words\": top_words_per_topic[np.argmax(topic_distributions[i])]\n",
    "                }\n",
    "                topic_results.append(result)\n",
    "            \n",
    "            return {\"topic_results\": topic_results}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in topic modeling: {e}\")\n",
    "            return {\"topic_results\": []}\n",
    "\n",
    "# Apply topic modeling\n",
    "topic_modeling = sentiment_analysis.map_batches(\n",
    "    TopicModeler(num_topics=8, num_clusters=4),\n",
    "    batch_size=100,\n",
    "    concurrency=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5826f4f",
   "metadata": {},
   "source": [
    "### 5. **Named Entity Recognition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70f2b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import defaultdict\n",
    "\n",
    "class NERExtractor:\n",
    "    \"\"\"Extract named entities from text.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Load English language model\n",
    "        try:\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        except:\n",
    "            # Download if not available\n",
    "            import os\n",
    "            os.system(\"python -m spacy download en_core_web_sm\")\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Extract named entities from text batch.\"\"\"\n",
    "        ner_results = []\n",
    "        \n",
    "        for item in batch[\"topic_results\"]:\n",
    "            try:\n",
    "                text = item[\"processed_text\"]\n",
    "                \n",
    "                # Process text with spaCy\n",
    "                doc = self.nlp(text)\n",
    "                \n",
    "                # Extract entities by type\n",
    "                entities = defaultdict(list)\n",
    "                for ent in doc.ents:\n",
    "                    entities[ent.label_].append({\n",
    "                        \"text\": ent.text,\n",
    "                        \"start\": ent.start_char,\n",
    "                        \"end\": ent.end_char,\n",
    "                        \"confidence\": ent.label_\n",
    "                    })\n",
    "                \n",
    "                # Count entities\n",
    "                entity_counts = {label: len(ents) for label, ents in entities.items()}\n",
    "                \n",
    "                # Add NER results\n",
    "                result = {\n",
    "                    **item,\n",
    "                    \"entities\": dict(entities),\n",
    "                    \"entity_counts\": entity_counts,\n",
    "                    \"total_entities\": sum(entity_counts.values())\n",
    "                }\n",
    "                \n",
    "                ner_results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in NER extraction: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return {\"ner_results\": ner_results}\n",
    "\n",
    "# Apply NER extraction\n",
    "ner_extraction = topic_modeling.map_batches(\n",
    "    NERExtractor(),\n",
    "    batch_size=50,\n",
    "    concurrency=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ad7fe5",
   "metadata": {},
   "source": [
    "### 6. **Advanced NLP Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0297f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "class AdvancedNLPProcessor:\n",
    "    \"\"\"Perform advanced NLP tasks including summarization, language detection, and classification.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize multiple NLP pipelines\n",
    "        self.summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=-1)\n",
    "        self.classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\", device=-1)\n",
    "        self.language_detector = pipeline(\"text-classification\", model=\"papluca/xlm-roberta-base-language-detection\", device=-1)\n",
    "        \n",
    "        # Question answering pipeline\n",
    "        self.qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\", device=-1)\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Apply advanced NLP processing to text batch.\"\"\"\n",
    "        advanced_results = []\n",
    "        \n",
    "        for item in batch[\"ner_results\"]:\n",
    "            try:\n",
    "                text = item[\"processed_text\"]\n",
    "                original_text = item[\"original_text\"]\n",
    "                \n",
    "                # Text summarization (for longer texts)\n",
    "                summary = \"\"\n",
    "                if len(original_text) > 500:\n",
    "                    try:\n",
    "                        summary_result = self.summarizer(original_text[:1024], max_length=150, min_length=30, do_sample=False)\n",
    "                        summary = summary_result[0]['summary_text']\n",
    "                    except Exception as e:\n",
    "                        summary = f\"Summarization failed: {str(e)}\"\n",
    "                \n",
    "                # Language detection\n",
    "                try:\n",
    "                    language_result = self.language_detector(text[:512])\n",
    "                    detected_language = language_result[0]['label']\n",
    "                    language_confidence = language_result[0]['score']\n",
    "                except Exception as e:\n",
    "                    detected_language = \"unknown\"\n",
    "                    language_confidence = 0.0\n",
    "                \n",
    "                # Text classification (additional classification beyond sentiment)\n",
    "                try:\n",
    "                    classification_result = self.classifier(text[:512])\n",
    "                    text_category = classification_result[0]['label']\n",
    "                    category_confidence = classification_result[0]['score']\n",
    "                except Exception as e:\n",
    "                    text_category = \"unknown\"\n",
    "                    category_confidence = 0.0\n",
    "                \n",
    "                # Question answering (example questions)\n",
    "                qa_results = []\n",
    "                sample_questions = [\n",
    "                    \"What is the main topic?\",\n",
    "                    \"What is the sentiment?\",\n",
    "                    \"Who is mentioned?\"\n",
    "                ]\n",
    "                \n",
    "                for question in sample_questions:\n",
    "                    try:\n",
    "                        qa_result = self.qa_pipeline(question=question, context=original_text[:512])\n",
    "                        qa_results.append({\n",
    "                            \"question\": question,\n",
    "                            \"answer\": qa_result['answer'],\n",
    "                            \"confidence\": qa_result['score']\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        qa_results.append({\n",
    "                            \"question\": question,\n",
    "                            \"answer\": \"N/A\",\n",
    "                            \"confidence\": 0.0\n",
    "                        })\n",
    "                \n",
    "                # Text readability and complexity metrics\n",
    "                word_count = len(text.split())\n",
    "                sentence_count = len([s for s in original_text.split('.') if s.strip()])\n",
    "                avg_word_length = sum(len(word) for word in text.split()) / word_count if word_count > 0 else 0\n",
    "                \n",
    "                advanced_result = {\n",
    "                    **item,\n",
    "                    \"summary\": summary,\n",
    "                    \"detected_language\": detected_language,\n",
    "                    \"language_confidence\": language_confidence,\n",
    "                    \"text_category\": text_category,\n",
    "                    \"category_confidence\": category_confidence,\n",
    "                    \"qa_results\": qa_results,\n",
    "                    \"readability_metrics\": {\n",
    "                        \"word_count\": word_count,\n",
    "                        \"sentence_count\": sentence_count,\n",
    "                        \"avg_word_length\": avg_word_length,\n",
    "                        \"avg_sentence_length\": word_count / sentence_count if sentence_count > 0 else 0\n",
    "                    },\n",
    "                    \"advanced_processing_timestamp\": pd.Timestamp.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                advanced_results.append(advanced_result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in advanced NLP processing: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return {\"advanced_nlp_results\": advanced_results}\n",
    "\n",
    "# Apply advanced NLP processing\n",
    "advanced_nlp = ner_extraction.map_batches(\n",
    "    AdvancedNLPProcessor(),\n",
    "    batch_size=16,\n",
    "    concurrency=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc170b83",
   "metadata": {},
   "source": [
    "### 7. **Ray Data LLM Package Integration (Optional)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd09ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Use Ray Data LLM package for large-scale language model inference\n",
    "try:\n",
    "    from ray.data.llm import LLMPredictor\n",
    "    \n",
    "    class LLMTextAnalyzer:\n",
    "        \"\"\"Use Ray Data LLM package for advanced text analysis.\"\"\"\n",
    "        \n",
    "        def __init__(self, model_name=\"microsoft/DialoGPT-medium\"):\n",
    "            \"\"\"Initialize LLM predictor for text analysis.\"\"\"\n",
    "            self.model_name = model_name\n",
    "            \n",
    "            # Initialize LLM predictor with Ray Data LLM package\n",
    "            self.llm_predictor = LLMPredictor.from_checkpoint(\n",
    "                checkpoint=model_name,\n",
    "                torch_dtype=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "        \n",
    "        def __call__(self, batch):\n",
    "            \"\"\"Perform LLM-based text analysis.\"\"\"\n",
    "            llm_results = []\n",
    "            \n",
    "            for item in batch[\"advanced_nlp_results\"]:\n",
    "                try:\n",
    "                    original_text = item[\"original_text\"]\n",
    "                    \n",
    "                    # Create prompts for different analysis tasks\n",
    "                    analysis_prompts = [\n",
    "                        f\"Analyze the sentiment and key themes in this text: {original_text[:500]}\",\n",
    "                        f\"Extract the main topics and entities from: {original_text[:500]}\",\n",
    "                        f\"Provide a brief summary of: {original_text[:500]}\"\n",
    "                    ]\n",
    "                    \n",
    "                    llm_analyses = []\n",
    "                    for prompt in analysis_prompts:\n",
    "                        try:\n",
    "                            # Use LLM predictor for inference\n",
    "                            response = self.llm_predictor.predict(prompt)\n",
    "                            llm_analyses.append({\n",
    "                                \"prompt_type\": prompt.split(':')[0],\n",
    "                                \"response\": response,\n",
    "                                \"prompt_length\": len(prompt)\n",
    "                            })\n",
    "                        except Exception as e:\n",
    "                            llm_analyses.append({\n",
    "                                \"prompt_type\": prompt.split(':')[0],\n",
    "                                \"response\": f\"LLM inference failed: {str(e)}\",\n",
    "                                \"error\": True\n",
    "                            })\n",
    "                    \n",
    "                    llm_result = {\n",
    "                        **item,\n",
    "                        \"llm_analyses\": llm_analyses,\n",
    "                        \"llm_model_used\": self.model_name,\n",
    "                        \"llm_processing_timestamp\": pd.Timestamp.now().isoformat()\n",
    "                    }\n",
    "                    \n",
    "                    llm_results.append(llm_result)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in LLM processing: {e}\")\n",
    "                    llm_results.append({\n",
    "                        **item,\n",
    "                        \"llm_error\": str(e)\n",
    "                    })\n",
    "            \n",
    "            return {\"llm_results\": llm_results}\n",
    "    \n",
    "    # Apply LLM analysis (optional - requires Ray Data LLM package)\n",
    "    llm_analysis = advanced_nlp.map_batches(\n",
    "        LLMTextAnalyzer(),\n",
    "        batch_size=8,  # Smaller batch for LLM processing\n",
    "        num_gpus=1 if ray.cluster_resources().get(\"GPU\", 0) > 0 else 0,\n",
    "        concurrency=1  # Single concurrency for LLM to avoid resource conflicts\n",
    "    )\n",
    "    \n",
    "    print(\"LLM analysis completed using Ray Data LLM package\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Ray Data LLM package not available. Skipping LLM analysis.\")\n",
    "    print(\"To use LLM features, install with: pip install ray[data,llm]\")\n",
    "    llm_analysis = advanced_nlp\n",
    "\n",
    "# Alternative: Simple LLM integration without Ray Data LLM package\n",
    "class SimpleLLMProcessor:\n",
    "    \"\"\"Simple LLM integration using transformers directly.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        from transformers import pipeline\n",
    "        \n",
    "        # Initialize text generation pipeline\n",
    "        self.text_generator = pipeline(\n",
    "            \"text-generation\", \n",
    "            model=\"gpt2\", \n",
    "            device=-1,\n",
    "            max_length=100\n",
    "        )\n",
    "        \n",
    "        # Initialize summarization pipeline\n",
    "        self.summarizer = pipeline(\n",
    "            \"summarization\",\n",
    "            model=\"facebook/bart-large-cnn\",\n",
    "            device=-1\n",
    "        )\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Apply simple LLM processing.\"\"\"\n",
    "        simple_llm_results = []\n",
    "        \n",
    "        for item in batch[\"advanced_nlp_results\"]:\n",
    "            try:\n",
    "                text = item[\"original_text\"]\n",
    "                \n",
    "                # Text summarization\n",
    "                summary = \"\"\n",
    "                if len(text) > 200:\n",
    "                    try:\n",
    "                        summary_result = self.summarizer(text[:1024], max_length=100, min_length=30)\n",
    "                        summary = summary_result[0]['summary_text']\n",
    "                    except Exception as e:\n",
    "                        summary = f\"Summarization error: {str(e)}\"\n",
    "                \n",
    "                # Simple text generation\n",
    "                generation_prompt = f\"Based on this text: {text[:100]}... the key insight is\"\n",
    "                try:\n",
    "                    generation_result = self.text_generator(generation_prompt, max_length=50, num_return_sequences=1)\n",
    "                    generated_insight = generation_result[0]['generated_text'][len(generation_prompt):].strip()\n",
    "                except Exception as e:\n",
    "                    generated_insight = f\"Generation error: {str(e)}\"\n",
    "                \n",
    "                simple_llm_result = {\n",
    "                    **item,\n",
    "                    \"text_summary\": summary,\n",
    "                    \"generated_insight\": generated_insight,\n",
    "                    \"llm_processing_method\": \"transformers_direct\",\n",
    "                    \"llm_timestamp\": pd.Timestamp.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                simple_llm_results.append(simple_llm_result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in simple LLM processing: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return {\"simple_llm_results\": simple_llm_results}\n",
    "\n",
    "# Apply simple LLM processing as alternative\n",
    "simple_llm_analysis = advanced_nlp.map_batches(\n",
    "    SimpleLLMProcessor(),\n",
    "    batch_size=8,\n",
    "    concurrency=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3e19fc",
   "metadata": {},
   "source": [
    "### 8. **Text Similarity and Semantic Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3454f678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "class TextSimilarityAnalyzer:\n",
    "    \"\"\"Analyze text similarity and enable semantic search.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.similarity_threshold = 0.7\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Calculate text similarity within batch.\"\"\"\n",
    "        similarity_results = []\n",
    "        \n",
    "        # Extract embeddings from batch\n",
    "        embeddings = []\n",
    "        texts = []\n",
    "        items = []\n",
    "        \n",
    "        for item in batch.get(\"embedding_results\", []):\n",
    "            if \"embeddings\" in item and item[\"embeddings\"]:\n",
    "                embeddings.append(np.array(item[\"embeddings\"]))\n",
    "                texts.append(item[\"processed_text\"])\n",
    "                items.append(item)\n",
    "        \n",
    "        if len(embeddings) < 2:\n",
    "            return {\"similarity_results\": items}\n",
    "        \n",
    "        # Calculate pairwise similarities\n",
    "        embeddings_matrix = np.array(embeddings)\n",
    "        similarity_matrix = cosine_similarity(embeddings_matrix)\n",
    "        \n",
    "        # Find similar text pairs\n",
    "        for i, item in enumerate(items):\n",
    "            similar_texts = []\n",
    "            \n",
    "            for j, other_item in enumerate(items):\n",
    "                if i != j and similarity_matrix[i][j] > self.similarity_threshold:\n",
    "                    similar_texts.append({\n",
    "                        \"similar_text_id\": other_item.get(\"document_id\", j),\n",
    "                        \"similarity_score\": float(similarity_matrix[i][j]),\n",
    "                        \"similar_text_preview\": other_item[\"processed_text\"][:100]\n",
    "                    })\n",
    "            \n",
    "            # Sort by similarity score\n",
    "            similar_texts.sort(key=lambda x: x[\"similarity_score\"], reverse=True)\n",
    "            \n",
    "            result = {\n",
    "                **item,\n",
    "                \"similar_texts\": similar_texts[:5],  # Top 5 similar texts\n",
    "                \"similarity_count\": len(similar_texts),\n",
    "                \"has_similar_content\": len(similar_texts) > 0\n",
    "            }\n",
    "            \n",
    "            similarity_results.append(result)\n",
    "        \n",
    "        return {\"similarity_results\": similarity_results}\n",
    "\n",
    "# Apply similarity analysis\n",
    "similarity_analysis = ner_extraction.map_batches(\n",
    "    TextSimilarityAnalyzer(),\n",
    "    batch_size=20,\n",
    "    concurrency=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d03a85",
   "metadata": {},
   "source": [
    "### 9. **Language Detection and Multilingual Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74385c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "class MultilingualProcessor:\n",
    "    \"\"\"Handle multilingual text processing.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Language detection pipeline\n",
    "        self.language_detector = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=\"papluca/xlm-roberta-base-language-detection\",\n",
    "            device=-1\n",
    "        )\n",
    "        \n",
    "        # Multilingual sentiment analysis\n",
    "        self.multilingual_sentiment = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n",
    "            device=-1\n",
    "        )\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Process multilingual text.\"\"\"\n",
    "        multilingual_results = []\n",
    "        \n",
    "        for item in batch[\"similarity_results\"]:\n",
    "            try:\n",
    "                text = item[\"processed_text\"]\n",
    "                original_text = item[\"original_text\"]\n",
    "                \n",
    "                # Detect language\n",
    "                try:\n",
    "                    lang_result = self.language_detector(text[:512])\n",
    "                    detected_language = lang_result[0]['label']\n",
    "                    language_confidence = lang_result[0]['score']\n",
    "                except Exception as e:\n",
    "                    detected_language = \"unknown\"\n",
    "                    language_confidence = 0.0\n",
    "                \n",
    "                # Multilingual sentiment analysis\n",
    "                try:\n",
    "                    multilingual_sentiment = self.multilingual_sentiment(text[:512])\n",
    "                    ml_sentiment = multilingual_sentiment[0]['label']\n",
    "                    ml_sentiment_score = multilingual_sentiment[0]['score']\n",
    "                except Exception as e:\n",
    "                    ml_sentiment = \"unknown\"\n",
    "                    ml_sentiment_score = 0.0\n",
    "                \n",
    "                # Text complexity analysis\n",
    "                complexity_metrics = {\n",
    "                    \"character_count\": len(original_text),\n",
    "                    \"word_count\": len(text.split()),\n",
    "                    \"unique_words\": len(set(text.split())),\n",
    "                    \"lexical_diversity\": len(set(text.split())) / len(text.split()) if len(text.split()) > 0 else 0,\n",
    "                    \"avg_word_length\": sum(len(word) for word in text.split()) / len(text.split()) if len(text.split()) > 0 else 0\n",
    "                }\n",
    "                \n",
    "                multilingual_result = {\n",
    "                    **item,\n",
    "                    \"detected_language\": detected_language,\n",
    "                    \"language_confidence\": language_confidence,\n",
    "                    \"multilingual_sentiment\": ml_sentiment,\n",
    "                    \"multilingual_sentiment_score\": ml_sentiment_score,\n",
    "                    \"complexity_metrics\": complexity_metrics,\n",
    "                    \"is_english\": detected_language.lower() in ['en', 'english'],\n",
    "                    \"is_complex_text\": complexity_metrics[\"lexical_diversity\"] > 0.6,\n",
    "                    \"multilingual_timestamp\": pd.Timestamp.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                multilingual_results.append(multilingual_result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in multilingual processing: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return {\"multilingual_results\": multilingual_results}\n",
    "\n",
    "# Apply multilingual processing\n",
    "multilingual_analysis = similarity_analysis.map_batches(\n",
    "    MultilingualProcessor(),\n",
    "    batch_size=16,\n",
    "    concurrency=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e0222b",
   "metadata": {},
   "source": [
    "### 10. **Ray Data LLM Package for Production Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70866d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production-scale LLM inference using Ray Data LLM package\n",
    "try:\n",
    "    from ray.data.llm import LLMPredictor\n",
    "    \n",
    "    class ProductionLLMAnalyzer:\n",
    "        \"\"\"Production-scale LLM analysis using Ray Data LLM package.\"\"\"\n",
    "        \n",
    "        def __init__(self, model_name=\"microsoft/DialoGPT-small\"):\n",
    "            \"\"\"Initialize production LLM analyzer.\"\"\"\n",
    "            self.model_name = model_name\n",
    "            \n",
    "            # Configure LLM predictor for production use\n",
    "            self.llm_predictor = LLMPredictor.from_checkpoint(\n",
    "                checkpoint=model_name,\n",
    "                torch_dtype=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                max_batch_size=16,\n",
    "                max_concurrent_requests=4\n",
    "            )\n",
    "        \n",
    "        def __call__(self, batch):\n",
    "            \"\"\"Perform production LLM inference.\"\"\"\n",
    "            production_results = []\n",
    "            \n",
    "            # Prepare prompts for batch inference\n",
    "            prompts = []\n",
    "            items = []\n",
    "            \n",
    "            for item in batch[\"multilingual_results\"]:\n",
    "                text = item[\"original_text\"]\n",
    "                \n",
    "                # Create structured prompt for analysis\n",
    "                analysis_prompt = f\"\"\"\n",
    "                Analyze this text and provide insights:\n",
    "                \n",
    "                Text: {text[:800]}\n",
    "                \n",
    "                Please provide:\n",
    "                1. Main theme\n",
    "                2. Key entities\n",
    "                3. Emotional tone\n",
    "                4. Business relevance\n",
    "                \"\"\"\n",
    "                \n",
    "                prompts.append(analysis_prompt)\n",
    "                items.append(item)\n",
    "            \n",
    "            # Batch LLM inference using Ray Data LLM package\n",
    "            try:\n",
    "                llm_responses = self.llm_predictor.predict_batch(prompts)\n",
    "                \n",
    "                for item, response in zip(items, llm_responses):\n",
    "                    production_result = {\n",
    "                        **item,\n",
    "                        \"llm_analysis\": response,\n",
    "                        \"llm_model\": self.model_name,\n",
    "                        \"llm_method\": \"ray_data_llm_package\",\n",
    "                        \"production_timestamp\": pd.Timestamp.now().isoformat()\n",
    "                    }\n",
    "                    production_results.append(production_result)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"LLM batch inference failed: {e}\")\n",
    "                # Fallback to individual processing\n",
    "                for item in items:\n",
    "                    production_results.append({\n",
    "                        **item,\n",
    "                        \"llm_analysis\": \"LLM processing unavailable\",\n",
    "                        \"llm_error\": str(e)\n",
    "                    })\n",
    "            \n",
    "            return {\"production_llm_results\": production_results}\n",
    "    \n",
    "    # Apply production LLM analysis\n",
    "    production_llm = multilingual_analysis.map_batches(\n",
    "        ProductionLLMAnalyzer(),\n",
    "        batch_size=8,\n",
    "        num_gpus=1 if ray.cluster_resources().get(\"GPU\", 0) > 0 else 0,\n",
    "        concurrency=1\n",
    "    )\n",
    "    \n",
    "    print(\"Production LLM analysis completed using Ray Data LLM package\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Ray Data LLM package not available.\")\n",
    "    print(\"To use production LLM features, install with: pip install ray[data] vllm\")\n",
    "    production_llm = multilingual_analysis\n",
    "\n",
    "# Final results collection\n",
    "final_nlp_results = production_llm.take_all()\n",
    "print(f\"Complete NLP pipeline processed {len(final_nlp_results)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec9665a",
   "metadata": {},
   "source": [
    "## Advanced Features\n",
    "\n",
    "### **Ray Data's NLP Superpowers**\n",
    "\n",
    "**1. Distributed Model Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2099618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process 100K documents across cluster with automatic load balancing\n",
    "large_dataset.map_batches(\n",
    "    TransformerModel(), \n",
    "    batch_size=64,      # Optimal GPU utilization\n",
    "    concurrency=8,      # Parallel model instances\n",
    "    num_gpus=1          # GPU acceleration per instance\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8262a93c",
   "metadata": {},
   "source": [
    "**2. Memory-Efficient Text Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90905e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle datasets larger than cluster memory\n",
    "massive_text_dataset.map_batches(\n",
    "    TextProcessor(),\n",
    "    batch_size=1000,    # Process in manageable chunks\n",
    "    concurrency=16      # Maximize CPU utilization\n",
    ")\n",
    "# Ray Data automatically manages memory and spilling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7916b0c",
   "metadata": {},
   "source": [
    "**3. Fault-Tolerant NLP Pipelines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af4837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in error recovery for production NLP\n",
    "nlp_pipeline = (dataset\n",
    "    .map_batches(TextCleaner())      # Automatic retry on failures\n",
    "    .map_batches(SentimentAnalyzer()) # Worker failure recovery\n",
    "    .map_batches(TopicModeler())     # Data block replication\n",
    ")\n",
    "# No additional fault tolerance code needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd78ff92",
   "metadata": {},
   "source": [
    "**4. Seamless Model Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bb25b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale from single GPU to multi-GPU automatically\n",
    "if ray.cluster_resources().get(\"GPU\", 0) >= 4:\n",
    "    # Multi-GPU configuration\n",
    "    concurrency = 4\n",
    "    num_gpus = 1\n",
    "else:\n",
    "    # CPU fallback\n",
    "    concurrency = 8\n",
    "    num_gpus = 0\n",
    "\n",
    "# Ray Data handles resource allocation automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a6e32d",
   "metadata": {},
   "source": [
    "### **Enterprise NLP Use Case Patterns**\n",
    "\n",
    "**Customer Feedback Analysis Pipeline**\n",
    "- **Scale**: Process 50K+ daily reviews\n",
    "- **Speed**: 2-hour processing vs 40+ hours manual\n",
    "- **Accuracy**: Consistent analysis across all content\n",
    "- **Insights**: Product satisfaction, feature requests, issue identification\n",
    "\n",
    "**Support Ticket Intelligence**\n",
    "- **Automation**: 90% tickets auto-classified and routed\n",
    "- **Response Time**: 15-minute vs 24-hour issue identification\n",
    "- **Efficiency**: reduction in manual ticket triage\n",
    "- **Quality**: Consistent urgency and intent classification\n",
    "\n",
    "**Brand Monitoring at Scale**\n",
    "- **Coverage**: 100% social media mention analysis\n",
    "- **Real-time**: Immediate sentiment tracking and alerts\n",
    "- **Competitive**: Multi-brand comparison and positioning\n",
    "- **Actionable**: Trend identification and response recommendations\n",
    "\n",
    "## Production Considerations\n",
    "\n",
    "### **Model Management**\n",
    "- Model versioning and deployment\n",
    "- A/B testing for model performance\n",
    "- Model monitoring and retraining\n",
    "\n",
    "### **Scalability**\n",
    "- Horizontal scaling across multiple nodes\n",
    "- Load balancing for NLP workloads\n",
    "- Resource optimization for different model types\n",
    "\n",
    "### **Quality Assurance**\n",
    "- Text data validation\n",
    "- Model performance monitoring\n",
    "- Output quality checks\n",
    "\n",
    "## Example Workflows\n",
    "\n",
    "### **1. Customer Experience Intelligence**\n",
    "**Use Case**: E-commerce company analyzing 50K daily reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ca97a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load customer reviews at scale\n",
    "reviews = from_huggingface(\"amazon_reviews_multi\", split=\"train[:50000]\")\n",
    "\n",
    "# Multi-dimensional sentiment analysis\n",
    "sentiment_pipeline = reviews.map_batches(SentimentAnalyzer(), batch_size=100, concurrency=8)\n",
    "\n",
    "# Product feature extraction\n",
    "feature_pipeline = sentiment_pipeline.map_batches(ProductFeatureExtractor(), batch_size=50)\n",
    "\n",
    "# Business insights generation\n",
    "insights = feature_pipeline.groupby('product_category').agg({\n",
    "    'sentiment_score': 'mean',\n",
    "    'feature_mentions': 'count',\n",
    "    'recommendation_score': 'mean'\n",
    "})\n",
    "\n",
    "# Results: Product satisfaction by category, feature improvement priorities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e20799a",
   "metadata": {},
   "source": [
    "### **2. Brand Monitoring and Competitive Analysis**\n",
    "**Use Case**: Marketing team tracking 25K daily social mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d628a9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load social media mentions\n",
    "social_data = read_text(\"s3://social-media-feeds/mentions/\")\n",
    "\n",
    "# Brand sentiment tracking\n",
    "brand_analysis = social_data.map_batches(BrandSentimentAnalyzer(), batch_size=200)\n",
    "\n",
    "# Competitive comparison\n",
    "competitor_analysis = brand_analysis.map_batches(CompetitorMentionTracker(), batch_size=100)\n",
    "\n",
    "# Trend identification\n",
    "trending_topics = competitor_analysis.groupby('mention_type').agg({\n",
    "    'sentiment_score': 'mean',\n",
    "    'engagement_rate': 'mean',\n",
    "    'virality_score': 'max'\n",
    "})\n",
    "\n",
    "# Results: Real-time brand health, competitive positioning insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e4c3f4",
   "metadata": {},
   "source": [
    "### **3. Support Ticket Intelligence**\n",
    "**Use Case**: Customer service team processing 15K daily tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d849feba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load support tickets\n",
    "# Support tickets - JSON format (typical for customer support systems)\n",
    "tickets = ray.data.read_json(\"s3://ray-benchmark-data/support/tickets/*.json\")\n",
    "\n",
    "# Urgency classification and routing\n",
    "classified_tickets = tickets.map_batches(TicketClassifier(), batch_size=150)\n",
    "\n",
    "# Issue categorization and solution matching\n",
    "categorized_tickets = classified_tickets.map_batches(IssueCategorizer(), batch_size=100)\n",
    "\n",
    "# Knowledge base enhancement\n",
    "knowledge_updates = categorized_tickets.map_batches(KnowledgeExtractor(), batch_size=75)\n",
    "\n",
    "# Results: Automated ticket routing, solution recommendations, knowledge base updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64f9da0",
   "metadata": {},
   "source": [
    "### **4. Content Moderation at Scale**\n",
    "**Use Case**: Social platform moderating 100K daily posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96315b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load user-generated content\n",
    "user_content = read_json(\"s3://platform-content/posts/\")\n",
    "\n",
    "# Multi-layer content analysis\n",
    "safety_analysis = user_content.map_batches(ContentSafetyAnalyzer(), batch_size=500)\n",
    "\n",
    "# Toxicity and harmful content detection\n",
    "toxicity_analysis = safety_analysis.map_batches(ToxicityDetector(), batch_size=300)\n",
    "\n",
    "# Automated moderation decisions\n",
    "moderation_actions = toxicity_analysis.map_batches(ModerationDecisionEngine(), batch_size=200)\n",
    "\n",
    "# Results: Automated content moderation, safety scoring, action recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12815eb6",
   "metadata": {},
   "source": [
    "### **5. Document Intelligence and Compliance**\n",
    "**Use Case**: Legal firm processing 10K daily documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d81b532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load legal documents\n",
    "legal_docs = read_text(\"s3://legal-documents/filings/\")\n",
    "\n",
    "# Contract analysis and clause extraction\n",
    "contract_analysis = legal_docs.map_batches(ContractAnalyzer(), batch_size=50)\n",
    "\n",
    "# Compliance checking and risk assessment\n",
    "compliance_check = contract_analysis.map_batches(ComplianceValidator(), batch_size=25)\n",
    "\n",
    "# Key information extraction for case management\n",
    "case_intelligence = compliance_check.map_batches(CaseIntelligenceExtractor(), batch_size=30)\n",
    "\n",
    "# Results: Contract risk assessment, compliance validation, case insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe56ba90",
   "metadata": {},
   "source": [
    "### **6. Multilingual Customer Support**\n",
    "**Use Case**: Global company handling 30K daily multilingual inquiries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4318a900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multilingual customer inquiries\n",
    "inquiries = read_text(\"s3://global-support/inquiries/\")\n",
    "\n",
    "# Language detection and routing\n",
    "language_analysis = inquiries.map_batches(LanguageDetector(), batch_size=300)\n",
    "\n",
    "# Multilingual sentiment and intent analysis\n",
    "intent_analysis = language_analysis.map_batches(MultilingualIntentAnalyzer(), batch_size=150)\n",
    "\n",
    "# Automated response generation\n",
    "response_generation = intent_analysis.map_batches(ResponseGenerator(), batch_size=100)\n",
    "\n",
    "# Results: Automated multilingual support, intent classification, response suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044311bd",
   "metadata": {},
   "source": [
    "## Performance analysis\n",
    "\n",
    "### **NLP Pipeline Processing Flow**\n",
    "\n",
    "```\n",
    "Text Data Processing Pipeline:\n",
    "┌─────────────────┐\n",
    "│  Raw Text Data  │ (IMDB, Amazon, News)\n",
    "│  1M+ documents  │\n",
    "└────────┬────────┘\n",
    "         │\n",
    "         ▼\n",
    "┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n",
    "│ Text Cleaning   │    │ BERT Embeddings │    │ Sentiment       │\n",
    "│ & Preprocessing │───▶│ Generation      │───▶│ Analysis        │\n",
    "│                 │    │ (384-dim)       │    │ (Pos/Neg/Neu)  │\n",
    "└─────────────────┘    └─────────────────┘    └────────┬────────┘\n",
    "                                                       │\n",
    "         ┌─────────────────┐    ┌─────────────────┐    │\n",
    "         │ Named Entity    │    │ Topic Modeling  │    │\n",
    "         │ Recognition     │◀───│ & Clustering    │◀───┘\n",
    "         │ (Persons/Orgs)  │    │ (LDA/K-means)   │\n",
    "         └─────────────────┘    └─────────────────┘\n",
    "```\n",
    "\n",
    "### **Performance measurement framework**\n",
    "\n",
    "| Analysis Type | Benchmark Method | Visualization Output |\n",
    "|--------------|------------------|---------------------|\n",
    "| **Text Processing** | Throughput measurement | Processing speed charts |\n",
    "| **Model Inference** | GPU utilization tracking | Resource usage graphs |\n",
    "| **Sentiment Analysis** | Accuracy vs speed | Performance trade-offs |\n",
    "| **Topic Modeling** | Convergence analysis | Topic quality metrics |\n",
    "\n",
    "### **Expected Output Visualizations**\n",
    "\n",
    "The demo generates comprehensive analysis charts:\n",
    "\n",
    "| Chart Type | File Output | Content Description |\n",
    "|-----------|-------------|-------------------|\n",
    "| **Sentiment Distribution** | `sentiment_analysis.html` | Positive/Negative/Neutral breakdown |\n",
    "| **Topic Modeling Results** | `topic_visualization.html` | Word clouds and topic clusters |\n",
    "| **Performance Metrics** | `nlp_performance.html` | Throughput and resource usage |\n",
    "| **Entity Analysis** | `entity_extraction.html` | Named entity frequency charts |\n",
    "\n",
    "### **Sample Output Structure**\n",
    "\n",
    "```\n",
    "NLP Analysis Results:\n",
    "├── Text Statistics\n",
    "│   ├── Document count: [Actual count]\n",
    "│   ├── Average length: [Measured]\n",
    "│   └── Language distribution: [Detected]\n",
    "├── Sentiment Analysis\n",
    "│   ├── Positive: [%]\n",
    "│   ├── Negative: [%]\n",
    "│   └── Neutral: [%]\n",
    "├── Topic Modeling\n",
    "│   ├── Topics discovered: [Number]\n",
    "│   ├── Top keywords per topic\n",
    "│   └── Document-topic assignments\n",
    "└── Named Entities\n",
    "    ├── Persons: [Count]\n",
    "    ├── Organizations: [Count]\n",
    "    └── Locations: [Count]\n",
    "```\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### **Common Issues and Solutions**\n",
    "\n",
    "| Issue | Symptoms | Solution | Prevention |\n",
    "|-------|----------|----------|------------|\n",
    "| **Model Download Failures** | `OSError: Can't load model` | Check internet connection, try different model | Use local model cache, verify model names |\n",
    "| **GPU Memory Issues** | `CUDA out of memory` | Reduce batch size to 8-16, use CPU fallback | Monitor GPU memory, start with small batches |\n",
    "| **Text Encoding Errors** | `UnicodeDecodeError` | Add encoding handling, clean text | Validate text encoding, use UTF-8 |\n",
    "| **Slow Processing** | Long processing times | Use GPU acceleration, optimize batch size | Profile operations, check resource utilization |\n",
    "| **Model Compatibility** | Version conflicts | Update transformers library | Pin dependency versions in requirements.txt |\n",
    "\n",
    "### **Performance Optimization for Different Text Sizes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b44906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive batch sizing based on text characteristics\n",
    "def get_optimal_batch_size(sample_texts):\n",
    "    \"\"\"Calculate optimal batch size based on text characteristics.\"\"\"\n",
    "    avg_length = sum(len(text) for text in sample_texts) / len(sample_texts)\n",
    "    \n",
    "    if avg_length > 1000:\n",
    "        return 4   # Long texts - small batches\n",
    "    elif avg_length > 500:\n",
    "        return 8   # Medium texts - medium batches\n",
    "    else:\n",
    "        return 16  # Short texts - larger batches\n",
    "\n",
    "# Memory-efficient text processing\n",
    "def process_with_memory_management(dataset, processor_class):\n",
    "    \"\"\"Process text with automatic memory management.\"\"\"\n",
    "    import torch\n",
    "    \n",
    "    # Start with conservative batch size\n",
    "    batch_size = 8\n",
    "    \n",
    "    try:\n",
    "        result = dataset.map_batches(\n",
    "            processor_class(),\n",
    "            batch_size=batch_size,\n",
    "            num_gpus=1 if torch.cuda.is_available() else 0\n",
    "        )\n",
    "        return result\n",
    "        \n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        print(\"GPU memory error, reducing batch size and retrying...\")\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Retry with smaller batch\n",
    "        return dataset.map_batches(\n",
    "            processor_class(),\n",
    "            batch_size=batch_size // 2,\n",
    "            num_gpus=0  # Use CPU fallback\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13f6fea",
   "metadata": {},
   "source": [
    "### **Text Data Quality Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5e0e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate text data quality before processing\n",
    "def validate_text_quality(batch):\n",
    "    \"\"\"Validate text data quality and clean problematic entries.\"\"\"\n",
    "    clean_texts = []\n",
    "    \n",
    "    for item in batch:\n",
    "        text = item.get('text', '')\n",
    "        \n",
    "        # Quality checks\n",
    "        issues = []\n",
    "        \n",
    "        if not text or len(text.strip()) == 0:\n",
    "            issues.append(\"empty_text\")\n",
    "        \n",
    "        if len(text) > 10000:\n",
    "            issues.append(\"text_too_long\")\n",
    "            text = text[:10000]  # Truncate\n",
    "        \n",
    "        if not text.isascii():\n",
    "            try:\n",
    "                text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "                issues.append(\"non_ascii_characters\")\n",
    "            except:\n",
    "                issues.append(\"encoding_error\")\n",
    "                continue\n",
    "        \n",
    "        clean_item = {\n",
    "            **item,\n",
    "            'text': text,\n",
    "            'quality_issues': issues,\n",
    "            'is_clean': len(issues) == 0\n",
    "        }\n",
    "        \n",
    "        clean_texts.append(clean_item)\n",
    "    \n",
    "    return clean_texts\n",
    "\n",
    "# Apply validation before processing\n",
    "validated_texts = text_dataset.map_batches(validate_text_quality, batch_size=100)\n",
    "clean_texts = validated_texts.filter(lambda x: x['is_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74427b1",
   "metadata": {},
   "source": [
    "### **Debug Mode and Monitoring**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5fbe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "from transformers import logging as transformers_logging\n",
    "\n",
    "# Enable comprehensive debugging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "transformers_logging.set_verbosity_info()\n",
    "\n",
    "# Simple timing for performance tracking\n",
    "def track_processing_time():\n",
    "    \"\"\"Track processing time for performance analysis.\"\"\"\n",
    "    return time.time()\n",
    "\n",
    "print(\"Use Ray Dashboard for detailed resource monitoring and cluster metrics\")\n",
    "\n",
    "# Enable progress tracking\n",
    "class ProgressTracker:\n",
    "    def __init__(self, total_items):\n",
    "        self.total_items = total_items\n",
    "        self.processed_items = 0\n",
    "    \n",
    "    def update(self, batch_size):\n",
    "        self.processed_items += batch_size\n",
    "        progress = (self.processed_items / self.total_items) * 100\n",
    "        print(f\"Progress: {progress:.1f}% ({self.processed_items:,}/{self.total_items:,})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4917b54b",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "1. **Memory Errors**: Reduce batch size or increase cluster memory allocation\n",
    "2. **Slow Processing**: Optimize batch sizes and enable parallel text processing\n",
    "3. **Model Loading Failures**: Ensure NLP models are accessible to all workers\n",
    "4. **Text Encoding Issues**: Handle Unicode and special characters properly\n",
    "\n",
    "### Debug Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250e2a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "from ray.data.context import DataContext\n",
    "ctx = DataContext.get_current()\n",
    "ctx.enable_progress_bars = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa96e9dd",
   "metadata": {},
   "source": [
    "## Performance considerations\n",
    "\n",
    "- Use Ray Dashboard to monitor throughput and resource utilization.\n",
    "- Tune batch sizes and concurrency based on your actual cluster resources and dataset size.\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **Ray Data democratizes large-scale NLP**: Enterprise text processing without complex infrastructure\n",
    "- **Distributed processing essential for modern text volumes**: Social media and document analysis require parallel processing\n",
    "- **Preprocessing optimization provides major gains**: Proper text cleaning and tokenization improve downstream performance\n",
    "- **Production NLP requires monitoring**: Text data quality and model performance need continuous validation\n",
    "\n",
    "## Action Items\n",
    "\n",
    "### Immediate Goals (Next 2 weeks)\n",
    "1. **Implement text analytics pipeline** for your specific document processing needs\n",
    "2. **Add sentiment analysis** to understand text sentiment at scale\n",
    "3. **Set up text preprocessing** with tokenization and cleaning\n",
    "4. **Create text quality monitoring** to ensure processing accuracy\n",
    "\n",
    "### Long-term Goals (Next 3 months)\n",
    "1. **Deploy production NLP systems** with real-time text processing\n",
    "2. **Implement advanced NLP features** like entity recognition and topic modeling\n",
    "3. **Build text search and recommendation** systems using embeddings\n",
    "4. **Create multilingual support** for global text analysis\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Ray Data Documentation](https://docs.ray.io/en/latest/data/index.html)\n",
    "- [Transformers Documentation](https://huggingface.co/docs/transformers/)\n",
    "- [spaCy Documentation](https://spacy.io/usage)\n",
    "- [NLTK Documentation](https://www.nltk.org/)\n",
    "\n",
    "## Cleanup and Resource Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab6f431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up Ray resources\n",
    "ray.shutdown()\n",
    "print(\"Ray cluster shutdown complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff1624f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*This template provides a foundation for enterprise-scale text analytics with Ray Data. Start with basic sentiment analysis and systematically add advanced NLP capabilities based on your specific requirements.*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
