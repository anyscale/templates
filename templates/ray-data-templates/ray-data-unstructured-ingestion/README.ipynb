{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90859d89-e0b5-4def-ba99-dc38849757d9",
   "metadata": {},
   "source": [
    "# Unstructured Data Ingestion and Processing With Ray Data\n",
    "\n",
    "**Time to complete**: 35 min \\| **Difficulty**: Advanced \\| **Prerequisites**: Data engineering experience, document processing, basic NLP knowledge\n",
    "\n",
    "## What you’ll build\n",
    "\n",
    "Build a comprehensive document ingestion pipeline that transforms unstructured documents from data lakes into structured, analytics-ready datasets using Ray Data’s distributed processing capabilities for enterprise data warehouse workflows.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1.  [Data Lake Document Discovery](#step-1-data-lake-document-discovery) (8 min)\n",
    "2.  [Document Processing and Classification](#step-2-document-processing-and-classification) (10 min)\n",
    "3.  [Text Extraction and Enrichment](#step-3-text-extraction-and-enrichment) (8 min)\n",
    "4.  [LLM-Powered Content Analysis](#step-4-llm-powered-content-analysis) (6 min)\n",
    "5.  [Data Warehouse Output](#step-5-data-warehouse-output) (3 min)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "**Why unstructured data ingestion matters**: Enterprise data lakes contain vast amounts of unstructured documents (PDFs, Word docs, presentations, reports) that need systematic processing to extract business value for analytics and reporting.\n",
    "\n",
    "**Ray Data’s ingestion capabilities**: Distribute document processing across clusters to handle large-scale document collections, extract structured data, and prepare analytics-ready datasets for data warehouse consumption.\n",
    "\n",
    "**Data lake to warehouse patterns**: Techniques used by data engineering teams to systematically process document collections, extract structured information, and create queryable datasets for business intelligence.\n",
    "\n",
    "**Production ingestion workflows**: Scalable document processing patterns that handle diverse file formats, extract metadata, and create structured schemas for downstream analytics systems.\n",
    "\n",
    "**LLM integration strategies**: Document processing workflows that use Ray Data LLM package for content analysis and structured data extraction from unstructured text.\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Challenge**: Enterprise data lakes contain millions of unstructured documents (PDFs, Word docs, presentations) across multiple formats that need systematic processing to extract business value. Traditional document processing approaches struggle with:\n",
    "- **Scale**: Single-machine processing limits document volume\n",
    "- **Consistency**: Manual extraction creates inconsistent schemas  \n",
    "- **LLM integration**: Complex infrastructure for AI-powered analysis\n",
    "- **Warehouse integration**: Manual data modeling and ETL processes\n",
    "\n",
    "**Solution**: Ray Data enables end-to-end document ingestion pipelines:\n",
    "\n",
    "| Pipeline Stage | Traditional Approach | Ray Data Approach | Benefit |\n",
    "|------------------|-----------------------|---------------------|-----------|\n",
    "| **Document Discovery** | Sequential file listing | Parallel `read_binary_files()` | Process millions of files |\n",
    "| **Text Extraction** | Single-threaded parsing | Distributed `map_batches()` | Extract from all docs simultaneously |\n",
    "| **LLM Analysis** | Manual API calls | Integrated `ray.data.llm` package | Built-in batch inference |\n",
    "| **Data Warehouse** | Custom ETL scripts | Native `write_parquet()` with partitioning | Production-ready output |\n",
    "\n",
    ":::tip Ray Data LLM Integration\n",
    "Ray Data’s `ray.data.llm` package integrates vLLM directly into data pipelines, enabling document analysis at scale:\n",
    "- **Batch inference**: Process 1000s of documents with optimized GPU utilization\n",
    "- **Built-in preprocessing**: Custom prompt formatting integrated in pipeline\n",
    "- **Automatic postprocessing**: Extract structured data from LLM responses\n",
    "- **Fault tolerance**: Retry failed documents without losing progress\n",
    "- **Cost efficiency**: GPU sharing across document batches\n",
    ":::\n",
    "\n",
    "**Data Lake to Warehouse Flow**: This template demonstrates a complete pipeline from raw documents in data lakes to structured, queryable datasets ready for business intelligence and analytics workflows using Ray Data native operations and integrated LLM processing.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Approach comparison\n",
    "\n",
    "| Traditional Approach | Ray Data Approach | Key Benefit |\n",
    "|----------------------------|--------------------------|------------------|\n",
    "| **Single-machine processing** | Distributed across cluster | Horizontal scalability |\n",
    "| **Memory-limited** | Streaming execution | Handle large datasets |\n",
    "| **Sequential operations** | Pipeline parallelism | Better resource utilization |\n",
    "| **Manual optimization** | Automatic resource management | Simplified deployment |\n",
    "\n",
    "## Prerequisites Checklist\n",
    "\n",
    "Before starting, ensure you have:\n",
    "- \\[ \\] Understanding of data lake and data warehouse concepts\n",
    "- \\[ \\] Experience with document processing and text extraction\n",
    "- \\[ \\] Knowledge of structured data formats (Parquet, Delta Lake, Iceberg)\n",
    "- \\[ \\] Python environment with Ray Data and document processing libraries\n",
    "- \\[ \\] Access to S3 or other cloud storage for document sources\n",
    "\n",
    "## Quick start (3 minutes)\n",
    "\n",
    "This section demonstrates large-scale document ingestion using Ray Data:\n",
    "\n",
    "``` python\n",
    "import json\n",
    "import logging\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ray\n",
    "from ray.data.expressions import col, lit\n",
    "\n",
    "# Initialize Ray for distributed processing\n",
    "ray.init()\n",
    "\n",
    "# Configure Ray Data for optimal performance monitoring\n",
    "ctx = ray.data.DataContext.get_current()\n",
    "ctx.enable_progress_bars = True\n",
    "ctx.enable_operator_progress_bars = True\n",
    "\n",
    "# Load document collection from data lake\n",
    "document_dataset = ray.data.read_binary_files(\n",
    "    \"s3://anyscale-rag-application/1000-docs/\",\n",
    "    include_paths=True,\n",
    "    num_cpus=0.025  # High I/O concurrency for large document collections\n",
    ")\n",
    "\n",
    "print(f\"Loaded document collection: {document_dataset.count()} documents\")\n",
    "print(f\"Dataset schema: {document_dataset.schema()}\")\n",
    "```\n",
    "\n",
    "## Step 1: Data Lake Document Discovery\n",
    "\n",
    "### Initialize Ray Data environment\n",
    "\n",
    "``` python\n",
    "import json\n",
    "import logging\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ray\n",
    "from ray.data.expressions import col, lit\n",
    "\n",
    "# Initialize Ray for distributed document processing\n",
    "ray.init()\n",
    "\n",
    "# Configure Ray Data for optimal performance monitoring\n",
    "ctx = ray.data.DataContext.get_current()\n",
    "ctx.enable_progress_bars = True\n",
    "ctx.enable_operator_progress_bars = True\n",
    "\n",
    "print(\"Ray Data initialized for large-scale document ingestion\")\n",
    "print(f\"Cluster resources: {ray.cluster_resources()}\")\n",
    "```\n",
    "\n",
    "### Discover document collections in data lake\n",
    "\n",
    "``` python\n",
    "# Configuration for document ingestion pipeline\n",
    "SOURCE_S3_PATH = \"s3://anyscale-rag-application/1000-docs/\"\n",
    "OUTPUT_WAREHOUSE_PATH = \"/tmp/document_warehouse\"\n",
    "\n",
    "# Use Ray Data to scan large document collections\n",
    "print(\"Discovering documents in data lake...\")\n",
    "\n",
    "document_collection = ray.data.read_binary_files(\n",
    "    SOURCE_S3_PATH,\n",
    "    include_paths=True,\n",
    "    num_cpus=0.025  # High I/O concurrency for document discovery\n",
    ")\n",
    "\n",
    "print(f\"Document discovery completed:\")\n",
    "print(f\"  Total documents: {document_collection.count():,}\")\n",
    "print(f\"  Schema: {document_collection.schema()}\")\n",
    "```\n",
    "\n",
    "### Document metadata extraction\n",
    "\n",
    "``` python\n",
    "def extract_document_metadata(record: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Extract comprehensive metadata from document files for data warehouse analysis.\"\"\"\n",
    "    \n",
    "    file_path = Path(record[\"path\"])\n",
    "    file_size = len(record[\"bytes\"])\n",
    "    \n",
    "    # Extract file system metadata\n",
    "    file_metadata = {\n",
    "        \"document_id\": str(uuid.uuid4()),\n",
    "        \"file_path\": str(file_path),\n",
    "        \"file_name\": file_path.name,\n",
    "        \"file_extension\": file_path.suffix.lower(),\n",
    "        \"file_size_bytes\": file_size,\n",
    "        \"file_size_mb\": round(file_size / (1024 * 1024), 2),\n",
    "        \"discovery_timestamp\": datetime.now().isoformat(),\n",
    "        \"source_bucket\": file_path.parts[0] if file_path.parts else \"unknown\"\n",
    "    }\n",
    "    \n",
    "    # Business classification for data warehouse categorization\n",
    "    filename_lower = file_path.name.lower()\n",
    "    \n",
    "    if any(keyword in filename_lower for keyword in [\"financial\", \"earnings\", \"revenue\", \"profit\"]):\n",
    "        doc_type = \"financial_document\"\n",
    "        business_category = \"finance\"\n",
    "    elif any(keyword in filename_lower for keyword in [\"legal\", \"contract\", \"agreement\", \"terms\"]):\n",
    "        doc_type = \"legal_document\" \n",
    "        business_category = \"legal\"\n",
    "    elif any(keyword in filename_lower for keyword in [\"regulatory\", \"compliance\", \"filing\", \"sec\"]):\n",
    "        doc_type = \"regulatory_document\"\n",
    "        business_category = \"compliance\"\n",
    "    elif any(keyword in filename_lower for keyword in [\"client\", \"customer\", \"portfolio\"]):\n",
    "        doc_type = \"client_document\"\n",
    "        business_category = \"client_services\"\n",
    "    elif any(keyword in filename_lower for keyword in [\"market\", \"research\", \"analysis\", \"report\"]):\n",
    "        doc_type = \"research_document\"\n",
    "        business_category = \"research\"\n",
    "    else:\n",
    "        doc_type = \"general_document\"\n",
    "        business_category = \"general\"\n",
    "    \n",
    "    # Processing priority for workflow optimization\n",
    "    if any(keyword in filename_lower for keyword in [\"urgent\", \"critical\", \"deadline\"]):\n",
    "        priority = \"high\"\n",
    "        priority_score = 3\n",
    "    elif any(keyword in filename_lower for keyword in [\"important\", \"quarterly\", \"annual\"]):\n",
    "        priority = \"medium\"\n",
    "        priority_score = 2\n",
    "    else:\n",
    "        priority = \"low\"\n",
    "        priority_score = 1\n",
    "    \n",
    "    return {\n",
    "        **file_metadata,\n",
    "        \"document_type\": doc_type,\n",
    "        \"business_category\": business_category,\n",
    "        \"processing_priority\": priority,\n",
    "        \"priority_score\": priority_score,\n",
    "        \"estimated_pages\": max(1, file_size // 50000),  # Rough page estimate\n",
    "        \"processing_status\": \"discovered\"\n",
    "    }\n",
    "\n",
    "# Apply metadata extraction using Ray Data map operation\n",
    "print(\"Extracting document metadata for data warehouse analysis...\")\n",
    "\n",
    "documents_with_metadata = document_collection.map(\n",
    "    extract_document_metadata,\n",
    "    num_cpus=0.5  # Medium complexity metadata extraction\n",
    ")\n",
    "\n",
    "print(f\"Metadata extraction completed: {documents_with_metadata.count():,} documents processed\")\n",
    "```\n",
    "\n",
    "### Document collection analytics\n",
    "\n",
    "``` python\n",
    "# Use Ray Data native operations for document collection analysis\n",
    "from ray.data.aggregate import Count, Sum, Mean, Max, Min\n",
    "\n",
    "print(\"Analyzing document collection using Ray Data native operations...\")\n",
    "\n",
    "# Document type distribution using native groupby\n",
    "doc_type_stats = documents_with_metadata.groupby(\"document_type\").aggregate(\n",
    "    Count(),\n",
    "    Sum(\"file_size_bytes\"),\n",
    "    Mean(\"file_size_mb\"),\n",
    "    Max(\"estimated_pages\")\n",
    ")\n",
    "\n",
    "print(\"Document Type Distribution:\")\n",
    "print(doc_type_stats.limit(10).to_pandas())\n",
    "\n",
    "# Business category analysis\n",
    "category_stats = documents_with_metadata.groupby(\"business_category\").aggregate(\n",
    "    Count(),\n",
    "    Mean(\"priority_score\"),\n",
    "    Sum(\"file_size_mb\")\n",
    ")\n",
    "\n",
    "print(\"Business Category Analysis:\")\n",
    "print(category_stats.limit(10).to_pandas())\n",
    "\n",
    "# File extension analysis using expressions API\n",
    "pdf_documents = documents_with_metadata.filter(\n",
    "    col(\"file_extension\") == lit(\".pdf\"),\n",
    "    num_cpus=0.1\n",
    ")\n",
    "\n",
    "word_documents = documents_with_metadata.filter(\n",
    "    col(\"file_extension\") == lit(\".docx\"),\n",
    "    num_cpus=0.1\n",
    ")\n",
    "\n",
    "ppt_documents = documents_with_metadata.filter(\n",
    "    col(\"file_extension\") == lit(\".pptx\"),\n",
    "    num_cpus=0.1\n",
    ")\n",
    "\n",
    "print(f\"File Format Distribution:\")\n",
    "print(f\"  PDF documents: {pdf_documents.count():,}\")\n",
    "print(f\"  Word documents: {word_documents.count():,}\")\n",
    "print(f\"  PowerPoint documents: {ppt_documents.count():,}\")\n",
    "```\n",
    "\n",
    "## Step 2: Document Processing and Classification\n",
    "\n",
    "### Text extraction pipeline\n",
    "\n",
    "``` python\n",
    "def extract_text_from_document(record: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Extract text content from various document formats for data warehouse processing.\"\"\"\n",
    "    \n",
    "    file_extension = record[\"file_extension\"]\n",
    "    file_name = record[\"file_name\"]\n",
    "    file_size = record[\"file_size_bytes\"]\n",
    "    \n",
    "    # Simulate format-specific text extraction\n",
    "    # In production, use libraries like PyPDF2, python-docx, python-pptx, BeautifulSoup\n",
    "    \n",
    "    if file_extension == \".pdf\":\n",
    "        extracted_text = simulate_pdf_extraction(file_name, file_size)\n",
    "    elif file_extension in [\".docx\", \".doc\"]:\n",
    "        extracted_text = simulate_word_extraction(file_name, file_size)\n",
    "    elif file_extension in [\".pptx\", \".ppt\"]:\n",
    "        extracted_text = simulate_powerpoint_extraction(file_name, file_size)\n",
    "    elif file_extension in [\".html\", \".htm\"]:\n",
    "        extracted_text = simulate_html_extraction(file_name, file_size)\n",
    "    else:\n",
    "        extracted_text = f\"Text content from {file_name} with business information and structured data.\"\n",
    "    \n",
    "    # Calculate text statistics for analytics\n",
    "    text_length = len(extracted_text)\n",
    "    word_count = len(extracted_text.split())\n",
    "    \n",
    "    return {\n",
    "        **record,\n",
    "        \"extracted_text\": extracted_text,\n",
    "        \"text_length\": text_length,\n",
    "        \"word_count\": word_count,\n",
    "        \"text_extraction_timestamp\": datetime.now().isoformat(),\n",
    "        \"extraction_status\": \"success\"\n",
    "    }\n",
    "\n",
    "def simulate_pdf_extraction(file_name: str, file_size: int) -> str:\n",
    "    \"\"\"Simulate PDF text extraction with realistic business content.\"\"\"\n",
    "    if \"financial\" in file_name.lower():\n",
    "        return f\"Financial Report: Quarterly earnings data showing revenue growth, profit margins, and cash flow analysis. Balance sheet information includes assets, liabilities, and equity positions. Income statement details operating expenses, net income, and earnings per share metrics. Document contains structured financial tables and regulatory compliance information.\"\n",
    "    elif \"regulatory\" in file_name.lower():\n",
    "        return f\"Regulatory Filing: Compliance documentation for regulatory requirements including risk assessments, audit findings, and regulatory framework adherence. Contains mandatory reporting data, compliance metrics, and regulatory disclosure information.\"\n",
    "    else:\n",
    "        return f\"Business Document: Professional document containing business information, operational data, and structured content for enterprise analysis and reporting. Includes business metrics, process documentation, and analytical insights.\"\n",
    "\n",
    "def simulate_word_extraction(file_name: str, file_size: int) -> str:\n",
    "    \"\"\"Simulate Word document text extraction with business context.\"\"\"\n",
    "    if \"client\" in file_name.lower():\n",
    "        return f\"Client Report: Client portfolio analysis including investment performance, asset allocation, and risk assessment. Contains client-specific recommendations, market outlook information, and personalized financial advice.\"\n",
    "    elif \"policy\" in file_name.lower():\n",
    "        return f\"Policy Document: Corporate policies, procedures, guidelines, and operational frameworks. Contains structured policy information, compliance requirements, and operational procedures.\"\n",
    "    else:\n",
    "        return f\"Business Document: Corporate document with operational information, business processes, professional correspondence, and structured information for internal and external stakeholders.\"\n",
    "\n",
    "def simulate_powerpoint_extraction(file_name: str, file_size: int) -> str:\n",
    "    \"\"\"Simulate PowerPoint text extraction with presentation context.\"\"\"\n",
    "    if \"executive\" in file_name.lower():\n",
    "        return f\"Executive Presentation: Strategic business presentation with performance metrics, market analysis, and executive insights. Contains key performance indicators, strategic initiatives, and business analytics content.\"\n",
    "    elif \"quarterly\" in file_name.lower():\n",
    "        return f\"Quarterly Report: Quarterly business review with financial metrics, operational performance, market trends, and strategic initiatives. Contains structured quarterly data and business analytics.\"\n",
    "    else:\n",
    "        return f\"Business Presentation: Corporate presentation with charts, data visualizations, and structured content suitable for business meetings and stakeholder communications.\"\n",
    "\n",
    "def simulate_html_extraction(file_name: str, file_size: int) -> str:\n",
    "    \"\"\"Simulate HTML text extraction with web content context.\"\"\"\n",
    "    if \"report\" in file_name.lower():\n",
    "        return f\"Web Report: Online business report with structured data, tables, and analytical content. Contains web-based business intelligence and reporting information.\"\n",
    "    else:\n",
    "        return f\"HTML Document: Web content with structured information, data tables, and business content suitable for data warehouse ingestion.\"\n",
    "\n",
    "# Apply text extraction using Ray Data distributed processing\n",
    "print(\"Extracting text content from documents...\")\n",
    "\n",
    "try:\n",
    "    documents_with_text = documents_with_metadata.map_batches(\n",
    "        lambda batch: [extract_text_from_document(record) for record in batch],\n",
    "        num_cpus=1.0,  # Heavy text extraction processing\n",
    "        batch_size=500\n",
    "    )\n",
    "    \n",
    "    text_count = documents_with_text.count()\n",
    "    print(f\"Text extraction completed: {text_count:,} documents processed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Text extraction failed: {e}\")\n",
    "    raise\n",
    "```\n",
    "\n",
    "### Document quality assessment\n",
    "\n",
    "``` python\n",
    "def assess_document_quality(record: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Assess document quality for data warehouse ingestion suitability.\"\"\"\n",
    "    \n",
    "    quality_score = 0\n",
    "    quality_issues = []\n",
    "    \n",
    "    # File size quality check\n",
    "    if record[\"file_size_mb\"] > 0.01:  # At least 10KB\n",
    "        quality_score += 1\n",
    "    else:\n",
    "        quality_issues.append(\"file_too_small\")\n",
    "    \n",
    "    # Text content quality check\n",
    "    if record[\"text_length\"] > 100:  # At least 100 characters\n",
    "        quality_score += 1\n",
    "    else:\n",
    "        quality_issues.append(\"insufficient_text\")\n",
    "    \n",
    "    # Business relevance check\n",
    "    if record[\"business_category\"] != \"general\":\n",
    "        quality_score += 1\n",
    "    else:\n",
    "        quality_issues.append(\"low_business_relevance\")\n",
    "    \n",
    "    # Word count check\n",
    "    if record[\"word_count\"] > 20:  # At least 20 words\n",
    "        quality_score += 1\n",
    "    else:\n",
    "        quality_issues.append(\"insufficient_content\")\n",
    "    \n",
    "    # File format check\n",
    "    supported_formats = [\".pdf\", \".docx\", \".pptx\", \".html\", \".txt\"]\n",
    "    if record[\"file_extension\"] in supported_formats:\n",
    "        quality_score += 1\n",
    "    else:\n",
    "        quality_issues.append(\"unsupported_format\")\n",
    "    \n",
    "    # Determine overall quality rating\n",
    "    if quality_score >= 4:\n",
    "        quality_rating = \"high\"\n",
    "    elif quality_score >= 2:\n",
    "        quality_rating = \"medium\"\n",
    "    else:\n",
    "        quality_rating = \"low\"\n",
    "    \n",
    "    return {\n",
    "        **record,\n",
    "        \"quality_score\": quality_score,\n",
    "        \"quality_rating\": quality_rating,\n",
    "        \"quality_issues\": json.dumps(quality_issues),\n",
    "        \"quality_assessment_timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "# Apply quality assessment using Ray Data batch processing\n",
    "print(\"Assessing document quality for data warehouse ingestion...\")\n",
    "\n",
    "quality_assessed_docs = documents_with_text.map_batches(\n",
    "    lambda batch: [assess_document_quality(record, batch_format=\"pandas\") for record in batch],\n",
    "    num_cpus=0.25,  # Light quality assessment processing\n",
    "    batch_size=2000\n",
    ")\n",
    "\n",
    "# Filter high-quality documents using Ray Data expressionshigh_quality_docs = quality_assessed_docs.filter(\n",
    "    col(\"quality_rating\") == lit(\"high\"),\n",
    "    num_cpus=0.1\n",
    ")\n",
    "\n",
    "print(f\"Quality assessment completed:\")\n",
    "print(f\"  Total documents assessed: {quality_assessed_docs.count():,}\")\n",
    "print(f\"  High quality documents: {high_quality_docs.count():,}\")\n",
    "```\n",
    "\n",
    "### Document filtering and prioritization\n",
    "\n",
    "``` python\n",
    "# Use Ray Data native filtering for document prioritization\n",
    "print(\"Filtering and prioritizing documents for processing...\")\n",
    "\n",
    "# Filter by processing priority using expressions API\n",
    "high_priority_docs = high_quality_docs.filter(\n",
    "    col(\"priority_score\") >= lit(2),\n",
    "    num_cpus=0.1\n",
    ")\n",
    "\n",
    "# Filter by business category for targeted processing\n",
    "financial_docs = high_quality_docs.filter(\n",
    "    col(\"business_category\") == lit(\"finance\"),\n",
    "    num_cpus=0.1\n",
    ")\n",
    "\n",
    "compliance_docs = high_quality_docs.filter(\n",
    "    col(\"business_category\") == lit(\"compliance\"),\n",
    "    num_cpus=0.1\n",
    ")\n",
    "\n",
    "# Filter by file size for processing optimizationlarge_documents = high_quality_docs.filter(\n",
    "    col(\"file_size_mb\") > lit(5.0),  # Documents larger than 5MB\n",
    "    num_cpus=0.1\n",
    ")\n",
    "\n",
    "small_documents = high_quality_docs.filter(\n",
    "    col(\"file_size_mb\") <= lit(5.0),  # Documents 5MB or smaller\n",
    "    num_cpus=0.1\n",
    ")\n",
    "\n",
    "print(f\"Document filtering results:\")\n",
    "print(f\"  High priority documents: {high_priority_docs.count():,}\")\n",
    "print(f\"  Financial documents: {financial_docs.count():,}\")\n",
    "print(f\"  Compliance documents: {compliance_docs.count():,}\")\n",
    "print(f\"  Large documents (>5MB): {large_documents.count():,}\")\n",
    "print(f\"  Small documents (5MB): {small_documents.count():,}\")\n",
    "```\n",
    "\n",
    "## Step 3: Text Extraction and Enrichment\n",
    "\n",
    "### Text chunking for LLM processing\n",
    "\n",
    "``` python\n",
    "def create_text_chunks_for_analytics(record: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Create text chunks optimized for LLM processing and analytics.\"\"\"\n",
    "    \n",
    "    text = record[\"extracted_text\"]\n",
    "    chunk_size = 1500  # Optimal size for LLM context window\n",
    "    overlap = 150      # 10% overlap for context preservation\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    chunk_index = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunk_text = text[start:end]\n",
    "        \n",
    "        # Create analytics-optimized chunk record\n",
    "        chunk_record = {\n",
    "            **record,\n",
    "            \"chunk_id\": str(uuid.uuid4()),\n",
    "            \"chunk_index\": chunk_index,\n",
    "            \"chunk_text\": chunk_text,\n",
    "            \"chunk_length\": len(chunk_text),\n",
    "            \"chunk_word_count\": len(chunk_text.split()),\n",
    "            \"chunk_start_position\": start,\n",
    "            \"chunk_end_position\": end,\n",
    "            \"chunking_timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        chunks.append(chunk_record)\n",
    "        \n",
    "        # Move to next chunk with overlap\n",
    "        start = end - overlap\n",
    "        chunk_index += 1\n",
    "        \n",
    "        if start >= len(text):\n",
    "            break\n",
    "    \n",
    "    # Update total chunks for all records\n",
    "    for chunk in chunks:\n",
    "        chunk[\"total_chunks\"] = len(chunks)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Apply text chunking using Ray Data flat_map\n",
    "print(\"Creating text chunks for LLM processing...\")\n",
    "\n",
    "chunked_documents = high_quality_docs.flat_map(\n",
    "    create_text_chunks_for_analytics,\n",
    "    num_cpus=0.5  # Medium complexity chunking operation\n",
    ")\n",
    "\n",
    "print(f\"Text chunking completed: {chunked_documents.count():,} text chunks created\")\n",
    "```\n",
    "\n",
    "### Content preprocessing for analytics\n",
    "\n",
    "``` python\n",
    "def preprocess_content_for_analytics(record: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Preprocess text content for analytics and LLM processing.\"\"\"\n",
    "    \n",
    "    text = record[\"chunk_text\"]\n",
    "    \n",
    "    # Basic text cleaning and normalization\n",
    "    cleaned_text = text.strip()\n",
    "    cleaned_text = ' '.join(cleaned_text.split())  # Normalize whitespace\n",
    "    \n",
    "    # Extract content indicators for analytics\n",
    "    content_indicators = {\n",
    "        \"contains_financial_terms\": any(term in text.lower() for term in [\"revenue\", \"profit\", \"earnings\", \"financial\"]),\n",
    "        \"contains_compliance_terms\": any(term in text.lower() for term in [\"compliance\", \"regulatory\", \"audit\", \"risk\"]),\n",
    "        \"contains_dates\": any(term in text for term in [\"2023\", \"2024\", \"January\", \"February\", \"March\"]),\n",
    "        \"contains_numbers\": any(char.isdigit() for char in text),\n",
    "        \"contains_entities\": any(word[0].isupper() for word in text.split() if len(word) > 2)\n",
    "    }\n",
    "    \n",
    "    # Calculate content metrics\n",
    "    sentences = text.split('. ')\n",
    "    paragraphs = text.split('\\n')\n",
    "    \n",
    "    content_metrics = {\n",
    "        \"sentence_count\": len(sentences),\n",
    "        \"paragraph_count\": len(paragraphs),\n",
    "        \"avg_sentence_length\": sum(len(s.split()) for s in sentences) / len(sentences) if sentences else 0,\n",
    "        \"text_density\": record[\"chunk_word_count\"] / record[\"chunk_length\"] if record[\"chunk_length\"] > 0 else 0\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        **record,\n",
    "        \"cleaned_text\": cleaned_text,\n",
    "        \"content_indicators\": json.dumps(content_indicators),\n",
    "        \"content_metrics\": json.dumps(content_metrics),\n",
    "        \"preprocessing_timestamp\": datetime.now().isoformat(),\n",
    "        \"llm_ready\": len(cleaned_text) > 50  # Minimum text for LLM processing\n",
    "    }\n",
    "\n",
    "# Apply content preprocessing using Ray Data\n",
    "print(\"Preprocessing content for analytics...\")\n",
    "\n",
    "preprocessed_chunks = chunked_documents.map_batches(\n",
    "    lambda batch: [preprocess_content_for_analytics(record, batch_format=\"pandas\") for record in batch],\n",
    "    num_cpus=0.25,  # Light preprocessing\n",
    "    batch_size=2000\n",
    ")\n",
    "\n",
    "# Filter chunks ready for LLM processingllm_ready_chunks = preprocessed_chunks.filter(\n",
    "    col(\"llm_ready\") == lit(True),\n",
    "    num_cpus=0.1\n",
    ")\n",
    "\n",
    "print(f\"Content preprocessing completed: {llm_ready_chunks.count():,} chunks ready for LLM\")\n",
    "```\n",
    "\n",
    "## Step 4: LLM-Powered Content Analysis\n",
    "\n",
    "### Configure Ray Data LLM processing\n",
    "\n",
    "``` python\n",
    "# Configure LLM processing using Ray Data LLM package\n",
    "print(\"Configuring Ray Data LLM processing...\")\n",
    "\n",
    "# Install required packages for LLM processing\n",
    "# Pip install -U vllm==0.7.2\n",
    "try:\n",
    "    from ray.data.llm import vLLMEngineProcessorConfig, build_llm_processor\n",
    "    \n",
    "    # Configure vLLM engine for document analysis\n",
    "    llm_config = vLLMEngineProcessorConfig(\n",
    "        model_source=\"unsloth/Llama-3.1-8B-Instruct\",\n",
    "        engine_kwargs={\n",
    "            \"max_model_len\": 16384,\n",
    "            \"enable_chunked_prefill\": True,\n",
    "            \"max_num_batched_tokens\": 4096,\n",
    "            \"tensor_parallel_size\": 1,\n",
    "            \"pipeline_parallel_size\": 1,\n",
    "        },\n",
    "        concurrency=1,\n",
    "        batch_size=32,\n",
    "        accelerator_type=\"A10G\"\n",
    "    )\n",
    "    \n",
    "    print(\"vLLM configuration created successfully\")\n",
    "    llm_available = True\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Ray Data LLM package not available - will use fallback processing\")\n",
    "    llm_config = None\n",
    "    llm_available = False\n",
    "```\n",
    "\n",
    "### Business intelligence prompt engineering\n",
    "\n",
    "``` python\n",
    "def create_business_analysis_prompt(row: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Create specialized prompts for business document analysis using Ray Data LLM.\"\"\"\n",
    "    \n",
    "    business_context = f\"Document Type: {row['document_type']}, Category: {row['business_category']}\"\n",
    "    text_content = row[\"cleaned_text\"]\n",
    "    \n",
    "    # Create system prompt for structured business analysis\n",
    "    system_prompt = \"\"\"You are a business analyst specializing in document intelligence for enterprise data warehouses. \n",
    "    Analyze documents and extract structured business information for analytics and reporting.\n",
    "    \n",
    "    For each document chunk, provide analysis in this exact format:\n",
    "    SUMMARY: [2-3 sentence business summary]\n",
    "    KEY_METRICS: [Extract numerical data, percentages, financial figures]\n",
    "    ENTITIES: [Companies, people, locations, products mentioned]\n",
    "    CATEGORY: [financial/operational/strategic/compliance/research]\n",
    "    PRIORITY: [high/medium/low based on business importance]\n",
    "    \n",
    "    Keep responses structured and factual.\"\"\"\n",
    "    \n",
    "    # Create user prompt with business context\n",
    "    user_prompt = f\"\"\"Analyze this business document chunk for data warehouse ingestion:\n",
    "    \n",
    "    Context: {business_context}\n",
    "    \n",
    "    Content: {text_content}\n",
    "    \n",
    "    Provide structured analysis following the format above.\"\"\"\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        \"sampling_params\": {\n",
    "            \"temperature\": 0.2,  # Low temperature for consistent business analysis\n",
    "            \"max_tokens\": 300,   # Sufficient for structured output\n",
    "            \"top_p\": 0.9\n",
    "        }\n",
    "    }\n",
    "\n",
    "def extract_structured_analysis(row: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Extract structured analysis from LLM response for data warehouse storage.\"\"\"\n",
    "    \n",
    "    llm_response = row.get(\"generated_text\", \"\")\n",
    "    \n",
    "    # Parse LLM response for structured data warehouse fields\n",
    "    summary = extract_summary_from_response(llm_response)\n",
    "    metrics = extract_metrics_from_response(llm_response)\n",
    "    entities = extract_entities_from_response(llm_response)\n",
    "    category = extract_category_from_response(llm_response)\n",
    "    priority = extract_priority_from_response(llm_response)\n",
    "    \n",
    "    return {\n",
    "        **row,\n",
    "        \"llm_summary\": summary,\n",
    "        \"llm_metrics\": json.dumps(metrics),\n",
    "        \"llm_entities\": json.dumps(entities),\n",
    "        \"llm_category\": category,\n",
    "        \"llm_priority\": priority,\n",
    "        \"llm_analysis_timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "def extract_summary_from_response(response: str) -> str:\n",
    "    \"\"\"Extract summary from LLM response.\"\"\"\n",
    "    if \"SUMMARY:\" in response:\n",
    "        start = response.find(\"SUMMARY:\") + len(\"SUMMARY:\")\n",
    "        end = response.find(\"KEY_METRICS:\") if \"KEY_METRICS:\" in response else len(response)\n",
    "        return response[start:end].strip()\n",
    "    return \"Summary not available\"\n",
    "\n",
    "def extract_metrics_from_response(response: str) -> List[str]:\n",
    "    \"\"\"Extract metrics from LLM response.\"\"\"\n",
    "    if \"KEY_METRICS:\" in response:\n",
    "        start = response.find(\"KEY_METRICS:\") + len(\"KEY_METRICS:\")\n",
    "        end = response.find(\"ENTITIES:\") if \"ENTITIES:\" in response else len(response)\n",
    "        metrics_text = response[start:end].strip()\n",
    "        # Extract numerical values and percentages\n",
    "        metrics = []\n",
    "        words = metrics_text.split()\n",
    "        for word in words:\n",
    "            if any(char.isdigit() for char in word) or '%' in word:\n",
    "                metrics.append(word.strip(\".,!?;:()[]{}\"))\n",
    "        return metrics[:10]  # Limit to top 10 metrics\n",
    "    return []\n",
    "\n",
    "def extract_entities_from_response(response: str) -> List[str]:\n",
    "    \"\"\"Extract business entities from LLM response.\"\"\"\n",
    "    if \"ENTITIES:\" in response:\n",
    "        start = response.find(\"ENTITIES:\") + len(\"ENTITIES:\")\n",
    "        end = response.find(\"CATEGORY:\") if \"CATEGORY:\" in response else len(response)\n",
    "        entity_text = response[start:end].strip()\n",
    "        # Extract entities (companies, people, locations)\n",
    "        entities = []\n",
    "        words = entity_text.split()\n",
    "        for word in words:\n",
    "            clean_word = word.strip(\".,!?;:()[]{}\\\"'\")\n",
    "            if len(clean_word) > 2 and clean_word[0].isupper():\n",
    "                entities.append(clean_word)\n",
    "        return list(set(entities))[:15]  # Unique entities, max 15\n",
    "    return []\n",
    "\n",
    "def extract_category_from_response(response: str) -> str:\n",
    "    \"\"\"Extract category classification from LLM response.\"\"\"\n",
    "    if \"CATEGORY:\" in response:\n",
    "        start = response.find(\"CATEGORY:\") + len(\"CATEGORY:\")\n",
    "        end = response.find(\"PRIORITY:\") if \"PRIORITY:\" in response else len(response)\n",
    "        category_text = response[start:end].strip().lower()\n",
    "        \n",
    "        if \"financial\" in category_text:\n",
    "            return \"financial\"\n",
    "        elif \"operational\" in category_text:\n",
    "            return \"operational\"\n",
    "        elif \"strategic\" in category_text:\n",
    "            return \"strategic\"\n",
    "        elif \"compliance\" in category_text:\n",
    "            return \"compliance\"\n",
    "        elif \"research\" in category_text:\n",
    "            return \"research\"\n",
    "    return \"general\"\n",
    "\n",
    "def extract_priority_from_response(response: str) -> str:\n",
    "    \"\"\"Extract priority assessment from LLM response.\"\"\"\n",
    "    if \"PRIORITY:\" in response:\n",
    "        start = response.find(\"PRIORITY:\") + len(\"PRIORITY:\")\n",
    "        priority_text = response[start:].strip().lower()\n",
    "        \n",
    "        if \"high\" in priority_text:\n",
    "            return \"high\"\n",
    "        elif \"medium\" in priority_text:\n",
    "            return \"medium\"\n",
    "        else:\n",
    "            return \"low\"\n",
    "    return \"medium\"\n",
    "\n",
    "# Build and apply LLM processor using Ray Data LLM packageif llm_available:\n",
    "    print(\"Building LLM processor for business analysis...\")\n",
    "    \n",
    "    llm_processor = build_llm_processor(\n",
    "        llm_config,\n",
    "        preprocess=create_business_analysis_prompt,\n",
    "        postprocess=extract_structured_analysis\n",
    "    )\n",
    "    \n",
    "    # Apply LLM processing to document chunks\n",
    "    print(\"Applying LLM analysis to document chunks...\")\n",
    "    \n",
    "    llm_analyzed_docs = llm_processor(llm_ready_chunks)\n",
    "    \n",
    "    print(f\"LLM analysis completed: {llm_analyzed_docs.count():,} chunks analyzed\")\n",
    "    \n",
    "else:\n",
    "    print(\"Using fallback analysis without LLM...\")\n",
    "    \n",
    "    def fallback_analysis(record: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Fallback analysis when Ray Data LLM package is not available.\"\"\"\n",
    "        return {\n",
    "            **record,\n",
    "            \"llm_summary\": f\"Document analysis for {record['document_type']} in {record['business_category']} category\",\n",
    "            \"llm_metrics\": json.dumps([]),\n",
    "            \"llm_entities\": json.dumps([]),\n",
    "            \"llm_category\": record[\"business_category\"],\n",
    "            \"llm_priority\": record[\"processing_priority\"],\n",
    "            \"llm_analysis_timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    llm_analyzed_docs = llm_ready_chunks.map_batches(\n",
    "        lambda batch: [fallback_analysis(record, batch_format=\"pandas\") for record in batch],\n",
    "        num_cpus=0.25,  # Light fallback processing\n",
    "        batch_size=2000\n",
    "    )\n",
    "    \n",
    "    print(f\"Fallback analysis completed: {llm_analyzed_docs.count():,} chunks processed\")\n",
    "```\n",
    "\n",
    "### Content enrichment and entity extraction\n",
    "\n",
    "``` python\n",
    "def enrich_content_with_business_intelligence(record: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Enrich content with additional business intelligence for analytics.\"\"\"\n",
    "    \n",
    "    # Parse LLM-extracted data\n",
    "    try:\n",
    "        llm_metrics = json.loads(record.get(\"llm_metrics\", \"[]\"))\n",
    "        llm_entities = json.loads(record.get(\"llm_entities\", \"[]\"))\n",
    "        content_indicators = json.loads(record.get(\"content_indicators\", \"{}\"))\n",
    "    except:\n",
    "        llm_metrics = []\n",
    "        llm_entities = []\n",
    "        content_indicators = {}\n",
    "    \n",
    "    # Calculate enrichment metrics\n",
    "    enrichment_metrics = {\n",
    "        \"metrics_extracted_count\": len(llm_metrics),\n",
    "        \"entities_extracted_count\": len(llm_entities),\n",
    "        \"has_financial_indicators\": content_indicators.get(\"contains_financial_terms\", False),\n",
    "        \"has_compliance_indicators\": content_indicators.get(\"contains_compliance_terms\", False),\n",
    "        \"has_date_references\": content_indicators.get(\"contains_dates\", False),\n",
    "        \"has_numerical_data\": content_indicators.get(\"contains_numbers\", False),\n",
    "        \"content_richness_score\": len(llm_metrics) + len(llm_entities)\n",
    "    }\n",
    "    \n",
    "    # Determine analytics value for data warehouse\n",
    "    analytics_value = \"high\" if enrichment_metrics[\"content_richness_score\"] > 5 else \"medium\" if enrichment_metrics[\"content_richness_score\"] > 2 else \"low\"\n",
    "    \n",
    "    return {\n",
    "        **record,\n",
    "        \"enrichment_metrics\": json.dumps(enrichment_metrics),\n",
    "        \"analytics_value\": analytics_value,\n",
    "        \"enrichment_timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "# Apply content enrichment using Ray Dataprint(\"Enriching content with business intelligence...\")\n",
    "\n",
    "enriched_documents = llm_analyzed_docs.map_batches(\n",
    "    lambda batch: [enrich_content_with_business_intelligence(record, batch_format=\"pandas\") for record in batch],\n",
    "    num_cpus=0.25,  # Light enrichment processing\n",
    "    batch_size=2000\n",
    ")\n",
    "\n",
    "print(f\"Content enrichment completed: {enriched_documents.count():,} documents enriched\")\n",
    "```\n",
    "\n",
    "## Step 4: Structured Data Transformation\n",
    "\n",
    "### Data warehouse schema creation\n",
    "\n",
    "``` python\n",
    "def create_data_warehouse_schema(record: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Create final data warehouse schema optimized for analytics and BI tools.\"\"\"\n",
    "    \n",
    "    # Parse JSON fields for structured storage\n",
    "    try:\n",
    "        llm_metrics = json.loads(record.get(\"llm_metrics\", \"[]\"))\n",
    "        llm_entities = json.loads(record.get(\"llm_entities\", \"[]\"))\n",
    "        enrichment_metrics = json.loads(record.get(\"enrichment_metrics\", \"{}\"))\n",
    "    except:\n",
    "        llm_metrics = []\n",
    "        llm_entities = []\n",
    "        enrichment_metrics = {}\n",
    "    \n",
    "    # Create analytics-optimized record for data warehouse\n",
    "    warehouse_record = {\n",
    "        # Primary identifiers for data warehouse\n",
    "        \"document_id\": record[\"document_id\"],\n",
    "        \"chunk_id\": record[\"chunk_id\"],\n",
    "        \n",
    "        # Dimensional attributes for analytics\n",
    "        \"business_category\": record[\"business_category\"],\n",
    "        \"document_type\": record[\"document_type\"],\n",
    "        \"department\": record.get(\"department\", \"unknown\"),\n",
    "        \"file_extension\": record[\"file_extension\"],\n",
    "        \"quality_rating\": record[\"quality_rating\"],\n",
    "        \"llm_category\": record[\"llm_category\"],\n",
    "        \"llm_priority\": record[\"llm_priority\"],\n",
    "        \"analytics_value\": record[\"analytics_value\"],\n",
    "        \n",
    "        # Fact measures for analytics\n",
    "        \"file_size_mb\": record[\"file_size_mb\"],\n",
    "        \"text_length\": record[\"text_length\"],\n",
    "        \"word_count\": record[\"word_count\"],\n",
    "        \"chunk_length\": record[\"chunk_length\"],\n",
    "        \"chunk_word_count\": record[\"chunk_word_count\"],\n",
    "        \"quality_score\": record[\"quality_score\"],\n",
    "        \"priority_score\": record[\"priority_score\"],\n",
    "        \"chunk_index\": record[\"chunk_index\"],\n",
    "        \"total_chunks\": record[\"total_chunks\"],\n",
    "        \"estimated_pages\": record[\"estimated_pages\"],\n",
    "        \"metrics_count\": len(llm_metrics),\n",
    "        \"entities_count\": len(llm_entities),\n",
    "        \"content_richness_score\": enrichment_metrics.get(\"content_richness_score\", 0),\n",
    "        \n",
    "        # Content fields\n",
    "        \"text_content\": record[\"chunk_text\"],\n",
    "        \"llm_summary\": record[\"llm_summary\"],\n",
    "        \"source_path\": record[\"file_path\"],\n",
    "        \"file_name\": record[\"file_name\"],\n",
    "        \n",
    "        # Analytics flags\n",
    "        \"has_financial_content\": enrichment_metrics.get(\"has_financial_indicators\", False),\n",
    "        \"has_compliance_content\": enrichment_metrics.get(\"has_compliance_indicators\", False),\n",
    "        \"has_numerical_data\": enrichment_metrics.get(\"has_numerical_data\", False),\n",
    "        \"has_date_references\": enrichment_metrics.get(\"has_date_references\", False),\n",
    "        \n",
    "        # Processing timestamps for data lineage\n",
    "        \"discovery_date\": record[\"discovery_timestamp\"][:10],  # Extract date only\n",
    "        \"processing_date\": datetime.now().isoformat()[:10],\n",
    "        \"discovery_timestamp\": record[\"discovery_timestamp\"],\n",
    "        \"extraction_timestamp\": record[\"text_extraction_timestamp\"],\n",
    "        \"llm_analysis_timestamp\": record[\"llm_analysis_timestamp\"],\n",
    "        \"enrichment_timestamp\": record[\"enrichment_timestamp\"],\n",
    "        \n",
    "        # Data warehouse metadata\n",
    "        \"pipeline_version\": \"1.0\",\n",
    "        \"processing_engine\": \"ray_data\",\n",
    "        \"llm_engine\": \"ray_data_llm_vllm\" if llm_available else \"fallback\",\n",
    "        \"source_system\": \"enterprise_data_lake\"\n",
    "    }\n",
    "    \n",
    "    return warehouse_record\n",
    "\n",
    "# Apply data warehouse schema transformationprint(\"Creating data warehouse schema...\")\n",
    "\n",
    "warehouse_dataset = enriched_documents.map_batches(\n",
    "    lambda batch: [create_data_warehouse_schema(record, batch_format=\"pandas\") for record in batch],\n",
    "    num_cpus=0.25,  # Light schema transformation\n",
    "    batch_size=2000\n",
    ")\n",
    "\n",
    "print(f\"Data warehouse schema created: {warehouse_dataset.count():,} records\")\n",
    "```\n",
    "\n",
    "### Data validation for warehouse integration\n",
    "\n",
    "``` python\n",
    "def validate_warehouse_data(record: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Validate data quality for data warehouse integration.\"\"\"\n",
    "    \n",
    "    validation_results = {\n",
    "        \"has_required_ids\": all(field in record and record[field] for field in [\"document_id\", \"chunk_id\"]),\n",
    "        \"has_content\": len(record.get(\"text_content\", \"\")) > 10,\n",
    "        \"has_business_classification\": record.get(\"business_category\") != \"general\",\n",
    "        \"has_quality_metrics\": record.get(\"quality_score\", 0) > 0,\n",
    "        \"has_processing_timestamps\": all(field in record for field in [\"discovery_timestamp\", \"llm_analysis_timestamp\"]),\n",
    "        \"has_analytics_metadata\": record.get(\"analytics_value\") in [\"high\", \"medium\", \"low\"]\n",
    "    }\n",
    "    \n",
    "    # Calculate validation score\n",
    "    validation_score = sum(validation_results.values())\n",
    "    validation_passed = validation_score >= 5  # Require 5/6 validations to pass\n",
    "    \n",
    "    return {\n",
    "        **record,\n",
    "        \"validation_results\": json.dumps(validation_results),\n",
    "        \"validation_score\": validation_score,\n",
    "        \"validation_passed\": validation_passed,\n",
    "        \"validation_timestamp\": datetime.now().isoformat(),\n",
    "        \"warehouse_ready\": validation_passed\n",
    "    }\n",
    "\n",
    "# Apply data validation using Ray Dataprint(\"Validating data for warehouse integration...\")\n",
    "\n",
    "validated_documents = warehouse_dataset.map_batches(\n",
    "    lambda batch: [validate_warehouse_data(record, batch_format=\"pandas\") for record in batch],\n",
    "    num_cpus=0.25,  # Light validation processing\n",
    "    batch_size=2000\n",
    ")\n",
    "\n",
    "# Filter documents ready for warehouse storagewarehouse_ready_docs = validated_documents.filter(\n",
    "    col(\"warehouse_ready\") == lit(True),\n",
    "    num_cpus=0.1\n",
    ")\n",
    "\n",
    "print(f\"Data validation completed:\")\n",
    "print(f\"  Total documents validated: {validated_documents.count():,}\")\n",
    "print(f\"  Warehouse-ready documents: {warehouse_ready_docs.count():,}\")\n",
    "```\n",
    "\n",
    "## Step 5: Data Warehouse Output\n",
    "\n",
    "### Document processing analytics\n",
    "\n",
    "``` python\n",
    "# Visualize document processing pipeline results\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate document processing analytics using utility function\n",
    "from util.viz_utils import visualize_document_processing, create_processing_funnel\n",
    "\n",
    "fig = visualize_document_processing()\n",
    "print(\"Document processing visualization created\")\n",
    "\n",
    "# Create interactive processing funnel\n",
    "funnel_fig = create_processing_funnel(quality_assessed_docs.to_pandas())\n",
    "funnel_fig.write_html('document_processing_funnel.html')\n",
    "print(\"Interactive processing funnel saved\")\n",
    "```\n",
    "\n",
    "### Write to data warehouse formats\n",
    "\n",
    "``` python\n",
    "# Write main warehouse table with partitioning for query optimizationprint(\"Writing to data warehouse formats...\")\n",
    "\n",
    "# Main warehouse table partitioned by business category and processing datewarehouse_ready_docs.write_parquet(\n",
    "    f\"{OUTPUT_WAREHOUSE_PATH}/main_table/\",\n",
    "    partition_cols=[\"business_category\", \"processing_date\"],\n",
    "    compression=\"snappy\",\n",
    "    num_cpus=0.1  # Moderate write concurrency\n",
    ")\n",
    "\n",
    "print(\"Main warehouse table written with partitioning\")\n",
    "\n",
    "# Create business-specific datasets for targeted analyticsprint(\"Creating business-specific analytics datasets...\")\n",
    "\n",
    "# Financial documents for financial analyticsfinancial_analytics = warehouse_ready_docs.filter(\n",
    "    col(\"business_category\") == lit(\"finance\"),\n",
    "    num_cpus=0.1\n",
    ").select_columns([\n",
    "    \"document_id\", \"chunk_id\", \"text_content\", \"llm_summary\", \n",
    "    \"file_size_mb\", \"word_count\", \"quality_score\", \"processing_date\",\n",
    "    \"has_financial_content\", \"metrics_count\"\n",
    "])\n",
    "\n",
    "financial_analytics.write_parquet(\n",
    "    f\"{OUTPUT_WAREHOUSE_PATH}/analytics/financial/\",\n",
    "    partition_cols=[\"processing_date\"],\n",
    "    compression=\"snappy\",\n",
    "    num_cpus=0.1\n",
    ")\n",
    "\n",
    "# Compliance documents for regulatory reportingcompliance_analytics = warehouse_ready_docs.filter(\n",
    "    col(\"business_category\") == lit(\"compliance\"),\n",
    "    num_cpus=0.1\n",
    ").select_columns([\n",
    "    \"document_id\", \"chunk_id\", \"text_content\", \"llm_summary\",\n",
    "    \"quality_score\", \"llm_priority\", \"processing_date\",\n",
    "    \"has_compliance_content\", \"entities_count\"\n",
    "])\n",
    "\n",
    "compliance_analytics.write_parquet(\n",
    "    f\"{OUTPUT_WAREHOUSE_PATH}/analytics/compliance/\",\n",
    "    partition_cols=[\"processing_date\"],\n",
    "    compression=\"snappy\",\n",
    "    num_cpus=0.1\n",
    ")\n",
    "\n",
    "# Research documents for business intelligenceresearch_analytics = warehouse_ready_docs.filter(\n",
    "    col(\"business_category\") == lit(\"research\"),\n",
    "    num_cpus=0.1\n",
    ").select_columns([\n",
    "    \"document_id\", \"chunk_id\", \"text_content\", \"llm_summary\",\n",
    "    \"analytics_value\", \"content_richness_score\", \"processing_date\"\n",
    "])\n",
    "\n",
    "research_analytics.write_parquet(\n",
    "    f\"{OUTPUT_WAREHOUSE_PATH}/analytics/research/\",\n",
    "    partition_cols=[\"processing_date\"],\n",
    "    compression=\"snappy\",\n",
    "    num_cpus=0.1\n",
    ")\n",
    "\n",
    "print(f\"Business-specific datasets created:\")\n",
    "print(f\"  Financial analytics: {financial_analytics.count():,} records\")\n",
    "print(f\"  Compliance analytics: {compliance_analytics.count():,} records\")\n",
    "print(f\"  Research analytics: {research_analytics.count():,} records\")\n",
    "```\n",
    "\n",
    "### Create analytics summary tables\n",
    "\n",
    "``` python\n",
    "# Create comprehensive analytics summaries using Ray Data native operationsprint(\"Creating analytics summary tables for data warehouse...\")\n",
    "\n",
    "from ray.data.aggregate import Count, Sum, Mean, Max, Min\n",
    "\n",
    "# Document processing metrics by category and dateprocessing_metrics = warehouse_ready_docs.groupby([\"business_category\", \"processing_date\"]).aggregate(\n",
    "    Count(),\n",
    "    Sum(\"file_size_mb\"),\n",
    "    Mean(\"text_length\"),\n",
    "    Mean(\"word_count\"),\n",
    "    Mean(\"quality_score\"),\n",
    "    Sum(\"chunk_word_count\"),\n",
    "    Mean(\"content_richness_score\")\n",
    ")\n",
    "\n",
    "processing_metrics.write_parquet(\n",
    "    f\"{OUTPUT_WAREHOUSE_PATH}/summaries/processing_metrics/\",\n",
    "    partition_cols=[\"processing_date\"],\n",
    "    compression=\"snappy\",\n",
    "    num_cpus=0.1\n",
    ")\n",
    "\n",
    "# Document quality distribution analysisquality_distribution = warehouse_ready_docs.groupby([\"quality_rating\", \"business_category\"]).aggregate(\n",
    "    Count(),\n",
    "    Mean(\"text_length\"),\n",
    "    Mean(\"entities_count\"),\n",
    "    Mean(\"metrics_count\"),\n",
    "    Sum(\"file_size_mb\")\n",
    ")\n",
    "\n",
    "quality_distribution.write_parquet(\n",
    "    f\"{OUTPUT_WAREHOUSE_PATH}/summaries/quality_distribution/\",\n",
    "    compression=\"snappy\",\n",
    "    num_cpus=0.1\n",
    ")\n",
    "\n",
    "# Llm analysis effectiveness summaryllm_analysis_summary = warehouse_ready_docs.groupby([\"llm_category\", \"llm_priority\"]).aggregate(\n",
    "    Count(),\n",
    "    Mean(\"chunk_length\"),\n",
    "    Sum(\"word_count\"),\n",
    "    Mean(\"content_richness_score\")\n",
    ")\n",
    "\n",
    "llm_analysis_summary.write_parquet(\n",
    "    f\"{OUTPUT_WAREHOUSE_PATH}/summaries/llm_analysis/\",\n",
    "    compression=\"snappy\",\n",
    "    num_cpus=0.1\n",
    ")\n",
    "\n",
    "# Document type and format analysisformat_analysis = warehouse_ready_docs.groupby([\"document_type\", \"file_extension\"]).aggregate(\n",
    "    Count(),\n",
    "    Mean(\"file_size_mb\"),\n",
    "    Sum(\"estimated_pages\"),\n",
    "    Mean(\"analytics_value\")\n",
    ")\n",
    "\n",
    "format_analysis.write_parquet(\n",
    "    f\"{OUTPUT_WAREHOUSE_PATH}/summaries/format_analysis/\",\n",
    "    compression=\"snappy\",\n",
    "    num_cpus=0.1\n",
    ")\n",
    "\n",
    "print(\"Analytics summary tables created:\")\n",
    "print(\"  - Processing metrics by category and date\")\n",
    "print(\"  - Quality distribution analysis\")\n",
    "print(\"  - LLM analysis effectiveness summary\")\n",
    "print(\"  - Document format analysis\")\n",
    "```\n",
    "\n",
    "### Data catalog and lineage\n",
    "\n",
    "``` python\n",
    "# Create comprehensive data catalog for warehouse integrationdef create_data_warehouse_catalog():\n",
    "    \"\"\"Create data catalog metadata for all warehouse tables.\"\"\"\n",
    "    \n",
    "    catalog_metadata = {\n",
    "        \"catalog_version\": \"1.0\",\n",
    "        \"created_timestamp\": datetime.now().isoformat(),\n",
    "        \"pipeline_name\": \"ray_data_document_ingestion\",\n",
    "        \"total_input_documents\": document_collection.count(),\n",
    "        \"total_warehouse_records\": warehouse_ready_docs.count(),\n",
    "        \"processing_engine\": \"ray_data\",\n",
    "        \"llm_integration\": \"ray_data_llm_package\" if llm_available else \"fallback\",\n",
    "        \n",
    "        \"warehouse_tables\": {\n",
    "            \"main_table\": {\n",
    "                \"path\": f\"{OUTPUT_WAREHOUSE_PATH}/main_table/\",\n",
    "                \"description\": \"Main document warehouse table with full content and metadata\",\n",
    "                \"partitioning\": [\"business_category\", \"processing_date\"],\n",
    "                \"record_count\": warehouse_ready_docs.count(),\n",
    "                \"schema_version\": \"1.0\",\n",
    "                \"update_frequency\": \"daily\",\n",
    "                \"retention_policy\": \"7_years\"\n",
    "            },\n",
    "            \"financial_analytics\": {\n",
    "                \"path\": f\"{OUTPUT_WAREHOUSE_PATH}/analytics/financial/\",\n",
    "                \"description\": \"Financial documents optimized for financial analytics\",\n",
    "                \"partitioning\": [\"processing_date\"],\n",
    "                \"record_count\": financial_analytics.count(),\n",
    "                \"specialized_for\": \"financial_reporting_and_analysis\"\n",
    "            },\n",
    "            \"compliance_analytics\": {\n",
    "                \"path\": f\"{OUTPUT_WAREHOUSE_PATH}/analytics/compliance/\",\n",
    "                \"description\": \"Compliance documents for regulatory reporting\",\n",
    "                \"partitioning\": [\"processing_date\"],\n",
    "                \"record_count\": compliance_analytics.count(),\n",
    "                \"specialized_for\": \"regulatory_compliance_monitoring\"\n",
    "            },\n",
    "            \"research_analytics\": {\n",
    "                \"path\": f\"{OUTPUT_WAREHOUSE_PATH}/analytics/research/\",\n",
    "                \"description\": \"Research documents for business intelligence\",\n",
    "                \"partitioning\": [\"processing_date\"],\n",
    "                \"record_count\": research_analytics.count(),\n",
    "                \"specialized_for\": \"business_intelligence_research\"\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        \"summary_tables\": {\n",
    "            \"processing_metrics\": {\n",
    "                \"path\": f\"{OUTPUT_WAREHOUSE_PATH}/summaries/processing_metrics/\",\n",
    "                \"description\": \"Daily processing metrics by business category\",\n",
    "                \"aggregation_level\": \"daily_by_category\"\n",
    "            },\n",
    "            \"quality_distribution\": {\n",
    "                \"path\": f\"{OUTPUT_WAREHOUSE_PATH}/summaries/quality_distribution/\",\n",
    "                \"description\": \"Document quality distribution analysis\",\n",
    "                \"aggregation_level\": \"quality_rating_by_category\"\n",
    "            },\n",
    "            \"llm_analysis\": {\n",
    "                \"path\": f\"{OUTPUT_WAREHOUSE_PATH}/summaries/llm_analysis/\",\n",
    "                \"description\": \"LLM analysis effectiveness metrics\",\n",
    "                \"aggregation_level\": \"llm_category_by_priority\"\n",
    "            },\n",
    "            \"format_analysis\": {\n",
    "                \"path\": f\"{OUTPUT_WAREHOUSE_PATH}/summaries/format_analysis/\",\n",
    "                \"description\": \"Document format and type analysis\",\n",
    "                \"aggregation_level\": \"document_type_by_format\"\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        \"data_lineage\": {\n",
    "            \"source_system\": \"enterprise_data_lake\",\n",
    "            \"processing_pipeline\": \"ray_data_document_ingestion\",\n",
    "            \"llm_processing\": \"ray_data_llm_package\",\n",
    "            \"output_format\": \"parquet_partitioned\",\n",
    "            \"compression\": \"snappy\",\n",
    "            \"schema_version\": \"1.0\"\n",
    "        },\n",
    "        \n",
    "        \"ray_data_operations_used\": [\n",
    "            \"read_binary_files() - Large-scale document discovery\",\n",
    "            \"map() - Metadata extraction and preprocessing\",\n",
    "            \"map_batches() - Text extraction and content analysis\",\n",
    "            \"filter() - Quality assessment and document filtering\",\n",
    "            \"flat_map() - Text chunking for LLM processing\",\n",
    "            \"groupby().aggregate() - Business analytics and summaries\",\n",
    "            \"select_columns() - Schema optimization for analytics\",\n",
    "            \"write_parquet() - Data warehouse output with partitioning\",\n",
    "            \"ray.data.llm - Integrated LLM processing for content analysis\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return catalog_metadata\n",
    "\n",
    "# Create and save data catalog using Ray Datacatalog_data = create_data_warehouse_catalog()\n",
    "\n",
    "# Save catalog metadata as JSONcatalog_dataset = ray.data.from_items([catalog_data])\n",
    "catalog_dataset.write_json(\n",
    "    f\"{OUTPUT_WAREHOUSE_PATH}/catalog/\",\n",
    "    compression=\"gzip\",\n",
    "    num_cpus=0.1\n",
    ")\n",
    "\n",
    "print(\"Data warehouse catalog created and saved\")\n",
    "```\n",
    "\n",
    "## Verification and Analytics Validation\n",
    "\n",
    "### Comprehensive output verification\n",
    "\n",
    "``` python\n",
    "# Verify all data warehouse outputsprint(\"Verifying data warehouse integration...\")\n",
    "\n",
    "# Verify main warehouse tablemain_table_verification = ray.data.read_parquet(\n",
    "    f\"{OUTPUT_WAREHOUSE_PATH}/main_table/\",\n",
    "    num_cpus=0.025\n",
    ")\n",
    "\n",
    "# Verify summary tablesprocessing_metrics_verification = ray.data.read_parquet(\n",
    "    f\"{OUTPUT_WAREHOUSE_PATH}/summaries/processing_metrics/\",\n",
    "    num_cpus=0.025\n",
    ")\n",
    "\n",
    "quality_verification = ray.data.read_parquet(\n",
    "    f\"{OUTPUT_WAREHOUSE_PATH}/summaries/quality_distribution/\",\n",
    "    num_cpus=0.025\n",
    ")\n",
    "\n",
    "print(f\"Data warehouse verification:\")\n",
    "print(f\"  Main table records: {main_table_verification.count():,}\")\n",
    "print(f\"  Processing metrics: {processing_metrics_verification.count():,}\")\n",
    "print(f\"  Quality analysis: {quality_verification.count():,}\")\n",
    "print(f\"  Schema compatibility:  Verified\")\n",
    "\n",
    "# Display sample analytics dataprint(\"Sample analytics data:\")\n",
    "sample_analytics = main_table_verification.take(3)\n",
    "for i, record in enumerate(sample_analytics):\n",
    "    print(f\"  {i+1}. Doc: {record['document_id'][:8]}, Category: {record['business_category']}, \"\n",
    "          f\"Type: {record['document_type']}, Words: {record['word_count']}, Quality: {record['quality_rating']}\")\n",
    "```\n",
    "\n",
    "### Business Intelligence Integration\n",
    "\n",
    "``` python\n",
    "# Create BI-ready views using Ray Data operationsprint(\"Creating business intelligence views...\")\n",
    "\n",
    "# Executive dashboard viewexecutive_view = warehouse_ready_docs.select_columns([\n",
    "    \"document_id\", \"business_category\", \"document_type\", \"llm_summary\",\n",
    "    \"quality_score\", \"analytics_value\", \"processing_date\"\n",
    "]).filter(\n",
    "    col(\"analytics_value\") == lit(\"high\"),\n",
    "    num_cpus=0.1\n",
    ")\n",
    "\n",
    "executive_view.write_parquet(\n",
    "    f\"{OUTPUT_WAREHOUSE_PATH}/bi_views/executive_dashboard/\",\n",
    "    partition_cols=[\"business_category\"],\n",
    "    compression=\"snappy\",\n",
    "    num_cpus=0.1\n",
    ")\n",
    "\n",
    "# Operational metrics view\n",
    "operational_view = warehouse_ready_docs.select_columns([\n",
    "    \"document_id\", \"file_size_mb\", \"text_length\", \"word_count\",\n",
    "    \"quality_score\", \"processing_date\", \"llm_category\"\n",
    "]).filter(\n",
    "    col(\"quality_score\") >= lit(3),\n",
    "    num_cpus=0.1\n",
    ")\n",
    "\n",
    "operational_view.write_parquet(\n",
    "    f\"{OUTPUT_WAREHOUSE_PATH}/bi_views/operational_metrics/\",\n",
    "    partition_cols=[\"processing_date\"],\n",
    "    compression=\"snappy\",\n",
    "    num_cpus=0.1\n",
    ")\n",
    "\n",
    "print(f\"Business intelligence views created:\")\n",
    "print(f\"  Executive dashboard: {executive_view.count():,} records\")\n",
    "print(f\"  Operational metrics: {operational_view.count():,} records\")\n",
    "```\n",
    "\n",
    "## Pipeline Performance Summary\n",
    "\n",
    "``` python\n",
    "# Calculate comprehensive pipeline processing metrics\n",
    "print(\"=\" * 80)\n",
    "print(\"DOCUMENT INGESTION PIPELINE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Processing metrics\n",
    "total_input_docs = document_collection.count()\n",
    "total_chunks_created = enriched_documents.count()\n",
    "total_warehouse_records = warehouse_ready_docs.count()\n",
    "total_financial_records = financial_analytics.count()\n",
    "total_compliance_records = compliance_analytics.count()\n",
    "\n",
    "print(f\" PROCESSING METRICS:\")\n",
    "print(f\"    Input documents discovered: {total_input_docs:,}\")\n",
    "print(f\"    Text chunks created: {total_chunks_created:,}\")\n",
    "print(f\"    Warehouse records generated: {total_warehouse_records:,}\")\n",
    "print(f\"    Financial records: {total_financial_records:,}\")\n",
    "print(f\"    Compliance records: {total_compliance_records:,}\")\n",
    "print(f\"    Average chunks per document: {total_chunks_created / total_input_docs:.1f}\")\n",
    "\n",
    "# Data warehouse statistics\n",
    "business_categories = warehouse_ready_docs.select_columns([\"business_category\"]).distinct(num_cpus=0.1).take_all()\n",
    "document_types = warehouse_ready_docs.select_columns([\"document_type\"]).distinct(num_cpus=0.1).take_all()\n",
    "\n",
    "print(f\" DATA WAREHOUSE STATISTICS:\")\n",
    "print(f\"    Business categories: {len(business_categories)}\")\n",
    "print(f\"    Document types: {len(document_types)}\")\n",
    "print(f\"    Output formats: Parquet with Snappy compression\")\n",
    "print(f\"    Partitioning strategy: business_category, processing_date\")\n",
    "print(f\"    Summary tables: 4 analytics tables created\")\n",
    "print(f\"    BI views: 2 specialized views for business intelligence\")\n",
    "\n",
    "print(f\" RAY DATA OPERATIONS DEMONSTRATED:\")\n",
    "print(f\"    read_binary_files() - Large-scale document discovery\")\n",
    "print(f\"    map() - Metadata extraction and preprocessing\")\n",
    "print(f\"    map_batches() - Text extraction and content analysis\")\n",
    "print(f\"    filter() - Quality assessment and document filtering\")\n",
    "print(f\"    flat_map() - Text chunking for LLM processing\")\n",
    "print(f\"    groupby().aggregate() - Business analytics and summaries\")\n",
    "print(f\"    select_columns() - Schema optimization for analytics\")\n",
    "print(f\"    distinct() - Data deduplication and analysis\")\n",
    "print(f\"    write_parquet() - Data warehouse output with partitioning\")\n",
    "print(f\"    write_json() - Metadata and catalog management\")\n",
    "print(f\"    ray.data.llm - Integrated LLM processing for content analysis\")\n",
    "\n",
    "print(f\" DATA WAREHOUSE INTEGRATION:\")\n",
    "print(f\"    Partitioned tables for query optimization\")\n",
    "print(f\"    Business-specific analytics datasets\")\n",
    "print(f\"    Summary tables for operational monitoring\")\n",
    "print(f\"    BI views for executive dashboards\")\n",
    "print(f\"    Data catalog for metadata management\")\n",
    "print(f\"    Schema standardization for BI tool integration\")\n",
    "print(f\"    Data lineage tracking for governance\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "```\n",
    "\n",
    "## Advanced Ray Data Features Demonstrated\n",
    "\n",
    "### Native Operations Usage\n",
    "\n",
    "This template showcases comprehensive Ray Data native operations:\n",
    "\n",
    "**Data Discovery and Loading:**\n",
    "- `ray.data.read_binary_files()` - Efficient binary file reading from S3\n",
    "- `include_paths=True` - Path tracking for data lineage\n",
    "- `num_cpus=0.025` - High I/O concurrency for large collections\n",
    "\n",
    "**Data Transformation:**\n",
    "- `map()` - Row-wise metadata extraction and preprocessing\n",
    "- `map_batches()` - Vectorized text extraction and content analysis\n",
    "- `flat_map()` - One-to-many text chunking operations\n",
    "\n",
    "**Data Filtering and Selection:**\n",
    "- `filter()` with expressions API - Optimized filtering using `col()` and `lit()`\n",
    "- `select_columns()` - Schema optimization for analytics\n",
    "- `distinct()` - Data deduplication and unique value analysis\n",
    "\n",
    "**Analytics and Aggregation:**\n",
    "- `groupby().aggregate()` - Distributed business analytics\n",
    "- Native aggregation functions: `Count()`, `Sum()`, `Mean()`, `Max()`, `Min()`\n",
    "- Multi-dimensional grouping for comprehensive analytics\n",
    "\n",
    "**Data Output and Storage:**\n",
    "- `write_parquet()` - Efficient columnar storage with compression\n",
    "- `write_json()` - Metadata and catalog management\n",
    "- `partition_cols` - Query optimization through partitioning\n",
    "- `compression=\"snappy\"` - Storage optimization\n",
    "\n",
    "**LLM Integration:**\n",
    "- `ray.data.llm.vLLMEngineProcessorConfig` - LLM engine configuration\n",
    "- `build_llm_processor()` - Integrated LLM processing pipeline\n",
    "- Custom preprocessing and postprocessing functions\n",
    "- Batch inference optimization for document analysis\n",
    "\n",
    "## Ray Data Architecture for Document Processing\n",
    "\n",
    "Understanding Ray Data’s architecture is essential for building efficient document ingestion pipelines.\n",
    "\n",
    "### Streaming Execution Model\n",
    "\n",
    "Ray Data’s streaming execution enables processing millions of documents with constant memory usage:\n",
    "\n",
    "**Traditional Batch Processing:**\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/cko-2025-q1/batch-processing.png\" width=\"800\" alt=\"Traditional Batch Processing\">\n",
    "\n",
    "**Problems with traditional approach:**\n",
    "- High memory - requires loading all documents\n",
    "- No parallelism - stages run sequentially\n",
    "- Long latency - wait for complete load before processing\n",
    "- Wasted resources - CPUs/GPUs idle during load/write stages\n",
    "\n",
    "**Ray Data Streaming Execution:**\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/cko-2025-q1/pipelining.png\" width=\"800\" alt=\"Ray Data Streaming Execution\">\n",
    "\n",
    "**Benefits of streaming execution:**\n",
    "- Low memory - constant 128MB blocks regardless of document count\n",
    "- Pipeline parallelism - all stages active simultaneously\n",
    "- Fast first result - processing starts immediately\n",
    "- Maximum throughput - all resources utilized continuously\n",
    "\n",
    "**Practical example for document processing:**\n",
    "\n",
    "``` python\n",
    "# This pipeline runs all stages simultaneously\n",
    "processed_docs = (\n",
    "    # Stage 1: Discover and load documents\n",
    "    ray.data.read_binary_files(\"s3://documents/\", num_cpus=0.025)\n",
    "    \n",
    "    # Stage 2: Extract metadata (parallel with stage 1)\n",
    "    .map(extract_metadata, num_cpus=0.25)\n",
    "    \n",
    "    # Stage 3: Extract text (parallel with stages 1-2)\n",
    "    .map_batches(extract_text, batch_size=500, num_cpus=1.0)\n",
    "    \n",
    "    # Stage 4: Chunk for LLM (parallel with stages 1-3)\n",
    "    .flat_map(chunk_text, num_cpus=0.5)\n",
    "    \n",
    "    # Stage 5: Write to warehouse (starts as soon as first chunks ready)\n",
    "    .write_parquet(\"s3://warehouse/\", num_cpus=0.1)\n",
    ")\n",
    "\n",
    "# All stages run simultaneously!\n",
    "# Document 1 can be written while Document 1000 is being discovered\n",
    "# Memory stays constant for 1000 or 1,000,000 documents\n",
    "```\n",
    "\n",
    "### Datasets and Blocks\n",
    "\n",
    "Ray Data processes documents in **blocks**:\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/dataset-arch.svg\" width=\"700\" alt=\"Ray Data Block Architecture\">\n",
    "\n",
    "**Key concepts for document processing:**\n",
    "- **Blocks**: Groups of documents (typically ~128 MB of content)\n",
    "- **Distributed storage**: Blocks stored in Ray Object Store\n",
    "- **Independent processing**: Each block processed in parallel\n",
    "- **Configurable size**: Tune via `DataContext.target_max_block_size`\n",
    "\n",
    "**Why blocks matter for document ingestion:**\n",
    "- **Memory efficiency**: Process 100 documents at a time, not all 1M at once\n",
    "- **Parallelism**: 1000 blocks = 1000 parallel processing units\n",
    "- **Scalability**: Same code for 100 docs or 100M docs\n",
    "- **Performance**: Optimal throughput without manual tuning\n",
    "\n",
    "### Ray Memory Model\n",
    "\n",
    "Ray manages memory efficiently for document processing:\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/memory.svg\" width=\"600\" alt=\"Ray Memory Model\">\n",
    "\n",
    "**1. Object Store Memory (30% of node memory):**\n",
    "- Stores document blocks as shared memory\n",
    "- Enables zero-copy sharing between pipeline stages\n",
    "- Automatically spills to disk when full\n",
    "- Critical for passing documents through pipeline\n",
    "\n",
    "**2. Task Execution Memory (remaining memory):**\n",
    "- Used for text extraction, LLM inference, transformations\n",
    "- Allocated per worker\n",
    "- Released after document batch processing\n",
    "\n",
    "**Why this matters for document ingestion:**\n",
    "- **Resource planning**: Size cluster for peak document size + LLM models\n",
    "- **Performance tuning**: Avoid object store pressure with proper `num_cpus`\n",
    "- **Batch sizing**: Match batch sizes to available execution memory\n",
    "\n",
    "### Operators and Resource Management\n",
    "\n",
    "Ray Data uses **physical operators** for document processing:\n",
    "\n",
    "**Common operators:**\n",
    "- **TaskPoolMapOperator**: For stateless document transformations\n",
    "- **ActorPoolMapOperator**: For stateful operations (LLM inference)\n",
    "- **AllToAllOperator**: For document grouping and aggregations\n",
    "\n",
    "**Operator fusion for document processing:**\n",
    "\n",
    "``` python\n",
    "# These operations get fused automatically\n",
    "docs.map(extract_metadata).map(validate_metadata)\n",
    "# Becomes: TaskPoolMapOperator[extract_metadata->validate_metadata]\n",
    "# Result: No data transfer, single task per document block\n",
    "```\n",
    "\n",
    "**Resource management and backpressure:**\n",
    "- **Dynamic allocation**: Resources distributed across pipeline stages\n",
    "- **Backpressure**: Prevents memory overflow during heavy LLM processing\n",
    "- **Automatic tuning**: No manual configuration needed\n",
    "\n",
    "**Why this matters for document pipelines:**\n",
    "- **LLM integration**: Actors enable efficient model loading and reuse\n",
    "- **Memory safety**: Backpressure prevents OOM during text extraction\n",
    "- **Resource efficiency**: All stages utilize resources simultaneously\n",
    "\n",
    "### Performance Optimization Patterns\n",
    "\n",
    "**Resource Allocation Following Ray Data Best Practices:**\n",
    "- **I/O Operations**: `num_cpus=0.025-0.05` for maximum parallelism\n",
    "- **Light Processing**: `num_cpus=0.25` for quality assessment and validation\n",
    "- **Medium Processing**: `num_cpus=0.5` for text chunking and content analysis\n",
    "- **Heavy Processing**: `num_cpus=1.0` for text extraction and complex transformations\n",
    "- **Write Operations**: `num_cpus=0.1` for balanced output concurrency\n",
    "\n",
    "**Batch Size Optimization:**\n",
    "- **Small batches (500)** for heavy text extraction\n",
    "- **Medium batches (1000-2000)** for content analysis and validation\n",
    "- **Large batches (2000)** for light schema transformations\n",
    "- **LLM batches (32)** for optimal GPU utilization\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "``` python\n",
    "# Cleanup Ray resources following best practices\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "print(\"Ray shutdown completed\")\n",
    "```\n",
    "\n",
    "## Ray Data Performance Summary\n",
    "\n",
    "This template demonstrates comprehensive Ray Data best practices for enterprise document ingestion:\n",
    "\n",
    "- **Native operations**: Extensive use of `read_binary_files()`, `filter()`, `groupby()`, `aggregate()`, `select_columns()`, `distinct()`\n",
    "- **Proper resource allocation**: All operations specify `num_cpus` following optimization guidelines\n",
    "- **Expressions API**: Using `col()` and `lit()` for query optimization and performance\n",
    "- **LLM integration**: Using `ray.data.llm` package for content analysis and structured extraction\n",
    "- **Monitoring setup**: Progress bars enabled for performance visibility and bottleneck identification\n",
    "- **Data warehouse patterns**: Partitioned output, summary tables, BI views, and data catalog\n",
    "- **Analytics integration**: Business-specific datasets optimized for different analytics use cases\n",
    "- **Large-scale processing**: Optimized for enterprise document collections and data lake ingestion\n",
    "- **Resource cleanup**: Proper Ray shutdown procedures following best practices\n",
    "\n",
    "This pipeline transforms unstructured data lake documents into structured, analytics-ready datasets suitable for enterprise data warehouse consumption, business intelligence tools, and advanced analytics workflows."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
