{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the Ray AI Libraries\n",
    "\n",
    "Let's start with a quick end-to-end example to get a sense of what the Ray AI Libraries can do.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Here is the roadmap for this notebook:</b>\n",
    "<ul>\n",
    "    <li><b>Part 1:</b> Overview of the Ray AI Libraries</a></li>\n",
    "    <li><b>Part 2:</b> Quick end-to-end example</a></li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview of the Ray AI Libraries\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_AI_Libraries/Ray+AI+Libraries.png\" width=\"70%\" loading=\"lazy\">\n",
    "\n",
    "Built on top of Ray Core, the Ray AI Libraries inherit all the performance and scalability benefits offered by Core while providing a convenient abstraction layer for machine learning. These Python-first native libraries allow ML practitioners to distribute individual workloads, end-to-end applications, and build custom use cases in a unified framework.\n",
    "\n",
    "The Ray AI Libraries bring together an ever-growing ecosystem of integrations with popular machine learning frameworks to create a common interface for development.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Introduction_to_Ray_AIR/e2e_air.png\" width=\"100%\" loading=\"lazy\">|\n",
    "|:-:|\n",
    "|Ray AI Libraries enable end-to-end ML development and provides multiple options for integrating with other tools and libraries form the MLOps ecosystem.|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quick end-to-end example\n",
    "\n",
    "|Ray AIR Component|NYC Taxi Use Case|\n",
    "|:--|:--|\n",
    "|Ray Data|Ingest and transform raw data; perform batch inference by mapping the checkpointed model to batches of data.|\n",
    "|Ray Train|Use `Trainer` to scale XGBoost model training.|\n",
    "|Ray Tune|Use `Tuner` for hyperparameter search.|\n",
    "|Ray Serve|Deploy the model for online inference.|\n",
    "\n",
    "For this classification task, you will apply a simple [XGBoost](https://xgboost.readthedocs.io/en/stable/) (a gradient boosted trees framework) model to the June 2021 [New York City Taxi & Limousine Commission's Trip Record Data](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). This dataset contains over 2 million samples of yellow cab rides, and the goal is to predict whether a trip will result in a tip greater than 20% or not.\n",
    "\n",
    "**Dataset features**\n",
    "* **`passenger_count`**\n",
    "    * Float (whole number) representing number of passengers.\n",
    "* **`trip_distance`** \n",
    "    * Float representing trip distance in miles.\n",
    "* **`fare_amount`**\n",
    "    * Float representing total price including tax, tip, fees, etc.\n",
    "* **`trip_duration`**\n",
    "    * Integer representing seconds elapsed.\n",
    "* **`hour`**\n",
    "    * Hour that the trip started.\n",
    "    * Integer in the range `[0, 23]`\n",
    "* **`day_of_week`**\n",
    "    * Integer in the range `[1, 7]`.\n",
    "* **`is_big_tip`**\n",
    "    * Whether the tip amount was greater than 20%.\n",
    "    * Boolean `[True, False]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import xgboost\n",
    "from starlette.requests import Request\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.train import ScalingConfig, RunConfig\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "from ray.tune import Tuner, TuneConfig\n",
    "from ray import serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Read, preprocess with Ray Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "dataset = ray.data.read_parquet(\"s3://anonymous@anyscale-training-data/intro-to-ray-air/nyc_taxi_2021.parquet\")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Fit model with Ray Train__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the trainer\n",
    "trainer = XGBoostTrainer(\n",
    "    label_column=\"is_big_tip\",\n",
    "    scaling_config=ScalingConfig(num_workers=4, use_gpu=False),\n",
    "    params={\"objective\": \"binary:logistic\"},\n",
    "    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    "    run_config=RunConfig(storage_path=\"/mnt/cluster_storage/\"),\n",
    ")\n",
    "\n",
    "# Fit the trainer\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Optimize hyperparameters with Ray Tune__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the tuner\n",
    "tuner = Tuner(\n",
    "    trainer,\n",
    "    param_space={\"params\": {\"max_depth\": tune.randint(2, 12)}},\n",
    "    tune_config=TuneConfig(num_samples=3, metric=\"valid-logloss\", mode=\"min\"),\n",
    "    run_config=RunConfig(storage_path=\"/mnt/cluster_storage/\"),\n",
    ")\n",
    "\n",
    "# Fit the tuner and get the best checkpoint\n",
    "checkpoint = tuner.fit().get_best_result().checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Batch inference with Ray Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OfflinePredictor:\n",
    "    def __init__(self):\n",
    "        # Load expensive state\n",
    "        self._model = xgboost.Booster()\n",
    "        self._model.load_model(checkpoint.path + \"/model.ubj\")\n",
    "\n",
    "    def __call__(self, batch: dict) -> dict:\n",
    "        # Make prediction in batch\n",
    "        dmatrix = xgboost.DMatrix(pd.DataFrame(batch))\n",
    "        outputs = self._model.predict(dmatrix)\n",
    "        return {\"prediction\": outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply the predictor to the validation dataset\n",
    "valid_dataset_inputs = valid_dataset.drop_columns(['is_big_tip'])\n",
    "predicted_probabilities = valid_dataset_inputs.map_batches(OfflinePredictor, concurrency=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Materialize a batch\n",
    "predicted_probabilities.take_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Online prediction with Ray Serve__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class OnlinePredictor:\n",
    "    def __init__(self, checkpoint):\n",
    "        # Load expensive state\n",
    "        self._model = xgboost.Booster()\n",
    "        self._model.load_model(checkpoint.path + \"/model.ubj\")\n",
    "\n",
    "    async def __call__(self, request: Request) -> dict:\n",
    "        # Handle HTTP request\n",
    "        data = await request.json()\n",
    "        data = json.loads(data)\n",
    "        return {\"prediction\": self.predict(data)}\n",
    "\n",
    "    def predict(self, data: list[dict]) -> list[float]:\n",
    "        # Make prediction\n",
    "        dmatrix = xgboost.DMatrix(pd.DataFrame(data))\n",
    "        return self._model.predict(dmatrix)\n",
    "\n",
    "# Run the deployment\n",
    "handle = serve.run(OnlinePredictor.bind(checkpoint=checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Form payload\n",
    "valid_dataset_inputs = valid_dataset.drop_columns([\"is_big_tip\"])\n",
    "sample_batch = valid_dataset_inputs.take_batch(1)\n",
    "data = pd.DataFrame(sample_batch).to_json(orient=\"records\")\n",
    "\n",
    "# Send HTTP request\n",
    "requests.post(\"http://localhost:8000/\", json=data).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Shutdown Ray Serve\n",
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "!rm -rf /mnt/cluster_storage/XGBoostTrainer*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
