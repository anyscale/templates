{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Intro to Ray Tune\n","\n","This notebook will walk you through the basics of hyperparameter tuning with Ray Tune.\n","\n","<div class=\"alert alert-block alert-info\">\n","<p> Here is the roadmap for this notebook:</p>\n","<ul>\n","    <li> <b>Part 1:</b> Loading the data </li>\n","    <li> <b>Part 2:</b> Starting out with vanilla PyTorch </li>\n","    <li> <b>Part 3:</b> Hyperparameter tuning with Ray Tune </li>\n","</ul>\n","</div>\n"]},{"cell_type":"markdown","metadata":{},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tempfile\n","import os\n","from typing import Any\n","\n","import torch\n","import numpy as np\n","from torchvision.datasets import MNIST\n","from torchvision.transforms import Compose, ToTensor, Normalize\n","from torchvision.models import resnet18\n","from torch.utils.data import DataLoader\n","from torch.optim import Adam\n","from torch.nn import CrossEntropyLoss\n","\n","import ray\n","from ray import tune, train\n","from ray.tune.search import optuna"]},{"cell_type":"markdown","metadata":{},"source":["## Loading the data\n","\n","Our Dataset is the MNIST dataset\n","\n","The MNIST dataset consists of 28x28 pixel grayscale images of handwritten digits (0-9).\n","\n","Dataset details:\n","- Training set: 60,000 images\n","- Test set: 10,000 images\n","- Image size: 28x28 pixels\n","- Number of classes: 10 (digits 0-9)\n","\n","Data format:\n","Each image is represented as a 2D array of pixel values, where each pixel is a grayscale intensity between 0 (black) and 255 (white).\n","\n","Task:\n","The task is to classify each image into one of the 10 digit classes (0-9)."]},{"cell_type":"markdown","metadata":{},"source":["## Starting out with vanilla PyTorch\n","\n","Here is a high level overview of the model training process:\n","\n","- **Objective**: Classify handwritten digits (0-9)\n","- **Model**: Simple Neural Network using PyTorch\n","- **Evaluation Metric**: Accuracy\n","- **Dataset**: MNIST\n","\n","We'll start with a basic PyTorch implementation to establish a baseline before moving on to more advanced techniques. This will give us a good foundation for understanding the benefits of hyperparameter tuning and distributed training in later sections."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_loop_torch(num_epochs: int = 2, batch_size: int = 128, lr: float = 1e-5):\n","    criterion = CrossEntropyLoss()\n","\n","    model = resnet18()\n","    model.conv1 = torch.nn.Conv2d(\n","        1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n","    )\n","    model.to(\"cuda\")\n","\n","    optimizer = Adam(model.parameters(), lr=lr)\n","    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n","    train_data = MNIST(root=\"./data\", train=True, download=True, transform=transform)\n","    data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n","\n","    for epoch in range(num_epochs):\n","        for images, labels in data_loader:\n","            images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        # Report the metrics\n","        print(f\"Epoch {epoch}, Loss: {loss}\")"]},{"cell_type":"markdown","metadata":{},"source":["We fit the model by submitting it onto a GPU node using Ray Core"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_loop_torch(num_epochs=2)"]},{"cell_type":"markdown","metadata":{},"source":["**Can we do any better ?** let's see if we can tune the hyperparameters of our model to get a better loss.\n","\n","But hyperparameter tuning is a computationally expensive task, and it will take a long time to run sequentially.\n","\n","[Ray Tune](https://docs.ray.io/en/master/tune/) is a distributed hyperparameter tuning library that can help us speed up the process!"]},{"cell_type":"markdown","metadata":{},"source":["## Hyperparameter tuning with Ray Tune"]},{"cell_type":"markdown","metadata":{},"source":["### Intro to Ray Tune\n","\n","<img src=\"https://docs.ray.io/en/master/_images/tune_overview.png\" width=\"250\">\n","\n","Tune is a Python library for experiment execution and hyperparameter tuning at any scale.\n","\n","Let's take a look at a very simple example of how to use Ray Tune to tune the hyperparameters of our XGBoost model."]},{"cell_type":"markdown","metadata":{},"source":["### Getting started\n","\n","We start by defining our training function"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def my_simple_model(distance: np.ndarray, a: float) -> np.ndarray:\n","    return distance * a\n","\n","# Step 1: Define the training function\n","def train_my_simple_model(config: dict[str, Any]) -> None: # Expected function signature for Ray Tune\n","    distances = np.array([0.1, 0.2, 0.3, 0.4, 0.5])\n","    total_amts = distances * 10\n","    \n","    a = config[\"a\"]\n","    predictions = my_simple_model(distances, a)\n","    rmse = np.sqrt(np.mean((total_amts - predictions) ** 2))\n","\n","    train.report({\"rmse\": rmse}) # This is how we report the metric to Ray Tune"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-block alert-warning\">\n","<b>Note:</b> how the training function needs to accept a config argument. This is because Ray Tune will pass the hyperparameters to the training function as a dictionary.\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["Next, we define and run the hyperparameter tuning job by following these steps:\n","\n","1. Create a `Tuner` object (in our case named `tuner`)\n","2. Call `tuner.fit`"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Step 2: Set up the Tuner\n","tuner = tune.Tuner(\n","    trainable=train_my_simple_model,  # Training function or class to be tuned\n","    param_space={\n","        \"a\": tune.randint(0, 20),  # Hyperparameter: a\n","    },\n","    tune_config=tune.TuneConfig(\n","        metric=\"rmse\",  # Metric to optimize (minimize)\n","        mode=\"min\",     # Minimize the metric\n","        num_samples=5,  # Number of samples to try\n","    ),\n",")\n","\n","# Step 3: Run the Tuner and get the results\n","results = tuner.fit()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Step 4: Get the best result\n","best_result = results.get_best_result()\n","best_result"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["best_result.config"]},{"cell_type":"markdown","metadata":{},"source":["So let's recap what actually happened here ?\n","\n","```python\n","tuner = tune.Tuner(\n","    trainable=train_my_simple_model,  # Training function or class to be tuned\n","    param_space={\n","        \"a\": tune.randint(0, 20),  # Hyperparameter: a\n","    },\n","    tune_config=tune.TuneConfig(\n","        metric=\"rmse\",  # Metric to optimize (minimize)\n","        mode=\"min\",     # Minimize the metric\n","        num_samples=5,  # Number of samples to try\n","    ),\n",")\n","\n","results = tuner.fit()\n","```\n","\n","A Tuner accepts:\n","- A training function or class which is specified by `trainable`\n","- A search space which is specified by `param_space`\n","- A metric to optimize which is specified by `metric` and the direction of optimization `mode`\n","- `num_samples` which correlates to the number of trials to run\n","\n","`tuner.fit` then runs multiple trials in parallel, each with a different set of hyperparameters, and returns the best set of hyperparameters found.\n"]},{"cell_type":"markdown","metadata":{},"source":["### Diving deeper into Ray Tune concepts\n","\n","You might be wondering:\n","- How does the tuner allocate resources to trials?\n","- How does it decide how to tune - i.e. which trials to run next?\n","    - e.g. A random search, or a more sophisticated search algorithm like a bayesian optimization algorithm.\n","- How does it decide when to stop - i.e. whether to kill a trial early?\n","    - e.g. If a trial is performing poorly compared to other trials, it perhaps makes sense to stop it early (successive halving, hyperband)\n","\n","By default: \n","- Each trial will run in a separate process and consume 1 CPU core.\n","- Ray Tune uses a search algorithm to decide which trials to run next.\n","- Ray Tune uses a scheduler to decide if/when to stop trials, or to prioritize certain trials over others."]},{"cell_type":"markdown","metadata":{},"source":["Here is the same code with the default settings for Ray Tune *explicitly* specified."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tuner = tune.Tuner(\n","    trainable=tune.with_resources(train_my_simple_model, {\"cpu\": 1}), # this is how to specify resources for your trainable\n","    param_space={\"a\": tune.randint(0, 20)},\n","    tune_config=tune.TuneConfig(\n","        mode=\"min\",\n","        metric=\"rmse\",\n","        num_samples=5, \n","        search_alg=tune.search.BasicVariantGenerator(), # performs a basic variation of random/grid search based on parameter space\n","        scheduler=tune.schedulers.FIFOScheduler(), # this is a simple scheduler - no early stopping, just run all trials in submission order\n","    ),\n",")\n","results = tuner.fit()"]},{"cell_type":"markdown","metadata":{},"source":["Below is a diagram showing the relationship between the different Ray Tune components we have discussed.\n","\n","<img src=\"https://docs.ray.io/en/latest/_images/tune_flow.png\" width=\"800\" />\n"]},{"cell_type":"markdown","metadata":{},"source":["Here is the same experiment table annotated \n","\n","<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/attentive-ray-101/experiment_table_annotated.png\" width=\"800\" />\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Exercise\n","\n","<div class=\"alert alert-block alert-info\">\n","    \n","__Lab activity: Finetune a linear regression model.__\n","    \n","\n","Given the below code to train a linear regression model from scratch: \n","\n","```python\n","def train_linear_model(lr: float, epochs: int) -> None:\n","    x = np.array([0, 1, 2, 3, 4])\n","    y = x * 2\n","    w = 0\n","    for _ in range(epochs):\n","        loss = np.sqrt(np.mean((w * x - y) ** 2))\n","        dl_dw = np.mean(2 * x * (w * x - y)) \n","        w -= lr * dl_dw\n","        print({\"rmse\": loss})\n","\n","# Hint: Step 1 update the function signature\n","\n","# Hint: Step 2 Create the tuner object\n","tuner = tune.Tuner(...)\n","\n","# Hint: Step 3: Run the tuner\n","results = tuner.fit()\n","```\n","\n","Use Ray Tune to tune the hyperparameters `lr` and `epochs`. \n","\n","Perform a search using the optuna.OptunaSearch search algorithm with 5 samples over the following ranges:\n","- `lr`: loguniform(1e-4, 1e-1)\n","- `epochs`: randint(1, 100)\n","\n","</div>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Write your code here\n"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-block alert-info\">\n","<details>\n","<summary>Click here to view the solution</summary>\n","\n","```python\n","def train_linear_model(config) -> None:\n","    epochs = config[\"epochs\"]\n","    lr = config[\"lr\"]\n","    x = np.array([0, 1, 2, 3, 4])\n","    y = x * 2\n","    w = 0\n","    for _ in range(epochs):\n","        loss = np.sqrt(np.mean((w * x - y) ** 2))\n","        dl_dw = np.mean(2 * x * (w * x - y)) \n","        w -= lr * dl_dw\n","        train.report({\"rmse\": loss})\n","\n","tuner = tune.Tuner(\n","    trainable=train_linear_model,  # Training function or class to be tuned\n","    param_space={\n","        \"lr\": tune.loguniform(1e-4, 1e-1),  # Hyperparameter: learning rate\n","        \"epochs\": tune.randint(1, 100),  # Hyperparameter: number of epochs\n","    },\n","    tune_config=tune.TuneConfig(\n","        metric=\"rmse\",  # Metric to optimize (minimize)\n","        mode=\"min\",     # Minimize the metric\n","        num_samples=5,  # Number of samples to try\n","        search_alg=optuna.OptunaSearch(), # Use Optuna for hyperparameter search\n","    ),\n",")\n","\n","results = tuner.fit()\n","```\n","\n","</details>\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["### Hyperparameter tune the PyTorch model using Ray Tune\n","\n","The first step is to move in all the PyTorch code into a function that we can pass to the `trainable` argument of the `tune.run` function."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_pytorch(config): # we change the function so it accepts a config dictionary\n","    criterion = CrossEntropyLoss()\n","\n","    model = resnet18()\n","    model.conv1 = torch.nn.Conv2d(\n","        1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n","    )\n","    model.to(\"cuda\")\n","\n","    optimizer = Adam(model.parameters(), lr=config[\"lr\"])\n","    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n","    train_data = MNIST(root=\"./data\", train=True, download=True, transform=transform)\n","    data_loader = DataLoader(train_data, batch_size=config[\"batch_size\"], shuffle=True, drop_last=True)\n","\n","    for epoch in range(config[\"num_epochs\"]):\n","        for images, labels in data_loader:\n","            images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        # Report the metrics using train.report instead of print\n","        train.report({\"loss\": loss.item()})"]},{"cell_type":"markdown","metadata":{},"source":["The second and third steps are the same as before. We define the tuner and run it by calling the fit method."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tuner = tune.Tuner(\n","    trainable=tune.with_resources(train_pytorch, {\"gpu\": 1}), # we will dedicate 1 GPU to each trial\n","    param_space={\n","        \"num_epochs\": 1,\n","        \"batch_size\": 128,\n","        \"lr\": tune.loguniform(1e-4, 1e-1),\n","    },\n","    tune_config=tune.TuneConfig(\n","        mode=\"min\",\n","        metric=\"loss\",\n","        num_samples=2,\n","        search_alg=tune.search.BasicVariantGenerator(),\n","        scheduler=tune.schedulers.FIFOScheduler(),\n","    ),\n",")\n","\n","results = tuner.fit()"]},{"cell_type":"markdown","metadata":{},"source":["Finally, we can get the best result and its configuration:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["best_result = results.get_best_result()\n","best_result.config"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":2}
