{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Hyperparameter tuning of distributed training with Ray Tune and Ray Train\n",
    "\n",
    "This is a bonus notebook that shows how to perform hyperparameter tuning of distributed training with Ray Tune and Ray Train.\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/train-tuner.svg\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torchvision.models import resnet18\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import ray\n",
    "from ray import tune, train\n",
    "from ray.train.torch import TorchTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will use the example of training a ResNet18 model on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_ray_train(config: dict):  # pass in hyperparameters in config\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    model = resnet18()\n",
    "    model.conv1 = torch.nn.Conv2d(\n",
    "        1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
    "    )\n",
    "    model = train.torch.prepare_model(model) # Wrap the model in DistributedDataParallel\n",
    "\n",
    "    global_batch_size = config[\"global_batch_size\"]\n",
    "    batch_size = global_batch_size // ray.train.get_context().get_world_size()\n",
    "    optimizer = Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "    train_data = MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    data_loader = train.torch.prepare_data_loader(data_loader) # Wrap the data loader in a DistributedSampler\n",
    "\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        # Ensure data is on the correct device\n",
    "        data_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        for (\n",
    "            images,\n",
    "            labels,\n",
    "        ) in data_loader:  # images, labels are now sharded across the workers\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()  # Gradients are accumulated across the workers\n",
    "            optimizer.step()\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            torch.save(\n",
    "                model.module.state_dict(), os.path.join(temp_checkpoint_dir, \"model.pt\")\n",
    "            )\n",
    "            # Report the loss to Ray Tune\n",
    "            ray.train.report(\n",
    "                {\"loss\": loss.item()},\n",
    "                checkpoint=ray.train.Checkpoint.from_directory(temp_checkpoint_dir),\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now pass the training loop into the `train.torch.TorchTrainer` to perform distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TorchTrainer(\n",
    "    train_loop_ray_train,\n",
    "    train_loop_config={\"num_epochs\": 2, \"global_batch_size\": 128},\n",
    "    run_config=train.RunConfig(\n",
    "        storage_path=\"/mnt/cluster_storage/dist_train_tune_example/\",\n",
    "        name=\"tune_example\",\n",
    "    ),\n",
    "    scaling_config=train.ScalingConfig(\n",
    "        num_workers=2,\n",
    "        use_gpu=True,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out a Ray Train trainer is itself a Ray Tune trainable, so we can pass it directly into the `tune.Tuner` as we have done before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = tune.Tuner(\n",
    "    trainer,\n",
    "    param_space={\n",
    "        \"train_loop_config\": {\n",
    "            \"num_epochs\": 1,\n",
    "            \"global_batch_size\": 128,\n",
    "            \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        }\n",
    "    },\n",
    "    tune_config=tune.TuneConfig(\n",
    "        mode=\"min\",\n",
    "        metric=\"loss\",\n",
    "        num_samples=2,\n",
    "    ),\n",
    ")\n",
    "\n",
    "results = tuner.fit()\n",
    "\n",
    "best_result = results.get_best_result()\n",
    "best_result.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /mnt/cluster_storage/dist_train_tune_example/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
