{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ray aDAG Developer Guide - Hands-on Walkthrough\n",
    "\n",
    "## 1. Introduction to Ray aDAGs\n",
    "# Note: Transition to slides to explain \"What is Ray aDAG?\" and \"Why Use aDAGs?\"\n",
    "# (Discuss performance benefits and specific use cases like LLM inference.)\n",
    "\n",
    "# Also note that this requires both torch and ray installed (obviously) but both are prepped already as part of the image \n",
    "# for Ray Summit Training 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc02b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define and Create Actors with Ray Core\n",
    "import ray\n",
    "\n",
    "@ray.remote\n",
    "class EchoActor:\n",
    "    def echo(self, msg):\n",
    "        return msg\n",
    "\n",
    "# Create two actors\n",
    "a = EchoActor.remote()\n",
    "b = EchoActor.remote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27097255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a message and get a response\n",
    "msg_ref = a.echo.remote(\"hello\")\n",
    "msg_ref = b.echo.remote(msg_ref)\n",
    "print(ray.get(msg_ref))  # Expected output: \"hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa7187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Using Ray aDAGs for Performance Optimization\n",
    "# Note: Transition to slides to explain \"How Ray Core traditionally executes tasks\" \n",
    "# and \"Challenges with dynamic control flow\" (discuss overheads with serialization and object store).\n",
    "\n",
    "# Step 3: Define and Execute with Ray DAG API (Classic Ray Core)\n",
    "import ray.dag\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58f8f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a lazy DAG\n",
    "with ray.dag.InputNode() as inp:\n",
    "    intermediate_inp = a.echo.bind(inp)\n",
    "    dag = b.echo.bind(intermediate_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85954667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the DAG with inputs\n",
    "print(ray.get(dag.execute(\"hello\")))\n",
    "print(ray.get(dag.execute(\"world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af998905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time the execution\n",
    "for _ in range(5):\n",
    "    start = time.perf_counter()\n",
    "    ray.get(dag.execute(\"hello\"))\n",
    "    print(\"Took\", time.perf_counter() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75fecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Optimizing with Ray aDAGs\n",
    "\n",
    "# Step 4: Compile and Execute with aDAG Backend and time and compare the difference in exec speed\n",
    "adag = dag.experimental_compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f46cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the aDAG and measure the time\n",
    "for _ in range(5):\n",
    "    start = time.perf_counter()\n",
    "    ray.get(adag.execute(\"hello\"))\n",
    "    print(\"Took\", time.perf_counter() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4658867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tear down the DAG\n",
    "adag.teardown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee49cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. [BONUS #1] Multi-Actor Execution in Ray aDAG\n",
    "\n",
    "# Step 5: Executing Across Multiple Actors with Ray aDAG\n",
    "# Create multiple actors\n",
    "N = 3\n",
    "actors = [EchoActor.remote() for _ in range(N)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b0cc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the DAG with multiple outputs\n",
    "with ray.dag.InputNode() as inp:\n",
    "    outputs = [actor.echo.bind(inp) for actor in actors]\n",
    "    dag = ray.dag.MultiOutputNode(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af593a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile and execute the DAG\n",
    "adag = dag.experimental_compile()\n",
    "print(ray.get(adag.execute(\"hello\")))  # Expected: [\"hello\", \"hello\", \"hello\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2638f3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tear down the DAG\n",
    "adag.teardown()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47fa55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. [BONUS #2] GPU-GPU Communication with aDAGs\n",
    "\n",
    "# Note: Transition to slides to discuss \"GPU-GPU communication and NCCL\".\n",
    "\n",
    "# Step 6: GPU to GPU Data Transfer Example\n",
    "import torch\n",
    "from ray.experimental.channel.torch_tensor_type import TorchTensorType\n",
    "\n",
    "@ray.remote(num_gpus=1)\n",
    "class GPUSender:\n",
    "    def send(self, shape):\n",
    "        return torch.zeros(shape, device=\"cuda\")\n",
    "@ray.remote(num_gpus=1)\n",
    "class GPUReceiver:\n",
    "    def recv(self, tensor: torch.Tensor):\n",
    "        assert tensor.device.type == \"cuda\"\n",
    "        return tensor.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0898fec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sender and receiver actors\n",
    "sender = GPUSender.remote()\n",
    "receiver = GPUReceiver.remote()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a1cb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and compile a DAG for GPU-GPU communication\n",
    "with ray.dag.InputNode() as inp:\n",
    "    dag = sender.send.bind(inp)\n",
    "    dag = dag.with_type_hint(TorchTensorType())\n",
    "    dag = receiver.recv.bind(dag)\n",
    "adag = dag.experimental_compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d03b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the DAG and check the results\n",
    "assert ray.get(adag.execute((10, ))) == (10, )\n",
    "adag.teardown()\n",
    "\n",
    "## 7. Conclusion and Summary\n",
    "# Note: Transition to slides for summarizing key takeaways and discussing \n",
    "# limitations of aDAGs (e.g., actor constraints, NCCL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1a177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a message and get a response\n",
    "msg_ref = a.echo.remote(\"hello\")\n",
    "msg_ref = b.echo.remote(msg_ref)\n",
    "print(ray.get(msg_ref))  # Expected output: \"hello\"\n",
    "## 3. Using Ray aDAGs for Performance Optimization\n",
    "\n",
    "# Note: Transition to slides to explain \"How Ray Core traditionally executes tasks\" \n",
    "# and \"Challenges with dynamic control flow\" (discuss overheads with serialization and object store).\n",
    "# Step 3: Define and Execute with Ray DAG API (Classic Ray Core)\n",
    "import ray.dag\n",
    "import time\n",
    "\n",
    "# Define a lazy DAG\n",
    "with ray.dag.InputNode() as inp:\n",
    "    intermediate_inp = a.echo.bind(inp)\n",
    "    dag = b.echo.bind(intermediate_inp)\n",
    "# Execute the DAG with inputs\n",
    "print(ray.get(dag.execute(\"hello\")))\n",
    "print(ray.get(dag.execute(\"world\")))\n",
    "# Time the execution\n",
    "for _ in range(5):\n",
    "    start = time.perf_counter()\n",
    "    ray.get(dag.execute(\"hello\"))\n",
    "    print(\"Took\", time.perf_counter() - start)\n",
    "## 4. Optimizing with Ray aDAGs\n",
    "\n",
    "# Step 4: Compile and Execute with aDAG Backend\n",
    "# Compile the DAG for aDAG backend\n",
    "\n",
    "adag = dag.experimental_compile()\n",
    "# Execute the aDAG and measure the time\n",
    "for _ in range(5):\n",
    "    start = time.perf_counter()\n",
    "    ray.get(adag.execute(\"hello\"))\n",
    "    print(\"Took\", time.perf_counter() - start)\n",
    "# Tear down the DAG\n",
    "adag.teardown()\n",
    "\n",
    "## 5. [BONUS #1] Multi-Actor Execution in Ray aDAG\n",
    "\n",
    "# Step 5: Executing Across Multiple Actors with Ray aDAG\n",
    "# Create multiple actors\n",
    "N = 3\n",
    "actors = [EchoActor.remote() for _ in range(N)]\n",
    "# Define the DAG with multiple outputs\n",
    "with ray.dag.InputNode() as inp:\n",
    "    outputs = [actor.echo.bind(inp) for actor in actors]\n",
    "    dag = ray.dag.MultiOutputNode(outputs)\n",
    "# Compile and execute the DAG\n",
    "adag = dag.experimental_compile()\n",
    "print(ray.get(adag.execute(\"hello\")))  # Expected: [\"hello\", \"hello\", \"hello\"]\n",
    "# Tear down the DAG\n",
    "adag.teardown()\n",
    "\n",
    "## 6. [BONUS #2] GPU-GPU Communication with aDAGs\n",
    "\n",
    "# Note: Transition to slides to discuss \"GPU-GPU communication and NCCL\".\n",
    "\n",
    "# Step 6: GPU to GPU Data Transfer Example\n",
    "import torch\n",
    "from ray.experimental.channel.torch_tensor_type import TorchTensorType\n",
    "\n",
    "@ray.remote(num_gpus=1)\n",
    "class GPUSender:\n",
    "    def send(self, shape):\n",
    "        return torch.zeros(shape, device=\"cuda\")\n",
    "@ray.remote(num_gpus=1)\n",
    "class GPUReceiver:\n",
    "    def recv(self, tensor: torch.Tensor):\n",
    "        assert tensor.device.type == \"cuda\"\n",
    "        return tensor.shape\n",
    "# Create the sender and receiver actors\n",
    "sender = GPUSender.remote()\n",
    "receiver = GPUReceiver.remote()\n",
    "# Define and compile a DAG for GPU-GPU communication\n",
    "with ray.dag.InputNode() as inp:\n",
    "    dag = sender.send.bind(inp)\n",
    "    dag = dag.with_type_hint(TorchTensorType())\n",
    "    dag = receiver.recv.bind(dag)\n",
    "adag = dag.experimental_compile()\n",
    "# Execute the DAG and check the results\n",
    "assert ray.get(adag.execute((10, ))) == (10, )\n",
    "adag.teardown()\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2318fda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Conclusion and Summary\n",
    "# Note: Transition to slides for summarizing key takeaways and discussing \n",
    "# limitations of aDAGs (e.g., actor constraints, NCCL)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
