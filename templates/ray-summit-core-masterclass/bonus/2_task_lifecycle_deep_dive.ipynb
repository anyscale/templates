{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b752be-c1a2-428b-be5c-bcd3fbaeaf50",
   "metadata": {},
   "source": [
    "# Deep-dive into a Ray task's lifecycle\n",
    "\n",
    "In this notebook, we will provide a deep-dive into a Ray task's lifecycle. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "__Roadmap: Deep dive into a Ray task's lifecycle__\n",
    "\n",
    "1. High-Level Overview of a Task's Lifecycle\n",
    "2. The Main Components of a Ray Cluster\n",
    "3. Task Execution Stages in Detail:\n",
    "    1. Mapping Execution Stages to Cluster Components\n",
    "    2. Task Submission in Detail\n",
    "    4. Autoscaling in Detail\n",
    "    5. Task Scheduling in Detail\n",
    "    6. Result Handling in Detail\n",
    "4. Overview of Scheduling Strategies\n",
    "    1. How does a raylet classify nodes?\n",
    "    2. Default Scheduling Strategy\n",
    "    3. Node Affinity Strategy\n",
    "    4. SPREAD Scheduling Strategy\n",
    "    5. Placement Group Scheduling Strategy\n",
    "</div>\n",
    "\n",
    "Note in most cases, the notebook applies to Java, C++, and Python tasks. However certain remarks mainly focus on peculiarities of python tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df571bb5-5ef5-4a85-9106-429762fc3248",
   "metadata": {},
   "source": [
    "# High-level overview of a task's lifecycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964a4ab2-0760-4a1b-83de-00c08a27433e",
   "metadata": {},
   "source": [
    "We start by visualizing a task's execution using the following diagram:\n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/task_execution_detail_0_v3.svg.svg\" width=\"800px\">\n",
    "\n",
    "In case you skipped it, this same diagram was presented in the high-level overview notebook of Ray tasks.\n",
    "\n",
    "We will proceed to add more color to this diagram providing useful details for each step of the process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9065e725-7bb2-4fe9-8a54-78dc1f52eee5",
   "metadata": {},
   "source": [
    "# The main components of a Ray cluster\n",
    "\n",
    "A Ray cluster consists of:\n",
    "- One or more **worker nodes**, where each worker node consists of the following processes:\n",
    "    - **worker processes** responsible for task submission and execution.\n",
    "    - A **raylet** responsible for resource management and task placement.\n",
    "- One of the worker nodes is designated a **head node** and is responsible for running \n",
    "  - A **global control service** responsible for keeping track of the **cluster-level state** that is not supposed to change too frequently.\n",
    "  - An **autoscaler** service responsible for allocating and removing worker nodes by integrating with different infrastructure providers (e.g. AWS, GCP, ...) to match the resource requirements of the cluster.\n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/ray_cluster_detail_0_v3.svg\" width=\"800px\">\n",
    "\n",
    "<!-- \n",
    "Reference: \n",
    "- See [V2 architecture document -> Architecture Overview -> Design -> Components](https://docs.google.com/document/d/1tBw9A4j62ruI5omIJbMxly-la5w4q_TjyJgJL_jN2fI/preview#heading=h.cclei73t0j5p)\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1837554-89d5-46bb-9343-96d2bc125518",
   "metadata": {},
   "source": [
    "# Task execution stages in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530ceb15-bd56-4078-9186-3a17adccde04",
   "metadata": {},
   "source": [
    "## Mapping execution stages to cluster components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2bdf5e-5d3b-4dca-a922-51c90fe84e24",
   "metadata": {},
   "source": [
    "Now that we are familiar with the different components on a Ray cluster, here is our same task execution diagram revisited with colors indicating which component is responsible for each step.\n",
    "\n",
    "- One **worker process** submits the task\n",
    "- The cluster **autoscaler** will handle upscaling nodes to meet new resource requirements\n",
    "- **Raylet(s)** will handle task scheduling/placement on a worker\n",
    "- **One worker process** executes the task\n",
    "- The result information is sent back to the **submitter worker** once complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313528c7-5685-4828-a5da-f45efcdbb186",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/task_execution_detail_1_v3.svg.svg\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e771bd14-45ab-4a01-ac9d-a85f246acb12",
   "metadata": {},
   "source": [
    "## Task submission in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8520fc88-a756-43e9-8ad3-ad457d4ea288",
   "metadata": {},
   "source": [
    "### Exporting and loading function code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9e8d8f-54c3-49d8-84f7-b073e06dba5c",
   "metadata": {},
   "source": [
    "Remember a task wraps around a given function. The worker executing a task, is executing its underlying function.\n",
    "\n",
    "Here are the steps that Ray follows to export and load a task's function:\n",
    "\n",
    "1. The submitter worker will serialize a task's function definition\n",
    "    - In the case of Python, Ray makes use of a variant of pickle (cloudpickle) to serialize the function\n",
    "2. The submitter worker will export the function definition to the GCS Store\n",
    "3. The executor worker will load and cache the function definition from the GCS Store\n",
    "4. The executor worker will deserialize the code and execute the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff498cea-99fc-4619-b295-c5e1ee0fc6e8",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/task_execution_detail_export_load_v3.svg\" width=\"900px\">\n",
    "\n",
    "<!-- References:\n",
    "- See code:\n",
    "    - Exporting Function to GCS Store\n",
    "        1. [Python .remote calls ._remote](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L140)\n",
    "        2. [Python ._remote pickles the function](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L299)\n",
    "        3. [Python ._remote call exports the function via the function manager.export](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_private/function_manager.py#L273)\n",
    "        4. [Which calls the cython GcsClient.internal_kv_put](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L2579)\n",
    "        5. [Which calls the gcs_client.cc PythonGcsClient::InternalKVPut](https://github.com/ray-project/ray/blob/55ab6dfd6b415f8795dd1dfed7b3fde2558efc46/src/ray/gcs/gcs_client/gcs_client.cc#L312) that sets the key, value in the proper namespace \n",
    "    - Importing Function from GCS Store\n",
    "        1. [When instantiating a CoreWorker, we add task receivers which will callback CoreWorker::ExecuteTask](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/core_worker.cc#L147)\n",
    "        2. [CoreWorker::ExecuteTask() will prepare a RayFunction and submit it to its execution callback](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/core_worker.cc#L2721C21-L2721C44)\n",
    "        3. [The task execution callback in the case of python will execute the function from cython given the set task_execution_handler](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L3075C43-L3075C65)\n",
    "        4. [The task execution handler will execute the task with a cancellation handler](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L2064C17-L2064C55)\n",
    "        5. [The handler will call the function_manager.get_execution_info](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L1944)\n",
    "        6. [function_manager.get_execution_info will in turn call function_manager._wait_for_function](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_private/function_manager.py#L393)\n",
    "        7. [function_manager._wait_for_function will in turn call function_manager.fetch_and_register_remote_function](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_private/function_manager.py#L455C33-L455C67)\n",
    "        8. [function_manager.fetch_and_register_remote_function will in turn call function_manager.fetch_registered_method](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_private/function_manager.py#L299C37-L299C60)\n",
    "        9. [function_manager.fetch_registered_method will in turn call gcs_client.internal_kv_get to read the function defintion from the GCS KV store](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_private/function_manager.py#L281)\n",
    "    - Caching the function definition:\n",
    "        10. [As a continuation the execution_infos in-memory dictionary mapping is updated to store the function definition] (https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L1946)\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b76c87-2f76-4b49-94ef-b41b45c608e7",
   "metadata": {},
   "source": [
    "### Resolving dependencies and data locality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310873ef-d5df-4c8e-95f5-0997bd698384",
   "metadata": {},
   "source": [
    "Here are some key steps in task submission:\n",
    "\n",
    "1. A submitter worker will resolve a task's dependency locations before creating and submitting the task.\n",
    "2. A submitter worker will choose the worker node that has most of the dependency data local to it.\n",
    "3. A submitter worker will request what Ray calls a \"Worker Lease\" from the raylet on the chosen data-locality-optimal node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552d341d-8471-48af-b1b8-a9c68a04d37d",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/task_execution_detail_resolving_deps_data_locality_v3.svg\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063e5323-3bd8-4cf9-aa9e-5b2db5520623",
   "metadata": {},
   "source": [
    "let's unpack the above steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0de1b1-0887-4d50-b2eb-f07b834eeda7",
   "metadata": {},
   "source": [
    "#### Resolving dependencies in detail\n",
    "\n",
    "Given a particular task `task1` that depends on, objects `A` and `B` as inputs\n",
    "\n",
    "The submitter worker process will perform these two main steps\n",
    "\n",
    "1. Wait for each object to be available via async callbacks\n",
    "    - remember `A` and `B` could very well be the outputs of a different task, hence why we need to wait \n",
    "2. Proceed with scheduling now that all dependencies are resolved\n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/resolving_deps_v3.svg\" width=\"600px\">\n",
    "\n",
    "Note: Later in the notebook we will discuss ray's distributed ownership and object store which will clarify how the worker can check if the objects are ready. \n",
    "\n",
    "<!-- References:\n",
    "- See code:\n",
    "    1. [Python .remote calls ._remote](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L140)\n",
    "    2. [python ._remote call calls submit_task](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L420)\n",
    "    3. [submit_task calls the cython submit_task function defined here](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L3574)\n",
    "    4. [cython submit_task Delegates to C++ CoreWorker::SubmitTask]((https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L3643)\n",
    "    5. [CoreWorker::SubmitTask calls direct_task_submitter.SubmitTask](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/core_worker.cc#L1949)\n",
    "    5. [direct_task_submitter.SubmitTask calls ResolveDependencies to resolve dependencies](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/direct_task_transport.cc#L28)\n",
    "    6. [ResolveDependencies calls InlineDependencies](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/dependency_resolver.cc#L117)\n",
    "    7. [InlineDependencies fetches task metadata like the size](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/dependency_resolver.cc#L44C10-L44C10)\n",
    "  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59848ed9-8bd9-410e-93ed-262838613072",
   "metadata": {},
   "source": [
    "#### Data locality resolution in detail\n",
    "\n",
    "The submitter process will choose the node that has the **most number of object argument bytes** already local.\n",
    "\n",
    "The diagram shows the same particular task `task1` we saw before. \n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/data_locality_v3.svg\" width=\"600px\">\n",
    "\n",
    "Note: Small caveat: \"enforcing data locality\" stage is skipped in case the task's specified scheduling policy is stringent (e.g. a node-affinity policy). Scheduling policies will be discussed in more detail later in the notebook.\n",
    "\n",
    "<!-- References:\n",
    "- See code:\n",
    "    1. [Python .remote calls ._remote](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L140)\n",
    "    2. [python ._remote call calls submit_task](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/remote_function.py#L420)\n",
    "    3. [submit_task calls the cython submit_task function](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L3643)\n",
    "    4. [cython submit_task Delegates to SubmitTask from c++ Core Worker with calls direct_task_submitter.SubmitTask](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/core_worker.cc#L1949)\n",
    "    5. [direct_task_submitter.SubmitTask calls ResolveDependencies to resolve dependencies](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/direct_task_transport.cc#L28)\n",
    "    6. [direct_task_submitter.SubmitTask as a callback will now call RequestNewWorkerIfNeeded](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/direct_task_transport.cc#L135)\n",
    "    7. [RequestNewWorkerIfNeeded will in turn call GetBestNodeForTask](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/direct_task_transport.cc#L394)\n",
    "    8. [GetBestNodeForTask will pick a node for locality in case the scheduling strategy is not stringent (i.e. node affinity or spread)](https://github.com/ray-project/ray/blob/master/src/ray/core_worker/lease_policy.cc#L39)\n",
    "    9. [GetBestNodeIdForTask will find the node with the most object bytes](https://github.com/ray-project/ray/blob/master/src/ray/core_worker/lease_policy.cc#L47C1-L48C1) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ab9247-07f5-4f45-a317-fd226e729a38",
   "metadata": {},
   "source": [
    "## Task scheduling in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce112e4-a12a-4725-930b-0ab9608922c7",
   "metadata": {},
   "source": [
    "Now that a worker lease request is sent, here are the steps that follow to schedule a task\n",
    "\n",
    "- The **raylet on the data-locality-optimal node**:\n",
    "    - Receives the worker lease request \n",
    "    - Receives a view of the entire cluster state from the GCS via a periodic broadcast\n",
    "    - Makes a decision: which node is the best to schedule the task on\n",
    "- The **raylet on the best node** now:\n",
    "    - Attempts to reserve the resources on the node to satisfy the lease\n",
    "    - Updates the GCS in case it succeeds to reserve the resources via a periodic message\n",
    " \n",
    "This is shown in the below diagram, the potential autoscaling step prior to finding a best node is left out to simplify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334bdd22-9963-4369-a7a6-0212baca2810",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/task_execution_detail_find_best_node_allocate_resources_v3.svg\" width=\"900px\">\n",
    "\n",
    "<!-- References:\n",
    "- Raylet Receives Worker Lease Request:\n",
    "    1. [When a worker lease request comes in a raylet's NodeManager::HandleRequestWorkerLease gets called](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/node_manager.cc#L1783)\n",
    "    2. [NodeManager::HandleRequestWorkerLease will delegate a call to ClusterTaskManager::QueueAndScheduleTask()](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/node_manager.cc#L1830)\n",
    "    3. [ClusterTaskManager::QueueAndScheduleTask it will delegate a call to ClusterTaskManager::ScheduleAndDispatchTasks()](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/scheduling/cluster_task_manager.cc#L64)\n",
    "    4. [ClusterTaskManager.ScheduleAndDispatchTasks() will call ClusterTaskManager.ScheduleOnNode()](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/scheduling/cluster_task_manager.cc#L192)\n",
    "- Best Scheduleable Node is a different Node\n",
    "    1. [Check that indeed we can allocate resources on the remote node by calling ClusterResourceManager.AllocateRemoteTaskResources](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/scheduling/cluster_task_manager.cc#L440)\n",
    "    2. [Send a redirection back to the task submitter by sending a reply callback with the remote raylet address calling send_reply_callback()](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/scheduling/cluster_task_manager.cc#L456)\n",
    "    3. [The task submitter CoreWorkerDirectTaskSubmitter::SubmitTask will now handle the callback by sending the worker lease request to the remote raylet](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/transport/direct_task_transport.cc#L505C1-L506C1) \n",
    "- Best Scheduleable Node is the same as the Local (data-locality-optimal) Node\n",
    "    1. [If the node id is the same as the local node id, then LocalTaskManager.QueueAndScheduleTask() is called](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/scheduling/cluster_task_manager.cc#L421)\n",
    "    2. [LocalTaskManager.QueueAndScheduleTask will first wait for the task arguments to be ready calling LocalTaskManager.WaitForTaskArgsRequests](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/local_task_manager.cc#L66)\n",
    "    3. [LocalTaskManager.ScheduleAndDispatchTasks() will in turn call LocalTaskManager.DispatchScheduledTasksToWorkers()](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/local_task_manager.cc#L96)\n",
    "    4. [LocalTaskManager.DispatchScheduledTasksToWorkers() will in turn pop a worker from the WorkerPool.PopWorker](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/local_task_manager.cc#L267)\n",
    "- [Calling WorkerPool.PopWorker to assing a worker to the task by popping a worker from the pool](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/worker_pool.cc#L1150) will perform the following :\n",
    "    - Worker Allocation: [Try to reuse existing idle workers that match the task's requirements. If no suitable worker is available, it starts a new one.](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/worker_pool.cc#L1250)\n",
    "    - Runtime Environment Management: [Handles the creation or retrieval of a runtime environment necessary for the task](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/worker_pool.cc#L1261) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5863d0d3-a7be-4d2d-bc44-5fdec1615639",
   "metadata": {},
   "source": [
    "### Scheduling hotpath: leveraging leases and caches\n",
    "\n",
    "- A scheduling request at task submission can reuse a leased worker if it has the same:\n",
    "    - Resource requirements as these must be acquired from the node during task execution.\n",
    "    - Shared-memory task arguments, as these must be made local on the node before task execution.\n",
    "- This \"hot path\" most commonly occurs for **subsequent task executions**. We visualize it in the diagram below. Note how we skip:\n",
    "    - sending a request to a raylet altogether\n",
    "    - storing and fetching the function code in GCS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1e8c3e-4950-4afb-884c-c36e8657c800",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/task_execution_scheduling_hot_path_v3.svg\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e540aed-7af5-4274-9a4a-c1193c308ca9",
   "metadata": {},
   "source": [
    "## Autoscaling in detail\n",
    "\n",
    "- **Question**: What happens if a raylet fails to find a \"best node for a task\"? Imagine a task that is requesting GPU resources when all the running worker nodes are CPU only.\n",
    "- **Answer**: The task gets stuck in a pending state until the autoscaler adds GPU nodes to the cluster.\n",
    "\n",
    "More specifically, here is how the autoscaling loop works:\n",
    "\n",
    "- The worker process submits tasks which request resources such as GPU.\n",
    "- The raylet attempts to find the best node for the task.\n",
    "- The raylet fails to find a node that satisfies the task requirements\n",
    "- The GCS will periodically pull resource usage and receive resource updates from all the raylets\n",
    "- The autoscaler will periodically fetch the snapshots from GCS.\n",
    "- The autoscaler looks at the resources available in the cluster, resources requested, what is pending and calculates the number of nodes to satisfy both running and pending tasks.\n",
    "- The autoscaler then adds or removes nodes from the cluster via the node provider interface (e.g. AWS interface)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889ac2c6-5d9c-43ff-b156-0ce783861550",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/task_execution_autoscaling_v2.svg\" width=\"900px\">\n",
    "\n",
    "<!-- References:\n",
    "\n",
    "- GCS pulling resource usage:\n",
    "    - [GcsServer::DoStart calls GcsServer::InitGcsResourceManager](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/gcs/gcs_server/gcs_server.cc#L195)\n",
    "    - [GcsServer::InitGcsResourceManager periodically pulls resource load data from each node by calling GetResourceLoad](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/gcs/gcs_server/gcs_server.cc#L374C1-L375C1)\n",
    "    - [NodeManager::HandleGetResourceLoad responds to requests for resource load data from individual nodes by calling ClusterTaskManager::FillResourceUsage](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/node_manager.cc#L1763)\n",
    "    - [ClusterTaskManager::FillResourceUsage in turn calls SchedulerResourceReporter::FillResourceUsage which populates load information and PopulateResourceViewSyncMessage which poipulates usage information](https://github.com/ray-project/ray/blob/master/src/ray/raylet/scheduling/cluster_task_manager.cc#L351C1-L352C1)    \n",
    "\n",
    "- Raylet sending resource updates:\n",
    "    - [GcsServer::DoStart calls GcsServer::InitGcsResourceManager](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/gcs/gcs_server/gcs_server.cc#L195)\n",
    "    - [The GCS resource manager](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/gcs/gcs_server/gcs_server.cc#L341) and raylet share the the cluster resource manager\n",
    "    - [The cluster resource manager will periodically register a call from the raylet AddorUpdateNode which is called only after the resource is modified for a given time delta GetNodeResourceModifiedTs](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/scheduling/cluster_resource_manager.cc#L34)\n",
    "    - [AddorUpdateNode will call Node(node_resources) updating the cluster resource manager's view of a given node's resources](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/scheduling/cluster_resource_manager.cc#L70)\n",
    "\n",
    "- Autoscaler logic\n",
    "    - [InitMonitorServer() in GcsServer::DoStart creates a GcsMonitorServer a shim responsible for providing a compatible interface between GCS and `monitor.py`](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/gcs/gcs_server/gcs_server.cc#L231)\n",
    "    - [python/ray/autoscaler/_private/monitor.py -> Monitor: periodically collect stats from GCS and trigger autoscaler update](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/autoscaler/_private/monitor.py#L126)\n",
    "    - [python/ray/autoscaler/_private/resource_demand_scheduler.py contains core autoscaling logic](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/autoscaler/_private/resource_demand_scheduler.py#L102C7-L102C30)\n",
    "    - [python/ray/autoscaler/node_provider.py provides node provider interface](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/autoscaler/node_provider.py#L13)\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab2dccb-3420-482f-8d7a-53dbbe10c074",
   "metadata": {},
   "source": [
    "## Result handling in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cb730d-9308-4328-9cc6-b846f89c943e",
   "metadata": {},
   "source": [
    "### Cluster components zoom-in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b73d55-40c2-4af8-b04b-d62ea8012224",
   "metadata": {},
   "source": [
    "Let's revisit our mental model for the Ray cluster and add some more detail to which components control and manage objects in Ray.\n",
    "\n",
    "- Each worker process stores:\n",
    "    - **An ownership table** contains system metadata (object sizes, locations and reference counts) for the objects to which the worker has a reference\n",
    "    - **An in-process store** used to store small objects.\n",
    "- Each raylet runs:\n",
    "    - A **shared-memory object store** responsible for storing, transferring, and spilling large objects. The individual object stores in a cluster comprise the _Ray distributed object store_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e73764b-947b-4a7e-a0a4-00850d3a189d",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/ray_cluster_distributed_ownership_v3.svg\" width=\"800px\">\n",
    "\n",
    "<!-- \n",
    "Reference: \n",
    "- See [V2 architecture document -> Architecture Overview -> Design -> Components](https://docs.google.com/document/d/1tBw9A4j62ruI5omIJbMxly-la5w4q_TjyJgJL_jN2fI/preview#heading=h.cclei73t0j5p)\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070dd72b-48d0-4f41-8d87-ab6cc47a8207",
   "metadata": {},
   "source": [
    "### Object handling post execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1da0a9-572b-460c-9ea6-a1ddda00bcca",
   "metadata": {},
   "source": [
    "Let's take a look at the steps involved in object handling:\n",
    "\n",
    "- The submitter worker creates an object reference for the output of the task in its ownership table\n",
    "- The submitter worker then submits the task for scheduling\n",
    "- The executor worker will execute the task function\n",
    "- The executor worker will then prepare the return object\n",
    "    - If the return object is small <100KB:\n",
    "        - Return the values inline directly to the submitter's in-process object store.\n",
    "    - If the return object is large:\n",
    "        - Store the objects in the raylet object store\n",
    "- The executor updates the submitter's ownership table with the location of the object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33201b6-6850-47ff-97da-48dbb9c82313",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/task_execution_detail_distributed_object_store_ownership_v4.svg\" width=\"800px\">\n",
    "\n",
    "<!-- References: \n",
    "- See code:\n",
    "    1. [When instantiating a CoreWorker, we add task receivers which will callback CoreWorker::ExecuteTask](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/core_worker.cc#L147)\n",
    "    2. [CoreWorker::ExecuteTask() will prepare a RayFunction and submit it to its execution callback](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/core_worker/core_worker.cc#L2721C21-L2721C44)\n",
    "    3. [The task execution callback in the case of python will execute the function from cython given the set task_execution_handler](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L3075C43-L3075C65)\n",
    "    4. [The task execution handler will execute the task with a cancellation handler](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L2064C17-L2064C55)\n",
    "    5. [The handler will call execute_task handling a KeyboardInterrupt error](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L1960C7-L1960C7)\n",
    "    6. [execute_task will invoke the function_executor](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L1675)\n",
    "    7. [execute_task will store the outputs in the object store](https://github.com/ray-project/ray/blob/releases/2.8.1/python/ray/_raylet.pyx#L1810C12-L1810C12) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a2ca85-2a78-467c-aa56-f80cf23c40a6",
   "metadata": {},
   "source": [
    "### Distributed ownership work in Ray\n",
    "\n",
    "#### How does it work?\n",
    "The process that submits a task is considered to be the owner of the result of the task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65c4990-a466-4044-8f81-edc649a8d3e5",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/distributed_ownership_overview_v4.svg\" width=\"800px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89923e26-9910-43a6-a991-757f07917d0d",
   "metadata": {},
   "source": [
    "#### Upsides to distributed ownership\n",
    "\n",
    "- Latency: Faster than communicating all ownership information back to a head node.\n",
    "- Scalability: There is no central bottleneck when attempting to scale the cluster given every worker maintains its own ownership information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48a0ba6-de22-4168-971a-ea0b8f465026",
   "metadata": {},
   "source": [
    "#### Downsides to distributed ownership\n",
    "\n",
    "- objects fate-share with their owner\n",
    "    - i.e. even though the object is available on an object store in node 2, if node 1 fails, the owner fails, and the object is no longer reachable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f19405b-3a65-4b01-9670-60dbaf33b3c3",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/distributed_ownership_fate_share_with_owner_v4.svg\" width=\"900px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a04790-a38f-4e56-aa8a-cdcbbc795dcf",
   "metadata": {},
   "source": [
    "### Distributed object store\n",
    "\n",
    "The raylet's object store can be thought of as shared memory across all workers on a node.\n",
    "\n",
    "For values that can be zero-copy deserialized, passing the ObjectRef to `ray.get` or as a task argument will return a direct pointer to the shared memory buffer to the worker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8fc3c1-f86e-4157-86d6-73a93e41acfe",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/distributed_ownership_data_sharing_v4.svg\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc916b30-6ab0-47e0-b587-b6db4b43e4dd",
   "metadata": {},
   "source": [
    "#### Downside to a shared object-store\n",
    "\n",
    "This also means that \"worker\" processes fate-share with their local raylet process.\n",
    "\n",
    "A simple mental model to have is `raylet = node` if a raylet fails, all workloads on node will fail "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a11ba7-2e48-49f0-9f8c-f17dbc4d3e79",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/distributed_ownership_fate_share_with_raylet_v4.svg\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d18be57-14d8-4ac8-bc97-9dd2103f5b83",
   "metadata": {},
   "source": [
    "# Overview of scheduling strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0946813d-9312-4e63-9872-7d9c9e14bc96",
   "metadata": {},
   "source": [
    "Ray provides different scheduling strategies that you can set on your task.\n",
    "\n",
    "We will go over:\n",
    "- How a raylet assess feasibility and availability of nodes\n",
    "- How every scheduling strategy/policy works and when you should use it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ee7a35-f5d1-428d-a3e1-a66c4407fb98",
   "metadata": {},
   "source": [
    "## Node classification\n",
    "\n",
    "Given a resource requirement, a raylet classifies a node as one of the following:\n",
    "- feasible\n",
    "    - available\n",
    "    - not available\n",
    "- infeasible node \n",
    "\n",
    "Let's understand this by looking at an example task `my_task` that has a resource requirement of 3 CPUs:\n",
    "\n",
    "- all nodes with >= 3 CPUs are classified as **feasible**\n",
    "    - all **feasible nodes** that have >= 3 CPUs **idle** are classified as **available**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007b8d8d-cb3e-49c9-baf9-55aae5a1b8bd",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/raylet_node_classification_v3.svg\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9755b82f-b86d-4e47-ac4f-5c9238dca42c",
   "metadata": {},
   "source": [
    "## Default scheduling strategy\n",
    "\n",
    "This is the default scheduling policy used by Ray\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Ray attempts to strike a balance between favoring nodes that already cater for data locality and favoring those that have low resource utilization.\n",
    "\n",
    "### How does it work?\n",
    "It is a hybrid policy that combines the following two heuristics:\n",
    "- Bin packing heuristic\n",
    "- Load balancing heuristic\n",
    "\n",
    "<!-- ### References:\n",
    "- See code here:\n",
    "    - [Default Hybrid Scheduling Policy is defined here](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/scheduling/policy/hybrid_scheduling_policy.cc) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa50e575-7810-4762-92c5-ba307c64f1bb",
   "metadata": {},
   "source": [
    "The diagram below shows the policy in action in a bin-packing heuristic/mode\n",
    "\n",
    "Note the **Local Node** shown in the diagram is the node that is local to the raylet that received the worker lease request - which in almost all cases is the raylet that satisfies data locality requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc93fb1-0036-4328-902a-fccb85a4afa8",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_policy_hybrid_policy_binpacking_v4.svg\" width=\"900px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637ed34d-1d44-4b58-bbd0-3641f7c365cd",
   "metadata": {},
   "source": [
    "The diagram below shows the policy in action in a load balancing heuristic. \n",
    "\n",
    "This occurs when our preferred local node is heavily being utilized. The strategy will now spread new tasks among other feasible and available nodes.\n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_policy_hybrid_policy_balancing_v4.svg\" width=\"900px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2d5f03-2742-45e0-9939-e4300c98e748",
   "metadata": {},
   "source": [
    "## SPREAD scheduling strategy\n",
    "\n",
    "### How does it work?\n",
    "It behaves like a best-effort round-robin. It spreads across all the available nodes first and then the feasible nodes.\n",
    "\n",
    "### Use-cases\n",
    "- When you want to load-balance your tasks across nodes. e.g. you are building a web service and want to avoid overloading certain nodes.\n",
    "\n",
    "\n",
    "<!-- ### References:\n",
    "- See code here\n",
    "    - [Spread Scheduling Policy is defined here](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/scheduling/policy/spread_scheduling_policy.cc)\n",
    "  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c1a5e4-2aae-462d-b4c3-f71280568ec5",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_policy_spread_v3.svg\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfbf117-865f-4b6f-88f6-6c9671ffa172",
   "metadata": {},
   "source": [
    "### Sample code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1610bdc8-ab5e-4de4-9a03-12f6609a0687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "\n",
    "@ray.remote(scheduling_strategy=\"SPREAD\")\n",
    "def spread_default_func():\n",
    "    return 2\n",
    "\n",
    "\n",
    "ray.get(spread_default_func.remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbf3515-9c48-42a3-a61f-83d722dc7aee",
   "metadata": {},
   "source": [
    "## Placement Group Scheduling Strategy\n",
    "\n",
    "In cases when we want to treat a set of resources as a single unit, we can use placement groups.\n",
    "\n",
    "### How does it work?\n",
    "\n",
    "- A **placement group** is formed from a set of **resource bundles**\n",
    "  - A **resource bundle** is a list of resource requirements that fit in a single node\n",
    "- A **placement group** can specify a **placement strategy** that determines how the **resource bundles** are placed\n",
    "  - The **placement strategy** can be one of the following:\n",
    "    - **PACK**: pack the **resource bundles** into as few nodes as possible\n",
    "    - **SPREAD**: spread the **resource bundles** across as many nodes as possible\n",
    "    - **STRICT_PACK**: pack the **resource bundles** into as few nodes as possible and fail if not possible\n",
    "    - **STRICT_SPREAD**: spread the **resource bundles** across as many nodes as possible and fail if not possible\n",
    "- **Placement Groups** are **atomic** \n",
    "  -  i.e. either all the **resource bundles** are placed or none are placed\n",
    "  -  GCS uses a two-phase commit protocol to ensure atomicity\n",
    "\n",
    "### Use-cases\n",
    "\n",
    "Placement groups are used for **atomic gang scheduling**. Imagine the use case of a distributed training that requires 4 GPU nodes total. Other distributed schedulers might first reserve 3 GPUs and hang waiting for the fourth hogging resources in the meantime. Ray, instead, will either reserve all 4 GPUs or it will fail scheduling.\n",
    "\n",
    "- Use SPREAD when you want to load-balance your tasks across nodes. e.g. you are building a web service and want to avoid overloading certain nodes.\n",
    "- Use PACK when you want to maximize resource utilization. e.g. you are running training and want to cut costs by packing all your resource bundles on a small subset of nodes.\n",
    "\n",
    "<!-- ### References\n",
    "- [See code here for Bundle Scheduling Policy](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/scheduling/policy/bundle_scheduling_policy.cc) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd86af99-9c06-4a9c-b73b-0c9f1d1cbe94",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_policy_placement_group_v3.svg\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9553a1dc-a980-42f6-a542-48974140578a",
   "metadata": {},
   "source": [
    "### Example Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbf1929-0bcc-4abf-a372-e09eea922907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy\n",
    "from ray.util.placement_group import (\n",
    "    placement_group,\n",
    "    placement_group_table,\n",
    "    remove_placement_group,\n",
    ")\n",
    "\n",
    "# Reserve a placement group of 1 bundle that reserves 0.1 CPU\n",
    "pg = placement_group([{\"CPU\": 0.1}], strategy=\"PACK\", name=\"my_pg\")\n",
    "\n",
    "# Wait until placement group is created.\n",
    "ray.get(pg.ready(), timeout=10)\n",
    "\n",
    "# Inspect placement group states using the table\n",
    "print(placement_group_table(pg))\n",
    "\n",
    "\n",
    "@ray.remote(\n",
    "    scheduling_strategy=PlacementGroupSchedulingStrategy(\n",
    "        placement_group=pg,\n",
    "    ),\n",
    "    # task requirement needs to be less than placement group capacity\n",
    "    num_cpus=0.1,\n",
    ")\n",
    "def placement_group_schedule():\n",
    "    return 2\n",
    "\n",
    "\n",
    "out = ray.get(placement_group_schedule.remote())\n",
    "print(out)\n",
    "\n",
    "# Remove placement group.\n",
    "remove_placement_group(pg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1358afc-669c-4ea7-a873-e5747fbec9a2",
   "metadata": {},
   "source": [
    "## Node affinity strategy\n",
    "\n",
    "### How does it work?\n",
    "It assigns a task to a given node in either a strict or soft manner.\n",
    "\n",
    "### Use-cases\n",
    "- When you want to ensure that your task runs on a specific node\n",
    "\n",
    "\n",
    "<!-- ### References:\n",
    "- See code here\n",
    "    - [Node Affinity Policy is defined here](https://github.com/ray-project/ray/blob/releases/2.8.1/src/ray/raylet/scheduling/policy/node_affinity_scheduling_policy.cc)\n",
    "  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e958ad-4d1f-4513-be26-524b9d3e7958",
   "metadata": {},
   "source": [
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/scheduling_policy_node_affinity_v3.svg\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66136a5-980f-455c-b97a-28e29597a49a",
   "metadata": {},
   "source": [
    "### Sample code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f9ad3f-ce48-4579-a58b-023409575197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_a_node_with(resource, amount):\n",
    "    for node in ray.nodes():\n",
    "        if resource in node['Resources'] and node['Resources'][resource] >= amount:\n",
    "            return node\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c03228-23bf-464e-a895-ce752a03401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "node = find_a_node_with('CPU', 1)\n",
    "\n",
    "node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61371ae5-ffa8-48a0-b4e0-be5378c7874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.util.scheduling_strategies import NodeAffinitySchedulingStrategy\n",
    "\n",
    "# pin this task to only run on the current node id\n",
    "run_on_same_node = NodeAffinitySchedulingStrategy(\n",
    "    node_id=node['NodeID'], \n",
    "    soft=False,\n",
    ")\n",
    "\n",
    "@ray.remote(\n",
    "    scheduling_strategy=run_on_same_node\n",
    ")\n",
    "def node_affinity_schedule():\n",
    "    return 2\n",
    "\n",
    "\n",
    "ray.get(node_affinity_schedule.remote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ff64bc-f5c5-4278-b193-1311ad612f84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
