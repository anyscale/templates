{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning LLMs\n",
    "\n",
    "In this notebook we will be making use of Anyscale's LLMForge to finetune our first LLM model. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Here is the roadmap for this notebook:</b>\n",
    "<ul>\n",
    "    <li><b>Part 0:</b> Why finetune LLMs?</li>\n",
    "    <li><b>Part 1:</b> Introduction to LLMForge</li>\n",
    "    <li><b>Part 2:</b> Submitting an LLM Finetuning Job</li>\n",
    "    <li><b>Part 3:</b> Tracking the Progress of the Job</li>\n",
    "    <li><b>Part 4:</b> Tailoring LLMForge to Your Needs</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anyscale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Why finetune LLMs?\n",
    "\n",
    "The main usecase for finetuning LLMs is to adapt a pre-trained model to a specific task or dataset.\n",
    "\n",
    "- **Task-Specific Performance**: Fine-tuning hones an LLM's capabilities for a particular task, leading to superior performance.\n",
    "- **Resource Efficiency**: We can use smaller LLMs that require less computational resources to achieve better performance than larger general-purpose models.\n",
    "- **Privacy and Security**: We can self-host finetuned models to ensure that our data is not shared with third parties.\n",
    "\n",
    "In this guide, we will be finetuning an LLM model on a custom video gaming dataset. \n",
    "\n",
    "The task is a functional representation task where we want to extract structured data from user input on video games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1. Introduction to LLMForge\n",
    "\n",
    "<!-- get one liner from docs -->\n",
    "Anyscale's [LLMForge](https://docs.anyscale.com/llms/finetuning/intro/#what-is-llmforge) provides an easy to use library for fine-tuning LLMs.\n",
    "\n",
    "<!-- add diagram on how to work with LLMForge -->\n",
    "Here is a diagram that shows a *typical workflow* when working with LLMForge:\n",
    "\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-summit/e2e-llms/llmforge-finetune-workflow-v2.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Preparing an LLMForge configuration file\n",
    "\n",
    "We have already prepared a configuration file for you under `configs/training/lora/mistral-7b.yaml`\n",
    "\n",
    "Here are the file contents:\n",
    "\n",
    "```yaml\n",
    "# Change this to the model you want to fine-tune\n",
    "model_id: mistralai/Mistral-7B-Instruct-v0.1\n",
    "\n",
    "# Change this to the path to your training data\n",
    "train_path: s3://anyscale-public-materials/llm-finetuning/viggo_inverted/train/subset-500.jsonl\n",
    "\n",
    "# Change this to the path to your validation data. This is optional\n",
    "valid_path: s3://anyscale-public-materials/llm-finetuning/viggo_inverted/valid/data.jsonl\n",
    "\n",
    "# Change this to the context length you want to use. Examples with longer\n",
    "# context length will be truncated.\n",
    "context_length: 512\n",
    "\n",
    "# Change this to total number of GPUs that you want to use\n",
    "num_devices: 2\n",
    "\n",
    "# Change this to the number of epochs that you want to train for\n",
    "num_epochs: 3\n",
    "\n",
    "# Change this to the batch size that you want to use\n",
    "train_batch_size_per_device: 16\n",
    "eval_batch_size_per_device: 16\n",
    "\n",
    "# Change this to the learning rate that you want to use\n",
    "learning_rate: 1e-4\n",
    "\n",
    "# This will pad batches to the longest sequence. Use \"max_length\" when profiling to profile the worst case.\n",
    "padding: \"longest\"\n",
    "\n",
    "# By default, we will keep the best checkpoint. You can change this to keep more checkpoints.\n",
    "num_checkpoints_to_keep: 1\n",
    "\n",
    "# Deepspeed configuration, you can provide your own deepspeed setup\n",
    "deepspeed:\n",
    "  config_path: configs/deepspeed/zero_3_offload_optim+param.json\n",
    "\n",
    "# Lora configuration\n",
    "lora_config:\n",
    "  r: 8\n",
    "  lora_alpha: 16\n",
    "  lora_dropout: 0.05\n",
    "  target_modules:\n",
    "    - q_proj\n",
    "    - v_proj\n",
    "    - k_proj\n",
    "    - o_proj\n",
    "    - gate_proj\n",
    "    - up_proj\n",
    "    - down_proj\n",
    "    - embed_tokens\n",
    "    - lm_head\n",
    "  task_type: \"CAUSAL_LM\"\n",
    "  bias: \"none\"\n",
    "  modules_to_save: []\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<!-- LLMForge config explained -->\n",
    "Anyscale's LLMForge's finetune config can be split into the following:\n",
    "\n",
    "- **Model Configuration:**\n",
    "    - `model_id`: The Hugging Face model name.\n",
    "    \n",
    "- **Data Configuration:**\n",
    "    - `train_path`: The path to the training data.\n",
    "    - `valid_path`: The path to the validation data.\n",
    "    - `context_length`: The maximum number of tokens in the input.\n",
    "    \n",
    "- **Training Configuration:**\n",
    "    - `learning_rate`: The learning rate for the optimizer.\n",
    "    - `num_epochs`: The number of epochs to train for.\n",
    "    - `train_batch_size_per_device`: The batch size per device for training.\n",
    "    - `eval_batch_size_per_device`: The evaluation batch size per device.\n",
    "    - `num_devices`: The number of devices to train on.\n",
    "    \n",
    "- **Output Configuration:**\n",
    "    - `num_checkpoints_to_keep`: The number of checkpoints to retain.\n",
    "    - `output_dir`: The output directory for the model outputs.\n",
    "\n",
    "- **Advanced Training Configuration:**\n",
    "    - **LoRA Configuration:**\n",
    "        - `lora_config`: The LoRA configuration. Key parameters include:\n",
    "            - `r`: The rank of the LoRA matrix.\n",
    "            - `target_modules`: The modules to which LoRA will be applied.\n",
    "            - `lora_alpha`: The LoRA alpha parameter (a scaling factor).\n",
    "    - **DeepSpeed Configuration:**\n",
    "        - `deepspeed`: Settings for distributed training strategies such as FSDP (Fully Sharded Data Parallel).\n",
    "            - This may include specifying the FSDP stage.\n",
    "            - Optionally, enable CPU offloading for parameter and optimizer states.\n",
    "    \n",
    "\n",
    "Default configurations for all popular models are available in the `llm-forge` library, which serve as a good starting point for most tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2. Submitting an LLM Finetuning Job\n",
    "\n",
    "To run the finetuning, we will be using the Anyscale Job SDK.\n",
    "\n",
    "We start by defining a JobConfig object with the following content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = anyscale.job.JobConfig(\n",
    "    # The command to run the finetuning process\n",
    "    entrypoint=\"llmforge anyscale finetune configs/training/lora/mistral-7b.yaml\",\n",
    "    # The image to use for the job\n",
    "    image_uri=\"localhost:5555/anyscale/llm-forge:0.5.4\",\n",
    "    # Retry the job up to 1 times\n",
    "    max_retries=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then run the following command to submit the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = anyscale.job.submit(config=job_config)\n",
    "job_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Note:</b> by default the job will make use of the same compute configuration as the current workspace that is submitting the job unless specified otherwise.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 3. Tracking the Progress of the Job\n",
    "\n",
    "Once the job is submitted, we can make use of the observability features of the Anyscale platform to track the progress of the job at the following location: https://console.anyscale.com/jobs/{job_id}\n",
    "\n",
    "More specifically, we can inspect the following:\n",
    "- Logs to view which stage of the finetuning process the job is currently at.\n",
    "- Hardware utilization metrics to ensure that the job is making full use of the resources allocated to it.\n",
    "- Training metrics to see how the model is performing on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to follow the job logs in real-time, you can run the following command:\n",
    "\n",
    "```bash\n",
    "!anyscale job logs --id {job_id} -f\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "If you head to the Job's dashboard, you can see the hardware utilization metrics showcasing the GPU utilization and the memory usage:\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-summit/e2e-llms/hardware-utilization-metrics-v2.jpg\" width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the job's log tab, you can see a snippet of the logs showcasing the training metrics:\n",
    "\n",
    "```\n",
    "2024-09-04, 17:36:21.824\tdriver\t╭───────────────────────────────────────────────╮\n",
    "2024-09-04, 17:36:21.824\tdriver\t│ Training result                               │\n",
    "2024-09-04, 17:36:21.824\tdriver\t├───────────────────────────────────────────────┤\n",
    "2024-09-04, 17:36:21.824\tdriver\t│ checkpoint_dir_name                           │\n",
    "2024-09-04, 17:36:21.824\tdriver\t│ time_this_iter_s                      9.07254 │\n",
    "2024-09-04, 17:36:21.824\tdriver\t│ time_total_s                          414.102 │\n",
    "2024-09-04, 17:36:21.824\tdriver\t│ training_iteration                         29 │\n",
    "2024-09-04, 17:36:21.824\tdriver\t│ avg_bwd_time_per_epoch                        │\n",
    "2024-09-04, 17:36:21.824\tdriver\t│ avg_fwd_time_per_epoch                        │\n",
    "2024-09-04, 17:36:21.824\tdriver\t│ avg_train_loss_epoch                          │\n",
    "2024-09-04, 17:36:21.824\tdriver\t│ bwd_time                              5.13469 │\n",
    "2024-09-04, 17:36:21.824\tdriver\t│ epoch                                       1 │\n",
    "2024-09-04, 17:36:21.824\tdriver\t│ eval_loss                                     │\n",
    "2024-09-04, 17:36:21.824\tdriver\t│ eval_time_per_epoch                           │\n",
    "2024-09-04, 17:36:21.824\tdriver\t│ fwd_time                              3.94241 │\n",
    "2024-09-04, 17:36:21.824\tdriver\t│ learning_rate                           5e-05 │\n",
    "2024-09-04, 17:36:21.824\tdriver\t│ num_iterations                             13 │\n",
    "2024-09-04, 17:36:21.824\tdriver\t│ perplexity                                    │\n",
    "2024-09-04, 17:36:21.824\tdriver\t│ step                                       12 │\n",
    "2024-09-04, 17:36:21.824\tdriver\t│ total_trained_steps                        29 │\n",
    "2024-09-04, 17:36:21.824\tdriver\t│ total_update_time                     268.125 │\n",
    "2024-09-04, 17:36:21.824\tdriver\t│ train_loss_batch                      0.28994 │\n",
    "2024-09-04, 17:36:21.824\tdriver\t│ train_time_per_epoch                          │\n",
    "2024-09-04, 17:36:21.824\tdriver\t│ train_time_per_step                   9.07861 │\n",
    "2024-09-04, 17:36:21.824\tdriver\t│ trained_tokens                         280128 │\n",
    "2024-09-04, 17:36:21.824\tdriver\t│ trained_tokens_this_iter                10752 │\n",
    "2024-09-04, 17:36:21.824\tdriver\t│ trained_tokens_throughput             1044.76 │\n",
    "2024-09-04, 17:36:21.824\tdriver\t│ trained_tokens_throughput_this_iter   1184.51 │\n",
    "2024-09-04, 17:36:21.824\tdriver\t╰───────────────────────────────────────────────╯\n",
    "2024-09-04, 17:36:21.824\tdriver\t(RayTrainWorker pid=2484, ip=10.0.32.0) [epoch 1 step 12] loss: 0.28619903326034546 step-time: 9.077147483825684\n",
    "```\n",
    "\n",
    "\n",
    "Note, you can also run tools like tensorboard to visualize the training metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tailoring LLMForge to Your Needs\n",
    "\n",
    "### 1. Start with a default configuration\n",
    "\n",
    "Use the Anyscale [finetuning LLMs template](https://console.anyscale.com/v2/template-preview/finetuning_llms_v2) which contains a default configuration for the most common models.\n",
    "\n",
    "### 2. Customize to point to your data\n",
    "\n",
    "Use the `train_path` and `valid_path` to point to your data. Update the `context_length` to fit your expected input/output length.\n",
    "\n",
    "### 3. Run the job and monitor for performance bottlenecks\n",
    "\n",
    "Here are some common performance bottlenecks:\n",
    "\n",
    "#### Minimize GPU communication overhead\n",
    "If you can secure a large instance and perform the finetuning on a single node, then this will be advisable to reduce the communication overhead during distributed training. You can specify a larger node instances by setting a custom compute configuration in the `job.yaml` file.\n",
    "\n",
    "#### Maximize GPU memory utilization\n",
    "\n",
    "The following parameters affect your GPU memory utilization\n",
    "\n",
    "1. The batch size per device\n",
    "2. The chosen context length\n",
    "3. The padding type\n",
    "\n",
    "In addition, other configurations like deepspeed will also have an effect on your memory.\n",
    "\n",
    "You will want to tune these parameters to maximize your hardware utilization.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b> Note:</b> For an advanced tuning guide check out [this guide here](https://docs.anyscale.com/canary/llms/finetuning/guides/optimize_cost/)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "We jumped directly into finetuning an LLM but in the next notebooks we will cover the following topics:\n",
    "\n",
    "1. How did we prepare the data for finetuning?\n",
    "2. How should we evaluate the model?\n",
    "3. How do we deploy the model?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
