{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data for Fine-Tuning a Large Language Model\n",
    "\n",
    "It is critical to prepare quality data in the correct format to fine-tune a large language model.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Here is the roadmap for this notebook:</b>\n",
    "\n",
    "<ul>\n",
    "    <li><b>Part 1:</b> Preparing a sample dataset.</li>\n",
    "    <li><b>Part 2:</b> Introduction to Ray Data.</li>\n",
    "    <li><b>Part 3:</b> Migrating to a scalable pipeline.</li>\n",
    "    <li><b>Part 4:</b> Using the Anyscale Datasets registry.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from typing import Any\n",
    "\n",
    "import anyscale\n",
    "import pandas as pd\n",
    "import ray\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = ray.data.DataContext.get_current()\n",
    "ctx.enable_operator_progress_bars = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1. Preparing a sample dataset\n",
    "\n",
    "Let's start by preparing a small dataset for fine-tuning a large language model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Dataset\n",
    "\n",
    "We'll be using the [ViGGO dataset](https://huggingface.co/datasets/GEM/viggo) dataset, where the input (`meaning_representation`) is a structured collection of the overall intent (ex. `inform`) and entities (ex. `release_year`) and the output (`target`) is an unstructured sentence that incorporates all the structured input information. \n",
    "\n",
    "But for our task, we'll **reverse** this dataset where the input will be the unstructured sentence and the output will be the structured information.\n",
    "\n",
    "```python\n",
    "# Input (unstructured sentence):\n",
    "\"Dirt: Showdown from 2012 is a sport racing game for the PlayStation, Xbox, PC rated E 10+ (for Everyone 10 and Older). It's not available on Steam, Linux, or Mac.\"\n",
    "\n",
    "# Output (function + attributes): \n",
    "\"inform(name[Dirt: Showdown], release_year[2012], esrb[E 10+ (for Everyone 10 and Older)], genres[driving/racing, sport], platforms[PlayStation, Xbox, PC], available_on_steam[no], has_linux_release[no], has_mac_release[no])\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Schema\n",
    "\n",
    "The preprocessing we'll do involves formatting our dataset into the schema required for fine-tuning (`system`, `user`, `assistant`) conversations.\n",
    "\n",
    "- `system`: description of the behavior or personality of the model. As a best practice, this should be the same for all examples in the fine-tuning dataset, and should remain the same system prompt when moved to production.\n",
    "- `user`: user message, or \"prompt,\" that provides a request for the model to respond to.\n",
    "- `assistant`: stores previous responses but can also contain examples of intended responses for the LLM to return.\n",
    "\n",
    "```python\n",
    "conversations = [\n",
    "    {\"messages\": [\n",
    "        {'role': 'system', 'content': system_content},\n",
    "        {'role': 'user', 'content': item['target']},\n",
    "        {'role': 'assistant', 'content': item['meaning_representation']}\n",
    "    ]},\n",
    "    {\"messages\": [...]},\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Loading a sample dataset\n",
    "\n",
    "We will make use of the `datasets` library to load the ViGGO dataset and prepare a sample dataset for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"GEM/viggo\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the data splits available in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splits\n",
    "train_set = dataset['train']\n",
    "val_set = dataset['validation']\n",
    "test_set = dataset['test']\n",
    "print (f\"train: {len(train_set)}\")\n",
    "print (f\"val: {len(val_set)}\")\n",
    "print (f\"test: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a single row of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in test_set:\n",
    "    break\n",
    "row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a function that will transform the row into a format that can be used by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_schema(row: dict[str, Any], system_content: str) -> dict[str, Any]:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": row[\"target\"]},\n",
    "        {\"role\": \"assistant\", \"content\": row[\"meaning_representation\"]},\n",
    "    ]\n",
    "    return {\"messages\": messages}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System content\n",
    "system_content = (\n",
    "    \"Given a target sentence construct the underlying meaning representation of the input \"\n",
    "    \"sentence as a single function with attributes and attribute values. This function \"\n",
    "    \"should describe the target string accurately and the function must be one of the \"\n",
    "    \"following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', \"\n",
    "    \"'suggest', 'request_explanation', 'recommend', 'request_attribute']. The attributes \"\n",
    "    \"must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', \"\n",
    "    \"'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', \"\n",
    "    \"'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now convert the data to the schema format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_data = []\n",
    "\n",
    "for row in train_set:\n",
    "    row[\"schema\"] = to_schema(row, system_content)\n",
    "    converted_data.append(row[\"schema\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how the schema looks like for a single row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row[\"schema\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then make use of pandas to first view our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_df = pd.DataFrame(converted_data)\n",
    "converted_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we then store our training dataset which is now ready for finetuning via LLMForge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_df.to_json(\"train.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to Ray Data\n",
    "\n",
    "<!-- One liner about Ray Data -->\n",
    "Ray Data is a scalable data processing library for ML workloads, particularly suited for the following workloads:\n",
    "\n",
    "\n",
    "<!-- Diagram showing streaming and heterogenous cluster -->\n",
    "Ray Data is particularly useful for streaming data on a heterogenous cluster:\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/stream-example.png\" width=\"600\">\n",
    "\n",
    "Your production pipeline for preparing data for fine-tuning a large language model could require:\n",
    "1. Loading mutli-modal datasets\n",
    "2. Inferencing against guardrail models to remove low-quality and PII data.\n",
    "3. Preprocessing data to the schema required for fine-tuning.\n",
    "\n",
    "You will want to make the most efficient use of your cluster to process this data. Ray Data can help you do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray Data's API\n",
    "\n",
    "Here are the steps to make use of Ray Data:\n",
    "1. Create a Ray Dataset usually by pointing to a data source.\n",
    "2. Apply transformations to the Ray Dataset.\n",
    "3. Write out the results to a data source.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data\n",
    "\n",
    "Ray Data has a number of [IO connectors](https://docs.ray.io/en/latest/data/api/input_output.html) to most commonly used formats.\n",
    "\n",
    "For purposes of this introduction, we will use the `from_huggingface` function to read the dataset we prepared in the previous section but this time we enable streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_streaming_ds = load_dataset(\n",
    "    path=\"GEM/viggo\",\n",
    "    name=\"default\",\n",
    "    streaming=True, # Enable streaming\n",
    "    split=\"train\",\n",
    ")\n",
    "\n",
    "train_ds = ray.data.from_huggingface(train_streaming_ds)\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<b>Note</b> that we can also stream data directly from huggingface or from any other source (e.g. parquet on S3)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Data\n",
    "\n",
    "Datasets can be transformed by applying a row-wise `map` operation. We do this by providing a user-defined function that takes a row as input and returns a row as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_schema_map(row: dict[str, Any]) -> dict[str, Any]:\n",
    "    return to_schema(row, system_content=system_content)\n",
    "\n",
    "train_ds_with_schema = train_ds.map(to_schema_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lazy execution\n",
    "\n",
    "By default, `map` is lazy, meaning that it will not actually execute the function until you consume it. This allows for optimizations like pipelining and fusing of operations.\n",
    "\n",
    "To inspect a few rows of the dataset, you can use the `take` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_with_schema.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Data\n",
    "\n",
    "We can then write out the data to disk using the avialable IO connector methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_storage_path = os.environ[\"ANYSCALE_ARTIFACT_STORAGE\"]\n",
    "uuid_ = str(uuid.uuid4())\n",
    "storage_path = artifact_storage_path.rstrip(\"/\") +  f\"/ray_summit/e2e_llms/{uuid_}\"\n",
    "storage_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make use of the `write_json` method to write the dataset to the storage path in a distributed manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_with_schema.write_json(f\"{storage_path}/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the generated files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls {storage_path}/train/ --human-readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap of our Ray Data pipeline\n",
    "\n",
    "Here is our Ray data pipeline condensed into the following chained operations:\n",
    "\n",
    "```python\n",
    "(\n",
    "    ray.data.from_huggingface(train_streaming_ds)\n",
    "    .map(to_schema_map)\n",
    "    .write_json(f\"{storage_path}/train\")\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Lab activity: Apply more elaborate preprocessing\n",
    "\n",
    "Assume you have a function that you would like to apply to remove all `give_opinion` messages to avoid finetuning on sensitive user opinions.\n",
    "\n",
    "In a production setting, think of this as applying a Guardrail model that you use to detect and filter out poor quality data or PII data.\n",
    "\n",
    "i.e. given this code:\n",
    "\n",
    "```python\n",
    "def is_give_opinion(conversation):\n",
    "    sys, user, assistant = conversation\n",
    "    return \"give_opinion\" in assistant[\"content\"]\n",
    "\n",
    "\n",
    "def filter_opinions(row) -> bool:\n",
    "    # Hint: call is_give_opinion on the row\n",
    "    ...\n",
    "\n",
    "(\n",
    "    ray.data.from_huggingface(train_streaming_ds)\n",
    "    .map(to_schema_map)\n",
    "    .filter(filter_opinions)\n",
    "    .write_json(f\"{storage_path}/train_without_opinion\")\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<details>\n",
    "<summary>Click here to view the solution</summary>\n",
    "\n",
    "```python\n",
    "def is_give_opinion(conversation):\n",
    "    sys, user, assistant = conversation\n",
    "    return \"give_opinion\" in assistant[\"content\"]\n",
    "\n",
    "\n",
    "def filter_opinions(row) -> bool:\n",
    "    return not is_give_opinion(row[\"messages\"])\n",
    "\n",
    "(\n",
    "    ray.data.from_huggingface(train_streaming_ds)\n",
    "    .map(to_schema_map)\n",
    "    .filter(filter_opinions)\n",
    "    .write_json(f\"{storage_path}/train_without_opinion\")\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Anyscale Datasets\n",
    "\n",
    "Anyscale Datasets is a managed dataset registry and discovery service that allows you to:\n",
    "\n",
    "- Centralize dataset storage\n",
    "- Version datasets\n",
    "- Track dataset usage\n",
    "- Manage dataset access\n",
    "\n",
    "Let's upload our training data to the Anyscale Datasets registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anyscale_dataset = anyscale.llm.dataset.upload(\n",
    "    \"train.jsonl\",\n",
    "    name=\"viggo_train\",\n",
    "    description=(\n",
    "        \"VIGGO dataset for E2E LLM template: train split\"\n",
    "    ),\n",
    "    )\n",
    "\n",
    "anyscale_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is now saved to the Anyscale Datasets registry.\n",
    "\n",
    "To load the Anyscale Dataset back into a Ray Dataset, you can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anyscale_dataset = anyscale.llm.dataset.get(\"viggo_train\")\n",
    "train_ds_with_schema = ray.data.read_json(anyscale_dataset.storage_uri)\n",
    "train_ds_with_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also want to download the contents of the Dataset file directly, in this case, a `.jsonl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_contents: bytes = anyscale.llm.dataset.download(\"viggo_train\")\n",
    "lines = dataset_contents.decode().splitlines()\n",
    "print(\"# of rows:\", len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or version the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anyscale_dataset = anyscale.llm.dataset.get(\"viggo_train\")\n",
    "latest_version = anyscale_dataset.version\n",
    "anyscale_dataset = anyscale.llm.dataset.upload(\n",
    "    \"train.jsonl\",\n",
    "    name=\"viggo_train\",\n",
    "    description=(\n",
    "        f\"VIGGO dataset for E2E LLM template: train split, version {latest_version + 1}\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Latest version:\", anyscale.llm.dataset.get(\"viggo_train\"))\n",
    "print(\"Second latest version:\", anyscale.llm.dataset.get(\"viggo_train\", version=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can use the Anyscale dataset in your LLMForge fine-tuning jobs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
