{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Now we'll evaluate our fine-tuned LLM to see how well it performs on our task. Here is the roadmap for our notebook:\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b> Here is the roadmap for this notebook:</b>\n",
    "\n",
    "<ul>\n",
    "    <li><b>Part1:</b> Overview of LLM Evaluation</li>\n",
    "    <li><b>Part2:</b> Loading Test Data</li>\n",
    "    <li><b>Part3:</b> Forming our Inputs and Outputs</li>\n",
    "    <li><b>Part4:</b> Running Model Inference</li>\n",
    "    <li><b>Part5:</b> Generating Evaluation Metrics</li>\n",
    "    <li><b>Part6:</b> Comparing with a Baseline Model</li>\n",
    "</ul>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Optional\n",
    "\n",
    "import anyscale\n",
    "import numpy as np\n",
    "import ray\n",
    "import re\n",
    "\n",
    "from rich import print as rprint\n",
    "from transformers import AutoTokenizer\n",
    "from vllm.lora.request import LoRARequest\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = ray.data.DataContext.get_current()\n",
    "ctx.enable_operator_progress_bars = False\n",
    "ctx.enable_progress_bars = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 0. Overview of LLM Evaluation\n",
    "\n",
    "Here are the main steps for evaluating a language model:\n",
    "\n",
    "1. Prepare Evaluation Data:\n",
    "    1. Get data representative of the task you want to evaluate the model on.\n",
    "    2. Prepare it in the proper format for the model.\n",
    "2. Generate responses using your LLM\n",
    "    1. Run batch inference on the evaluation data.\n",
    "3. Produce evaluation metrics\n",
    "    1. Choose a metric based on the model's output.\n",
    "    2. Compare the model's performance to a baseline model to see if it's better.\n",
    "\n",
    "Here is a diagram of the evaluation process:\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-summit/e2e-llms/evaluation_metrics_v3.png\" width=\"700\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load model artifacts\n",
    "\n",
    "Now that our finetuning is complete, we can load the model artifacts from cloud storage to a local [cluster storage](https://docs.anyscale.com/workspaces/storage/#cluster-storage) to use for other workloads.\n",
    "\n",
    "To retrieve information about your fine-tuned model, Anyscale provides a convenient model registry SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"background-color: yellow;\">&nbsp;ðŸ”„ REPLACE&nbsp;</b>: Use the job ID of your fine-tuning run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = anyscale.llm.model.get(job_id=\"prodjob_123\") # REPLACE with the job ID for your fine-tuning run\n",
    "rprint(model_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the model ID from the model info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = model_info.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will download the artifacts from the cloud storage bucket to our local cluster storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_storage_uri =  (\n",
    "    f\"{os.environ['ANYSCALE_ARTIFACT_STORAGE']}\"\n",
    "    f\"/lora_fine_tuning/{model_id}\"\n",
    ")\n",
    "# s3_storage_uri = model_info.storage_uri \n",
    "s3_path_wo_bucket = '/'.join(s3_storage_uri.split('/')[3:])\n",
    "\n",
    "local_artifacts_dir = \"/mnt/cluster_storage\"\n",
    "local_artifacts_path = os.path.join(local_artifacts_dir, s3_path_wo_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync {s3_storage_uri} {local_artifacts_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>Backup:</b> In case you don't have access to a successful finetuning job, you can download the artifacts by running this code in a python cell.\n",
    "\n",
    "```python\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.1:aitra:qzoyg\"\n",
    "local_artifacts_path = f\"/mnt/cluster_storage/llm-finetuning/lora_fine_tuning/{model_id}\"\n",
    "!aws s3 sync s3://anyscale-public-materials/llm-finetuning/lora_fine_tuning/{model_id} {local_artifacts_path}\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2. Reading the test data\n",
    "\n",
    "Let's start by reading the test data to evaluate our fine-tuned LLM. This test data has undergone the same preparation process as the training data - i.e. it is in the correct schema format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = (\n",
    "    ray.data.read_json(\n",
    "        \"s3://anyscale-public-materials/llm-finetuning/viggo_inverted/test/data.jsonl\"\n",
    "    ).limit(100)  # We limit to 100 for the sake of time but still sufficient size.\n",
    ")\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<b>NOTE:</b> It is important to split the dataset into a train, validation, and test set. The test set should be used only for evaluation purposes. The model should not be trained or tuned on the test set.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 3. Forming our Inputs and Outputs\n",
    "\n",
    "Let's split the test data into inputs and outputs. Our inputs are the \"system\" and \"user\" prompts, and the outputs are the responses generated by the \"assistant\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_inputs_outputs(row):\n",
    "    row[\"input_messages\"] = [\n",
    "        message for message in row[\"messages\"] if message[\"role\"] != \"assistant\"\n",
    "    ]\n",
    "    row[\"output_messages\"] = [\n",
    "        message for message in row[\"messages\"] if message[\"role\"] == \"assistant\"\n",
    "    ]\n",
    "    del row[\"messages\"]\n",
    "    return row\n",
    "\n",
    "test_ds_inputs_outputs = test_ds.map(split_inputs_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect a sample batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = test_ds_inputs_outputs.take_batch(1)\n",
    "sample_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to fetch the LLM model files from an s3 bucket instead of huggingface. This is much more likely what you might do in a production environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"/mnt/cluster_storage/mistralai--Mistral-7B-Instruct-v0.1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync \"s3://anyscale-public-materials/llm/mistralai--Mistral-7B-Instruct-v0.1/\" {base_model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load the appropriate tokenizer to apply to our input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokenizer encodes the input text into a list of token ids that the model can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"Hello there\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The token ids are simply the indices of the tokens in the model's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"Hello there\", add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to tokenizing, we will need to convert the prompt into the template format that the model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.apply_chat_template(\n",
    "    conversation=sample_batch[\"input_messages\"][0],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    "    return_tensors=\"np\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the prompt template and tokenize the input data, we'll use the following stateful transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralTokenizer:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "    def __call__(self, row: dict[str, Any]) -> dict[str, Any]:\n",
    "        row[\"input_tokens\"] = self.tokenizer.apply_chat_template(\n",
    "            conversation=row[\"input_messages\"],\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_tensors=\"np\",\n",
    "        ).squeeze()\n",
    "        return row\n",
    "\n",
    "\n",
    "test_ds_inputs_tokenized = test_ds_inputs_outputs.map(\n",
    "    MistralTokenizer,\n",
    "    concurrency=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tokenized_batch = test_ds_inputs_tokenized.take_batch(1)\n",
    "sample_tokenized_batch[\"input_tokens\"][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then proceed to materialize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds_inputs_tokenized = test_ds_inputs_tokenized.materialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Materializing the dataset could be useful if we want to compute metrics on the tokens like the maximum input token length for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_token_length(row: dict) -> dict:\n",
    "    row[\"token_length\"] = len(row[\"input_tokens\"])\n",
    "    return row\n",
    "\n",
    "max_input_length = test_ds_inputs_tokenized.map(compute_token_length).max(on=\"token_length\")\n",
    "max_input_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 5. Running Model Inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Quick Intro to vLLM\n",
    "\n",
    "vLLM is a library for high throughput generation of LLM models by leveraging various performance optimizations, primarily: \n",
    "\n",
    "* Efficient management of attention key and value memory with PagedAttention \n",
    "* Fast model execution with CUDA/HIP graph\n",
    "* Quantization: GPTQ, AWQ, SqueezeLLM, FP8 KV Cache\n",
    "* Optimized CUDA kernels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vLLM makes available an `LLM` class which can be called along with sampling parameters to generate outputs.\n",
    "\n",
    "Here is how we can build a stateful transformation to perform batch inference on our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMPredictor:\n",
    "    def __init__(\n",
    "        self, hf_model: str, sampling_params: SamplingParams, lora_path: str = None\n",
    "    ):\n",
    "        # 1. Load the LLM\n",
    "        self.llm = LLM(\n",
    "            model=hf_model,\n",
    "            enable_lora=bool(lora_path),\n",
    "            gpu_memory_utilization=0.95,\n",
    "            kv_cache_dtype=\"fp8\",\n",
    "        )\n",
    "\n",
    "        self.sampling_params = sampling_params\n",
    "        # 2. Prepare a LoRA request if a LoRA path is provided\n",
    "        self.lora_request = (\n",
    "            LoRARequest(\n",
    "                lora_name=\"lora_adapter\", lora_int_id=1, lora_local_path=lora_path\n",
    "            )\n",
    "            if lora_path\n",
    "            else None\n",
    "        )\n",
    "\n",
    "    def __call__(self, batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "        # 3. Generate outputs\n",
    "        responses = self.llm.generate(\n",
    "            prompt_token_ids=[ids.squeeze().tolist() for ids in batch[\"input_tokens\"]],\n",
    "            sampling_params=self.sampling_params,\n",
    "            lora_request=self.lora_request,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"prompt\": [\n",
    "                \" \".join(message[\"content\"] for message in messages)\n",
    "                for messages in batch[\"input_messages\"]\n",
    "            ],\n",
    "            \"expected_output\": [\n",
    "                message[\"content\"]\n",
    "                for messages in batch[\"output_messages\"]\n",
    "                for message in messages\n",
    "            ],\n",
    "            \"generated_text\": [resp.outputs[0].text for resp in responses],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then apply the transformation like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0, max_tokens=1024, detokenize=True)\n",
    "\n",
    "test_ds_responses = test_ds_inputs_tokenized.map_batches(\n",
    "    LLMPredictor,\n",
    "    fn_constructor_kwargs={\n",
    "        \"hf_model\": base_model,\n",
    "        \"sampling_params\": sampling_params,\n",
    "        \"lora_path\": local_artifacts_path,\n",
    "    },\n",
    "    concurrency=1,  # number of LLM instances\n",
    "    num_gpus=1,  # GPUs per LLM instance\n",
    "    accelerator_type=\"A10G\",  # A10G\n",
    "    batch_size=40,\n",
    ")\n",
    "\n",
    "test_ds_responses = test_ds_responses.materialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<b>Note:</b> Running inference can take a long time depending on the size of the dataset and the model. Additional time may be required for the model to automatically scale up to handle the workload.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_response = test_ds_responses.take_batch(2)\n",
    "sample_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Activity: Find the optimal batch size\n",
    "\n",
    "To run batch inference efficiently, we should always look to maximize our hardware utilization. \n",
    "\n",
    "To that end, you need to find the batch size that will maximize our GPU memory usage. \n",
    "\n",
    "Hint: make use of the metrics tab to look at the hardware utilization and iteratively find your batch size.\n",
    "\n",
    "\n",
    "```python\n",
    "test_ds_inputs_tokenized.map_batches(\n",
    "    LLMPredictor,\n",
    "    fn_constructor_kwargs={\n",
    "        \"hf_model\": base_model,\n",
    "        \"sampling_params\": sampling_params,\n",
    "        \"lora_path\": local_artifacts_path,\n",
    "    },\n",
    "    concurrency=1,  \n",
    "    num_gpus=1,  \n",
    "    accelerator_type=\"A10G\",\n",
    "    batch_size=40, # Hint: find the optimal batch size.\n",
    ").materialize()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary> Click here to see the solution </summary>\n",
    "\n",
    "```python\n",
    "test_ds_inputs_tokenized.map_batches(\n",
    "    LLMPredictor,\n",
    "    fn_constructor_kwargs={\n",
    "        \"hf_model\": base_model,\n",
    "        \"sampling_params\": sampling_params,\n",
    "        \"lora_path\": local_artifacts_path,\n",
    "    },\n",
    "    concurrency=1,  \n",
    "    num_gpus=1, \n",
    "    accelerator_type=\"A10G\",\n",
    "    batch_size=70,\n",
    ").materialize()\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 6. Generating Evaluation Metrics\n",
    "\n",
    "Depending on your task, you will want to choose the proper evaluation metric. \n",
    "\n",
    "In our functional representation task, the output is constrained into a limited set of categories and therefore standard classification evaluation metrics are a good choice.\n",
    "\n",
    "In more open-ended response generation tasks, you might want to consider making use of an LLM as a judge to generate a scoring metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing the responses\n",
    "\n",
    "We will evaluate the accuracy at two levels:\n",
    "- accuracy of predicting the correct function type\n",
    "- accuracy of predicting the correct attribute types (a much more difficult task)\n",
    "\n",
    "Lets post process the outputs to extract the ground-truth vs model predicted function types and attriute types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_function_type(response: str) -> Optional[str]:\n",
    "    \"\"\"Extract the function type from the response.\"\"\"\n",
    "    if response is None:\n",
    "        return None\n",
    "\n",
    "    # pattern to match is \"{function_type}({attributes})\"\n",
    "    expected_pattern = re.compile(r\"^(?P<function_type>.+?)\\((?P<attributes>.+)\\)$\")\n",
    "\n",
    "    # remove any \"Output: \" prefix and strip the response\n",
    "    match = expected_pattern.match(response.split(\"Output: \")[-1].strip())\n",
    "\n",
    "    if match is None:\n",
    "        return None\n",
    "\n",
    "    # return the function type\n",
    "    ret = match.group(\"function_type\")\n",
    "    return ret.replace(\"\\\\_\", \"_\") # handle escapes of underscores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this expected response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_output = sample_response['expected_output'][0]\n",
    "expected_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract its function type like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_function_type(expected_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the generated output from our finetuned LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_output = sample_response[\"generated_text\"][0]\n",
    "generated_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract its function type like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_function_type(generated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a similar function to extract the attribute types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attribute_types(response: Optional[str]) -> list[str]:\n",
    "    if response is None:\n",
    "        return []\n",
    "\n",
    "    # pattern to match is \"{function_type}({attributes})\"\n",
    "    expected_pattern = re.compile(r\"^(?P<function_type>.+?)\\((?P<attributes>.+)\\)$\")\n",
    "\n",
    "    # remove any \"Output: \" prefix and strip the response\n",
    "    match = expected_pattern.match(response.split(\"Output: \")[-1].strip())\n",
    "\n",
    "    if match is None:\n",
    "        return []\n",
    "\n",
    "    attributes = match.group(\"attributes\")\n",
    "\n",
    "    # pattern is \"{attribute_type}[{attribute_value}], ...\"\n",
    "    attr_types = re.findall(r\"(\\w+)\\[\", attributes)\n",
    "\n",
    "    return attr_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a sample expected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the expected attribute types to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_attribute_types(expected_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take our finetuned LLM generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now extract its attribute types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_attribute_types(generated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this post processing to our entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(row: dict[str, Any]) -> dict[str, Any]:\n",
    "    row.update(\n",
    "        {\n",
    "            \"ground_truth_fn_type\": extract_function_type(row[\"expected_output\"]),\n",
    "            \"ground_truth_attr_types\": extract_attribute_types(row[\"expected_output\"]),\n",
    "            \"model_fn_type\": extract_function_type(row[\"generated_text\"]),\n",
    "            \"model_attr_types\": extract_attribute_types(row[\"generated_text\"]),\n",
    "        }\n",
    "    )\n",
    "    return row\n",
    "\n",
    "\n",
    "test_ds_responses_processed = test_ds_responses.map(post_process)\n",
    "sample_processed = test_ds_responses_processed.take_batch(2)\n",
    "sample_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_function_type_accuracy(batch: dict) -> dict:\n",
    "    batch[\"fn_type_match\"] = batch[\"ground_truth_fn_type\"] == batch[\"model_fn_type\"]\n",
    "    return batch\n",
    "\n",
    "fn_type_accuracy_percent = test_ds_responses_processed.map(check_function_type_accuracy).mean(on=\"fn_type_match\") * 100 \n",
    "print(f\"The correct function type is predicted at {fn_type_accuracy_percent}% accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_attribute_types_accuracy(batch: dict) -> dict:\n",
    "    batch[\"attr_types_match\"] = batch[\"ground_truth_attr_types\"].apply(set) == batch[\"model_attr_types\"].apply(set)\n",
    "    return batch\n",
    "\n",
    "attr_types_accuracy_percent = test_ds_responses_processed.map_batches(check_attribute_types_accuracy, batch_format=\"pandas\").mean(on=\"attr_types_match\") * 100 \n",
    "print(f\"The correct attribute types are predicted at {attr_types_accuracy_percent}% accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Activity: Change the attribute types accuracy metric\n",
    "\n",
    "Our current metric for attribute types is not very strict. \n",
    "\n",
    "Can you make it stricter by setting `attr_types_match` to `True` only when the model's predicted attribute types and the ground truth attribute types are exactly the same in the order they appear?\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary> Click here to see the solution </summary>\n",
    "\n",
    "```python\n",
    "def check_attribute_types_accuracy(batch: dict) -> dict:\n",
    "    batch[\"attr_types_match\"] = batch[\"ground_truth_attr_types\"].apply(list) == batch[\"model_attr_types\"].apply(list)\n",
    "    return batch\n",
    "\n",
    "attr_types_accuracy_percent = test_ds_responses_processed.map_batches(check_attribute_types_accuracy, batch_format=\"pandas\").mean(on=\"attr_types_match\") * 100 \n",
    "print(f\"The correct attribute types are predicted at {attr_types_accuracy_percent}% accuracy\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 7. Running Baseline Model Inference\n",
    "\n",
    "We will benchmark the performance to the unfinetuned version of the same LLM. \n",
    "\n",
    "### Using Few-shot learning for the baseline model\n",
    "\n",
    "We will augment the prompt with few-shot examples as a prompt-engineering approach to provide a fair comparison between the finetuned and unfinetuned models given the unfinetuned model fails to perform the task out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us read in from our training data up to 20 examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_few_shot = ray.data.read_json(\"s3://anyscale-public-materials/llm-finetuning/viggo_inverted/train/subset-500.jsonl\").limit(20).to_pandas()\n",
    "examples = df_few_shot['messages'].tolist()\n",
    "examples[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a sample conversation from our test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_conversations = test_ds.take_batch(2)\n",
    "sample_conversations[\"messages\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we will build our prompt with few shot examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot(messages: list, examples: list) -> list:\n",
    "    \"\"\"Build a prompt for few-shot learning given a user input and examples.\"\"\"\n",
    "    system_message, user_message, assistant_message = messages\n",
    "    user_text = user_message[\"content\"]\n",
    "\n",
    "    example_preface = (\n",
    "        \"Examples are printed below.\"\n",
    "        if len(examples) > 1\n",
    "        else \"An example is printed below.\"\n",
    "    )\n",
    "    example_preface += (\n",
    "        ' Note: you are to respond with the string after \"Output: \" only.'\n",
    "    )\n",
    "    examples_parsed = \"\\n\\n\".join(\n",
    "        [\n",
    "            f\"{user['content']}\\nOutput: {assistant['content']}\"\n",
    "            for (system, user, assistant) in examples\n",
    "        ]\n",
    "    )\n",
    "    response_preface = \"Now please provide the output for:\"\n",
    "    user_text = f\"{example_preface}\\n\\n{examples_parsed}\\n\\n{response_preface}\\n{user_text}\\nOutput: \"\n",
    "    return [system_message, {\"role\": \"user\", \"content\": user_text}, assistant_message]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply `few_shot` function with only two examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = sample_conversations[\"messages\"][0]\n",
    "conversation_with_few_shot = few_shot(conversation, examples[:2])\n",
    "conversation_with_few_shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the updated user prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation_with_few_shot[1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's map this across our entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_few_shot(row: dict[str, Any]) -> dict[str, Any]:\n",
    "    row[\"messages\"] = few_shot(row[\"messages\"], examples)\n",
    "    return row\n",
    "\n",
    "test_ds_with_few_shot = test_ds.map(apply_few_shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to generate responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0, max_tokens=2048, detokenize=True)\n",
    "\n",
    "test_ds_responses_few_shot = (\n",
    "    test_ds_with_few_shot.map(split_inputs_outputs)\n",
    "    .map(\n",
    "        MistralTokenizer,\n",
    "        concurrency=2,\n",
    "    )\n",
    "    .map_batches(\n",
    "        LLMPredictor,\n",
    "        fn_constructor_kwargs={\n",
    "            \"hf_model\": base_model,\n",
    "            \"sampling_params\": sampling_params,\n",
    "        },\n",
    "        concurrency=1,  # number of LLM instances\n",
    "        num_gpus=1,  # GPUs per LLM instance\n",
    "        accelerator_type=\"A10G\",  # A10G or L4\n",
    "        batch_size=10,\n",
    "    )\n",
    "    .map(post_process)\n",
    "    .materialize()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparing Evaluation Metrics\n",
    "\n",
    "Let's produce the evaluation metrics on our baseline to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_type_accuracy_percent_few_shot = test_ds_responses_few_shot.map(check_function_type_accuracy).mean(on=\"fn_type_match\") * 100 \n",
    "print(f\"The correct function type is predicted at {fn_type_accuracy_percent_few_shot}% accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_types_accuracy_percent_few_shot = test_ds_responses_few_shot.map_batches(check_attribute_types_accuracy, batch_format=\"pandas\").mean(on=\"attr_types_match\") * 100 \n",
    "print(f\"The correct attribute types are predicted at {attr_types_accuracy_percent_few_shot}% accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /mnt/cluster_storage/llm-finetuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
