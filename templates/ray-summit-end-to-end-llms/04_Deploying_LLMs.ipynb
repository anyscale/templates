{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy, configure, and serve LLMs \n",
    "\n",
    "This guide benefits from an Anyscale library for serving LLMs on Anyscale called [RayLLM](http://https://docs.anyscale.com/llms/serving/intro).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Here is the roadmap for this notebook:</b>\n",
    "<ul>\n",
    "    <li><b>Part 1:</b> Overview of RayLLM</a></li>\n",
    "    <li><b>Part 2:</b> Generating a RayLLM Configuration</a></li>\n",
    "    <li><b>Part 3:</b> Running a RayLLM application </a></li>\n",
    "    <li><b>Part 4:</b> Querying our RayLLM application</a></li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "import anyscale\n",
    "import openai\n",
    "import ray\n",
    "from ray import serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = ray.data.DataContext.get_current()\n",
    "ctx.enable_operator_progress_bars = False\n",
    "ctx.enable_progress_bars = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview of RayLLM\n",
    "RayLLM provides a number of features that simplify LLM development, including:\n",
    "- An extensive suite of pre-configured open source LLMs.\n",
    "- An OpenAI-compatible REST API.\n",
    "\n",
    "As well as operational features to efficiently scale LLM apps:\n",
    "- Optimizations such as continuous batching, quantization and streaming.\n",
    "- Production-grade autoscaling support, including scale-to-zero.\n",
    "- Native multi-GPU & multi-node model deployments.\n",
    "\n",
    "To learn more about RayLLM, check out [the docs](http://https://docs.anyscale.com/llms/serving/intro). \n",
    "\n",
    "For a full guide on how to deploy LLMs, check out this [workspace template](https://docs.anyscale.com/examples/deploy-llms/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generating a RayLLM Configuration\n",
    "\n",
    "The first step is to set up a huggingface token in order to access the huggingface model hub. You can get a token by signing up at [huggingface](https://huggingface.co/login). \n",
    "\n",
    "You then will need to visit the [mistralai/Mistral-7B-Instruct-v0.1 model page ](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) and request access to the model.\n",
    "\n",
    "Once you have your token, you can proceed to open a terminal window (via Menu > Terminal > New Terminal) and run the `rayllm gen-config` command. \n",
    "\n",
    "Below are similar prompts to what you will see:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "(base) ray@ip-10-0-4-24:~/default/ray-summit-2024-training/End_to_End_LLMs/bonus$ rayllm gen-config\n",
    "We have provided the defaults for the following models:\n",
    "meta-llama/Llama-2-7b-chat-hf\n",
    "meta-llama/Llama-2-13b-chat-hf\n",
    "meta-llama/Llama-2-70b-chat-hf\n",
    "meta-llama/Meta-Llama-3-8B-Instruct\n",
    "meta-llama/Meta-Llama-3-70B-Instruct\n",
    "meta-llama/Meta-Llama-3.1-8B-Instruct\n",
    "meta-llama/Meta-Llama-3.1-70B-Instruct\n",
    "mistralai/Mistral-7B-Instruct-v0.1\n",
    "mistralai/Mixtral-8x7B-Instruct-v0.1\n",
    "mistralai/Mixtral-8x22B-Instruct-v0.1\n",
    "google/gemma-7b-it\n",
    "llava-hf/llava-v1.6-mistral-7b-hf\n",
    "Please enter the model ID you would like to serve, or enter your own custom model ID: mistralai/Mistral-7B-Instruct-v0.1\n",
    "GPU type [L4/A10/A100_40G/A100_80G/H100]: L4\n",
    "Tensor parallelism (1): 1\n",
    "Enable LoRA serving [y/n] (n): y\n",
    "LoRA weights storage URI. If not provided, the default will be used. \n",
    "(s3://anyscale-production-data-cld-91sl4yby42b2ivfp1inig5suuy/org_uhhav3lw5hg4risfz57ct1tg9s/cld_91sl4yby42b2ivfp1inig5suuy/artifact_storage/lora_fine_tuning): \n",
    "Maximum number of LoRA models per replica (16): \n",
    "Further customize the auto-scaling config [y/n] (n): n\n",
    "Enable token authentication?\n",
    "Note: Auth-enabled services require manual addition to playground. [y/n] (n): y\n",
    "\n",
    "Your serve configuration file is successfully written to ./serve_20240907010212.yaml\n",
    "\n",
    "Do you want to start up the server locally? [y/n] (y): y\n",
    "Run the serving command in the background: [y/n] (y): y\n",
    "Running: serve run ./serve_20240907010212.yaml --non-blocking\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Running a RayLLM application\n",
    "\n",
    "In the final steps of the interactive command we ran above, we can see that we ran the model locally by executing:\n",
    "\n",
    "```bash\n",
    "serve run ./serve_20240907010212.yaml --non-blocking\n",
    "```\n",
    "\n",
    "We can validate that the indeed our application is running by checking the Ray Serve dashboard. \n",
    "\n",
    "It should now look like this:\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-summit/e2e-llms/deploy_llm_v2.jpg\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Querying our LLM application\n",
    "\n",
    "Let's first build a client to query our LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_client(base_url: str, api_key: str) -> openai.OpenAI:\n",
    "    return openai.OpenAI(\n",
    "        base_url=base_url.rstrip(\"/\") + \"/v1\",\n",
    "        api_key=api_key,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = build_client(\"http://localhost:8000\", \"NOT A REAL KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we build a query function to send requests to our LLM application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(\n",
    "    client: openai.OpenAI,\n",
    "    llm_model: str,\n",
    "    system_message: dict[str, str],\n",
    "    user_message: dict[str, str],\n",
    "    temperature: float = 0,\n",
    "    timeout: float = 3 * 60,\n",
    ") -> Optional[str]:\n",
    "    model_response = client.chat.completions.create(\n",
    "        model=llm_model,\n",
    "        messages=[system_message, user_message],\n",
    "        temperature=temperature,\n",
    "        timeout=timeout,\n",
    "    )\n",
    "    model_output = model_response.choices[0].message.content\n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"background-color: yellow;\">&nbsp;ðŸ”„ REPLACE&nbsp;</b>: Use the job ID of your fine-tuning run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = anyscale.llm.model.get(job_id=\"prodjob_123\") # REPLACE with the job ID for your fine-tuning run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the base model ID and the model ID from the model info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = model_info.base_model_id\n",
    "finetuned_model_id = model_info.id\n",
    "finetuned_model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>Backup:</b> In case you don't have access to a successful finetuning job, you can copy the artifacts using the following command:\n",
    "\n",
    "```python\n",
    "base_model = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "finetuned_model_id = \"mistralai/Mistral-7B-Instruct-v0.1:aitra:qzoyg\"\n",
    "s3_lora_path = (\n",
    "    f\"{os.environ['ANYSCALE_ARTIFACT_STORAGE']}\"\n",
    "    f\"/lora_fine_tuning/{model_id}\"\n",
    ")\n",
    "!aws s3 sync s3://anyscale-public-materials/llm-finetuning/lora_fine_tuning/{model_id} {s3_lora_path}\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first test our base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query(\n",
    "    client=client,\n",
    "    llm_model=base_model,\n",
    "    system_message={\"content\": \"you are a helpful assistant\", \"role\": \"system\"},\n",
    "    user_message={\"content\": \"Hello there\", \"role\": \"user\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now query our finetuned LLM using the generated model id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query(\n",
    "    client=client,\n",
    "    llm_model=finetuned_model_id,\n",
    "    system_message={\"content\": \"you are a helpful assistant\", \"role\": \"system\"},\n",
    "    user_message={\"content\": \"Hello there\", \"role\": \"user\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"background-color: orange;\">&nbsp;ðŸ’¡ INSIGHT&nbsp;</b>: Ray Serve and Anyscale support [serving multiple LoRA adapters](https://github.com/anyscale/templates/blob/main/templates/endpoints_v2/examples/lora/DeployLora.ipynb) with a common base model in the same request batch which allows you to serve a wide variety of use-cases without increasing hardware spend. In addition, we use Serve multiplexing to reduce the number of swaps for LoRA adapters. There is a slight latency overhead to serving a LoRA model compared to the base model, typically 10-20%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this on our VIGGO dataset by reading in a sample conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = (\n",
    "    ray.data.read_json(\n",
    "        \"s3://anyscale-public-materials/llm-finetuning/viggo_inverted/test/data.jsonl\"\n",
    "    )\n",
    "    .to_pandas()[\"messages\"]\n",
    "    .tolist()\n",
    ")\n",
    "test_conversation = test_sample[0]\n",
    "test_conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check to see the response from our base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_base_model = query(\n",
    "    client=client,\n",
    "    llm_model=base_model,\n",
    "    system_message=test_conversation[0],\n",
    "    user_message=test_conversation[1]\n",
    ")\n",
    "print(response_base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if our finetuned model will provide a response with the format that we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_finetuned_model = query(\n",
    "    client=client,\n",
    "    llm_model=finetuned_model_id,\n",
    "    system_message=test_conversation[0],\n",
    "    user_message=test_conversation[1]\n",
    ")\n",
    "\n",
    "print(response_finetuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the finetuned model provides a more accurate and relevant response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_response = test_conversation[-1]\n",
    "expected_response[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Activity: Query the model with few-shot learning\n",
    "\n",
    "Confirm that indeed few-shot learning will assist our base model by augmenting the prompt.\n",
    "\n",
    "```python\n",
    "system_message = test_conversation[0]\n",
    "user_message = test_conversation[1]\n",
    "\n",
    "examples = \"\"\"\n",
    "Here is the target sentence:\n",
    "Dirt: Showdown from 2012 is a sport racing game for the PlayStation, Xbox, PC rated E 10+ (for Everyone 10 and Older). It's not available on Steam, Linux, or Mac.\n",
    "Output: inform(name[Dirt: Showdown], release_year[2012], esrb[E 10+ (for Everyone 10 and Older)], genres[driving/racing, sport], platforms[PlayStation, Xbox, PC], available_on_steam[no], has_linux_release[no], has_mac_release[no])\n",
    "\n",
    "Here is the target sentence:\n",
    "Dirt: Showdown is a sport racing game that was released in 2012. The game is available on PlayStation, Xbox, and PC, and it has an ESRB Rating of E 10+ (for Everyone 10 and Older). However, it is not yet available as a Steam, Linux, or Mac release.\n",
    "Output: inform(name[Dirt: Showdown], release_year[2012], esrb[E 10+ (for Everyone 10 and Older)], genres[driving/racing, sport], platforms[PlayStation, Xbox, PC], available_on_steam[no], has_linux_release[no], has_mac_release[no])\n",
    "\"\"\"\n",
    "\n",
    "user_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": ... # Hint: update the user message content to include the examples\n",
    "}, \n",
    "\n",
    "# Run the query\n",
    "query(\n",
    "    client=client,\n",
    "    llm_model=base_model,\n",
    "    system_message=system_message,\n",
    "    user_message=user_message\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary> Click here to see the solution </summary>\n",
    "\n",
    "```python\n",
    "system_message = test_conversation[0]\n",
    "user_message = test_conversation[1]\n",
    "\n",
    "examples = \"\"\"\n",
    "Here is the target sentence:\n",
    "Dirt: Showdown from 2012 is a sport racing game for the PlayStation, Xbox, PC rated E 10+ (for Everyone 10 and Older). It's not available on Steam, Linux, or Mac.\n",
    "Output: inform(name[Dirt: Showdown], release_year[2012], esrb[E 10+ (for Everyone 10 and Older)], genres[driving/racing, sport], platforms[PlayStation, Xbox, PC], available_on_steam[no], has_linux_release[no], has_mac_release[no])\n",
    "\n",
    "Here is the target sentence:\n",
    "Dirt: Showdown is a sport racing game that was released in 2012. The game is available on PlayStation, Xbox, and PC, and it has an ESRB Rating of E 10+ (for Everyone 10 and Older). However, it is not yet available as a Steam, Linux, or Mac release.\n",
    "Output: inform(name[Dirt: Showdown], release_year[2012], esrb[E 10+ (for Everyone 10 and Older)], genres[driving/racing, sport], platforms[PlayStation, Xbox, PC], available_on_steam[no], has_linux_release[no], has_mac_release[no])\n",
    "\"\"\"\n",
    "\n",
    "user_message_with_examples = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": (\n",
    "f\"\"\"\n",
    "Here are examples of the target output:\n",
    "{examples}\n",
    "\n",
    "Now please provide the output for:\n",
    "Here is the target sentence:\n",
    "{user_message[\"content\"]}\n",
    "Output: \n",
    "\"\"\"\n",
    ")\n",
    "}\n",
    "\n",
    "\n",
    "# Run the query\n",
    "query(\n",
    "    client=client,\n",
    "    llm_model=base_model,\n",
    "    system_message=system_message,\n",
    "    user_message=user_message_with_examples\n",
    ")\n",
    "```\n",
    "\n",
    "</details>\n",
    "<br/>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean up and shutdown our RayLLM application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Deploying as an Anyscale Service\n",
    "\n",
    "In case you want to productionize your LLM app, you can deploy it as an Anyscale Service. \n",
    "\n",
    "To do so, you can use the Anyscale CLI to deploy your application.\n",
    "\n",
    "```bash\n",
    "anyscale service deploy -f ./serve_20240907010212.yaml\n",
    "```\n",
    "\n",
    "You can then query your application using the same `query` function we defined earlier. Except this time, your client now points to the Anyscale endpoint and your API key is the generated authentication token.\n",
    "\n",
    "```python\n",
    "client = build_client(\"https://<your-endpoint>.serve.anyscale.com/\", \"<your-auth-token>\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
