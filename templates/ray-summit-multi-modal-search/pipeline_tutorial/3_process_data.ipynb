{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8253df45-49fb-4034-9aa7-679ada01abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Any, Literal, Type\n",
    "import numpy as np\n",
    "import requests\n",
    "import ray\n",
    "import torchvision\n",
    "from enum import Enum\n",
    "import typer\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from pyarrow import csv\n",
    "from pydantic import BaseModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "from vllm.multimodal.image import ImagePixelData\n",
    "from vllm import LLM, SamplingParams\n",
    "from PIL import Image\n",
    "from pymongo import MongoClient, UpdateOne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e432dd3d-f0cb-4884-8c8e-df3ac81f173c",
   "metadata": {},
   "source": [
    "# Reinventing Multi-Modal Search with Anyscale and MongoDB\n",
    "\n",
    "## Data processing pipeline tutorial\n",
    "\n",
    "__Let's look at the data flow logic__\n",
    "\n",
    "<img src='https://images.ctfassets.net/xjan103pcp94/Kb1UzXpXsig64xD1f29jE/945c113fd16f1badfd6d8c4400967062/image14.png' width=800px />\n",
    "\n",
    "__Let's look at the scaling opportunites__\n",
    "\n",
    "<img src='https://images.ctfassets.net/xjan103pcp94/4qBYwRfwQOD4DjA66N75Gq/8884421a98ea5013533e24f2a17e9cf5/image1.png' width=1000px />\n",
    "\n",
    "<img src='https://images.ctfassets.net/xjan103pcp94/6kROYvGncNbWlCsfklwEjz/e12cd36ed7be5cd45e603d38cb10dc95/image2.png' width=1000px />\n",
    "\n",
    "For this tutorial, we'll use a small number of records and a small scaling configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055ee1a1-2e25-4121-962f-fc758ff7b02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples=200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2dc142-3529-4dd1-981a-61471a2f9531",
   "metadata": {},
   "source": [
    "The workers below will corresponds to processes, each assigned a CPU and optionally a GPU. Ray allows more flexibility but we'll keep this pipeline as simples as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc8bd87-b22e-48f3-8c09-2065ae4a4fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_image_download_workers=2\n",
    "\n",
    "num_llava_tokenizer_workers=2\n",
    "\n",
    "num_llava_model_workers=1\n",
    "# llava_model_accelerator_type=NVIDIA_TESLA_A10G\n",
    "llava_model_batch_size=80\n",
    "\n",
    "num_mistral_tokenizer_workers_per_classifier=2\n",
    "\n",
    "num_mistral_model_workers_per_classifier=1\n",
    "mistral_model_batch_size=80\n",
    "# mistral_model_accelerator_type=NVIDIA_TESLA_A10G\n",
    "\n",
    "num_mistral_detokenizer_workers_per_classifier=2\n",
    "\n",
    "num_embedder_workers=1\n",
    "embedding_model_batch_size=80\n",
    "# embedding_model_accelerator_type=NVIDIA_TESLA_A10G\n",
    "\n",
    "db_update_batch_size=80\n",
    "\n",
    "num_db_workers=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94205417-4883-4bce-8c64-6ec535e2ea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name: str = \"myntra\"\n",
    "collection_name: str = \"myntra-items-offline\"\n",
    "cluster_size: str = \"m0\"\n",
    "scaling_config_path: str = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64303167",
   "metadata": {},
   "source": [
    "### Read raw data\n",
    "\n",
    "The data comes from a subset of the Myntra retail products dataset: https://www.kaggle.com/datasets/ronakbokaria/myntra-products-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf89b33-d854-4f1c-9423-d98be0a1a762",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 's3://anyscale-public-materials/mongodb-demo/raw/myntra_subset_deduped_10000.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478ad343-f631-4cc1-8c35-5010a2a300ac",
   "metadata": {},
   "source": [
    "The Ray Data `read` methods use PyArrow in most cases for the physical reads -- the schema here is provided as PyArrow types.\n",
    "\n",
    "> Learn more about the Apache Arrow project https://arrow.apache.org/docs/python/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e15f99-f28d-41f7-9aeb-c7f5fe0597c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path: str, nsamples: int) -> ray.data.Dataset:\n",
    "    ds = ray.data.read_csv(\n",
    "        path,\n",
    "        parse_options=csv.ParseOptions(newlines_in_values=True),\n",
    "        convert_options=csv.ConvertOptions(\n",
    "            column_types={\n",
    "                \"id\": pa.int64(),\n",
    "                \"name\": pa.string(),\n",
    "                \"img\": pa.string(),\n",
    "                \"asin\": pa.string(),\n",
    "                \"price\": pa.float64(),\n",
    "                \"mrp\": pa.float64(),\n",
    "                \"rating\": pa.float64(),\n",
    "                \"ratingTotal\": pa.int64(),\n",
    "                \"discount\": pa.int64(),\n",
    "                \"seller\": pa.string(),\n",
    "                \"purl\": pa.string(),\n",
    "            }\n",
    "        ),\n",
    "        override_num_blocks=nsamples,\n",
    "    )\n",
    "    return ds.limit(nsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e814ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = read_data(path, nsamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894bc732-5314-4dda-aed4-2c8f164fad80",
   "metadata": {},
   "source": [
    "### Preprocess data\n",
    "\n",
    "The following operations will be transforms applied to our data. \n",
    "\n",
    "We'll define them first...\n",
    "* functions for stateless operations\n",
    "* classes for operations which reuse state\n",
    "\n",
    "... and then plug them into our pipeline with Ray's `map_batches` API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377a5339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url: str) -> bytes:\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response.content\n",
    "    except Exception:\n",
    "        return b\"\"\n",
    "\n",
    "def download_images(batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        batch[\"url\"] = batch[\"img\"]\n",
    "        batch[\"img\"] = list(executor.map(download_image, batch[\"url\"]))  # type: ignore\n",
    "    return batch\n",
    "\n",
    "class LargestCenterSquare:\n",
    "    \"\"\"Largest center square crop for images.\"\"\"\n",
    "\n",
    "    def __init__(self, size: int) -> None:\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, row: dict[str, Any]) -> dict[str, Any]:\n",
    "        \"\"\"Crop the largest center square from an image.\"\"\"\n",
    "        img = Image.open(io.BytesIO(row[\"img\"]))\n",
    "\n",
    "        # First, resize the image such that the smallest side is self.size while preserving aspect ratio.\n",
    "        img = torchvision.transforms.functional.resize(\n",
    "            img=img,\n",
    "            size=self.size,\n",
    "        )\n",
    "\n",
    "        # Then take a center crop to a square.\n",
    "        w, h = img.size\n",
    "        c_top = (h - self.size) // 2\n",
    "        c_left = (w - self.size) // 2\n",
    "        row[\"img\"] = torchvision.transforms.functional.crop(\n",
    "            img=img,\n",
    "            top=c_top,\n",
    "            left=c_left,\n",
    "            height=self.size,\n",
    "            width=self.size,\n",
    "        )\n",
    "\n",
    "        return row\n",
    "\n",
    "DESCRIPTION_PROMPT_TEMPLATE = \"<image>\" * 1176 + (\n",
    "    \"\\nUSER: Generate an ecommerce product description given the image and this title: {title}.\"\n",
    "    \"Make sure to include information about the color of the product in the description.\"\n",
    "    \"\\nASSISTANT:\"\n",
    ")\n",
    "\n",
    "def gen_description_prompt(row: dict[str, Any]) -> dict[str, Any]:\n",
    "    title = row[\"name\"]\n",
    "    row[\"description_prompt\"] = DESCRIPTION_PROMPT_TEMPLATE.format(title=title)\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a617430f-bca6-4ef2-9977-01e0971742df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = (\n",
    "    ds.map_batches(download_images, num_cpus=4, concurrency=num_image_download_workers)\n",
    "    .filter(lambda x: bool(x[\"img\"]))\n",
    "    .map(LargestCenterSquare(size=336))\n",
    "    .map(gen_description_prompt)\n",
    "    .materialize()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47856d84-3d00-4693-aa58-c5884718a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.take_batch(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55ebe3b-0626-4207-936f-9edc2756f7c7",
   "metadata": {},
   "source": [
    "### Estimate input/output token distribution for LLAVA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf821e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlaVAMistralTokenizer:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "        )\n",
    "\n",
    "    def __call__(self, batch: dict[str, np.ndarray], input: str, output: str):\n",
    "        batch[output] = self.tokenizer.batch_encode_plus(batch[input].tolist())[\"input_ids\"]\n",
    "        return batch\n",
    "        \n",
    "def compute_num_tokens(row: dict[str, Any], col: str) -> dict[str, Any]:\n",
    "    row[\"num_tokens\"] = len(row[col])\n",
    "    return row\n",
    "        \n",
    "max_input_tokens = (\n",
    "    ds.map_batches(\n",
    "        LlaVAMistralTokenizer,\n",
    "        fn_kwargs={\n",
    "            \"input\": \"description_prompt\",\n",
    "            \"output\": \"description_prompt_tokens\",\n",
    "        },\n",
    "        concurrency=num_llava_tokenizer_workers,\n",
    "        num_cpus=1,\n",
    "    )\n",
    "    .select_columns([\"description_prompt_tokens\"])\n",
    "    .map(compute_num_tokens, fn_kwargs={\"col\": \"description_prompt_tokens\"})\n",
    "    .max(on=\"num_tokens\")\n",
    ")\n",
    "\n",
    "max_output_tokens = 256  # maximum size of desired product description\n",
    "max_model_length = max_input_tokens + max_output_tokens\n",
    "print(\n",
    "    f\"Description gen: {max_input_tokens=} {max_output_tokens=} {max_model_length=}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ad8191-cd7e-4718-934f-7e42a8bddeef",
   "metadata": {},
   "source": [
    "### Generate description using LLAVA model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61ba8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlaVAMistral:\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_model_len: int,\n",
    "        max_num_seqs: int = 400,\n",
    "        max_tokens: int = 1024,\n",
    "        # NOTE: \"fp8\" currently doesn't support FlashAttention-2 backend so while\n",
    "        # we can fit more sequences in memory, performance will be suboptimal\n",
    "        kv_cache_dtype: str = \"fp8\",\n",
    "    ):\n",
    "        self.llm = LLM(\n",
    "            model=\"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "            trust_remote_code=True,\n",
    "            enable_lora=False,\n",
    "            max_num_seqs=max_num_seqs,\n",
    "            max_model_len=max_model_len,\n",
    "            gpu_memory_utilization=0.95,\n",
    "            image_input_type=\"pixel_values\",\n",
    "            image_token_id=32000,\n",
    "            image_input_shape=\"1,3,336,336\",\n",
    "            image_feature_size=1176,\n",
    "            kv_cache_dtype=kv_cache_dtype,\n",
    "            preemption_mode=\"swap\",\n",
    "        )\n",
    "        self.sampling_params = SamplingParams(\n",
    "            n=1,\n",
    "            presence_penalty=0,\n",
    "            frequency_penalty=0,\n",
    "            repetition_penalty=1,\n",
    "            length_penalty=1,\n",
    "            top_p=1,\n",
    "            top_k=-1,\n",
    "            temperature=0,\n",
    "            use_beam_search=False,\n",
    "            ignore_eos=False,\n",
    "            max_tokens=max_tokens,\n",
    "            seed=None,\n",
    "            detokenize=True,\n",
    "        )\n",
    "\n",
    "    def __call__(self, batch: dict[str, np.ndarray], col: str) -> dict[str, np.ndarray]:\n",
    "        prompts = batch[col]\n",
    "        images = batch[\"img\"]\n",
    "        responses = self.llm.generate(\n",
    "            [\n",
    "                {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"multi_modal_data\": ImagePixelData(image),\n",
    "                }\n",
    "                for prompt, image in zip(prompts, images)\n",
    "            ],\n",
    "            sampling_params=self.sampling_params,\n",
    "        )\n",
    "\n",
    "        batch[\"description\"] = [resp.outputs[0].text for resp in responses]  # type: ignore\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6620303-8cf5-49d3-9f03-9c17131dae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map_batches(\n",
    "    LlaVAMistral,\n",
    "    fn_constructor_kwargs={\n",
    "        \"max_model_len\": max_model_length,\n",
    "        \"max_tokens\": max_output_tokens,\n",
    "        \"max_num_seqs\": 400,\n",
    "    },\n",
    "    fn_kwargs={\"col\": \"description_prompt\"},\n",
    "    batch_size=llava_model_batch_size,\n",
    "    num_gpus=1.0,\n",
    "    concurrency=num_llava_model_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc77eaa-e3b8-4252-9540-de13d4f86267",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.materialize()\n",
    "\n",
    "ds.take_batch(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b943c5-c1a6-42c1-bf68-1bda39dddc9f",
   "metadata": {},
   "source": [
    "### Generate classifier prompts and tokenize them\n",
    "\n",
    "In the classification step, we'll reduce the number of classifiers (vs. the full pipeline) to require fewer GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a82b384-3730-45da-b41a-28c03b3bef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_prompt_classifier(\n",
    "    row: dict[str, Any],\n",
    "    prompt_template: str,\n",
    "    classes: list[str],\n",
    "    col: str,\n",
    ") -> dict[str, Any]:\n",
    "    classes_str = \", \".join(classes)\n",
    "    title = row[\"name\"]\n",
    "    description = row[\"description\"]\n",
    "    row[f\"{col}_prompt\"] = prompt_template.format(\n",
    "        title=title,\n",
    "        description=description,\n",
    "        classes_str=classes_str,\n",
    "    )\n",
    "    return row\n",
    "    \n",
    "classifiers: dict[str, Any] = {\n",
    "    \"category\": {\n",
    "        \"classes\": [\"Tops\", \"Bottoms\", \"Dresses\", \"Footwear\", \"Accessories\"],\n",
    "        \"prompt_template\": (\n",
    "            \"Given the title of this product: {title} and \"\n",
    "            \"the description: {description}, what category does it belong to? \"\n",
    "            \"Chose from the following categories: {classes_str}. \"\n",
    "            \"Return the category that best fits the product. Only return the category name and nothing else.\"\n",
    "        ),\n",
    "        \"prompt_constructor\": construct_prompt_classifier,\n",
    "    },\n",
    "    \"season\": {\n",
    "        \"classes\": [\"Summer\", \"Winter\", \"Spring\", \"Fall\"],\n",
    "        \"prompt_template\": (\n",
    "            \"Given the title of this product: {title} and \"\n",
    "            \"the description: {description}, what season does it belong to? \"\n",
    "            \"Chose from the following seasons: {classes_str}. \"\n",
    "            \"Return the season that best fits the product. Only return the season name and nothing else.\"\n",
    "        ),\n",
    "        \"prompt_constructor\": construct_prompt_classifier,\n",
    "    },\n",
    "#    \"color\": {\n",
    "#        \"classes\": [\n",
    "#            \"Red\",\n",
    "#            \"Blue\",\n",
    "#            \"Green\",\n",
    "#            \"Yellow\",\n",
    "#            \"Black\",\n",
    "#            \"White\",\n",
    "#            \"Pink\",\n",
    "#            \"Purple\",\n",
    "#            \"Orange\",\n",
    "#            \"Brown\",\n",
    "#            \"Grey\",\n",
    "#        ],\n",
    "#        \"prompt_template\": (\n",
    "#            \"Given the title of this product: {title} and \"\n",
    "#            \"the description: {description}, what color does it belong to? \"\n",
    "#            \"Chose from the following colors: {classes_str}. \"\n",
    "#            \"Return the color that best fits the product. Only return the color name and nothing else.\"\n",
    "#        ),\n",
    "#        \"prompt_constructor\": construct_prompt_classifier,\n",
    "#    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e5700-c9b6-422a-89ad-c8df6ef88cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralTokenizer:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "        )\n",
    "\n",
    "    def __call__(self, batch: dict, input: str, output: str):\n",
    "        batch[output] = self.tokenizer.apply_chat_template(\n",
    "            conversation=[[{\"role\": \"user\", \"content\": input_}] for input_ in batch[input]],\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_tensors=\"np\",\n",
    "        )\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3516685f-0a4d-4213-832e-1e99f13b889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for classifier, classifier_spec in classifiers.items():\n",
    "    ds = (\n",
    "        ds.map(\n",
    "            classifier_spec[\"prompt_constructor\"],\n",
    "            fn_kwargs={\n",
    "                \"prompt_template\": classifier_spec[\"prompt_template\"],\n",
    "                \"classes\": classifier_spec[\"classes\"],\n",
    "                \"col\": classifier,\n",
    "            },\n",
    "        )\n",
    "        .map_batches(\n",
    "            MistralTokenizer,\n",
    "            fn_kwargs={\n",
    "                \"input\": f\"{classifier}_prompt\",\n",
    "                \"output\": f\"{classifier}_prompt_tokens\",\n",
    "            },\n",
    "            concurrency=num_mistral_tokenizer_workers_per_classifier,\n",
    "            num_cpus=1,\n",
    "        )\n",
    "        .materialize()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288048f4-ab7e-481e-926b-302022218871",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.take_batch(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1194ab8-fb02-4e8a-a891-08d35636b6c6",
   "metadata": {},
   "source": [
    "### Estimate input/output token distribution for Mistral models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384192e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for classifier, classifier_spec in classifiers.items():\n",
    "    max_output_tokens = (\n",
    "        ray.data.from_items(\n",
    "            [\n",
    "                {\n",
    "                    \"output\": max(classifier_spec[\"classes\"], key=len),\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        .map_batches(\n",
    "            MistralTokenizer,\n",
    "            fn_kwargs={\n",
    "                \"input\": \"output\",\n",
    "                \"output\": \"output_tokens\",\n",
    "            },\n",
    "            concurrency=1,\n",
    "            num_cpus=1,\n",
    "        )\n",
    "        .map(\n",
    "            compute_num_tokens,\n",
    "            fn_kwargs={\"col\": \"output_tokens\"},\n",
    "        )\n",
    "        .max(on=\"num_tokens\")\n",
    "    )\n",
    "    # allow for 40 tokens of buffer to account for non-exact outputs e.g \"the color is Red\" instead of just \"Red\"\n",
    "    buffer_size = 40\n",
    "    classifier_spec[\"max_output_tokens\"] = max_output_tokens + buffer_size\n",
    "\n",
    "    max_input_tokens = (\n",
    "        ds.select_columns([f\"{classifier}_prompt_tokens\"])\n",
    "        .map(compute_num_tokens, fn_kwargs={\"col\": f\"{classifier}_prompt_tokens\"})\n",
    "        .max(on=\"num_tokens\")\n",
    "    )\n",
    "    max_output_tokens = classifier_spec[\"max_output_tokens\"]\n",
    "    print(f\"{classifier=} {max_input_tokens=} {max_output_tokens=}\")\n",
    "    max_model_length = max_input_tokens + max_output_tokens\n",
    "    classifier_spec[\"max_model_length\"] = max_model_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b51961-b4be-41d9-9875-569cd183f86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralvLLM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_model_len: int = 4096,\n",
    "        max_tokens: int = 2048,\n",
    "        max_num_seqs: int = 256,\n",
    "        # NOTE: \"fp8\" currently doesn't support FlashAttention-2 backend so while\n",
    "        # we can fit more sequences in memory, performance will be suboptimal\n",
    "        kv_cache_dtype: str = \"fp8\",\n",
    "    ):\n",
    "        self.llm = LLM(\n",
    "            model=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "            trust_remote_code=True,\n",
    "            enable_lora=False,\n",
    "            max_num_seqs=max_num_seqs,\n",
    "            max_model_len=max_model_len,\n",
    "            gpu_memory_utilization=0.90,\n",
    "            skip_tokenizer_init=True,\n",
    "            kv_cache_dtype=kv_cache_dtype,\n",
    "            preemption_mode=\"swap\",\n",
    "        )\n",
    "        self.sampling_params = SamplingParams(\n",
    "            n=1,\n",
    "            presence_penalty=0,\n",
    "            frequency_penalty=0,\n",
    "            repetition_penalty=1,\n",
    "            length_penalty=1,\n",
    "            top_p=1,\n",
    "            top_k=-1,\n",
    "            temperature=0,\n",
    "            use_beam_search=False,\n",
    "            ignore_eos=False,\n",
    "            max_tokens=max_tokens,\n",
    "            seed=None,\n",
    "            detokenize=False,\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self, batch: dict[str, np.ndarray], input: str, output: str\n",
    "    ) -> dict[str, np.ndarray]:\n",
    "        responses = self.llm.generate(\n",
    "            prompt_token_ids=[ids.tolist() for ids in batch[input]],\n",
    "            sampling_params=self.sampling_params,\n",
    "        )\n",
    "        batch[output] = [resp.outputs[0].token_ids for resp in responses]  # type: ignore\n",
    "        return batch\n",
    "\n",
    "\n",
    "class MistralDeTokenizer:\n",
    "    def __init__(self) -> None:\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "        )\n",
    "\n",
    "    def __call__(self, batch: dict[str, np.ndarray], key: str) -> dict[str, np.ndarray]:\n",
    "        batch[key] = self.tokenizer.batch_decode(batch[key], skip_special_tokens=True)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaafe5df-0b34-48e2-92cb-23b75f933022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_response(\n",
    "    row: dict[str, Any], response_col: str, classes: list[str]\n",
    ") -> dict[str, Any]:\n",
    "    response_str = row[response_col]\n",
    "    matches = []\n",
    "    for class_ in classes:\n",
    "        if class_.lower() in response_str.lower():\n",
    "            matches.append(class_)\n",
    "    if len(matches) == 1:\n",
    "        response = matches[0]\n",
    "    else:\n",
    "        response = None\n",
    "    row[response_col] = response\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa0e675-24c0-4438-af7e-32f16a9758dd",
   "metadata": {},
   "source": [
    "### Generate classifier responses using Mistral model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcfecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for classifier, classifier_spec in classifiers.items():\n",
    "    ds = (\n",
    "        ds.map_batches(\n",
    "            MistralvLLM,\n",
    "            fn_kwargs={\n",
    "                \"input\": f\"{classifier}_prompt_tokens\",\n",
    "                \"output\": f\"{classifier}_response\",\n",
    "            },\n",
    "            fn_constructor_kwargs={\n",
    "                \"max_model_len\": classifier_spec[\"max_model_length\"],\n",
    "                \"max_tokens\": classifier_spec[\"max_output_tokens\"],\n",
    "            },\n",
    "            batch_size=mistral_model_batch_size,\n",
    "            num_gpus=1.0,\n",
    "            concurrency=num_mistral_model_workers_per_classifier,\n",
    "        )\n",
    "        .map_batches(\n",
    "            MistralDeTokenizer,\n",
    "            fn_kwargs={\"key\": f\"{classifier}_response\"},\n",
    "            concurrency=num_mistral_detokenizer_workers_per_classifier,\n",
    "            num_cpus=1,\n",
    "        )\n",
    "        .map(\n",
    "            clean_response,\n",
    "            fn_kwargs={\n",
    "                \"classes\": classifier_spec[\"classes\"],\n",
    "                \"response_col\": f\"{classifier}_response\",\n",
    "            },\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a173b1a6-94d9-4328-813f-6aef29de8b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.materialize()\n",
    "\n",
    "ds.take_batch(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37324ffe-ce01-447e-92de-a8d9aac22364",
   "metadata": {},
   "source": [
    "### Generate embeddings using embedding model inference\n",
    "\n",
    "To reduce resource requirements, we'll alter this code vs. the full pipeline, to run the embedder model on CPU. It's not quite as fast but performance is acceptable for small data scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2098e525",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedderSentenceTransformer:\n",
    "    def __init__(self, model: str = \"thenlper/gte-large\", device: str = \"cuda\"):\n",
    "        self.model = SentenceTransformer(model) # comment out the use of the device param to keep model on CPU\n",
    "\n",
    "    def __call__(\n",
    "        self, batch: dict[str, np.ndarray], cols: list[str]\n",
    "    ) -> dict[str, np.ndarray]:\n",
    "        for col in cols:\n",
    "            batch[f\"{col}_embedding\"] = self.model.encode(  # type: ignore\n",
    "                batch[col].tolist(), batch_size=len(batch[col])\n",
    "            )\n",
    "        return batch\n",
    "        \n",
    "ds = ds.map_batches(\n",
    "    EmbedderSentenceTransformer,\n",
    "    fn_kwargs={\"cols\": [\"name\", \"description\"]},\n",
    "    batch_size=embedding_model_batch_size,\n",
    "    #num_gpus=1.0,\n",
    "    concurrency=num_embedder_workers,\n",
    "    #accelerator_type=embedding_model_accelerator_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c11d4f-3ec8-4dab-ab31-af6b1fcb3b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.materialize()\n",
    "\n",
    "ds.take_batch(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce937ffd",
   "metadata": {},
   "source": [
    "### Upsert records into MongoDB collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab77777d-ce15-4114-a3ae-ccf197a34dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_record(batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "    batch[\"_id\"] = batch[\"name\"]\n",
    "    return {\n",
    "        \"_id\": batch[\"_id\"],\n",
    "        \"name\": batch[\"name\"],\n",
    "        \"img\": batch[\"url\"],\n",
    "        \"price\": batch[\"price\"],\n",
    "        \"rating\": batch[\"rating\"],\n",
    "        \"description\": batch[\"description\"],\n",
    "        \"category\": batch[\"category_response\"],\n",
    "        \"season\": batch[\"season_response\"],\n",
    "#        \"color\": batch[\"color_response\"],\n",
    "        \"name_embedding\": batch[\"name_embedding\"].tolist(),\n",
    "        \"description_embedding\": batch[\"description_embedding\"].tolist(),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2e5a17-6155-40b8-82e6-265c87d5a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MongoBulkUpdate:\n",
    "    def __init__(self, db: str, collection: str) -> None:\n",
    "        client = MongoClient(os.environ[\"DB_CONNECTION_STRING\"])\n",
    "        self.collection = client[db][collection]\n",
    "\n",
    "    def __call__(self, batch_df: pd.DataFrame) -> dict[str, np.ndarray]:\n",
    "        docs = batch_df.to_dict(orient=\"records\")\n",
    "        bulk_ops = [\n",
    "            UpdateOne(filter={\"_id\": doc[\"_id\"]}, update={\"$set\": doc}, upsert=True)\n",
    "            for doc in docs\n",
    "        ]\n",
    "        self.collection.bulk_write(bulk_ops)\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16007dc4-785b-42c0-83aa-0658b4870f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ds.map_batches(update_record)\n",
    "    .map_batches(\n",
    "        MongoBulkUpdate,\n",
    "        fn_constructor_kwargs={\n",
    "            \"db\": db_name,\n",
    "            \"collection\": collection_name,\n",
    "        },\n",
    "        batch_size=db_update_batch_size,\n",
    "        concurrency=num_db_workers,\n",
    "        num_cpus=0.1,\n",
    "        batch_format=\"pandas\",\n",
    "    )\n",
    "    .materialize()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68672d22-b9e0-46bd-926d-a6b0400c3e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
