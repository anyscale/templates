{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b72fa4-6e0a-4057-861c-4c61593c1007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Any, Literal, Type\n",
    "import numpy as np\n",
    "import requests\n",
    "import ray\n",
    "import torchvision\n",
    "from enum import Enum\n",
    "import typer\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from pyarrow import csv\n",
    "from pydantic import BaseModel\n",
    "from ray.util.accelerators import NVIDIA_TESLA_A10G\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "from vllm.multimodal.image import ImagePixelData\n",
    "from vllm import LLM, SamplingParams\n",
    "from PIL import Image\n",
    "from pymongo import MongoClient, UpdateOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9087e741-3ab2-436d-b5d3-412dd8592b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055ee1a1-2e25-4121-962f-fc758ff7b02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples=1_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc8bd87-b22e-48f3-8c09-2065ae4a4fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_image_download_workers=3\n",
    "num_llava_tokenizer_workers=2\n",
    "num_llava_model_workers=1\n",
    "llava_model_accelerator_type=NVIDIA_TESLA_A10G\n",
    "llava_model_batch_size=80\n",
    "num_mistral_tokenizer_workers_per_classifier=2\n",
    "num_mistral_model_workers_per_classifier=1\n",
    "num_mistral_detokenizer_workers_per_classifier=2\n",
    "mistral_model_batch_size=80\n",
    "mistral_model_accelerator_type=NVIDIA_TESLA_A10G\n",
    "num_embedder_workers=1\n",
    "embedding_model_batch_size=80\n",
    "embedding_model_accelerator_type=NVIDIA_TESLA_A10G\n",
    "db_update_batch_size=80\n",
    "num_db_workers=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94205417-4883-4bce-8c64-6ec535e2ea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name: str = \"myntra\"\n",
    "collection_name: str = \"myntra-items-offline\"\n",
    "cluster_size: str = \"m0\"\n",
    "scaling_config_path: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf89b33-d854-4f1c-9423-d98be0a1a762",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 's3://anyscale-public-materials/mongodb-demo/raw/myntra_subset_deduped_10000.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e15f99-f28d-41f7-9aeb-c7f5fe0597c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path: str, nsamples: int) -> ray.data.Dataset:\n",
    "    ds = ray.data.read_csv(\n",
    "        path,\n",
    "        parse_options=csv.ParseOptions(newlines_in_values=True),\n",
    "        convert_options=csv.ConvertOptions(\n",
    "            column_types={\n",
    "                \"id\": pa.int64(),\n",
    "                \"name\": pa.string(),\n",
    "                \"img\": pa.string(),\n",
    "                \"asin\": pa.string(),\n",
    "                \"price\": pa.float64(),\n",
    "                \"mrp\": pa.float64(),\n",
    "                \"rating\": pa.float64(),\n",
    "                \"ratingTotal\": pa.int64(),\n",
    "                \"discount\": pa.int64(),\n",
    "                \"seller\": pa.string(),\n",
    "                \"purl\": pa.string(),\n",
    "            }\n",
    "        ),\n",
    "        override_num_blocks=nsamples,\n",
    "    )\n",
    "    return ds.limit(nsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894bc732-5314-4dda-aed4-2c8f164fad80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read and preprocess data\n",
    "\n",
    "def download_image(url: str) -> bytes:\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response.content\n",
    "    except Exception:\n",
    "        return b\"\"\n",
    "\n",
    "def download_images(batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        batch[\"url\"] = batch[\"img\"]\n",
    "        batch[\"img\"] = list(executor.map(download_image, batch[\"url\"]))  # type: ignore\n",
    "    return batch\n",
    "\n",
    "class LargestCenterSquare:\n",
    "    \"\"\"Largest center square crop for images.\"\"\"\n",
    "\n",
    "    def __init__(self, size: int) -> None:\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, row: dict[str, Any]) -> dict[str, Any]:\n",
    "        \"\"\"Crop the largest center square from an image.\"\"\"\n",
    "        img = Image.open(io.BytesIO(row[\"img\"]))\n",
    "\n",
    "        # First, resize the image such that the smallest side is self.size while preserving aspect ratio.\n",
    "        img = torchvision.transforms.functional.resize(\n",
    "            img=img,\n",
    "            size=self.size,\n",
    "        )\n",
    "\n",
    "        # Then take a center crop to a square.\n",
    "        w, h = img.size\n",
    "        c_top = (h - self.size) // 2\n",
    "        c_left = (w - self.size) // 2\n",
    "        row[\"img\"] = torchvision.transforms.functional.crop(\n",
    "            img=img,\n",
    "            top=c_top,\n",
    "            left=c_left,\n",
    "            height=self.size,\n",
    "            width=self.size,\n",
    "        )\n",
    "\n",
    "        return row\n",
    "\n",
    "DESCRIPTION_PROMPT_TEMPLATE = \"<image>\" * 1176 + (\n",
    "    \"\\nUSER: Generate an ecommerce product description given the image and this title: {title}.\"\n",
    "    \"Make sure to include information about the color of the product in the description.\"\n",
    "    \"\\nASSISTANT:\"\n",
    ")\n",
    "\n",
    "def gen_description_prompt(row: dict[str, Any]) -> dict[str, Any]:\n",
    "    title = row[\"name\"]\n",
    "    row[\"description_prompt\"] = DESCRIPTION_PROMPT_TEMPLATE.format(title=title)\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a617430f-bca6-4ef2-9977-01e0971742df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = read_data(path, nsamples)\n",
    "\n",
    "ds = (\n",
    "    ds.map_batches(download_images, num_cpus=4, concurrency=num_image_download_workers)\n",
    "    .filter(lambda x: bool(x[\"img\"]))\n",
    "    .map(LargestCenterSquare(size=336))\n",
    "    .map(gen_description_prompt)\n",
    "    .materialize()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55ebe3b-0626-4207-936f-9edc2756f7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Estimate input/output token distribution for LLAVA model\n",
    "\n",
    "class LlaVAMistralTokenizer:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "        )\n",
    "\n",
    "    def __call__(self, batch: dict[str, np.ndarray], input: str, output: str):\n",
    "        batch[output] = self.tokenizer.batch_encode_plus(batch[input].tolist())[\"input_ids\"]\n",
    "        return batch\n",
    "        \n",
    "def compute_num_tokens(row: dict[str, Any], col: str) -> dict[str, Any]:\n",
    "    row[\"num_tokens\"] = len(row[col])\n",
    "    return row\n",
    "        \n",
    "max_input_tokens = (\n",
    "    ds.map_batches(\n",
    "        LlaVAMistralTokenizer,\n",
    "        fn_kwargs={\n",
    "            \"input\": \"description_prompt\",\n",
    "            \"output\": \"description_prompt_tokens\",\n",
    "        },\n",
    "        concurrency=num_llava_tokenizer_workers,\n",
    "        num_cpus=1,\n",
    "    )\n",
    "    .select_columns([\"description_prompt_tokens\"])\n",
    "    .map(compute_num_tokens, fn_kwargs={\"col\": \"description_prompt_tokens\"})\n",
    "    .max(on=\"num_tokens\")\n",
    ")\n",
    "\n",
    "max_output_tokens = 256  # maximum size of desired product description\n",
    "max_model_length = max_input_tokens + max_output_tokens\n",
    "print(\n",
    "    f\"Description gen: {max_input_tokens=} {max_output_tokens=} {max_model_length=}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ad8191-cd7e-4718-934f-7e42a8bddeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Generate description using LLAVA model inference\n",
    "\n",
    "class LlaVAMistral:\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_model_len: int,\n",
    "        max_num_seqs: int = 400,\n",
    "        max_tokens: int = 1024,\n",
    "        # NOTE: \"fp8\" currently doesn't support FlashAttention-2 backend so while\n",
    "        # we can fit more sequences in memory, performance will be suboptimal\n",
    "        kv_cache_dtype: str = \"fp8\",\n",
    "    ):\n",
    "        self.llm = LLM(\n",
    "            model=\"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "            trust_remote_code=True,\n",
    "            enable_lora=False,\n",
    "            max_num_seqs=max_num_seqs,\n",
    "            max_model_len=max_model_len,\n",
    "            gpu_memory_utilization=0.95,\n",
    "            image_input_type=\"pixel_values\",\n",
    "            image_token_id=32000,\n",
    "            image_input_shape=\"1,3,336,336\",\n",
    "            image_feature_size=1176,\n",
    "            kv_cache_dtype=kv_cache_dtype,\n",
    "            preemption_mode=\"swap\",\n",
    "        )\n",
    "        self.sampling_params = SamplingParams(\n",
    "            n=1,\n",
    "            presence_penalty=0,\n",
    "            frequency_penalty=0,\n",
    "            repetition_penalty=1,\n",
    "            length_penalty=1,\n",
    "            top_p=1,\n",
    "            top_k=-1,\n",
    "            temperature=0,\n",
    "            use_beam_search=False,\n",
    "            ignore_eos=False,\n",
    "            max_tokens=max_tokens,\n",
    "            seed=None,\n",
    "            detokenize=True,\n",
    "        )\n",
    "\n",
    "    def __call__(self, batch: dict[str, np.ndarray], col: str) -> dict[str, np.ndarray]:\n",
    "        prompts = batch[col]\n",
    "        images = batch[\"img\"]\n",
    "        responses = self.llm.generate(\n",
    "            [\n",
    "                {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"multi_modal_data\": ImagePixelData(image),\n",
    "                }\n",
    "                for prompt, image in zip(prompts, images)\n",
    "            ],\n",
    "            sampling_params=self.sampling_params,\n",
    "        )\n",
    "\n",
    "        batch[\"description\"] = [resp.outputs[0].text for resp in responses]  # type: ignore\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6620303-8cf5-49d3-9f03-9c17131dae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map_batches(\n",
    "    LlaVAMistral,\n",
    "    fn_constructor_kwargs={\n",
    "        \"max_model_len\": max_model_length,\n",
    "        \"max_tokens\": max_output_tokens,\n",
    "        \"max_num_seqs\": 400,\n",
    "    },\n",
    "    fn_kwargs={\"col\": \"description_prompt\"},\n",
    "    batch_size=llava_model_batch_size,\n",
    "    num_gpus=1.0,\n",
    "    concurrency=num_llava_model_workers,\n",
    "    accelerator_type=llava_model_accelerator_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7447b3-bb11-44fd-b3a1-cc6fa80a46d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_prompt_classifier(\n",
    "    row: dict[str, Any],\n",
    "    prompt_template: str,\n",
    "    classes: list[str],\n",
    "    col: str,\n",
    ") -> dict[str, Any]:\n",
    "    classes_str = \", \".join(classes)\n",
    "    title = row[\"name\"]\n",
    "    description = row[\"description\"]\n",
    "    row[f\"{col}_prompt\"] = prompt_template.format(\n",
    "        title=title,\n",
    "        description=description,\n",
    "        classes_str=classes_str,\n",
    "    )\n",
    "    return row\n",
    "    \n",
    "classifiers: dict[str, Any] = {\n",
    "    \"category\": {\n",
    "        \"classes\": [\"Tops\", \"Bottoms\", \"Dresses\", \"Footwear\", \"Accessories\"],\n",
    "        \"prompt_template\": (\n",
    "            \"Given the title of this product: {title} and \"\n",
    "            \"the description: {description}, what category does it belong to? \"\n",
    "            \"Chose from the following categories: {classes_str}. \"\n",
    "            \"Return the category that best fits the product. Only return the category name and nothing else.\"\n",
    "        ),\n",
    "        \"prompt_constructor\": construct_prompt_classifier,\n",
    "    },\n",
    "    \"season\": {\n",
    "        \"classes\": [\"Summer\", \"Winter\", \"Spring\", \"Fall\"],\n",
    "        \"prompt_template\": (\n",
    "            \"Given the title of this product: {title} and \"\n",
    "            \"the description: {description}, what season does it belong to? \"\n",
    "            \"Chose from the following seasons: {classes_str}. \"\n",
    "            \"Return the season that best fits the product. Only return the season name and nothing else.\"\n",
    "        ),\n",
    "        \"prompt_constructor\": construct_prompt_classifier,\n",
    "    },\n",
    "    \"color\": {\n",
    "        \"classes\": [\n",
    "            \"Red\",\n",
    "            \"Blue\",\n",
    "            \"Green\",\n",
    "            \"Yellow\",\n",
    "            \"Black\",\n",
    "            \"White\",\n",
    "            \"Pink\",\n",
    "            \"Purple\",\n",
    "            \"Orange\",\n",
    "            \"Brown\",\n",
    "            \"Grey\",\n",
    "        ],\n",
    "        \"prompt_template\": (\n",
    "            \"Given the title of this product: {title} and \"\n",
    "            \"the description: {description}, what color does it belong to? \"\n",
    "            \"Chose from the following colors: {classes_str}. \"\n",
    "            \"Return the color that best fits the product. Only return the color name and nothing else.\"\n",
    "        ),\n",
    "        \"prompt_constructor\": construct_prompt_classifier,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7d56b3-5ef5-4bd7-b7a0-c1618fba8957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralTokenizer:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "        )\n",
    "\n",
    "    def __call__(self, batch: dict, input: str, output: str):\n",
    "        batch[output] = self.tokenizer.apply_chat_template(\n",
    "            conversation=[[{\"role\": \"user\", \"content\": input_}] for input_ in batch[input]],\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_tensors=\"np\",\n",
    "        )\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244945a9-e4d4-455b-86f8-836c3c8ff0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Generate classifier prompts and tokenize them\n",
    "for classifier, classifier_spec in classifiers.items():\n",
    "    ds = (\n",
    "        ds.map(\n",
    "            classifier_spec[\"prompt_constructor\"],\n",
    "            fn_kwargs={\n",
    "                \"prompt_template\": classifier_spec[\"prompt_template\"],\n",
    "                \"classes\": classifier_spec[\"classes\"],\n",
    "                \"col\": classifier,\n",
    "            },\n",
    "        )\n",
    "        .map_batches(\n",
    "            MistralTokenizer,\n",
    "            fn_kwargs={\n",
    "                \"input\": f\"{classifier}_prompt\",\n",
    "                \"output\": f\"{classifier}_prompt_tokens\",\n",
    "            },\n",
    "            concurrency=num_mistral_tokenizer_workers_per_classifier,\n",
    "            num_cpus=1,\n",
    "        )\n",
    "        .materialize()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756f2c3d-9e23-4d7c-adee-4e5190a00567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Estimate input/output token distribution for Mistral models\n",
    "for classifier, classifier_spec in classifiers.items():\n",
    "    max_output_tokens = (\n",
    "        ray.data.from_items(\n",
    "            [\n",
    "                {\n",
    "                    \"output\": max(classifier_spec[\"classes\"], key=len),\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        .map_batches(\n",
    "            MistralTokenizer,\n",
    "            fn_kwargs={\n",
    "                \"input\": \"output\",\n",
    "                \"output\": \"output_tokens\",\n",
    "            },\n",
    "            concurrency=1,\n",
    "            num_cpus=1,\n",
    "        )\n",
    "        .map(\n",
    "            compute_num_tokens,\n",
    "            fn_kwargs={\"col\": \"output_tokens\"},\n",
    "        )\n",
    "        .max(on=\"num_tokens\")\n",
    "    )\n",
    "    # allow for 40 tokens of buffer to account for non-exact outputs e.g \"the color is Red\" instead of just \"Red\"\n",
    "    buffer_size = 40\n",
    "    classifier_spec[\"max_output_tokens\"] = max_output_tokens + buffer_size\n",
    "\n",
    "    max_input_tokens = (\n",
    "        ds.select_columns([f\"{classifier}_prompt_tokens\"])\n",
    "        .map(compute_num_tokens, fn_kwargs={\"col\": f\"{classifier}_prompt_tokens\"})\n",
    "        .max(on=\"num_tokens\")\n",
    "    )\n",
    "    max_output_tokens = classifier_spec[\"max_output_tokens\"]\n",
    "    print(f\"{classifier=} {max_input_tokens=} {max_output_tokens=}\")\n",
    "    max_model_length = max_input_tokens + max_output_tokens\n",
    "    classifier_spec[\"max_model_length\"] = max_model_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a14469d-84f4-44bf-b651-9fc76bceb1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralvLLM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_model_len: int = 4096,\n",
    "        max_tokens: int = 2048,\n",
    "        max_num_seqs: int = 256,\n",
    "        # NOTE: \"fp8\" currently doesn't support FlashAttention-2 backend so while\n",
    "        # we can fit more sequences in memory, performance will be suboptimal\n",
    "        kv_cache_dtype: str = \"fp8\",\n",
    "    ):\n",
    "        self.llm = LLM(\n",
    "            model=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "            trust_remote_code=True,\n",
    "            enable_lora=False,\n",
    "            max_num_seqs=max_num_seqs,\n",
    "            max_model_len=max_model_len,\n",
    "            gpu_memory_utilization=0.90,\n",
    "            skip_tokenizer_init=True,\n",
    "            kv_cache_dtype=kv_cache_dtype,\n",
    "            preemption_mode=\"swap\",\n",
    "        )\n",
    "        self.sampling_params = SamplingParams(\n",
    "            n=1,\n",
    "            presence_penalty=0,\n",
    "            frequency_penalty=0,\n",
    "            repetition_penalty=1,\n",
    "            length_penalty=1,\n",
    "            top_p=1,\n",
    "            top_k=-1,\n",
    "            temperature=0,\n",
    "            use_beam_search=False,\n",
    "            ignore_eos=False,\n",
    "            max_tokens=max_tokens,\n",
    "            seed=None,\n",
    "            detokenize=False,\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self, batch: dict[str, np.ndarray], input: str, output: str\n",
    "    ) -> dict[str, np.ndarray]:\n",
    "        responses = self.llm.generate(\n",
    "            prompt_token_ids=[ids.tolist() for ids in batch[input]],\n",
    "            sampling_params=self.sampling_params,\n",
    "        )\n",
    "        batch[output] = [resp.outputs[0].token_ids for resp in responses]  # type: ignore\n",
    "        return batch\n",
    "\n",
    "\n",
    "class MistralDeTokenizer:\n",
    "    def __init__(self) -> None:\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "        )\n",
    "\n",
    "    def __call__(self, batch: dict[str, np.ndarray], key: str) -> dict[str, np.ndarray]:\n",
    "        batch[key] = self.tokenizer.batch_decode(batch[key], skip_special_tokens=True)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2482e476-7377-4574-8ad1-c137d2ab6ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_response(\n",
    "    row: dict[str, Any], response_col: str, classes: list[str]\n",
    ") -> dict[str, Any]:\n",
    "    response_str = row[response_col]\n",
    "    matches = []\n",
    "    for class_ in classes:\n",
    "        if class_.lower() in response_str.lower():\n",
    "            matches.append(class_)\n",
    "    if len(matches) == 1:\n",
    "        response = matches[0]\n",
    "    else:\n",
    "        response = None\n",
    "    row[response_col] = response\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99683256-2ceb-4c50-9467-bc3d46676f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Generate classifier responses using Mistral model inference\n",
    "for classifier, classifier_spec in classifiers.items():\n",
    "    ds = (\n",
    "        ds.map_batches(\n",
    "            MistralvLLM,\n",
    "            fn_kwargs={\n",
    "                \"input\": f\"{classifier}_prompt_tokens\",\n",
    "                \"output\": f\"{classifier}_response\",\n",
    "            },\n",
    "            fn_constructor_kwargs={\n",
    "                \"max_model_len\": classifier_spec[\"max_model_length\"],\n",
    "                \"max_tokens\": classifier_spec[\"max_output_tokens\"],\n",
    "            },\n",
    "            batch_size=mistral_model_batch_size,\n",
    "            num_gpus=1.0,\n",
    "            concurrency=num_mistral_model_workers_per_classifier,\n",
    "            accelerator_type=mistral_model_accelerator_type,\n",
    "        )\n",
    "        .map_batches(\n",
    "            MistralDeTokenizer,\n",
    "            fn_kwargs={\"key\": f\"{classifier}_response\"},\n",
    "            concurrency=num_mistral_detokenizer_workers_per_classifier,\n",
    "            num_cpus=1,\n",
    "        )\n",
    "        .map(\n",
    "            clean_response,\n",
    "            fn_kwargs={\n",
    "                \"classes\": classifier_spec[\"classes\"],\n",
    "                \"response_col\": f\"{classifier}_response\",\n",
    "            },\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37324ffe-ce01-447e-92de-a8d9aac22364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Generate embeddings using embedding model inference\n",
    "\n",
    "class EmbedderSentenceTransformer:\n",
    "    def __init__(self, model: str = \"thenlper/gte-large\", device: str = \"cuda\"):\n",
    "        self.model = SentenceTransformer(model, device=device)\n",
    "\n",
    "    def __call__(\n",
    "        self, batch: dict[str, np.ndarray], cols: list[str]\n",
    "    ) -> dict[str, np.ndarray]:\n",
    "        for col in cols:\n",
    "            batch[f\"{col}_embedding\"] = self.model.encode(  # type: ignore\n",
    "                batch[col].tolist(), batch_size=len(batch[col])\n",
    "            )\n",
    "        return batch\n",
    "        \n",
    "ds = ds.map_batches(\n",
    "    EmbedderSentenceTransformer,\n",
    "    fn_kwargs={\"cols\": [\"name\", \"description\"]},\n",
    "    batch_size=embedding_model_batch_size,\n",
    "    num_gpus=1.0,\n",
    "    concurrency=num_embedder_workers,\n",
    "    accelerator_type=embedding_model_accelerator_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d5c7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_record(batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "    batch[\"_id\"] = batch[\"name\"]\n",
    "    return {\n",
    "        \"_id\": batch[\"_id\"],\n",
    "        \"name\": batch[\"name\"],\n",
    "        \"img\": batch[\"url\"],\n",
    "        \"price\": batch[\"price\"],\n",
    "        \"rating\": batch[\"rating\"],\n",
    "        \"description\": batch[\"description\"],\n",
    "        \"category\": batch[\"category_response\"],\n",
    "        \"season\": batch[\"season_response\"],\n",
    "        \"color\": batch[\"color_response\"],\n",
    "        \"name_embedding\": batch[\"name_embedding\"].tolist(),\n",
    "        \"description_embedding\": batch[\"description_embedding\"].tolist(),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db26b0c-2547-4109-baf1-efc80d1e79dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MongoBulkUpdate:\n",
    "    def __init__(self, db: str, collection: str) -> None:\n",
    "        client = MongoClient(os.environ[\"DB_CONNECTION_STRING\"])\n",
    "        self.collection = client[db][collection]\n",
    "\n",
    "    def __call__(self, batch_df: pd.DataFrame) -> dict[str, np.ndarray]:\n",
    "        docs = batch_df.to_dict(orient=\"records\")\n",
    "        bulk_ops = [\n",
    "            UpdateOne(filter={\"_id\": doc[\"_id\"]}, update={\"$set\": doc}, upsert=True)\n",
    "            for doc in docs\n",
    "        ]\n",
    "        self.collection.bulk_write(bulk_ops)\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16007dc4-785b-42c0-83aa-0658b4870f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ds.map_batches(update_record)\n",
    "    .map_batches(\n",
    "        MongoBulkUpdate,\n",
    "        fn_constructor_kwargs={\n",
    "            \"db\": db_name,\n",
    "            \"collection\": collection_name,\n",
    "        },\n",
    "        batch_size=db_update_batch_size,\n",
    "        concurrency=num_db_workers,\n",
    "        num_cpus=0.1,\n",
    "        batch_format=\"pandas\",\n",
    "    )\n",
    "    .materialize()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74de3db-6bb9-48dd-b061-5ddffd90a981",
   "metadata": {},
   "outputs": [],
   "source": [
    "! date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c7087d-fc99-45b5-95ee-17d7a568024e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
