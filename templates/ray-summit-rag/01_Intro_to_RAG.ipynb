{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to RAG\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Here is the roadmap for this notebook:</b>\n",
    "<ul>\n",
    "    <li><b>Part 1:</b> Prompting an LLM without RAG</a></li>\n",
    "    <li><b>Part 2:</b> In-context learning and LLMs</a></li>\n",
    "    <li><b>Part 3:</b> Retrieval and semantic search</a></li>\n",
    "    <li><b>Part 4:</b> RAG: High-level overview</a></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import openai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisite setup\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> <b> Important if you want to run this notebook:</b> \n",
    "\n",
    "This RAG notebook requires having a running LLM Anyscale service. To deploy an LLM as an Anyscale service, you can follow the step-by-step instructions in this [Deploy an LLM workspace template](https://console.anyscale.com/v2/template-preview/endpoints_v2). Make sure to choose the `mistralai/Mistral-7B-Instruct-v0.1` model when deploying.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "<b style=\"background-color: yellow;\">&nbsp;ðŸ”„ REPLACE&nbsp;</b>: Use the url and api key from the Anyscale service you deployed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANYSCALE_SERVICE_BASE_URL = \"replace-with-my-anyscale-service-url\"\n",
    "ANYSCALE_API_KEY = \"replace-with-my-anyscale-api-key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## What is RAG ?\n",
    "\n",
    "Retrieval augmented generation (RAG) combines Large Language models (LLMs) and information retrieval systems to provide a more robust and context-aware response generation system. It was introduced by Lewis et al. in the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Prompting an LLM without RAG\n",
    "\n",
    "Here is our system without RAG. \n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-summit/rag-app/RAG+App+-+Ray+Summit+-+without_rag+-+v2.png\" alt=\"Without RAG\" width=\"550px\"/>\n",
    "\n",
    "We prompt an LLM and get back a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_llm(user_prompt, model=\"mistralai/Mistral-7B-Instruct-v0.1\", temperature=0, **kwargs):\n",
    "    # Initialize a client to perform API requests\n",
    "    client = openai.OpenAI(\n",
    "        base_url=ANYSCALE_SERVICE_BASE_URL,\n",
    "        api_key=ANYSCALE_API_KEY,\n",
    "    )\n",
    "    \n",
    "    # Call the chat completions endpoint\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            # Prime the system with a system message - a common best practice\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            # Send the user message with the proper \"user\" role and \"content\"\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    return chat_completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we will prompt an LLM about the capital of France."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the capital of France?\"\n",
    "response = prompt_llm(prompt)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the case of prompting the LLM about **internal** company documents. \n",
    "\n",
    "Think of technical company documents and company policies that are not available on the internet.\n",
    "\n",
    "Given the LLM has not been trained on these documents, it will not be able to provide a good response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Can I rent the company car on weekends?\"\n",
    "response = prompt_llm(prompt)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## In-context learning and LLMs\n",
    "\n",
    "It turns out LLMs excel at in-context learning, meaning they can utilize additional context provided with a user prompt to generate a response that is grounded in the provided context. \n",
    "\n",
    "Here a diagram of the system with in-context learning:\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-summit/rag-app/RAG+App+-+Ray+Summit+-+in-context-learning+-++v2.png\" alt=\"In-context learning\" width=\"500px\"/>\n",
    "\n",
    "For a formal understanding, refer to the paper titled [In-Context Retrieval-Augmented Language Models](https://arxiv.org/pdf/2302.00083.pdf), which performs experiments to validate in-context learning.\n",
    "\n",
    "\n",
    "Let's consider the case of prompting the LLM about internal company policies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Here are the company policies that you need to know about:\n",
    "\n",
    "1. You are not allowed to use the company's computers between 9am and 5pm. \n",
    "2. You are not allowed to use the company car on weekends.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we provide the LLM with the company's policies as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Am I allowed to use the company car on weekends?\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Given the following context:\n",
    "{context}\n",
    "\n",
    "What is the answer to the following question:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "response = prompt_llm(prompt)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get back the correct answer to the question, which is \"You are not allowed to use the company car on weekends.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Retrieval and semantic search\n",
    "\n",
    "In a real-world scenario, we can't provide the LLM with the entire company's data as context. It would be inefficient to do so from both a cost and performance perspective.\n",
    "\n",
    "So we will need a retrieval system to find the most relevant context.\n",
    "\n",
    "One effective retrieval system is semantic search, which uses embeddings to find the most relevant context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### What is semantic search ?\n",
    "\n",
    "Semantic search enables us to find documents that share a similar meaning with our queries.\n",
    "\n",
    "To capture the \"meaning\" of a query, we use specialized encoders known as \"embedding models.\"\n",
    "\n",
    "Embedding models encode text into a high-dimensional vector, playing a crucial role in converting language into a mathematical format for efficient comparison and retrieval.\n",
    "\n",
    "\n",
    "### How do embedding models work?</h5>\n",
    "\n",
    "Embedding models are trained on a large corpus of text data to learn the relationships between words and phrases.\n",
    "\n",
    "The model represents each word or phrase as a high-dimensional vector, where similar words are closer together in the vector space.\n",
    "\n",
    "<img src='https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-summit/rag-app/word-embeddings.png' width=\"600px\" alt=\"Word Embeddings tSNE\"/>\n",
    "\n",
    "The diagram shows word embedding vectors in a 2D space. Semantically similar words end up close to each other in the reduced vector space. \n",
    "\n",
    "Note for semantic search, we use sequence embeddings with a much higher dimensionality offering much richer representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating embeddings\n",
    "\n",
    "Here is how to generate embeddings using the `sentence-transformers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "prompt = \"Am I allowed to use the company car on weekends?\"\n",
    "\n",
    "document_1 = \"You are not allowed to use the company's computers between 9am and 5pm.\"\n",
    "document_2 = \"You are not allowed to use the company car on weekends.\"\n",
    "\n",
    "prompt_embedding_vector = embedding_model.encode(prompt)\n",
    "document_1_embedding_vector = embedding_model.encode(document_1)\n",
    "document_2_embedding_vector = embedding_model.encode(document_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can find the similarity between the prompt and document vectors by computing the cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = cosine_similarity([prompt_embedding_vector], [document_1_embedding_vector, document_2_embedding_vector]).flatten()\n",
    "similarity_between_prompt_and_document_1, similarity_between_prompt_and_document_2 = similarities\n",
    "print(f\"{similarity_between_prompt_and_document_1=}\")\n",
    "print(f\"{similarity_between_prompt_and_document_2=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Activity: Find the most similar document\n",
    "\n",
    "Given the following two documents and prompt:\n",
    "\n",
    "```python\n",
    "prompt = \"What is the current king of england's name?\"\n",
    "\n",
    "document_1 = \"British monarchy head at present moment: Charles III\"\n",
    "document_2 = \"The current king of spain's name is Felipe VI\"\n",
    "\n",
    "# Hint: Compute the embedding vector for the prompt and the documents.\n",
    "\n",
    "# Hint: Use a similarity metric to find the most similar document to the prompt.\n",
    "\n",
    "```\n",
    "\n",
    "Find the closest document to the prompt using the `BAAI/bge-small-en-v1.5` model. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<details> \n",
    "\n",
    "<summary>Click here to see the solution </summary>\n",
    "\n",
    "```python\n",
    "prompt = \"What is the current king of england's name?\"\n",
    "\n",
    "document_1 = \"British monarchy head at present moment: Charles III\"\n",
    "document_2 = \"The current king of spain's name is Felipe VI\"\n",
    "\n",
    "# Compute the embedding vector for the prompt and the documents.\n",
    "prompt_embedding_vector = embedding_model.encode(prompt)\n",
    "document_1_embedding_vector = embedding_model.encode(document_1)\n",
    "document_2_embedding_vector = embedding_model.encode(document_2)\n",
    "\n",
    "# Use a similarity metric to find the most similar document to the prompt.\n",
    "similarities = cosine_similarity([prompt_embedding_vector], [document_1_embedding_vector, document_2_embedding_vector]).flatten()\n",
    "similarity_between_prompt_and_document_1, similarity_between_prompt_and_document_2 = similarities\n",
    "if similarity_between_prompt_and_document_1 > similarity_between_prompt_and_document_2:\n",
    "    print(\"Document 1 is more similar to the prompt\")\n",
    "else:\n",
    "    print(\"Document 2 is more similar to the prompt\")\n",
    "```\n",
    "\n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<b>Note:</b> how even though `document_2` has direct word matches to the provided prompt, such as \"the,\" \"current,\" \"king,\" and \"name,\" its meaning is less similar than `document_1`, which uses different terms like \"British monarchy head.\" This is an example of how semantic search can be more effective than lexical (keyword-based) search.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## RAG: High-level overview\n",
    "\n",
    "With RAG, we now have a retrieval system that finds the most relevant context and provides it to the LLM.\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-summit/rag-app/RAG+App+-+Ray+Summit+-+with_rag_simple_v2.png\" alt=\"With RAG\" width=\"600px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why RAG ?\n",
    "\n",
    "RAG systems enhance LLMs by:\n",
    "\n",
    "- Reducing hallucinations with relevant context.\n",
    "- Providing clear information attribution.\n",
    "- Enabling access control to information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we build a basic RAG system ?\n",
    "\n",
    "A common approach for building a basic RAG systems is by:\n",
    "\n",
    "1. Encoding our documents, commonly referred to as generating embeddings of our documents.\n",
    "2. Storing the generated embeddings in a vector store.\n",
    "3. Encoding our user query.\n",
    "4. Retrieving relevant documents from our vector store given the encoded user query.\n",
    "5. Augmenting the user prompt with the retrieved context.\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-summit/rag-app/RAG+App+-+Ray+Summit+-+with_rag_v2.png\" alt=\"With RAG Highlights\" width=\"800px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Stages:\n",
    "\n",
    "- **Stage 1: Indexing**\n",
    "  1. Loading the documents from a source like a website, API, or database.\n",
    "  2. Processing the documents into \"embeddable\" document chunks.\n",
    "  3. Encoding the documents chunks into embedding vectors.\n",
    "  4. Storing the document embedding vectors in a vector store.\n",
    "- **Stage 2: Retrieval**\n",
    "  1. Encoding the user query.\n",
    "  2. Retrieving the most similar documents from the vector store given the encoded user query.\n",
    "- **Stage 3: Generation**\n",
    "  1. Augmenting the prompt with the provided context.\n",
    "  2. Generating a response from the augmented prompt.\n",
    "\n",
    "Stage 1 is setup; Stages 2 and 3 are operational.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps: Building a RAG-based QA engine for the Ray documentation\n",
    "\n",
    "We will start to build a RAG-based QA engine for the Ray documentation. This will be an attempt to recreate the \"Ask AI\" bot on the Ray [documentation website](https://docs.ray.io/en/latest/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
