{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84acd29e",
   "metadata": {},
   "source": [
    "# Indexing Data for RAG using Ray Data\n",
    "\n",
    "The first stage of RAG is to index the data. This can be done by creating embeddings for the data and storing them in a vector store. \n",
    "\n",
    "This notebook will walk you through the process of creating an embedding pipeline and then scaling it with Ray Data.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Here is the roadmap for this notebook:</b>\n",
    "<ul>\n",
    "    <li><b>Part 0:</b> RAG overview recap</a></li>\n",
    "    <li><b>Part 1:</b> Embeddings pipeline overview</a></li>\n",
    "    <li><b>Part 2:</b> Simplest possible embedding pipeline</a></li>\n",
    "    <li><b>Part 3:</b> Simple pipeline for a real use-case</a></li>\n",
    "    <li><b>Part 4:</b> Migrating the simple pipeline to Ray Data</a></li>\n",
    "    <li><b>Part 5:</b> Building a vector store</a></li>\n",
    "    <li><b>Part 6:</b> Key takeaways</a></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038051e6",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c771f-2c84-4016-8dea-0eb9d844d6ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "\n",
    "import joblib\n",
    "import psutil\n",
    "import ray\n",
    "from cloudpathlib import CloudPath\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942484a5",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d847d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get(\"ANYSCALE_ARTIFACT_STORAGE\"):\n",
    "    DATA_DIR = Path(\"/mnt/cluster_storage/\")\n",
    "    shutil.copytree(Path(\"./data/\"), DATA_DIR, dirs_exist_ok=True)\n",
    "else:\n",
    "    DATA_DIR = Path(\"./data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9936f4",
   "metadata": {},
   "source": [
    "## RAG Overview Recap\n",
    "\n",
    "As a recap here are the three main phases of implementing RAG\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-summit/rag-app/RAG+App+-+Ray+Summit+-+with_rag_v2.png\" alt=\"With RAG Highlights\" width=\"800px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da494e7d",
   "metadata": {},
   "source": [
    "## Embeddings pipeline overview\n",
    "\n",
    "What are the steps involved in generating embeddings? In the most common case for text data, the steps are as follows:\n",
    "\n",
    "1. Load documents\n",
    "2. Process documents into chunks\n",
    "   1. Process documents into chunks\n",
    "   2. Optionally persist chunks\n",
    "3. Generate embeddings from chunks\n",
    "   1. Generate embeddings from chunks\n",
    "   2. Optionally persist embeddings\n",
    "4. Upsert embeddings into a database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c493e7",
   "metadata": {},
   "source": [
    "## Simple pipeline for a real use-case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bf7061",
   "metadata": {},
   "source": [
    "Let's now assume we want to \"embed the Ray documentation website\". \n",
    "\n",
    "We will circle back and start with a small sample dataset taken from the ray documentation. \n",
    "\n",
    "To visualize our pipeline, see the diagram below:\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-summit/rag-app/simple_embeddings_pipeline_v2.svg\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c035b5b",
   "metadata": {},
   "source": [
    "### 1. Load documents\n",
    "\n",
    "First step, we load the data using `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7680f3d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(DATA_DIR / \"small_sample\" / \"sample-input.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef6158e-febf-48ae-aa1d-2ddb1d3a99bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "We have a dataset of 4 documents fetched from online content and stored as objects in a json file.\n",
    "\n",
    "Here are some of the notable columns:\n",
    "- `text` column which contains the text of the document that we want to embed.\n",
    "- `section_url` column which contains the section under which the document is found.\n",
    "- `page_url` column which contains the page under which the document is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f189a745-6c07-432c-8a45-0e8f551e6a36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291194b7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "**Considerations for scaling the pipeline:**\n",
    "- Memory: We currently load the entire file into memory. This is not a problem for small files, but can be a problem for large files.\n",
    "- Latency: Reading the file from disk is slow. We can speed this up by using a faster disk, but we can also speed this up by parallelizing the read.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8b84e3-aca6-4b9d-bbb3-0b16ba5a6c30",
   "metadata": {},
   "source": [
    "### 2. Process documents into chunks\n",
    "\n",
    "We will use langchain's `RecursiveCharacterTextSplitter` to split the text into chunks. \n",
    "\n",
    "It works by first splitting on paragraphs, then sentences, then words, then characters. It is a recursive algorithm that will stop once the chunk size is satisfied.\n",
    "\n",
    "Let's try it out on a sampe document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c0df2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "This is the first part. Estimate me like 12 words long.\n",
    "\n",
    "This is the second part. Estimate me like 12 words long.\n",
    "\n",
    "This is the third part. Estimate me like 12 words long.\n",
    "\"\"\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # The default separators used by the splitter\n",
    "    chunk_size=24,\n",
    "    chunk_overlap=0,\n",
    "    length_function=lambda x: len(x.split(\" \")),\n",
    ")\n",
    "splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0810e6d8",
   "metadata": {},
   "source": [
    "If we change the paragraphs, the chunk contents will change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a22363",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "This is the first part. Estimate me like 12 words long.\n",
    "\n",
    "This is the second part. Estimate me like 12 words long.\n",
    "This is the third part. Estimate me like 12 words long.\n",
    "\"\"\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # The default separators used by the splitter\n",
    "    chunk_size=24,\n",
    "    chunk_overlap=0,\n",
    "    length_function=lambda x: len(x.split(\" \")),\n",
    ")\n",
    "splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14de14fc",
   "metadata": {},
   "source": [
    "We now proceed to:\n",
    "\n",
    "1. Configure the `RecursiveCharacterTextSplitter`\n",
    "2. Run it over all the documents in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80d46e9-7fd3-4c51-bf67-61dffbe6c8e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_size = 128  #  Chunk size is usually specified in tokens\n",
    "words_to_tokens = 1.2  # Heuristic for converting tokens to words\n",
    "chunk_size_in_words = int(chunk_size // words_to_tokens)\n",
    "\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size_in_words,\n",
    "    length_function=lambda x: len(x.split()),\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "chunks = []\n",
    "for idx, row in df.iterrows():\n",
    "    for chunk in splitter.split_text(row[\"text\"]):\n",
    "        chunks.append(\n",
    "            {\n",
    "                \"text\": chunk,\n",
    "                \"section_url\": row[\"section_url\"],\n",
    "                \"page_url\": row[\"page_url\"],\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9ff5d5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-secondary\">\n",
    "\n",
    "**Considerations for choosing the chunk size**\n",
    "\n",
    "  - We want the chunks small enough to:\n",
    "    - Fit into the context window of our chosen embedding model\n",
    "    - Be semantically coherent - i.e. concentrate on ideally a single topic\n",
    "  - We want the chunks large enough to:\n",
    "    - Contain enough information to be semantically meaningful.\n",
    "    - Avoid creating too many embeddings which can be expensive to store and query.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3a2912",
   "metadata": {},
   "source": [
    "Let's inspect the chunks produced for the first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6c9f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_document = df[\"text\"].iloc[0]\n",
    "print(\"first document is\", len(first_document.split()), \"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8416b2-5ca1-40ad-9dc0-7505cbc97c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k, v in chunks[0].items():\n",
    "    if k == \"text\":\n",
    "        print(\"first chunk of first document is\", len(v.split()), \"words\")\n",
    "    else:\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81caba36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k, v in chunks[1].items():\n",
    "    if k == \"text\":\n",
    "        print(\"second chunk of first document is\", len(v.split()), \"words\")\n",
    "    else:\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4617ebad-f9b8-4cc7-b29d-f69b4ca5036f",
   "metadata": {},
   "source": [
    "### 3. Generate embeddings from chunks\n",
    "\n",
    "For our third step, we want to load a good embedding model. \n",
    "\n",
    "**Suggested steps to choosing an embedding model:**\n",
    "1. Visit the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard) on HuggingFace.\n",
    "2. Find a model that satisfies the following considerations:\n",
    "  - Does the model perform well overall and in the task you are interested in?\n",
    "  - Is the model closed-source or open-source?\n",
    "    - If it is closed-source:\n",
    "      - What are the costs, security, and privacy implications?\n",
    "    - If it is open-source:\n",
    "      - What are its resource requirements if you want to self-host it?\n",
    "      - Is it readily available as a service by third-party providers like Anyscale, Fireworks, or Togther AI?\n",
    "\n",
    "We will use `thenlper/gte-large` model from the [HuggingFace Model Hub](https://huggingface.co/thenlper/gte-large) given it is an open-source model and is available as a service by Anyscale and performs relatively well in the MTEB leaderboard.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Note: be wary of models that overfit to the MTEB leaderboard. It is important to test the model on your own data.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c301f9a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "svmem = psutil.virtual_memory()\n",
    "\n",
    "# memory used in GB\n",
    "memory_used = svmem.total - svmem.available\n",
    "memory_used_gb_before_model_load = memory_used / (1024**3)\n",
    "memory_used_gb_before_model_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3112f3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = SentenceTransformer('thenlper/gte-large', device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25590d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "svmem = psutil.virtual_memory()\n",
    "memory_used = svmem.total - svmem.available\n",
    "memory_used_gb_after_model_load = memory_used / (1024**3)\n",
    "memory_used_gb_after_model_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd92639",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_memory_usage = memory_used_gb_after_model_load - memory_used_gb_before_model_load\n",
    "model_memory_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad64a18",
   "metadata": {},
   "source": [
    "Loading the embedding model took around 1 GB of memory.\n",
    "\n",
    "Let's see how slow it is to generate an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9340e099",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "embeddings = model.encode([chunk[\"text\"] for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f9601-0def-44fc-9852-74ea78abe329",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49537d91",
   "metadata": {},
   "source": [
    "It takes on the order of a few seconds to embed 8 chunks on our CPU. We will most definitely need a GPU to speed things up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb35f3d5-eaeb-4338-a00b-d07cd477d034",
   "metadata": {},
   "source": [
    "#### Save embeddings to disk\n",
    "\n",
    "As a fourth step, we want to store our generated embeddings as a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0153e93c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_output = pd.DataFrame(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75eb31f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_output[\"embeddings\"] = embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e2bca2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aad3a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_output.to_parquet(DATA_DIR / \"sample-output-pandas.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d5a657-90c6-4af7-91d2-4e0ef75005cb",
   "metadata": {},
   "source": [
    "### 4. Upsert embeddings to vector store\n",
    "\n",
    "The final step is to upsert the embeddings into a database. We will skip this step for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824ae888-4217-4240-9749-59997de57a36",
   "metadata": {},
   "source": [
    "## Migrating the simple pipeline to Ray Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2203991",
   "metadata": {},
   "source": [
    "We now want to migrate our implementation to use Ray Data to drastically scale our pipeline for larger datasets.\n",
    "\n",
    "### 1. Load documents\n",
    "\n",
    "Let's start with a first pass conversion of our data pipeline to use Ray Data. \n",
    "\n",
    "Instead of `pandas.read_json`, use `ray.data.read_json` to instantiate a `ray.data.Dataset` that will eventually read our file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02879966",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_sample_input = ray.data.read_json(DATA_DIR / \"small_sample\" / \"sample-input.jsonl\")\n",
    "type(ds_sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ee87a5",
   "metadata": {},
   "source": [
    "`ray.data.read_json` returns a `ray.data.Dataset` which is a distributed collection of data. Execution in Ray Data by default is:\n",
    "- **Lazy**: `Dataset` transformations aren’t executed until you call a consumption operation.\n",
    "- **Streaming**: `Dataset` transformations are executed in a streaming way, incrementally on the base data, one block at a time.\n",
    "\n",
    "Accordingly `ray.data.Dataset` will only fetch back some high-level metadata and schema information about the file, but not the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b079ec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sample_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c30d74",
   "metadata": {},
   "source": [
    "### Under the hood\n",
    "\n",
    "Ray Data uses Ray tasks to read files in parallel. Each read task reads one or more files and produces one or more output blocks.\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-summit/rag-app/dataset-read-cropped-v2.svg\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa786d3c",
   "metadata": {},
   "source": [
    "### 2. Process documents into chunks\n",
    "\n",
    "Given a `ray.data.Dataset`, we can apply transformations to it. There are two types of transformations:\n",
    "1. **row-wise transformations**\n",
    "  - `map`: a 1-to-1 function that is applied to each row in the dataset.\n",
    "  - `filter`: a 1-to-1 function that is applied to each row in the dataset and filters out rows that don’t satisfy the condition.\n",
    "  - `flat_map`: a 1-to-many function that is applied to each row in the dataset and then flattens the results into a single dataset.\n",
    "2. **batch-wise transformations**\n",
    "  - `map_batches`: a 1-to-n function that is applied to each batch in the dataset.\n",
    "\n",
    "\n",
    "We chose to make use of `flat_map` to generate a list of chunk rows. `flat_map` will create `FlatMap` tasks which will be scheduled in parallel to process as many rows as possible at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52bb9ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chunk_row(row):\n",
    "    chunk_size = 128\n",
    "    words_to_tokens = 1.2\n",
    "    num_tokens = int(chunk_size // words_to_tokens)\n",
    "\n",
    "    def get_num_words(text):\n",
    "        return len(text.split())\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=num_tokens,\n",
    "        keep_separator=True, \n",
    "        length_function=get_num_words, \n",
    "        chunk_overlap=0,\n",
    "    )\n",
    "\n",
    "    chunks = []\n",
    "    for chunk in splitter.split_text(row[\"text\"]):\n",
    "        chunks.append(\n",
    "            {\n",
    "                \"text\": chunk,\n",
    "                \"section_url\": row[\"section_url\"],\n",
    "                \"page_url\": row[\"page_url\"],\n",
    "            }\n",
    "        )\n",
    "    return chunks\n",
    "\n",
    "ds_sample_input_chunked = ds_sample_input.flat_map(chunk_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c5ee9b",
   "metadata": {},
   "source": [
    "To verify our `flat_map` is working, we can consume a limited number of rows from the dataset.\n",
    "\n",
    "To do so, we an either call\n",
    "- `take` to specify a limited number of rows from the dataset.\n",
    "- `take_batch` to specify a limited number of batches from the dataset.\n",
    "\n",
    "Here we call `take(2)` to return 2 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bf2c18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_sample_input_chunked.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6c2abb",
   "metadata": {},
   "source": [
    "### 3. Generate embeddings from chunks\n",
    "\n",
    "For our third step, we apply the embeddings using `map_batches`, which will be implemented using `MapBatches` tasks scheduled in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85a86f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def embed_batch(batch):\n",
    "    assert isinstance(batch, dict)\n",
    "    for key in batch.keys():\n",
    "        assert key in [\"text\", \"section_url\", \"page_url\"]\n",
    "    for val in batch.values():\n",
    "        assert isinstance(val, np.ndarray), type(val)\n",
    "\n",
    "    model = SentenceTransformer('thenlper/gte-large')\n",
    "    text = batch[\"text\"].tolist()\n",
    "    embeddings = model.encode(text, batch_size=len(text))\n",
    "    batch[\"embeddings\"] = embeddings.tolist()\n",
    "    return batch\n",
    "\n",
    "ds_sample_input_embedded = ds_sample_input_chunked.map_batches(embed_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a97e92",
   "metadata": {},
   "source": [
    "#### Save embeddings to disk\n",
    "\n",
    "For our fourth step, we write our dataset to parquet using `write_parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8c08db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "output_path = DATA_DIR / \"small_sample\" / \"sample-output\"\n",
    "if output_path.exists():\n",
    "    shutil.rmtree(output_path)\n",
    "\n",
    "ds_sample_input_embedded.write_parquet(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d6ccf4",
   "metadata": {},
   "source": [
    "We inspect the created parquet output directory. Every write task will create a separate file in the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a389e8f8-e2c1-484c-9df4-c2e383b34fc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -llah {output_path} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ef4b25",
   "metadata": {},
   "source": [
    "We can read the parquet file back into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a305d0-81e0-4c93-9770-9ed0a8b65e6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = ray.data.read_parquet(DATA_DIR / \"small_sample\" / \"sample-output\").to_pandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7f7b49-4a0c-4160-bc35-341894ad4149",
   "metadata": {},
   "source": [
    "### 4. Upsert embeddings to vector store\n",
    "\n",
    "The final step is to upsert the embeddings into a database. We will skip this step for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efabf980",
   "metadata": {},
   "source": [
    "**Recap**\n",
    "\n",
    "Here is our entire pipeline:\n",
    "\n",
    "```python\n",
    "(\n",
    "    ray.data.read_json(DATA_DIR / \"small_sample\" / \"sample-input.jsonl\")\n",
    "    .flat_map(chunk_row)\n",
    "    .map_batches(embed_batch)\n",
    "    .write_parquet(DATA_DIR / \"small_sample\" / \"sample-output\")\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380e0822",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Activity: Implement the pipeline using a different embedding model\n",
    "\n",
    "Re-implement the entire data pipeline but this time use a different embedding model `BAAI/bge-large-en-v1.5` which outperforms `thenlper/gte-large` on certain parts of the MTEB leaderboard.\n",
    "\n",
    "NOTE: make sure to output the results to a different directory.\n",
    "\n",
    "```python\n",
    "# Hint: Use the code in the recap section as a template but update the embedding transformation.\n",
    "```\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e64859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5664875b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<details> \n",
    "\n",
    "<summary>Click here to see the solution </summary>\n",
    "\n",
    "```python\n",
    "def embed_batch(batch):\n",
    "    # Load the embedding model\n",
    "    model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\")\n",
    "    text = batch[\"text\"].tolist()\n",
    "    embeddings = model.encode(text, batch_size=len(text))\n",
    "    batch[\"embeddings\"] = embeddings.tolist()\n",
    "    return batch\n",
    "\n",
    "(\n",
    "    ray.data.read_json(DATA_DIR / \"small_sample\" / \"sample-input.jsonl\")\n",
    "    .flat_map(chunk_row)\n",
    "    .map_batches(embed_batch)\n",
    "    .write_parquet(DATA_DIR / \"small_sample\" / \"sample-output-bge\")\n",
    ")\n",
    "\n",
    "# inspect output\n",
    "ray.data.read_parquet(DATA_DIR / \"small_sample\" / \"sample-output-bge\").to_pandas()\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7880df",
   "metadata": {},
   "source": [
    "## Scaling the pipeline with Ray Data\n",
    "\n",
    "Let's explore how to scale our pipeline to a larger dataset using Ray Data.\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-summit/rag-app/full_scale_embeddings_pipeline_v2.svg\" width=\"1000px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b675c09",
   "metadata": {},
   "source": [
    "### Phase 1: Preparing input files\n",
    "\n",
    "First, we need to prepare our documents by performing the following steps\n",
    "1. Fetch all the Ray documentation from the web.\n",
    "2. Parse the web pages to extract the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667c2c2f",
   "metadata": {},
   "source": [
    "#### 1. Fetch all the Ray documentation from the web.\n",
    "\n",
    "We have already fetched the Ray documentation and stored it on S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c88dbbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_web_pages_dir = CloudPath(\n",
    "    \"s3://anyscale-public-materials/ray-documentation-html-files/unzipped/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14628649",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_web_pages_dir.exists(), raw_web_pages_dir.is_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc2b87f",
   "metadata": {},
   "source": [
    "#### 2. Parse the web pages to extract the text.\n",
    "\n",
    "We first read all HTML files in the raw web pages directory into a `ray.data.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebe9021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_web_page_paths = ray.data.from_items(\n",
    "    [{\"path\": path} for path in raw_web_pages_dir.glob(\"**/*.html\")]\n",
    ")\n",
    "ds_web_page_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d448ad80",
   "metadata": {},
   "source": [
    "Note that this only includes the latest version of the ray documentation. This size would be drastically multiplied if we included all versions of the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f788923",
   "metadata": {},
   "source": [
    "##### Utilize inherent structure to improve the documents \n",
    "\n",
    "Documentation [webpages](https://docs.ray.io/en/latest/rllib/rllib-env.html) are naturally split into sections. We can use this to our advantage by returning our documents as sections. This will facilitate producing semantically coherent chunks. \n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-summit/rag-app/ray_docs_section_extraction_v2.png\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b5a4dc",
   "metadata": {},
   "source": [
    "We are producing multiple documents from each HTML file. We will use the `flat_map` method to produce multiple documents from each HTML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44037192",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def path_to_uri(\n",
    "    path: CloudPath, scheme: str = \"https://\", domain: str = \"docs.ray.io\"\n",
    ") -> str:\n",
    "    return scheme + domain + str(path).split(domain)[-1]\n",
    "\n",
    "def extract_sections_from_html(record: dict) -> list[dict]:\n",
    "    documents = []\n",
    "    # 1. Request the page and parse it using BeautifulSoup\n",
    "    with record[\"path\"].open(\"r\", encoding=\"utf-8\", force_overwrite_from_cloud=True) as html_file:\n",
    "        soup = BeautifulSoup(html_file, \"html.parser\")\n",
    "\n",
    "    page_url = path_to_uri(record[\"path\"])\n",
    "\n",
    "    # 2. Find all sections\n",
    "    sections = soup.find_all(\"section\")\n",
    "    for section in sections:\n",
    "        # 3. Extract text from the section but not from the subsections\n",
    "        section_text = \"\\n\".join(\n",
    "            [child.text for child in section.children if child.name != \"section\"]\n",
    "        )\n",
    "        # 4. Construct the section url\n",
    "        section_url = page_url + \"#\" + section[\"id\"]\n",
    "        # 5. Create a document object with the text, source page, source section uri\n",
    "        documents.append(\n",
    "            {\n",
    "                \"text\": section_text,\n",
    "                \"section_url\": section_url,\n",
    "                \"page_url\": page_url,\n",
    "            }\n",
    "        )\n",
    "    return documents\n",
    "\n",
    "\n",
    "ds_sections = ds_web_page_paths.flat_map(extract_sections_from_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f853bd4",
   "metadata": {},
   "source": [
    "Finally we store the produced dataset in parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fff8e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if (DATA_DIR / \"full_scale\" / \"02_sections\").exists():\n",
    "    shutil.rmtree(DATA_DIR / \"full_scale\" / \"02_sections\")\n",
    "ds_sections.write_parquet(DATA_DIR / \"full_scale\" / \"02_sections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b27d396",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -llh {DATA_DIR / \"full_scale\" / \"02_sections\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeda8999-9b56-4347-b3bd-6f740ece7b54",
   "metadata": {},
   "source": [
    "Let's count how many documents we will have after processing the sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d20170-970e-4c41-a830-19b28a36ed08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.data.read_parquet(DATA_DIR / \"full_scale\" / \"02_sections\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c55a29",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "**Considerations for reading input files into Ray Data:**\n",
    "\n",
    "Pruning columns and using filter pushdown can optimize parquet file reads:\n",
    "- Specify only necessary columns when dealing with column-oriented formats to reduce memory usage.\n",
    "- Apply filter pushdown in `ray.data.read_parquet` to retrieve only rows that meet certain conditions.\n",
    "\n",
    "However, as our dataset's memory footprint is predominantly due to the 'text' column, these optimizations will have a limited impact on reducing memory load.\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6349758",
   "metadata": {},
   "source": [
    "### Phase 2: Generating Embeddings\n",
    "\n",
    "Now that we have our documents, we can proceed to generate embeddings.\n",
    "\n",
    "#### 1. Load documents\n",
    "We begin by reading the documents from the \"02_sections\" directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cabac4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_sections = ray.data.read_parquet(DATA_DIR / \"full_scale\" / \"02_sections\")\n",
    "\n",
    "ds_sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75131c7b",
   "metadata": {},
   "source": [
    "#### Applying chunking as a transformation\n",
    "\n",
    "We apply our chunking transformation using `flat_map`, which applies a 1-to-many function to each row in the dataset and then flattens the results into a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed69ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_sections_chunked = ds_sections.flat_map(chunk_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07f2f96",
   "metadata": {},
   "source": [
    "We could have used `map_batches` instead to apply a many-to-many function to each batch of rows in the dataset. However, given our chunking transformation is not vectorized, `map_batches` will not be faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229450f6",
   "metadata": {},
   "source": [
    "Let's run the chunking and count our total number of chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8aaa46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_sections_chunked.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b485be",
   "metadata": {},
   "source": [
    "#### Applying embedding as a transformation\n",
    "\n",
    "We want to load the embedding model once and reuse it across multiple transformation tasks.\n",
    "\n",
    "To do so, we want to use call `map_batches` with **stateful transform** instead of a *stateless transform*. \n",
    "\n",
    "This means we create a pool of processes called actors where the model is already loaded in memory.\n",
    "\n",
    "Each actor will run a `MapBatch` process where:\n",
    "  - initial state is handled in `__init__`\n",
    "  - task is invoked using `__call__` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab910d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_workers = 2\n",
    "device = \"cuda\"\n",
    "\n",
    "class EmbedBatch:\n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer(\"thenlper/gte-large\", device=device)\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        text = batch[\"text\"].tolist()\n",
    "        embeddings = self.model.encode(text, batch_size=len(text))\n",
    "        batch[\"embeddings\"] = embeddings.tolist()\n",
    "        return batch\n",
    "\n",
    "ds_sections_embedded = ds_sections_chunked.map_batches(\n",
    "    EmbedBatch,\n",
    "    # Number of actors to launch.\n",
    "    concurrency=num_workers,\n",
    "    # Size of batch passed to embeddings actor.\n",
    "    batch_size=200,\n",
    "    # 1 GPU for each actor.\n",
    "    num_gpus=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaadfbcb",
   "metadata": {},
   "source": [
    "#### Writing the embeddings to disk\n",
    "\n",
    "Now that we need to write the embeddings to disk, the data pipeline will get executed and will stream the data to the GPU nodes to perform the embedding generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab59319-0e69-4fd5-8f0e-d556c5c4881e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if (DATA_DIR / \"full_scale\" / \"03_embeddings\").exists():\n",
    "    shutil.rmtree(DATA_DIR / \"full_scale\" / \"03_embeddings\")\n",
    "(\n",
    "    ds_sections_embedded.write_parquet(path=DATA_DIR / \"full_scale\" / \"03_embeddings\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a4cf7c",
   "metadata": {},
   "source": [
    "##### Inspecting the Ray Data dashboard\n",
    "\n",
    "If we take a look at the metrics tab of the ray data dashboard, we can check to see:\n",
    "\n",
    "- The GPU utilization\n",
    "    - Ideally, we would like to see the GPU utilization at 100% for the duration of the embedding process\n",
    "- The GPU memory (GRAM) percentage\n",
    "    - We would like to see the GPU memory utilization at 100% for the duration of the embedding process\n",
    "- The time spent on io and network by different tasks\n",
    "\n",
    "We can then use this information to optimize our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f68b208",
   "metadata": {},
   "source": [
    "##### Inspecting the output\n",
    "\n",
    "We check to see if the embeddings were written to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f381af-8c34-4e08-9014-c916f8ac3687",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -llh {DATA_DIR / \"full_scale\" / \"03_embeddings\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24e368b",
   "metadata": {},
   "source": [
    "### Recap of the pipeline\n",
    "\n",
    "Here is our entire pipeline so far:\n",
    "\n",
    "```python\n",
    "(\n",
    "    ray.data.read_json(\n",
    "        DATA_DIR / \"full_scale\" / \"02_sections\",\n",
    "    )\n",
    "    .flat_map(chunk_row)\n",
    "    .map_batches(\n",
    "        EmbedBatch,\n",
    "        concurrency=num_workers,\n",
    "        batch_size=200,\n",
    "        num_gpus=1,\n",
    "    )\n",
    "    .write_parquet(\n",
    "        path=DATA_DIR / \"full_scale\" / \"03_embeddings_tuning\",\n",
    "    )\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda2771a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Activity: Tuning the pipeline\n",
    "\n",
    "Proceed to tune your pipeline by changing the batch size on `map_batches` and see what effect it has on the GPU memory (GRAM) percentage.\n",
    "\n",
    "```python\n",
    "ds_sections_embedded = ds_sections_chunked.map_batches(\n",
    "    EmbedBatch,\n",
    "    concurrency=num_workers,\n",
    "    batch_size=200,  # Hint: Check how GRAM changes when you change the batch size\n",
    "    num_gpus=1,\n",
    ")\n",
    "\n",
    "ds_sections_embedded.materialize()\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288d98a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea228c6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Click here to see the solution</summary>\n",
    "\n",
    "```python\n",
    "ds_sections_embedded = ds_sections_chunked.map_batches(\n",
    "    EmbedBatch,\n",
    "    concurrency=num_workers,\n",
    "    batch_size=350,  # Optimal batch size for GRAM\n",
    "    num_gpus=1,\n",
    ")\n",
    "\n",
    "ds_sections_embedded.materialize()\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce8b0c3",
   "metadata": {},
   "source": [
    "### Upserting embeddings to a vector database\n",
    "\n",
    "We will use [chroma](https://www.trychroma.com/) to index our document embeddings in a vector store. Chroma is an open-source vector database optimized for similarity search and is user-friendly. We chose Chroma for its ease of use and its free tier, which meets our needs.\n",
    "\n",
    "<!-- \n",
    "We will use [Pinecone](https://www.pinecone.io/) to index our document embeddings in a vector store. Pinecone is a fully managed vector database optimized for similarity search and is user-friendly. We chose Pinecone for its ease of use and its free tier, which meets our needs.\n",
    "\n",
    "Index your document embeddings in Pinecone as follows:\n",
    "\n",
    "\n",
    "1. Create a Pinecone client.\n",
    "2. Create a Pinecone index.\n",
    "3. Load the embeddings from disk.\n",
    "4. Transform the embeddings into Pinecone’s index format.\n",
    "5. Upsert the embeddings into the Pinecone index.\n",
    "6. Query the Pinecone index. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88b8083",
   "metadata": {},
   "source": [
    "#### 1. Create a chroma client \n",
    "\n",
    "We create a chroma client using the `PersistentClient` class to connect to the chroma server against a persistent file store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3594d40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"/mnt/cluster_storage/vector_store\")\n",
    "chroma_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b815e31d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chroma_client.list_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ed9863",
   "metadata": {},
   "source": [
    "#### 2. Create a chroma collection\n",
    "\n",
    "Next, we create a collection in chroma to store our embeddings. A collection provides a vector store index for our embeddings.\n",
    "\n",
    "We specify `hnsw:space` to use the \"Hierarchical Navigable Small World\" algorithm for similarity search using cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134c2d70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "collection = chroma_client.get_or_create_collection(name=\"ray-docs\", metadata={\"hnsw:space\": \"cosine\"})\n",
    "collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff80c116",
   "metadata": {},
   "source": [
    "#### 3. Load the embeddings from disk \n",
    "\n",
    "We will load the embeddings from disk using `ray.data.read_parquet` to initiate a distributed upsert of the embeddings to chroma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a3f696",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_embeddings = ray.data.read_parquet(DATA_DIR / \"full_scale\" / \"03_embeddings/\")\n",
    "ds_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d883bcb2",
   "metadata": {},
   "source": [
    "#### 4. Transform the embeddings into chroma index format \n",
    "\n",
    "We construct an `id` column to uniquely identify each embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168a85f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_id(row):\n",
    "    row_hash = joblib.hash(row)\n",
    "    page_name = row[\"page_url\"].split(\"/\")[-1]\n",
    "    section_name = row[\"section_url\"].split(\"#\")[-1]\n",
    "    row[\"id\"] =  f\"{page_name}#{section_name}#{row_hash}\"\n",
    "    return row\n",
    "\n",
    "ds_embeddings_with_id = ds_embeddings.map(compute_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154edf44",
   "metadata": {},
   "source": [
    "We fetch back the data as a collection of objects and then upsert them into chroma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c6730f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chroma_data = ds_embeddings_with_id.to_pandas().drop_duplicates(subset=[\"id\"]).to_dict(orient=\"list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91cf418",
   "metadata": {},
   "source": [
    "Here is how to upsert documents into a collection in chroma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d749632",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.upsert(\n",
    "    ids=chroma_data[\"id\"],\n",
    "    embeddings=[arr.tolist() for arr in chroma_data[\"embeddings\"]],\n",
    "    documents=chroma_data[\"text\"],\n",
    "    metadatas=[\n",
    "        {\n",
    "            \"section_url\": section_url,\n",
    "            \"page_url\": page_url,\n",
    "        }\n",
    "        for section_url, page_url in zip(chroma_data[\"section_url\"], chroma_data[\"page_url\"])\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f2dee6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Note we can further parallelize the upsert using a `map_batches` operation. This is left as an exercise for the reader.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fb5d69",
   "metadata": {},
   "source": [
    "### Querying the chroma collection\n",
    "\n",
    "Given we have indexed our embeddings, we can now query the index to retrieve the most similar documents to a given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a48944",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What is the default number of maximum replicas for a Ray Serve deployment?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742abec1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer('thenlper/gte-large')\n",
    "query_embedding = model.encode(query).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6828281f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be32a627",
   "metadata": {},
   "source": [
    "Here is the most relevant text we found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58a34e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[\"documents\"][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ad9e4c",
   "metadata": {},
   "source": [
    "It was fetched from this page of the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b9b6be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result[\"metadatas\"][0][0][\"page_url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf59bf49",
   "metadata": {},
   "source": [
    "We can additionally retrieve the similarity score in case we want to only retrieve results with a score above a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db08e14d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = [1- distance for distance in result[\"distances\"][0]]\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e9c74f",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "With Ray and Anyscale we are able to achieve very fast and efficient embeddings generation at scale. See this [blog](https://www.anyscale.com/blog/rag-at-scale-10x-cheaper-embedding-computations-with-anyscale-and-pinecone) showcasing how we were able to achieve 10x cheaper embeddings generation of billions of documents using Ray and Pinecone.\n",
    "\n",
    "Ray Data's Lazy and Streaming execution model allows us to:\n",
    "- Efficiently scale our pipeline to large datasets\n",
    "- Avoid having to fully materialize the dataset in a store (memory/disk)\n",
    "- Easily saturate GPUs by scaling preprocessing across CPU nodes\n",
    "  \n",
    "Anyscale provides:\n",
    "- Access to spot instances with fallback to on-demand to run the pipeline in the most cost-efficient manner\n",
    "- Incremental metadata fetching of very large parquet datasets avoiding long \"boot times\" and idling instances\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
