{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c39cccf9",
   "metadata": {},
   "source": [
    "# Brief \n",
    "\n",
    "Having indexed the data, we can now build our RAG system. We will start by building the retriever, which will be responsible for finding the most relevant documents to a given query and then we will build an LLM client to generate the response.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b> Here is the roadmap for this notebook:</b>\n",
    "\n",
    "<ul>\n",
    "    <li><b>Part 1:</b> RAG Application Overview</li>\n",
    "    <li><b>Part 2:</b> Building Retriever components</li>\n",
    "    <li><b>Part 3:</b> Building Response Generation</li>\n",
    "    <li><b>Part 4:</b> Putting it all together into a QA Engine</li>\n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ef871a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef7fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from typing import Any, Iterator\n",
    "\n",
    "import openai\n",
    "import chromadb\n",
    "from openai.resources.chat.completions import ChatCompletion\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e171a305",
   "metadata": {},
   "source": [
    "## Pre-requisite setup\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> <b> Important if you want to run this notebook:</b> \n",
    "\n",
    "This RAG notebook requires having a running LLM Anyscale service. To deploy an LLM as an Anyscale service, you can follow the step-by-step instructions in this [Deploy an LLM workspace template](https://console.anyscale.com/v2/template-preview/endpoints_v2). Make sure to choose the `mistralai/Mistral-7B-Instruct-v0.1` model when deploying.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a1e90f",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbf9771",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANYSCALE_SERVICE_BASE_URL = \"replace-with-my-anyscale-service-url\"\n",
    "ANYSCALE_API_KEY = \"replace-with-my-anyscale-api-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc53d896",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get(\"ANYSCALE_ARTIFACT_STORAGE\"):\n",
    "    DATA_DIR = Path(\"/mnt/cluster_storage/\")\n",
    "    shutil.copytree(Path(\"./data/\"), DATA_DIR, dirs_exist_ok=True)\n",
    "else:\n",
    "    DATA_DIR = Path(\"./data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d3af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding model we used to build the search index on chroma\n",
    "EMBEDDING_MODEL_NAME = \"thenlper/gte-large\"\n",
    "# The chroma search index we built\n",
    "CHROMA_COLLECTION_NAME = \"ray-docs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c257a14",
   "metadata": {},
   "source": [
    "## 0. RAG Application Overview\n",
    "\n",
    "We are building a simple RAG application that can answer questions about [Ray](https://docs.ray.io/). \n",
    "\n",
    "As a recap, see the diagram below for a visual representation of the components required for RAG.\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-summit/rag-app/RAG+App+-+Ray+Summit+-+with_rag_simple_v2.png\" alt=\"With RAG\" width=\"600px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e3516c",
   "metadata": {},
   "source": [
    "## 1. Building Retriever components\n",
    "Retrieval is implemented in the following steps:\n",
    "\n",
    "1. Encode the user query\n",
    "2. Search the vector store\n",
    "3. Compose a context from the retrieved documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ac27aa",
   "metadata": {},
   "source": [
    "### 1. Encode the user query\n",
    "To encode the query, we will use the same embedding model that we used to encode the documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4db779",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryEncoder:\n",
    "    def __init__(self):\n",
    "        self.embedding_model_name = EMBEDDING_MODEL_NAME\n",
    "        self.model = SentenceTransformer(self.embedding_model_name)\n",
    "\n",
    "    def encode(self, query: str) -> list[float]:\n",
    "        return self.model.encode(query).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ce0b02",
   "metadata": {},
   "source": [
    "We try out our QueryEncoder by encoding a sample query relevant to our domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4c7627",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_encoder = QueryEncoder()\n",
    "query = \"How can I deploy Ray Serve to Kubernetes?\"\n",
    "embeddings_vector = query_encoder.encode(query)\n",
    "\n",
    "type(embeddings_vector), len(embeddings_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_vector[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73087086",
   "metadata": {},
   "source": [
    "### 2. Search the vector store\n",
    "Next, we will search the vector store to retrieve the closest documents to the query.\n",
    "\n",
    "We implement a `VectorStore` abstraction that reiles on the chroma client to search the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e07e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self):\n",
    "        chroma_client = chromadb.PersistentClient(\n",
    "            path=\"/mnt/cluster_storage/vector_store\"\n",
    "        )\n",
    "        self._collection = chroma_client.get_collection(CHROMA_COLLECTION_NAME)\n",
    "\n",
    "    def query(self, query_embedding: list[float], top_k: int) -> dict:\n",
    "        \"\"\"Retrieve the most similar chunks to the given query embedding.\"\"\"\n",
    "        if top_k == 0:\n",
    "            return {\"documents\": [], \"usage\": {}}\n",
    "\n",
    "        response = self._collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=top_k,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"documents\": [\n",
    "                {\n",
    "                    \"text\": text,\n",
    "                    \"section_url\": metadata[\"section_url\"],\n",
    "                }\n",
    "                for text, metadata in zip(\n",
    "                    response[\"documents\"][0], response[\"metadatas\"][0]\n",
    "                )\n",
    "            ],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2141b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = VectorStore()\n",
    "vector_store_response = vector_store.query(\n",
    "    query_embedding=embeddings_vector,\n",
    "    top_k=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ae23d4",
   "metadata": {},
   "source": [
    "We can inspect the retrieved document URLs given our query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e0e6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in vector_store_response[\"documents\"]:\n",
    "    print(doc[\"section_url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50af105",
   "metadata": {},
   "source": [
    "### 3. Compose a context from the retrieved documents\n",
    "\n",
    "We put together a `Retriever` that encapsulates the entire retrieval process so far.\n",
    "\n",
    "It also composes the context from the retrieved documents by simply concatenating the retrieved chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0a5a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retriever:\n",
    "    def __init__(self, query_encoder, vector_store):\n",
    "        self.query_encoder = query_encoder\n",
    "        self.vector_store = vector_store\n",
    "\n",
    "    def _compose_context(self, contexts: list[str]) -> str:\n",
    "        sep = 100 * \"-\"\n",
    "        return \"\\n\\n\".join([f\"{sep}\\n{context}\" for context in contexts])\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int) -> dict:\n",
    "        \"\"\"Retrieve the context and sources for the given query.\"\"\"\n",
    "        encoded_query = self.query_encoder.encode(query)\n",
    "        vector_store_response = self.vector_store.query(\n",
    "            query_embedding=encoded_query,\n",
    "            top_k=top_k,\n",
    "        )\n",
    "        contexts = [chunk[\"text\"] for chunk in vector_store_response[\"documents\"]]\n",
    "        sources = [chunk[\"section_url\"] for chunk in vector_store_response[\"documents\"]]\n",
    "        return {\n",
    "            \"contexts\": contexts,\n",
    "            \"composed_context\": self._compose_context(contexts),\n",
    "            \"sources\": sources,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a32543",
   "metadata": {},
   "source": [
    "We run the retriever to check it is working as expected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b5997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Retriever(query_encoder=query_encoder, vector_store=vector_store)\n",
    "retrieval_response = retriever.retrieve(\n",
    "    query=query,\n",
    "    top_k=3,\n",
    ")\n",
    "retrieval_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e350b905",
   "metadata": {},
   "source": [
    "We inspect the retrieved context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3438a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retrieval_response[\"composed_context\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6687d2a7",
   "metadata": {},
   "source": [
    "## 2. Building Response Generation\n",
    "\n",
    "We will generate a response using an LLM server offering an openai-compatible API.\n",
    "\n",
    "To do so we implement a simple LLM client class that encapsulates the generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8969eb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMClient:\n",
    "    def __init__(self):\n",
    "        # Initialize a client to perform API requests\n",
    "        self.client = openai.OpenAI(\n",
    "            base_url=ANYSCALE_SERVICE_BASE_URL,\n",
    "            api_key=ANYSCALE_API_KEY,\n",
    "        )\n",
    "\n",
    "    def generate(self, user_prompt: str, model=\"mistralai/Mistral-7B-Instruct-v0.1\", temperature: float = 0, **kwargs: Any) -> ChatCompletion:\n",
    "        \"\"\"Generate a completion from the given user prompt.\"\"\"\n",
    "        # Call the chat completions endpoint\n",
    "        chat_completion = self.client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                # Prime the system with a system message - a common best practice\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                # Send the user message with the proper \"user\" role and \"content\"\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        return chat_completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e493a",
   "metadata": {},
   "source": [
    "Note we are currently making use of an already deployed open-source LLM running on Anyscale.\n",
    "\n",
    "In case you want to deploy your own LLM, you can follow the instructions in the [Anyscale documentation](https://docs.anyscale.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ca2e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_client = LLMClient()\n",
    "response = llm_client.generate(\"What is the capital of France?\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1b49c6",
   "metadata": {},
   "source": [
    "## 3. Putting it all together into a QA Engine\n",
    "Given a user query we will want our RAG based QA engine to perform the following steps:\n",
    "\n",
    "1. Retrieve the closest documents to the query\n",
    "2. Augment the query with the context\n",
    "3. Generate a response to the augmented query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10197527",
   "metadata": {},
   "source": [
    "We decide on a simple prompt template to augment the user's query with the retrieved context. The template is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668f5d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_rag = \"\"\"\n",
    "Given the following context:\n",
    "{composed_context}\n",
    "\n",
    "Answer the following question:\n",
    "{query}\n",
    "\n",
    "If you cannot provide an answer based on the context, please say \"I don't know.\"\n",
    "Do not use the term \"context\" in your response.\"\"\"\n",
    "\n",
    "\n",
    "def augment_prompt(query: str, composed_context: str) -> str:\n",
    "    \"\"\"Augment the prompt with the given query and contexts.\"\"\"\n",
    "    return prompt_template_rag.format(composed_context=composed_context, query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100fabee",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_prompt = augment_prompt(\n",
    "    query=query,\n",
    "    composed_context=retrieval_response[\"composed_context\"],\n",
    ")\n",
    "print(augmented_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a380ede7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-secondary\">\n",
    "\n",
    "**Considerations for building a prompt-template for RAG:**\n",
    "\n",
    "Prompt engineering techniques can be used need to be purpose built for the usecase and chosen model. For example, if you want the model to still use its own knowledge in certain cases, you might want to use a different prompt template than if you want the model to only use the retrieved context.\n",
    "\n",
    "For comparison, here are the links to popular third-party library prompt templates which are fairly generic in nature:\n",
    "- [LangChain's default RAG prompt template](https://smith.langchain.com/hub/rlm/rag-prompt)\n",
    "- [LlamaIndex's RAG prompt template](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/prompts/default_prompts.py#L99)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23ad902",
   "metadata": {},
   "source": [
    "We implement our question answering `QA` class below that composed all the steps together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e476362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QA:\n",
    "    def __init__(self, retriever: Retriever, llm_client: LLMClient):\n",
    "        self.retriever = retriever\n",
    "        self.llm_client = llm_client\n",
    "\n",
    "    def answer(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int,\n",
    "        include_sources: bool = True,\n",
    "    ) -> Iterator[str]:\n",
    "        \"\"\"Answer the given question and provide sources.\"\"\"\n",
    "        retrieval_response = self.retriever.retrieve(\n",
    "            query=query,\n",
    "            top_k=top_k,\n",
    "        )\n",
    "        prompt = augment_prompt(query, retrieval_response[\"composed_context\"])\n",
    "        response = self.llm_client.generate(\n",
    "            user_prompt=prompt,\n",
    "            stream=True,\n",
    "        )\n",
    "        for chunk in response:\n",
    "            choice = chunk.choices[0]\n",
    "            if choice.delta.content is None:\n",
    "                continue\n",
    "            yield choice.delta.content\n",
    "\n",
    "        if include_sources:\n",
    "            yield \"\\n\" * 2\n",
    "            sources_str = \"\\n\".join(set(retrieval_response[\"sources\"]))\n",
    "            yield sources_str\n",
    "            yield \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147c965b",
   "metadata": {},
   "source": [
    "We now test out our `QA` implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f18fd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_agent = QA(retriever=retriever, llm_client=llm_client)\n",
    "response = qa_agent.answer(query=query, top_k=3)\n",
    "for r in response:\n",
    "    print(r, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b4a73",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Activity: Prompt the QA agent with different top_k values\n",
    "\n",
    "Prompt the same QA agent with the question \"How to deploy Ray Serve on Kubernetes?\" with `top_k=0` - is the answer still helpful and correct? \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c34d61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b610d1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<details>\n",
    "<summary>Click here to see the solution</summary>\n",
    "\n",
    "\n",
    "If you prompt the QA agent with `top_k=0`, the answer will not be meaningful. This is because the RAG application will not be able to retrieve any documents from the search index and therefore will not be able to generate an answer.\n",
    "\n",
    "```python\n",
    "qa_agent = QA(model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "response = qa_agent.answer(query=query, top_k=0)\n",
    "for r in response:\n",
    "    print(r, end=\"\")\n",
    "```\n",
    "\n",
    "This will now produce a hallucinated answer about using a helm chart that does not exist.\n",
    "\n",
    "\n",
    "</details>\n",
    "</summary>\n",
    "\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
