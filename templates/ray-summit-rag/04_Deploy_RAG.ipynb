{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c39cccf9",
   "metadata": {},
   "source": [
    "# Brief \n",
    "\n",
    "Having built a basic RAG application, we now need to deploy it. This guide will walk you through deploying the Retriever and Generation models on a server.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b> Here is the roadmap for this notebook:</b>\n",
    "\n",
    "<ul>\n",
    "    <li><b>Part 1:</b> RAG Backend Overview</li>\n",
    "    <li><b>Part 2:</b> Deploying the Retriever components</li>\n",
    "    <li><b>Part 3:</b> Deploying the Response Generation</li>\n",
    "    <li><b>Part 4:</b> Putting it all together into a QA Engine</li>\n",
    "    <li><b>Part 5:</b> Key Takeaways</li>\n",
    "    <li><b>Part 6:</b> Bonus: Adding HTTP Ingress</li>\n",
    "    <li><b>Part 7:</b> Bonus: Enabling streaming of response</li>\n",
    "    \n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ef871a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef7fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from fastapi import FastAPI\n",
    "from fastapi.responses import StreamingResponse\n",
    "from typing import Any, Iterator\n",
    "\n",
    "import openai\n",
    "import requests\n",
    "import chromadb\n",
    "from ray import serve\n",
    "from openai.resources.chat.completions import ChatCompletion\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e82c45",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "We will fetch some credentials from S3 and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a31994",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://anyscale-ray-summit-training-2024/anyscale_service_credentials.json ./credentials.json\n",
    "\n",
    "with open(\"credentials.json\", \"r\") as f:\n",
    "    credentials = json.load(f)\n",
    "    for key, value in credentials.items():\n",
    "        os.environ[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a1e90f",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc53d896",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get(\"ANYSCALE_ARTIFACT_STORAGE\"):\n",
    "    DATA_DIR = Path(\"/mnt/cluster_storage/\")\n",
    "    shutil.copytree(Path(\"./data/\"), DATA_DIR, dirs_exist_ok=True)\n",
    "else:\n",
    "    DATA_DIR = Path(\"./data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d3af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding model we used to build the search index on chroma\n",
    "EMBEDDING_MODEL_NAME = \"thenlper/gte-large\"\n",
    "# The chroma search index we built\n",
    "CHROMA_COLLECTION_NAME = \"ray-docs\"\n",
    "\n",
    "ANYSCALE_SERVICE_BASE_URL = os.environ[\"ANYSCALE_SERVICE_BASE_URL\"]\n",
    "ANYSCALE_API_KEY = os.environ[\"ANYSCALE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c257a14",
   "metadata": {},
   "source": [
    "## 0. RAG Backend Overview\n",
    "\n",
    "Here is the same diagram from the previous notebook, but with the services that we will deploy highlighted.\n",
    "\n",
    "All the services will be deployed as part of a single QA engine application.\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-summit/rag-app/RAG+App+-+Ray+Summit+-+with_rag_services_v2.png\" width=\"900px\"/>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Note: delineating which components are built as separate deployments is a design decision. It depends whether you want to scale them independently or not. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57455ffe",
   "metadata": {},
   "source": [
    "## 1. Building Retriever components\n",
    "\n",
    "As a reminder, Retrieval is implemented in the following steps:\n",
    "\n",
    "1. Encode the user query\n",
    "2. Search the vector store\n",
    "3. Compose a context from the retrieved documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6fd59b",
   "metadata": {},
   "source": [
    "### 1. Encode the user query\n",
    "\n",
    "To convert our QueryEncoder into a Ray deployment, simply need to wrap it with a `serve.deployment` decorator. \n",
    "\n",
    "Each deployment is a collection of replicas that can be scaled up or down based on the traffic.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/deployment.png' width=400/>\n",
    "\n",
    "\n",
    "The `autoscaling_config` parameter specifies the minimum and maximum number of replicas that can be created. \n",
    "\n",
    "The `ray_actor_options` parameter specifies the resources allocated to each replica. In this case, we allocate 1/10th (0.1) of a GPU to each replica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b846b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(\n",
    "    ray_actor_options={\"num_gpus\": 0.1},\n",
    "    autoscaling_config={\"min_replicas\": 1, \"max_replicas\": 2},\n",
    ")\n",
    "class QueryEncoder:\n",
    "    def __init__(self):\n",
    "        self.embedding_model_name = EMBEDDING_MODEL_NAME\n",
    "        self.model = SentenceTransformer(self.embedding_model_name, device=\"cuda\")\n",
    "\n",
    "    def encode(self, query: str) -> list[float]:\n",
    "        return self.model.encode(query).tolist()\n",
    "\n",
    "\n",
    "query_encoder = QueryEncoder.bind()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae07650",
   "metadata": {},
   "source": [
    "To send a gRPC request to the deployment, we need to:\n",
    "1. start running the deployment and fetch back its handle using `serve.run`\n",
    "2. send a request to the deployment using the handle using `.remote()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6db68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_encoder_handle = serve.run(query_encoder, route_prefix=\"/query-encoder\")\n",
    "query = \"How can I deploy Ray Serve to Kubernetes?\"\n",
    "embeddings_vector = await query_encoder_handle.encode.remote(query)\n",
    "\n",
    "type(embeddings_vector), len(embeddings_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a3528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_vector[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ce46aa",
   "metadata": {},
   "source": [
    "### 2. Search the vector store\n",
    "\n",
    "Next we would wrap the vector store with a `serve.deployment`. \n",
    "\n",
    "Note, we resort to a hack to ensure the vector store is running on the head node. This is because we are running a local chromadb in development mode which does not allow for concurrent access across nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e314b9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(\n",
    "    ray_actor_options={\"num_cpus\": 0, \"resources\": {\"is_head_node\": 1}},\n",
    ")\n",
    "class VectorStore:\n",
    "    def __init__(self):\n",
    "        chroma_client = chromadb.PersistentClient(\n",
    "            path=\"/mnt/cluster_storage/vector_store\"\n",
    "        )\n",
    "        self._collection = chroma_client.get_collection(CHROMA_COLLECTION_NAME)\n",
    "\n",
    "    async def query(self, query_embedding: list[float], top_k: int) -> dict:\n",
    "        \"\"\"Retrieve the most similar chunks to the given query embedding.\"\"\"\n",
    "        if top_k == 0:\n",
    "            return {\"documents\": [], \"usage\": {}}\n",
    "\n",
    "        response = self._collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=top_k,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"documents\": [\n",
    "                {\n",
    "                    \"text\": text,\n",
    "                    \"section_url\": metadata[\"section_url\"],\n",
    "                }\n",
    "                for text, metadata in zip(\n",
    "                    response[\"documents\"][0], response[\"metadatas\"][0]\n",
    "                )\n",
    "            ],\n",
    "        }\n",
    "\n",
    "vector_store = VectorStore.bind()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b7ebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_handle = serve.run(vector_store, route_prefix=\"/vector-store\")\n",
    "vector_store_response = await vector_store_handle.query.remote(\n",
    "    query_embedding=embeddings_vector,\n",
    "    top_k=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9381166d",
   "metadata": {},
   "source": [
    "We can inspect the retrieved document URLs given our query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91c7f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in vector_store_response[\"documents\"]:\n",
    "    print(doc[\"section_url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5310c44",
   "metadata": {},
   "source": [
    "### 3. Compose a context from the retrieved documents\n",
    "\n",
    "We put together a `Retriever` that encapsulates the entire retrieval process so far.\n",
    "\n",
    "It also composes the context from the retrieved documents by simply concatenating the retrieved chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842765a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(\n",
    "    ray_actor_options={\"num_cpus\": 0.1},\n",
    ")\n",
    "class Retriever:\n",
    "    def __init__(self, query_encoder, vector_store):\n",
    "        self.query_encoder = query_encoder\n",
    "        self.vector_store = vector_store\n",
    "\n",
    "    def _compose_context(self, contexts: list[str]) -> str:\n",
    "        sep = 100 * \"-\"\n",
    "        return \"\\n\\n\".join([f\"{sep}\\n{context}\" for context in contexts])\n",
    "\n",
    "    async def retrieve(self, query: str, top_k: int) -> dict:\n",
    "        \"\"\"Retrieve the context and sources for the given query.\"\"\"\n",
    "        encoded_query = await self.query_encoder.encode.remote(query)\n",
    "        vector_store_response = await self.vector_store.query.remote(\n",
    "            query_embedding=encoded_query,\n",
    "            top_k=top_k,\n",
    "        )\n",
    "        contexts = [chunk[\"text\"] for chunk in vector_store_response[\"documents\"]]\n",
    "        sources = [chunk[\"section_url\"] for chunk in vector_store_response[\"documents\"]]\n",
    "        return {\n",
    "            \"contexts\": contexts,\n",
    "            \"composed_context\": self._compose_context(contexts),\n",
    "            \"sources\": sources,\n",
    "        }\n",
    "\n",
    "\n",
    "retriever = Retriever.bind(query_encoder=query_encoder, vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f916b83c",
   "metadata": {},
   "source": [
    "We run the retriever to check it is working as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4496e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_handle = serve.run(retriever, route_prefix=\"/retriever\")\n",
    "retrieval_response = await retriever_handle.retrieve.remote(\n",
    "    query=query,\n",
    "    top_k=3,\n",
    ")\n",
    "retrieval_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7c0228",
   "metadata": {},
   "source": [
    "We inspect the retrieved context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8664a00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retrieval_response[\"composed_context\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28bd2bf",
   "metadata": {},
   "source": [
    "## 2. Building Response Generation\n",
    "\n",
    "Next we will wrap the LLM client as its own deployment. Here we showcase that we can also make use of fractional CPUs for this client deployment. \n",
    "\n",
    "Note: Separating the client as its own deployment is optional and could have been included in the QA engine deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7dfe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(\n",
    "    ray_actor_options={\"num_cpus\": 0.1},\n",
    ")\n",
    "class LLMClient:\n",
    "    def __init__(self):\n",
    "        # Initialize a client to perform API requests\n",
    "        self.client = openai.OpenAI(\n",
    "            base_url=ANYSCALE_SERVICE_BASE_URL,\n",
    "            api_key=ANYSCALE_API_KEY,\n",
    "        )\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        user_prompt: str,\n",
    "        model=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "        temperature: float = 0,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatCompletion:\n",
    "        \"\"\"Generate a completion from the given user prompt.\"\"\"\n",
    "        # Call the chat completions endpoint\n",
    "        chat_completion = self.client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                # Prime the system with a system message - a common best practice\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                # Send the user message with the proper \"user\" role and \"content\"\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        return chat_completion\n",
    "\n",
    "\n",
    "llm_client = LLMClient.bind()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36142cb0",
   "metadata": {},
   "source": [
    "Note we are currently making use of an already deployed open-source LLM running on Anyscale.\n",
    "\n",
    "In case you want to deploy your own LLM, you can follow this [ready-built Anyscale Deploy LLMs template](https://console.anyscale.com/v2/template-preview/endpoints_v2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22be87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_client_handle = serve.run(llm_client, route_prefix=\"/llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adde280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_response = await llm_client_handle.generate.remote( \n",
    "    user_prompt=\"What is the capital of France?\",\n",
    ")\n",
    "llm_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcab6183",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "Given a user query we will want our RAG based QA engine to perform the following steps:\n",
    "\n",
    "1. Retrieve the closest documents to the query\n",
    "2. Augment the query with the context\n",
    "3. Generate a response to the augmented query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34304b2f",
   "metadata": {},
   "source": [
    "We decide on a simple prompt template to augment the user's query with the retrieved context. The template is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d24422",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_rag = \"\"\"\n",
    "Given the following context:\n",
    "{composed_context}\n",
    "\n",
    "Answer the following question:\n",
    "{query}\n",
    "\n",
    "If you cannot provide an answer based on the context, please say \"I don't know.\"\n",
    "Do not use the term \"context\" in your response.\"\"\"\n",
    "\n",
    "\n",
    "def augment_prompt(query: str, composed_context: str) -> str:\n",
    "    \"\"\"Augment the prompt with the given query and contexts.\"\"\"\n",
    "    return prompt_template_rag.format(composed_context=composed_context, query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e67b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_prompt = augment_prompt(\n",
    "    \"How can I deploy Ray Serve to Kubernetes?\",\n",
    "    retrieval_response[\"composed_context\"],\n",
    ")\n",
    "print(augmented_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e56f6a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-secondary\">\n",
    "\n",
    "**Considerations for building a prompt-template for RAG:**\n",
    "\n",
    "Prompt engineering techniques can be used need to be purpose built for the usecase and chosen model. For example, if you want the model to still use its own knowledge in certain cases, you might want to use a different prompt template than if you want the model to only use the retrieved context.\n",
    "\n",
    "For comparison, here are the links to popular third-party library prompt templates which are fairly generic in nature:\n",
    "- [LangChain's default RAG prompt template](https://smith.langchain.com/hub/rlm/rag-prompt)\n",
    "- [LlamaIndex's RAG prompt template](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/prompts/default_prompts.py#L99)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0058b28",
   "metadata": {},
   "source": [
    "We follow a similar pattern and wrap the `QA` engine with a `serve.deployment` decorator. We update all calls to the retriever and generator to use the respective `remote` calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63931704",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(autoscaling_config=dict(min_replicas=1, max_replicas=3))\n",
    "class QA:\n",
    "    def __init__(self, retriever, llm_client):\n",
    "        self.retriever = retriever\n",
    "        self.llm_client = llm_client\n",
    "\n",
    "    async def answer(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int,\n",
    "        include_sources: bool = True,\n",
    "    ):\n",
    "        \"\"\"Answer the given question and provide sources.\"\"\"\n",
    "        retrieval_response = await self.retriever.retrieve.remote(\n",
    "            query=query,\n",
    "            top_k=top_k,\n",
    "        )\n",
    "        prompt = augment_prompt(query, retrieval_response[\"composed_context\"])\n",
    "        llm_response = await self.llm_client.generate.remote(user_prompt=prompt)\n",
    "        response = llm_response.choices[0].message.content\n",
    "\n",
    "        if include_sources:\n",
    "            response += \"\\n\" * 2\n",
    "            sources_str = \"\\n\".join(set(retrieval_response[\"sources\"]))\n",
    "            response += sources_str\n",
    "            response += \"\\n\"\n",
    "\n",
    "        return response\n",
    "\n",
    "\n",
    "qa_engine = QA.bind(\n",
    "    retriever=retriever,\n",
    "    llm_client=llm_client,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23af113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_handle = serve.run(qa_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154f06dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_response = await qa_handle.answer.remote(\n",
    "    query=\"How can I deploy Ray Serve to Kubernetes?\",\n",
    "    top_k=3,\n",
    "    include_sources=True,\n",
    ")\n",
    "print(qa_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68aae16",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "With Ray and Anyscale, we can easily deploy complex applications with multiple components.\n",
    "\n",
    "Ray Serve is:\n",
    "* **Flexible:** unlike other ML based serving platforms, Ray Serve is general purpose and allows for implementing complex logic which is almost always the case for production settings where multiple models need to be composed.\n",
    "* **Lightweight:** Much simpler than a micro-services set up where each service has to be containerized - doesn't require additional tooling enabling a simple python native approach to deploying apps\n",
    "* Offers **intuitive autoscaling** configuration instead of using proxies like CPU and network utilization.\n",
    "* Enables **fractional resource allocation**: allows for efficient resource utilization by allowing for fractional resource allocation to each replica.\n",
    "\n",
    "The Anyscale Platform allows us to deploy Ray serve applications with ease. It offers:\n",
    "* **Canary deployments**: to test new versions of the model\n",
    "* **Versioned Rollouts/Rollbacks** to manage deployments\n",
    "* **Replica compaction**: to reduce the number of replicas in a deployment\n",
    "\n",
    "To learn how to deploy an anyscale service, you can refer to the [Anyscale Services documentation](https://docs.anyscale.com/platform/services/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c287c8e",
   "metadata": {},
   "source": [
    "## Bonus: Adding HTTP Ingress\n",
    "\n",
    "FastAPI is a modern web framework for building APIs.\n",
    "\n",
    "Ray Serve offers an integration with FastAPI to easily expose Ray Serve deployments as HTTP endpoints and get benefits like request validation, OpenAPI documentation, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bee09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "@serve.deployment(autoscaling_config=dict(min_replicas=1, max_replicas=3))\n",
    "@serve.ingress(app)\n",
    "class QAGateway:\n",
    "    def __init__(self, qa_engine):\n",
    "        self.qa_engine = qa_engine\n",
    "\n",
    "    @app.get(\"/answer\")\n",
    "    async def answer(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 3,\n",
    "        include_sources: bool = True,\n",
    "    ):\n",
    "        return await self.qa_engine.answer.remote(\n",
    "            query=query,\n",
    "            top_k=top_k,\n",
    "            include_sources=include_sources,\n",
    "        )\n",
    "\n",
    "gateway = QAGateway.bind(qa_engine=qa_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0befdcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gateway_handle = serve.run(gateway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f525e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    query=\"How can I deploy Ray Serve to Kubernetes?\",\n",
    "    top_k=3,\n",
    ")\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/answer\", params=params)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7167a477",
   "metadata": {},
   "source": [
    "## Bonus: Streaming Responses\n",
    "\n",
    "Assuming we want to stream directly from our client, we can use the `StreamingResponse` from FastAPI to stream the response as it is generated.\n",
    "\n",
    "We first simplify to only deploy the LLM client and then stream the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a49cc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "@serve.deployment(\n",
    "    ray_actor_options={\"num_cpus\": 0.1},\n",
    ")\n",
    "@serve.ingress(app)\n",
    "class LLMClient:\n",
    "    def __init__(self):\n",
    "        # Initialize a client to perform API requests\n",
    "        self.client = openai.OpenAI(\n",
    "            base_url=ANYSCALE_SERVICE_BASE_URL,\n",
    "            api_key=ANYSCALE_API_KEY,\n",
    "        )\n",
    "    \n",
    "    @app.get(\"/generate\")\n",
    "    async def generate(\n",
    "        self,\n",
    "        user_prompt: str,\n",
    "        model: str = \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "        temperature: float = 0,\n",
    "    ) -> ChatCompletion:\n",
    "        \"\"\"Generate a completion from the given user prompt.\"\"\"\n",
    "        return StreamingResponse(\n",
    "            self._generate(\n",
    "                user_prompt=user_prompt, model=model, temperature=temperature\n",
    "            ),\n",
    "            media_type=\"text/event-stream\",\n",
    "        )\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        user_prompt: str,\n",
    "        model: str,\n",
    "        temperature: float,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[str]:\n",
    "        \"\"\"Generate a completion from the given user prompt.\"\"\"\n",
    "        # Call the chat completions endpoint\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                # Prime the system with a system message - a common best practice\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                # Send the user message with the proper \"user\" role and \"content\"\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            stream=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        for chunk in response:\n",
    "            choice = chunk.choices[0]\n",
    "            if choice.delta.content is None:\n",
    "                continue\n",
    "            yield choice.delta.content\n",
    "\n",
    "llm_client = LLMClient.bind()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c899089",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_client_handle = serve.run(llm_client, name=\"streaming-llm\", route_prefix=\"/stream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b131e056",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    user_prompt=\"What is the capital of France?\",\n",
    ")\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/stream/generate\", stream=True, params=params)\n",
    "for chunk in response.iter_content(chunk_size=None, decode_unicode=True):\n",
    "    print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9e787d",
   "metadata": {},
   "source": [
    "Next, we update the QA deployment to use the streaming LLM client.\n",
    "\n",
    "We start out by re-defining the `LLMClient`, this time just stripping the ingress decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e56cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(\n",
    "    ray_actor_options={\"num_cpus\": 0.1},\n",
    ")\n",
    "class LLMClient:\n",
    "    def __init__(self):\n",
    "        # Initialize a client to perform API requests\n",
    "        self.client = openai.OpenAI(\n",
    "            base_url=ANYSCALE_SERVICE_BASE_URL,\n",
    "            api_key=ANYSCALE_API_KEY,\n",
    "        )\n",
    "    \n",
    "    async def generate(\n",
    "        self,\n",
    "        user_prompt: str,\n",
    "        model: str = \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "        temperature: float = 0,\n",
    "    ) -> Iterator[str]:\n",
    "        \"\"\"Generate a completion from the given user prompt.\"\"\"\n",
    "        # Call the chat completions endpoint\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                # Prime the system with a system message - a common best practice\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                # Send the user message with the proper \"user\" role and \"content\"\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            stream=True,\n",
    "        )\n",
    "\n",
    "        for chunk in response:\n",
    "            choice = chunk.choices[0]\n",
    "            if choice.delta.content is None:\n",
    "                continue\n",
    "            yield choice.delta.content\n",
    "\n",
    "llm_client = LLMClient.bind()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a52743",
   "metadata": {},
   "source": [
    "Next, we'll update the QA deployment to use the streaming LLM client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81e863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(autoscaling_config=dict(min_replicas=1, max_replicas=3))\n",
    "@serve.ingress(app)\n",
    "class QA:\n",
    "    def __init__(self, retriever, llm_client):\n",
    "        self.retriever = retriever\n",
    "        # Enable streaming on the deployment handle\n",
    "        self.llm_client = llm_client.options(stream=True)\n",
    "\n",
    "    @app.get(\"/answer\")\n",
    "    async def answer(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int,\n",
    "        include_sources: bool = True,\n",
    "    ):\n",
    "        return StreamingResponse(\n",
    "            self._answer(\n",
    "                query=query,\n",
    "                top_k=top_k,\n",
    "                include_sources=include_sources,\n",
    "            ),\n",
    "            media_type=\"text/event-stream\",\n",
    "        )\n",
    "\n",
    "    async def _answer(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int,\n",
    "        include_sources: bool = True,\n",
    "    ) -> Iterator[str]:\n",
    "        \"\"\"Answer the given question and provide sources.\"\"\"\n",
    "        retrieval_response = await self.retriever.retrieve.remote(\n",
    "            query=query,\n",
    "            top_k=top_k,\n",
    "        )\n",
    "        prompt = augment_prompt(query, retrieval_response[\"composed_context\"])\n",
    "\n",
    "        # async for instead of await\n",
    "        async for chunk in self.llm_client.generate.remote(user_prompt=prompt):\n",
    "            yield chunk\n",
    "\n",
    "        if include_sources:\n",
    "            yield \"\\n\" * 2\n",
    "            sources_str = \"\\n\".join(set(retrieval_response[\"sources\"]))\n",
    "            yield sources_str\n",
    "            yield \"\\n\"\n",
    "\n",
    "\n",
    "qa_client = QA.bind(retriever=retriever, llm_client=llm_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7f8cbd",
   "metadata": {},
   "source": [
    "Note, we left out the gateway to reduce the complexity of the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cfa908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we shutdown the existing QA deployment\n",
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8962f26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_client_handle = serve.run(qa_client, name=\"streaming-qa\", route_prefix=\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe0986a",
   "metadata": {},
   "source": [
    "Let's request the streaming QA service in streaming mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e70e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    query=query,\n",
    "    top_k=3,\n",
    ")\n",
    "\n",
    "response = requests.get(\n",
    "    \"http://localhost:8000/answer\", stream=True, params=params\n",
    ")\n",
    "for chunk in response.iter_content(chunk_size=None, decode_unicode=True):\n",
    "    print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b662661",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "We shutdown the existing QA deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe984d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
