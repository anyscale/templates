{"text":"\n\n\nConfiguring Environments#\n\n\nYou can pass either a string name or a Python class to specify an environment. By default, strings will be interpreted as a gym environment name.\nCustom env classes passed directly to the algorithm must take a single env_config parameter in their constructor:\n\n\nimport gymnasium as gym\nimport ray\nfrom ray.rllib.algorithms import ppo\n\nclass MyEnv(gym.Env):\n    def __init__(self, env_config):\n        self.action_space = <gym.Space>\n        self.observation_space = <gym.Space>\n    def reset(self, seed, options):\n        return <obs>, <info>\n    def step(self, action):\n        return <obs>, <reward: float>, <terminated: bool>, <truncated: bool>, <info: dict>\n\nray.init()\nalgo = ppo.PPO(env=MyEnv, config={\n    \"env_config\": {},  # config to pass to env class\n})\n\nwhile True:\n    print(algo.train())\n\n\n\n\nYou can also register a custom env creator function with a string name. This function must take a single env_config (dict) parameter and return an env instance:\n\n\nfrom ray.tune.registry import register_env\n\ndef env_creator(env_config):\n    return MyEnv(...)  # return an env instance\n\nregister_env(\"my_env\", env_creator)\nalgo = ppo.PPO(env=\"my_env\")\n\n\n\n\nFor a full runnable code example using the custom environment API, see custom_env.py.\n\n\n\nWarning\nThe gymnasium registry is not compatible with Ray. Instead, always use the registration flows documented above to ensure Ray workers can access the environment.\n\n\n\nIn the above example, note that the env_creator function takes in an env_config object.\nThis is a dict containing options passed in through your algorithm.\nYou can also access env_config.worker_index and env_config.vector_index to get the worker id and env id within the worker (if num_envs_per_worker > 0).\nThis can be useful if you want to train over an ensemble of different environments, for example:\n\n\nclass MultiEnv(gym.Env):\n    def __init__(self, env_config):\n        # pick actual env based on worker and env indexes\n        self.env = gym.make(\n            choose_env_for(env_config.worker_index, env_config.vector_index))\n        self.action_space = self.env.action_space\n        self.observation_space = self.env.observation_space\n    def reset(self, seed, options):\n        return self.env.reset(seed, options)\n    def step(self, action):\n        return self.env.step(action)\n\nregister_env(\"multienv\", lambda config: MultiEnv(config))\n\n\n\n\n\nTip\nWhen using logging in an environment, the logging configuration needs to be done inside the environment, which runs inside Ray workers. Any configurations outside the environment, e.g., before starting Ray will be ignored.\n\n\n","section_url":"https:\/\/docs.ray.io\/en\/master\/rllib-env.html#configuring-environments","page_url":"https:\/\/docs.ray.io\/en\/master\/rllib-env.html"}
{"text":"\n\nGymnasium#\n\n\nRLlib uses Gymnasium as its environment interface for single-agent training. For more information on how to implement a custom Gymnasium environment, see the gymnasium.Env class definition. You may find the SimpleCorridor example useful as a reference.\n\n\n\n\n\n","section_url":"https:\/\/docs.ray.io\/en\/master\/rllib-env.html#gymnasium","page_url":"https:\/\/docs.ray.io\/en\/master\/rllib-env.html"}
{"text":"\n\nPerformance#\n\n\n\nTip\nAlso check out the scaling guide for RLlib training.\n\n\n\nThere are two ways to scale experience collection with Gym environments:\n\n\n\n\nVectorization within a single process: Though many envs can achieve high frame rates per core, their throughput is limited in practice by policy evaluation between steps. For example, even small TensorFlow models incur a couple milliseconds of latency to evaluate. This can be worked around by creating multiple envs per process and batching policy evaluations across these envs.\n\n\nYou can configure {\"num_envs_per_worker\": M} to have RLlib create M concurrent environments per worker. RLlib auto-vectorizes Gym environments via VectorEnv.wrap().\n\n\nDistribute across multiple processes: You can also have RLlib create multiple processes (Ray actors) for experience collection. In most algorithms this can be controlled by setting the {\"num_workers\": N} config.\n\n\n\n\n\n\n\nYou can also combine vectorization and distributed execution, as shown in the above figure. Here we plot just the throughput of RLlib policy evaluation from 1 to 128 CPUs. PongNoFrameskip-v4 on GPU scales from 2.4k to \u223c200k actions\/s, and Pendulum-v1 on CPU from 15k to 1.5M actions\/s. One machine was used for 1-16 workers, and a Ray cluster of four machines for 32-128 workers. Each worker was configured with num_envs_per_worker=64.\n\n","section_url":"https:\/\/docs.ray.io\/en\/master\/rllib-env.html#performance","page_url":"https:\/\/docs.ray.io\/en\/master\/rllib-env.html"}
{"text":"\n\nExpensive Environments#\n\n\nSome environments may be very resource-intensive to create. RLlib will create num_workers + 1 copies of the environment since one copy is needed for the driver process. To avoid paying the extra overhead of the driver copy, which is needed to access the env\u2019s action and observation spaces, you can defer environment initialization until reset() is called.\n\n","section_url":"https:\/\/docs.ray.io\/en\/master\/rllib-env.html#expensive-environments","page_url":"https:\/\/docs.ray.io\/en\/master\/rllib-env.html"}
