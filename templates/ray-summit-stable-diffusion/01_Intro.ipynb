{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Stable Diffusion and Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a gentle introduction to using Stable Diffusion and Ray\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Here is the roadmap for this notebook:</b>\n",
    "<ul>\n",
    "    <li><b>Part 1:</b> A simple data pipeline</li>\n",
    "    <li><b>Part 2:</b> Introduction to Ray Data</li>\n",
    "    <li><b>Part 3:</b> Batch Inference with Stable Diffusion</li>\n",
    "    <li><b>Part 4:</b> Stable Diffusion under the hood</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import json\n",
    "from typing import Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ray\n",
    "import torch\n",
    "from art import text2art\n",
    "from diffusers import DiffusionPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple data pipeline\n",
    "\n",
    "Let's begin with a very simple data pipeline which converts text into ASCII art. \n",
    "\n",
    "We start with a simple dataset of items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [\n",
    "    \"Astronaut\", \"Cat\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then apply a transformation to each item in the dataset to convert the text into ASCII art:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def artify(item: str) -> str:\n",
    "    return text2art(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will sequentially apply the `artify` function to each item in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for item in items:\n",
    "    data.append({\"prompt\": item, \"art\": artify(item)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0][\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0][\"art\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can write the data to a JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ascii_art.json\", \"w\") as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to Ray Data\n",
    "\n",
    "<!-- One liner about Ray Data -->\n",
    "Ray Data is a scalable data processing library for ML workloads, particularly suited for the following workloads:\n",
    "\n",
    "\n",
    "<!-- Diagram showing streaming and heterogenous cluster -->\n",
    "Ray Data is particularly useful for streaming data on a heterogenous cluster:\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/stream-example.png\" width=\"600\">\n",
    "\n",
    "Your production pipeline for generating images from text could require:\n",
    "1. Loading a large number of text prompts\n",
    "2. Generating images using large scale diffusion models\n",
    "3. Inferencing against guardrail models to remove low-quality and NSFW images\n",
    "\n",
    "You will want to make the most efficient use of your cluster to process this data. Ray Data can help you do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray Data's API\n",
    "\n",
    "Here are the steps to make use of Ray Data:\n",
    "1. Create a Ray Dataset usually by pointing to a data source.\n",
    "2. Apply transformations to the Ray Dataset.\n",
    "3. Write out the results to a data source.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data\n",
    "\n",
    "Ray Data has a number of [IO connectors](https://docs.ray.io/en/latest/data/api/input_output.html) to most commonly used formats.\n",
    "\n",
    "For purposes of this introduction, we will use the `from_items` function to create a dataset from a list of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_items = ray.data.from_items(items)\n",
    "ds_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Data\n",
    "\n",
    "Datasets can be transformed by applying a row-wise `map` operation. We do this by providing a user-defined function that takes a row as input and returns a row as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def artify_row(row: dict[str, Any]) -> dict[str, Any]:\n",
    "    row[\"art\"] = text2art(row[\"item\"])\n",
    "    return row\n",
    "\n",
    "ds_items_artified = ds_items.map(artify_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lazy execution\n",
    "\n",
    "By default, `map` is lazy, meaning that it will not actually execute the function until you consume it. This allows for optimizations like pipelining and fusing of operations.\n",
    "\n",
    "To inspect a few rows of the dataset, you can use the `take` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = ds_items_artified.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample[0][\"item\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample[0][\"art\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Data\n",
    "\n",
    "We can then write out the data to disk using the avialable [IO connector methods](https://docs.ray.io/en/latest/data/api/input_output.html).\n",
    "\n",
    "Here we will write the data to a JSON file to a shared storage location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_items_artified.write_json(\"/mnt/cluster_storage/ascii_art\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect the written files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /mnt/cluster_storage/ascii_art"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap of our Ray Data pipeline\n",
    "\n",
    "Here is our Ray data pipeline condensed into the following chained operations:\n",
    "\n",
    "```python\n",
    "(\n",
    "    ray.data.from_items(items)\n",
    "    .map(artify_row)\n",
    "    .write_json(\"/mnt/cluster_storage/ascii_art\")\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Inference with Stable Diffusion\n",
    "\n",
    "Now that we have a simple data pipeline, let's use Stable Diffusion to generate actual images from text.\n",
    "\n",
    "This will follow a very similar pattern. Let's say we are starting out with the following prompts:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"An astronaut on a horse\",\n",
    "    \"A cat with a jetpack\",\n",
    "] * 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a Ray Dataset from the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_prompts = ray.data.from_items(prompts)\n",
    "ds_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply want to apply a DiffusionPipeline to the dataset. \n",
    "\n",
    "We first define a function that creates and applies the pipeline to a single row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stable_diffusion(row: dict[str, Any]) -> dict[str, Any]:\n",
    "    # Create the stable diffusion pipeline\n",
    "    pipe = DiffusionPipeline.from_pretrained(\n",
    "        pretrained_model_name_or_path=\"stabilityai/stable-diffusion-2\",\n",
    "        torch_dtype=torch.float16,\n",
    "        use_safetensors=True,\n",
    "        variant=\"fp16\",\n",
    "    ).to(\"cuda\")\n",
    "    prompt = row[\"item\"]\n",
    "    # Apply the pipeline to the prompt\n",
    "    output = pipe(prompt, height=512, width=512)\n",
    "    # Extract the image from the output and construct the row\n",
    "    return {\"item\": prompt, \"image\": output.images[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply the function to each row in the dataset using the `map` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_images_generated_mapping_by_row = ds_prompts.map(\n",
    "    apply_stable_diffusion,\n",
    "    num_gpus=1, # specify the number of GPUs per task\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of parallelizing the inference per row, we can parallelize the inference per batch.\n",
    "\n",
    "Mapping over batches instead of rows is useful when we can benefit from vectorized operations on the batch level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stable_diffusion_batch(batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "    pipe = DiffusionPipeline.from_pretrained(\n",
    "        pretrained_model_name_or_path=\"stabilityai/stable-diffusion-2\",\n",
    "        torch_dtype=torch.float16,\n",
    "        use_safetensors=True,\n",
    "        variant=\"fp16\",\n",
    "    ).to(\"cuda\")\n",
    "    # Extract the prompts from the batch\n",
    "    prompts = batch[\"item\"].tolist()\n",
    "    # Apply the pipeline to the prompts\n",
    "    outputs = pipe(prompts, height=512, width=512)\n",
    "    # Extract the images from the outputs and construct the batch\n",
    "    return {\"item\": prompts, \"image\": outputs.images}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the function to each batch in the dataset using the `map_batches` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_images_generated_mapping_by_batch = ds_prompts.map_batches(\n",
    "    apply_stable_diffusion_batch,\n",
    "    batch_size=24, # specify the batch size per task to maximize GPU utilization\n",
    "    num_gpus=1, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current implementation requires us to load the pipeline for each batch we process.\n",
    "\n",
    "We can avoid reloading the pipeline for each batch by creating a stateful transformation, implemented as a callable class where:\n",
    "- `__init__`: initializes worker processes that will load the pipeline once and reuse it for transforming each batch.\n",
    "- `__call__`: applies the pipeline to the batch and returns the transformed batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StableDiffusion:\n",
    "    def __init__(self, model_id: str = \"stabilityai/stable-diffusion-2\") -> None:\n",
    "        self.pipe = DiffusionPipeline.from_pretrained(\n",
    "            model_id, torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\"\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "    def __call__(\n",
    "        self, batch: dict[str, np.ndarray], img_size: int = 512\n",
    "    ) -> dict[str, np.ndarray]:\n",
    "        prompts = batch[\"item\"].tolist()\n",
    "        batch[\"image\"] = self.pipe(prompts, height=img_size, width=img_size).images\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply the class to each batch in the dataset using the same `map_batches` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_images_generated_by_stateful_transform = ds_prompts.map_batches(\n",
    "    StableDiffusion,\n",
    "    batch_size=24,\n",
    "    num_gpus=1,  \n",
    "    concurrency=1,  # number of workers to launch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Activity: Visualize the generated images\n",
    "\n",
    "Lets fetch a batch of the generated images to the driver and visualize them.\n",
    "\n",
    "Use the `plot_images` function to visualize the images.\n",
    "\n",
    "```python\n",
    "def plot_images(batch: dict[str, np.ndarray]) -> None:\n",
    "    for item, image in zip(batch[\"item\"], batch[\"image\"]):\n",
    "        plt.imshow(image)\n",
    "        plt.title(item)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "# Hint: Implement the code below to fetch a batch from \n",
    "# ds_images_generated_by_stateful_transform\n",
    "batch = ...\n",
    "plot_images(batch)\n",
    "```\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<details>\n",
    "\n",
    "<summary>Click to expand/collapse</summary>\n",
    "\n",
    "```python\n",
    "def plot_images(batch: dict[str, np.ndarray]) -> None:\n",
    "    for item, image in zip(batch[\"item\"], batch[\"image\"]):\n",
    "        plt.imshow(image)\n",
    "        plt.title(item)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "size = 12\n",
    "batch = ds_images_generated.take_batch(batch_size=size)\n",
    "plot_images(batch)\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading/Writing to a Data Lake\n",
    "\n",
    "In a production setting, you will be building a Ray Dataset lazily by reading from a data source like a Data Lake (S3, GCS, HDFS, etc). \n",
    "\n",
    "To do so, let's make use of the artifact path that Anyscale provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid_str = str(uuid.uuid4())\n",
    "artifact_path = f\"/mnt/cluster_storage/stable-diffusion/{uuid_str}\"\n",
    "artifact_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start out by writing the prompts to a JSON directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_prompts.write_json(artifact_path + \"/prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the written files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {artifact_path}/prompts/ --human-readable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here is how the pipeline would look like if we want to read the prompts from S3, generate images and store the images back to S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ray.data.read_json(artifact_path + \"/prompts\")\n",
    "    .map_batches(StableDiffusion, batch_size=24, num_gpus=1, concurrency=1)\n",
    "    .write_parquet(artifact_path + \"/images\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<b>Note</b> how there is no need to explicitly materialize the dataset, instead the data will get streamed through the pipeline and written to the specified location. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {artifact_path}/images/ --human-readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stable Diffusion pipeline components\n",
    "\n",
    "Let's take a quick look at the components of the Stable Diffusion pipeline.\n",
    "\n",
    "First we load the pipeline on our local workspace node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"stabilityai/stable-diffusion-2\"\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    model_id, torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the text tokenizer and encoder shows how the text will be preprocessed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pipeline.tokenizer), type(pipeline.text_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the feature extractor and VAE shows how the images will be preprocessed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pipeline.feature_extractor), type(pipeline.vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our main model that predicts the noise level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pipeline.unet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the U-net will be used to predict which part of the image is noise, a scheduler needs to be used to sample the noise level.\n",
    "\n",
    "By default, diffusers will use the following scheduler, but other schedulers can be used as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pipeline.scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the inference data flow of the Stable Diffusion model simplified for generating an image of \"A person half Yoda and half Gandalf\":\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://www.paepper.com/blog/posts/everything-you-need-to-know-about-stable-diffusion/stable-diffusion-inference.png\" alt=\"Inference data flow of Stable Diffusion\" width=\"800\"/>\n",
    "  <figcaption>Image taken from <a href=\"https://www.paepper.com/blog/posts/everything-you-need-to-know-about-stable-diffusion/\">Everything you need to know about stable diffusion</a>\n",
    "</figcaption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /mnt/cluster_storage/ascii_art\n",
    "!rm -rf {artifact_path}\n",
    "!rm ascii_art.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
