{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beda561e",
   "metadata": {},
   "source": [
    "# Pre-processing for Stable Diffusion V2\n",
    "\n",
    "Let's build a scalable preprocessing pipeline for the Stable Diffusion V2 model.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Here is the roadmap for this notebook:</b>\n",
    "<ul>\n",
    "    <li><b>Part 0:</b> High-level overview of the preprocessing pipeline</li>\n",
    "    <li><b>Part 1:</b> Reading in the data</li>\n",
    "    <li><b>Part 2:</b> Transforming images and captions</li>\n",
    "    <li><b>Part 3:</b> Encoding of images and captions</li>\n",
    "    <li><b>Part 4:</b> Writing out the preprocessed data</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583b9839",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2459683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import uuid\n",
    "import io\n",
    "import logging\n",
    "from typing import Optional, Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyarrow as pa  # type: ignore\n",
    "import ray.data\n",
    "import torch\n",
    "import torchvision  # type: ignore\n",
    "from diffusers.models import AutoencoderKL\n",
    "from PIL import Image\n",
    "from transformers import CLIPTextModel, CLIPTokenizer  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec837f75-fefd-42c5-b614-e435f8de7432",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0. High-level overview of the preprocessing pipeline\n",
    "\n",
    "Here is a high-level overview of the preprocessing pipeline:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/stable-diffusion/preprocessing_architecture_v4.jpeg\" width=\"900px\">\n",
    "\n",
    "Ray Data loads the data from a remote storage system, then streams the data through two processing main stages:\n",
    "1. **Transformation**\n",
    "   1. Cropping and normalizing images.\n",
    "   2. Tokenizing the text captions using a CLIP tokenizer.\n",
    "2. **Encoding**\n",
    "   1. Compressing images into a latent space using a VAE encoder.\n",
    "   2. Generating text embeddings using a CLIP model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2e8517-65f9-4c41-9144-57df326f6c02",
   "metadata": {},
   "source": [
    "### 1. Reading in the data\n",
    "\n",
    "We're going to preprocess part of the LAION-art-8M dataset. To save time, we have provided a sample of the dataset on S3.\n",
    "\n",
    "We'll read this sample data and create a Ray dataset from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3c85d9-1e00-492d-b119-fca467a42f8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema = pa.schema(\n",
    "    [\n",
    "        pa.field(\"caption\", getattr(pa, \"string\")()),\n",
    "        pa.field(\"height\", getattr(pa, \"float64\")()),\n",
    "        pa.field(\"width\", getattr(pa, \"float64\")()),\n",
    "        pa.field(\"jpg\", getattr(pa, \"binary\")()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "ds = ray.data.read_parquet(\n",
    "    \"s3://anyscale-public-materials/ray-summit/stable-diffusion/data/raw/\",\n",
    "    schema=schema,\n",
    ")\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa9aad4-f273-4e8c-9fcf-8346fbff09a8",
   "metadata": {},
   "source": [
    "We know that when we run that step, we're not actually processing the whole dataset -- that's the whole idea behind lazy execution of the data pipeline.\n",
    "\n",
    "But Ray does sample the data to determine metadata like the number of files and data schema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101b2899-13ae-4063-8dab-fb9e47b51e5c",
   "metadata": {},
   "source": [
    "### 2. Transforming images and captions\n",
    "\n",
    "#### 2.1 Cropping and normalizing images\n",
    "We start by preprocessing the images: \n",
    "\n",
    "We need to perform these two operations on the images:\n",
    "1. Crop the images to a square aspect ratio.\n",
    "2. Normalize the pixel values to the distribution expected by the VAE encoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2c77f4",
   "metadata": {},
   "source": [
    "#### Step 1. Cropping the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d56d06-0546-4e40-9107-bb683e43fe95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LargestCenterSquare:\n",
    "    \"\"\"Largest center square crop for images.\"\"\"\n",
    "\n",
    "    def __init__(self, size: int) -> None:\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, img: Image.Image) -> Image.Image:\n",
    "        \"\"\"Crop the largest center square from an image.\"\"\"\n",
    "        # First, resize the image such that the smallest\n",
    "        # side is self.size while preserving aspect ratio.\n",
    "        img = torchvision.transforms.functional.resize(\n",
    "            img=img,\n",
    "            size=self.size,\n",
    "        )\n",
    "\n",
    "        # Then take a center crop to a square.\n",
    "        w, h = img.size\n",
    "        c_top = (h - self.size) // 2\n",
    "        c_left = (w - self.size) // 2\n",
    "        img = torchvision.transforms.functional.crop(\n",
    "            img=img,\n",
    "            top=c_top,\n",
    "            left=c_left,\n",
    "            height=self.size,\n",
    "            width=self.size,\n",
    "        )\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55618e3f-f60f-456d-ab0c-edabca9c87b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resolution = 512\n",
    "crop = LargestCenterSquare(resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f1eabd",
   "metadata": {},
   "source": [
    "Let's take a simple example to understand visualize how the crop function works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920fec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_example = ds.filter(lambda row: row[\"caption\"] == 'strawberry-lemonmousse-cake-3')\n",
    "example_image = ds_example.take(1)[0]\n",
    "image = Image.open(io.BytesIO(example_image[\"jpg\"]))\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cf17c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cee5af",
   "metadata": {},
   "source": [
    "#### Step 2. Normalizing the image\n",
    "\n",
    "We need to normalize the pixel values to the distribution expected by the VAE encoder. \n",
    "\n",
    "The VAE encoder expects pixel values in the range [-1, 1]\n",
    "\n",
    "Our images are in the range [0, 1] with an approximate mean of 0.5 in the center. \n",
    "\n",
    "To normalize the images, we'll subtract 0.5 from each pixel value and divide by 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f54b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d34b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = normalize(crop(image))\n",
    "\n",
    "normalized.min(), normalized.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0961204b",
   "metadata": {},
   "source": [
    "#### Putting it together into a single transform function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cef17d",
   "metadata": {},
   "source": [
    "We build a `transform_images` below to crop and normalize the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27d0a0d-260e-4782-bd7d-848228147cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tensor_to_array(tensor: torch.Tensor, dtype=np.float32) -> np.ndarray:\n",
    "    \"\"\"Convert a torch tensor to a numpy array.\"\"\"\n",
    "    array = tensor.detach().cpu().numpy()\n",
    "    return array.astype(dtype)\n",
    "\n",
    "\n",
    "def transform_images(row: dict[str, Any]) -> np.ndarray:\n",
    "    \"\"\"Transform image to a square-sized normalized tensor.\"\"\"\n",
    "    try:\n",
    "        image = Image.open(io.BytesIO(row[\"jpg\"]))\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error opening image: {e}\")\n",
    "        return []\n",
    "\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "    image = crop(image)\n",
    "    normalized_image_tensor = normalize(image)\n",
    "\n",
    "    row[f\"image_{resolution}\"] = convert_tensor_to_array(normalized_image_tensor)\n",
    "    return [row]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780d98a8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Note how we reference `crop` and `normalize` functions in the `transform_images` function. Those outer-scope objects are serialized and shipped along with the remote function definition.\n",
    "\n",
    "In this case, they are tiny, but in other cases -- say, we have a 16GB model we're referencing -- we would not want to rely on this scope behavior but would want to use other mechanisms to make those objects availabe to the workers.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28973d35",
   "metadata": {},
   "source": [
    "Now we call `flat_map` to apply the `transform_images` function to each row in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097ebdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_img_transformed = ds.flat_map(transform_images)\n",
    "\n",
    "ds_img_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b279cd81-1ad5-4c3d-8d83-393f3dcca767",
   "metadata": {
    "tags": []
   },
   "source": [
    "What happened to our schema?\n",
    "\n",
    "* `flat_map` is purely lazy ... applying didn't physically process any data at all, and since `flat_map` might have changed the schema of the records, Ray doesn't know what the resulting schema is\n",
    "\n",
    "If we want (or need) to inspect this behavior for development or debugging purposes, we can run the pipeline on a small part of the data using `take`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7b6346",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transformed = ds_img_transformed.take(2)[1]\n",
    "(\n",
    "    image_transformed[\"image_512\"].shape,\n",
    "    image_transformed[\"image_512\"].min(),\n",
    "    image_transformed[\"image_512\"].max(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe61534",
   "metadata": {},
   "source": [
    "### 3. Tokenize the text captions\n",
    "\n",
    "Now we'll want to tokenize the text captions using a CLIP tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eee876",
   "metadata": {},
   "source": [
    "Let's load a text tokenizer and inspect its behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9912591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-base\", subfolder=\"tokenizer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b20f409",
   "metadata": {},
   "source": [
    "Let's call the tokenizer on a simple string to get the token ids and tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e844989",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = text_tokenizer(\"strawberry-lemonmousse-cake-3\")[\"input_ids\"]\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a83f7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = text_tokenizer.convert_ids_to_tokens(token_ids)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da404a21",
   "metadata": {},
   "source": [
    "We can now define a function that will tokenize a batch of text captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073e0632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "    \"\"\"Tokenize the caption.\"\"\"\n",
    "    batch[\"caption_ids\"] = text_tokenizer(\n",
    "        batch[\"caption\"].tolist(),\n",
    "        padding=\"max_length\",\n",
    "        max_length=text_tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"np\",\n",
    "    )[\"input_ids\"]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ee2c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_img_txt_transformed = ds_img_transformed.map_batches(tokenize_text)\n",
    "example_txt_transformed = ds_img_txt_transformed.filter(\n",
    "    lambda row: row[\"caption\"] == \"strawberry-lemonmousse-cake-3\"\n",
    ").take(1)[0]\n",
    "print(example_txt_transformed[\"caption\"])\n",
    "example_txt_token_ids = example_txt_transformed[\"caption_ids\"]\n",
    "example_txt_token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedc6833",
   "metadata": {},
   "source": [
    "#### Understanding Ray Data's Operator Fusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cddaf93",
   "metadata": {},
   "source": [
    "Inspecting the execution plan of the dataset so far we see:\n",
    "\n",
    "```\n",
    "Execution plan of Dataset: \n",
    "InputDataBuffer[Input] \n",
    "-> TaskPoolMapOperator[ReadParquet]\n",
    "-> TaskPoolMapOperator[FlatMap(transform_images)->MapBatches(tokenize_text)]\n",
    "-> LimitOperator[limit=2]\n",
    "```\n",
    "\n",
    "Note how `transform_images` and `tokenize_text` functions are fused into a single operator.\n",
    "\n",
    "This is an optimization that Ray Data performs to reduce the number of times we need to serialize and deserialize data between Python processes.\n",
    "\n",
    "If Ray Data did not do this then it would have been advised to construct a `transform_images_and_text` transformation that combines the image and text transformations into a single function to reduce the number of times we need to serialize and deserialize data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfe6dab-3cc2-455e-83d6-f5c634b5d8f4",
   "metadata": {},
   "source": [
    "### 4. Encode images and captions\n",
    "\n",
    "We'll compress images into a latent space using a VAE encoder and generate text embeddings using a CLIP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9fcc5e-1e64-4670-ad39-3af591983eb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SDImageEncoder:\n",
    "    def __init__(self, model_name: str, device: torch.device) -> None:\n",
    "        self.vae = AutoencoderKL.from_pretrained(\n",
    "            model_name,\n",
    "            subfolder=\"vae\",\n",
    "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "        ).to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def encode_images(self, images: np.ndarray) -> np.ndarray:\n",
    "        input_images = torch.tensor(images, device=self.device)\n",
    "        if self.device == \"cuda\":\n",
    "            input_images = input_images.half()\n",
    "        latent_dist = self.vae.encode(input_images)[\"latent_dist\"]\n",
    "        image_latents = latent_dist.sample() * 0.18215\n",
    "        return convert_tensor_to_array(image_latents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a37d934",
   "metadata": {},
   "source": [
    "Let's run the image encoder against the sample image we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ef7ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder = SDImageEncoder(\"stabilityai/stable-diffusion-2-base\", \"cpu\")\n",
    "image_latents = image_encoder.encode_images(transform_images(example_image)[0][\"image_512\"][None])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76ab8c2",
   "metadata": {},
   "source": [
    "Let's plot the image latents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3de5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nchannels = image_latents.shape[0]\n",
    "fig, axes = plt.subplots(1, nchannels, figsize=(10, 10))\n",
    "\n",
    "for idx, ax in enumerate(axes):\n",
    "    ax.imshow(image_latents[idx], cmap=\"gray\")\n",
    "    ax.set_title(f\"Channel {idx}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "fig.suptitle(\"Image Latents\", fontsize=16, x=0.5, y=0.625)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023b5ba8",
   "metadata": {},
   "source": [
    "Next, let's encode the text using the CLIP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e314a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDTextEncoder:\n",
    "    def __init__(self, model_name: str, device: torch.device) -> None:\n",
    "        self.text_encoder = CLIPTextModel.from_pretrained(\n",
    "            model_name,\n",
    "            subfolder=\"text_encoder\",\n",
    "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "        ).to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def encode_text(self, caption_ids: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Encode text captions into a latent space.\"\"\"\n",
    "        caption_ids_tensor = torch.tensor(caption_ids, device=self.device)\n",
    "        caption_latents_tensor = self.text_encoder(caption_ids_tensor)[0]\n",
    "        return convert_tensor_to_array(caption_latents_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a396bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SDTextEncoder(\"stabilityai/stable-diffusion-2-base\", \"cpu\")\n",
    "example_text_embedding = encoder.encode_text([example_txt_token_ids])[0]\n",
    "example_text_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73088e0d",
   "metadata": {},
   "source": [
    "Given Ray Data doesn't support operator fusion between two different stateful transformations, we define a single `SDLatentSpaceEncoder` transformation that is composed of the image and text encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78405e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDLatentSpaceEncoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        resolution: int = 512,\n",
    "        device: Optional[str] = \"cuda\",\n",
    "        model_name: str = \"stabilityai/stable-diffusion-2-base\",\n",
    "    ) -> None:\n",
    "        self.device = torch.device(device)\n",
    "        self.resolution = resolution\n",
    "\n",
    "        # Instantiate image and text encoders\n",
    "        self.image_encoder = SDImageEncoder(model_name, self.device)\n",
    "        self.text_encoder = SDTextEncoder(model_name, self.device)\n",
    "\n",
    "    def __call__(self, batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "        with torch.no_grad():\n",
    "            # Step 1: Encode images.\n",
    "            input_images = batch[f\"image_{self.resolution}\"]\n",
    "            image_latents = self.image_encoder.encode_images(input_images)\n",
    "            batch[f\"image_latents_{self.resolution}\"] = image_latents\n",
    "\n",
    "            del batch[f\"image_{self.resolution}\"]\n",
    "            gc.collect()\n",
    "\n",
    "            # Step 2: Encode captions.\n",
    "            caption_ids = batch[\"caption_ids\"]\n",
    "            batch[\"caption_latents\"] = self.text_encoder.encode_text(caption_ids)\n",
    "\n",
    "            del batch[\"caption_ids\"]\n",
    "            gc.collect()\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8e05b1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<b>Note</b> how we are deleting the original image and caption_ids from the batch to free up memory. This is important when working with large datasets.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc547a90",
   "metadata": {},
   "source": [
    "We apply the encoder to the dataset to encode the images and text captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8c31e3-1289-4f1e-9dbc-e320125cc6a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_encoded = ds_img_txt_transformed.map_batches(\n",
    "    SDLatentSpaceEncoder,\n",
    "    concurrency=2,  # Total number of workers\n",
    "    num_gpus=1,  # number of GPUs per worker\n",
    "    batch_size=24,  # Use the largest batch size that can fit on our GPUs - depends on resolution\n",
    ")\n",
    "\n",
    "ds_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50f3178-3f33-436f-92f5-b2f0c4ad4802",
   "metadata": {},
   "source": [
    "### 5. Write outputs to parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bfdd3c-d8af-4442-8e60-ffe9e12b6918",
   "metadata": {},
   "source": [
    "Finally, we can write the output.\n",
    "\n",
    "We use the artifact store to write the output to a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebceecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid_str = str(uuid.uuid4())\n",
    "artifact_path = f\"/mnt/cluster_storage/stable-diffusion/{uuid_str}\"\n",
    "artifact_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb8de52",
   "metadata": {},
   "source": [
    "This operation requires physically moving the data, so it will trigger scheduling and execution of all of the upstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea15e02d-8f63-4347-af1b-66e064c89045",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_encoded.write_parquet(artifact_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b994a8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
