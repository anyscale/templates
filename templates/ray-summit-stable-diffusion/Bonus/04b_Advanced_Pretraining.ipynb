{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7078ab58-6ca4-4255-8050-b7c5fe7eae1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Distributed Training Optimizations for Stable Diffusion\n",
    "\n",
    "This notebook demonstrates certain optimizations that can be applied to the training process to improve performance and reduce costs.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>Here is the roadmap for this notebook:</b>\n",
    "\n",
    "<ul>\n",
    "    <li>Part 1: Setup</li>\n",
    "    <li>Part 2: Using Fully Sharded Data Parallel (FSDP)</li>\n",
    "    <li>Part 3: Online (end-to-end) preprocessing and training</li>\n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765f5851",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ecea11-fc44-4bc2-af6d-09db4753d78e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from diffusers import DDPMScheduler, UNet2DConditionModel\n",
    "from transformers import PretrainedConfig, get_linear_schedule_with_warmup\n",
    "from lightning.pytorch.utilities.types import OptimizerLRScheduler\n",
    "\n",
    "import ray.train\n",
    "from torch.distributed.fsdp import BackwardPrefetch\n",
    "from ray.train.lightning import RayLightningEnvironment, RayTrainReportCallback, RayFSDPStrategy\n",
    "from ray.train.torch import TorchTrainer, get_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7c50ba",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Let's begin with the same code as in the basic pretraining notebook.\n",
    "\n",
    "We first load the dataset and convert the precision to float16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40392506-334f-4b05-9bb0-f2815daff428",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_precision(batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "    for k, v in batch.items():\n",
    "        batch[k] = v.astype(np.float16)\n",
    "    return batch\n",
    "\n",
    "\n",
    "columns = [\"image_latents_256\", \"caption_latents\"]\n",
    "\n",
    "train_data_uri = (\n",
    "    \"s3://anyscale-public-materials/ray-summit/stable-diffusion/data/preprocessed/256/\"\n",
    ")\n",
    "train_ds = ray.data.read_parquet(train_data_uri, columns=columns, shuffle=\"files\")\n",
    "train_ds = train_ds.map_batches(convert_precision, batch_size=None)\n",
    "\n",
    "ray_datasets = {\"train\": train_ds}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f22554",
   "metadata": {},
   "source": [
    "We then define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bd6050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StableDiffusion(pl.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr: float,\n",
    "        resolution: int,\n",
    "        weight_decay: float,\n",
    "        num_warmup_steps: int,\n",
    "        model_name: str,\n",
    "    ) -> None:\n",
    "        self.lr = lr\n",
    "        self.resolution = resolution\n",
    "        self.weight_decay = weight_decay\n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # Initialize U-Net.\n",
    "        model_config = PretrainedConfig.get_config_dict(model_name, subfolder=\"unet\")[0]\n",
    "        self.unet = UNet2DConditionModel(**model_config)\n",
    "        # Define the training noise scheduler.\n",
    "        self.noise_scheduler = DDPMScheduler.from_pretrained(\n",
    "            model_name, subfolder=\"scheduler\"\n",
    "        )\n",
    "        # Setup loss function.\n",
    "        self.loss_fn = F.mse_loss\n",
    "        self.current_training_steps = 0\n",
    "\n",
    "    def on_fit_start(self) -> None:\n",
    "        \"\"\"Move cumprod tensor to GPU in advance to avoid data movement on each step.\"\"\"\n",
    "        self.noise_scheduler.alphas_cumprod = self.noise_scheduler.alphas_cumprod.to(\n",
    "            get_device()\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, batch: dict[str, torch.Tensor]\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward pass of the model.\"\"\"\n",
    "        # Extract inputs.\n",
    "        latents = batch[\"image_latents_256\"]\n",
    "        conditioning = batch[\"caption_latents\"]\n",
    "        # Sample the diffusion timesteps.\n",
    "        timesteps = self._sample_timesteps(latents)\n",
    "        # Add noise to the inputs (forward diffusion).\n",
    "        noise = torch.randn_like(latents)\n",
    "        noised_latents = self.noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "        # Forward through the model.\n",
    "        outputs = self.unet(noised_latents, timesteps, conditioning)[\"sample\"]\n",
    "        return outputs, noise\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: dict[str, torch.Tensor], batch_idx: int\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Training step of the model.\"\"\"\n",
    "        outputs, targets = self.forward(batch)\n",
    "        loss = self.loss_fn(outputs, targets)\n",
    "        self.log(\n",
    "            \"train/loss_mse\", loss.item(), prog_bar=False, on_step=True, sync_dist=False\n",
    "        )\n",
    "        self.current_training_steps += 1\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self) -> OptimizerLRScheduler:\n",
    "        \"\"\"Configure the optimizer and learning rate scheduler.\"\"\"\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.trainer.model.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "        # Set a large training step here to keep lr constant after warm-up.\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.num_warmup_steps,\n",
    "            num_training_steps=100000000000,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def _sample_timesteps(self, latents: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.randint(\n",
    "            0, len(self.noise_scheduler), (latents.shape[0],), device=latents.device\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f720ec43",
   "metadata": {},
   "source": [
    "## 2. Using Fully Sharded Data Parallel (FSDP)\n",
    "\n",
    "Ray Train also supports Fully Sharded Data Parallel (FSDP) for distributed training.\n",
    "\n",
    "FSDP is a new training paradigm that is designed to improve the performance of large-scale training by reducing the memory footprint of the model by sharding the model parameters across different GPUs.\n",
    "\n",
    "Here is a diagram to help illustrate how FSDP works.\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/26745457/236892936-d4b91751-4689-421e-ac5f-edfd2eeeb635.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfebb6c6",
   "metadata": {},
   "source": [
    "### FSDP configuration:\n",
    "\n",
    "#### Sharding strategy:\n",
    "\n",
    "There are three different modes of the FSDP sharding strategy:\n",
    "\n",
    "1. `NO_SHARD`: Parameters, gradients, and optimizer states are not sharded. Similar to DDP.\n",
    "2. `SHARD_GRAD_OP`: Gradients and optimizer states are sharded during computation, and additionally, parameters are sharded outside computation. Similar to ZeRO stage-2.\n",
    "3. `FULL_SHARD`: Parameters, gradients, and optimizer states are sharded. It has minimal GRAM usage among the 3 options. Similar to ZeRO stage-3.\n",
    "\n",
    "#### Auto-wrap policy:\n",
    "\n",
    "Model layers are often wrapped with FSDP in a layered fashion. This means that only the layers in a single FSDP instance are required to aggregate all parameters to a single device during forwarding or backward calculations.\n",
    "\n",
    "Depending on the model architecture, we might need to specify a custom auto-wrap policy.\n",
    "\n",
    "For example, we can use the `transformer_auto_wrap_policy` to automatically wrap each Transformer Block into a single FSDP instance.\n",
    "\n",
    "#### Overlap communication with computation:\n",
    "\n",
    "You can specify to overlap the upcoming all-gather while executing the current forward/backward pass. It can improve throughput but may slightly increase peak memory usage. Set `backward_prefetch` and `forward_prefetch` to overlap communication with computation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc993de5",
   "metadata": {},
   "source": [
    "Let's update our training loop to use FSDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed963f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_per_worker_fsdp(config):\n",
    "    train_ds = ray.train.get_dataset_shard(\"train\")\n",
    "    train_dataloader = train_ds.iter_torch_batches(\n",
    "        batch_size=config[\"batch_size_per_worker\"],\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "    model = StableDiffusion(\n",
    "        lr=config[\"lr\"],\n",
    "        resolution=config[\"resolution\"],\n",
    "        weight_decay=config[\"weight_decay\"],\n",
    "        num_warmup_steps=config[\"num_warmup_steps\"],\n",
    "        model_name=config[\"model_name\"],\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_steps=config[\"max_steps\"],\n",
    "        max_epochs=config[\"max_epochs\"],\n",
    "        accelerator=\"gpu\",\n",
    "        devices=\"auto\",\n",
    "        precision=\"bf16-mixed\",\n",
    "        strategy=RayFSDPStrategy( # Use RayFSDPStrategy instead of RayDDPStrategy\n",
    "            sharding_strategy=\"SHARD_GRAD_OP\", # Run FSDP with SHARD_GRAD_OP sharding strategy\n",
    "            backward_prefetch=BackwardPrefetch.BACKWARD_PRE, # Overlap communication with computation in backward pass\n",
    "        ),\n",
    "        plugins=[RayLightningEnvironment()],\n",
    "        callbacks=[RayTrainReportCallback()],\n",
    "        enable_checkpointing=False,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_dataloaders=train_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8ecabd",
   "metadata": {},
   "source": [
    "Let's run the training loop with FSDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152c6749",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_path = \"/mnt/cluster_storage/\"\n",
    "experiment_name = \"stable-diffusion-pretraining-fsdp\"\n",
    "\n",
    "train_loop_config = {\n",
    "    \"batch_size_per_worker\": 8,\n",
    "    \"prefetch_batches\": 2,\n",
    "    \"every_n_train_steps\": 10, # Report metrics and checkpoints every 10 steps\n",
    "    \"lr\": 0.0001,\n",
    "    \"num_warmup_steps\": 10_000,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"max_steps\": 550_000,\n",
    "    \"max_epochs\": 1,\n",
    "    \"resolution\": 256,\n",
    "    \"model_name\": \"stabilityai/stable-diffusion-2-base\",\n",
    "}\n",
    "\n",
    "run_config = ray.train.RunConfig(name=experiment_name, storage_path=storage_path)\n",
    "\n",
    "scaling_config = ray.train.ScalingConfig(\n",
    "    num_workers=2,\n",
    "    use_gpu=True,\n",
    ")\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker_fsdp,\n",
    "    train_loop_config=train_loop_config,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    "    datasets=ray_datasets,\n",
    ")\n",
    "\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab5d955",
   "metadata": {},
   "source": [
    "Let's load the model from the checkpoint and inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69903e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with result.checkpoint.as_directory() as checkpoint_dir:\n",
    "    ckpt_path = os.path.join(checkpoint_dir, \"checkpoint.ckpt\")\n",
    "    model = StableDiffusion.load_from_checkpoint(ckpt_path, map_location=\"cpu\")\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1c4b82",
   "metadata": {},
   "source": [
    "## 3. Online (end-to-end) preprocessing and training\n",
    "\n",
    "Looking ahead at more challenging Stable Diffusion training pipelines, we will need to handle data in a more sophisticated way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f79ea61",
   "metadata": {},
   "source": [
    "<img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9d74fd",
   "metadata": {},
   "source": [
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-summit/stable-diffusion/diagrams/sdxl_random_crop_limitation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac4c6e1",
   "metadata": {},
   "source": [
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-summit/stable-diffusion/diagrams/moving_preprocessors_to_gpu.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df44c586",
   "metadata": {},
   "source": [
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-summit/stable-diffusion/diagrams/online_preprocessing_as_a_solution.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a89b951",
   "metadata": {},
   "source": [
    "### Resources for online preprocessing and training\n",
    "\n",
    "Check out the following resources for more details:\n",
    "\n",
    "- [Reducing the Cost of Pre-training Stable Diffusion by 3.7x with Anyscale](https://www.anyscale.com/blog/scalable-and-cost-efficient-stable-diffusion-pre-training-with-ray)\n",
    "- [Pretraining Stable Diffusion (V2) workspace template](https://console.anyscale.com/v2/template-preview/stable-diffusion-pretraining)\n",
    "- [Processing 2 Billion Images for Stable Diffusion Model Training - Definitive Guides with Ray Series](https://www.anyscale.com/blog/processing-2-billion-images-for-stable-diffusion-model-training-definitive-guides-with-ray-series)\n",
    "- [We Pre-Trained Stable Diffusion Models on 2 billion Images and Didn't Break the Bank - Definitive Guides with Ray Series](https://www.anyscale.com/blog/we-pre-trained-stable-diffusion-models-on-2-billion-images-and-didnt-break-the-bank-definitive-guides-with-ray-series)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac62329a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
