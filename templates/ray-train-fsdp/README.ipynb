{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with PyTorch Fully Sharded Data Parallel (FSDP2) and Ray Train\n",
    "\n",
    "**Time to complete:** 30 min\n",
    "\n",
    "This template shows you how to unlock the memory and performance improvements of integrating PyTorch's Fully Sharded Data Parallel with Ray Train. \n",
    "\n",
    "PyTorch's Fully Sharded Data Parallel (FSDP2) enables model sharding across nodes, allowing distributed training of large models with a significantly smaller memory footprint compared to standard Distributed Data Parallel (DDP). For a more detailed overview of FSDP2, see [PyTorch's official documentation](https://docs.pytorch.org/tutorials/intermediate/FSDP_tutorial.html#getting-started-with-fully-sharded-data-parallel-fsdp2). \n",
    "\n",
    "This tutorial provides a comprehensive, step-by-step guide on integrating PyTorch FSDP2 with Ray Train. Specifically, this guide covers: \n",
    "\n",
    "- A hands-on example of training an image classification model\n",
    "- Model checkpoint saving and loading with PyTorch Distributed Checkpoint (DCP)\n",
    "- Configuring FSDP2 to mitigate out-of-memory (OOM) errors using mixed precision, CPU offloading, sharding granularity, and more\n",
    "- GPU memory profiling with PyTorch Profiler\n",
    "- Loading a distributed model for inference\n",
    "\n",
    "**Note:** This notebook uses FSDP2's `fully_sharded` API. If you're currently using FSDP1's `FullyShardedDataParallel`, consider migrating to FSDP2 for improved performance and features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "**Anyscale Specific Configuration**\n",
    "\n",
    "Note: This tutorial is optimized for the Anyscale platform. When running on open source Ray, additional configuration is required. For example, you’ll need to manually:\n",
    "\n",
    "- **Configure your Ray Cluster**: Set up your multi-node environment and manage resource allocation without Anyscale's automation.\n",
    "- **Manage Dependencies**: Manually install and manage dependencies on each node.\n",
    "- **Set Up Storage**: Configure your own distributed or shared storage system for model checkpointing.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Overview\n",
    "\n",
    "For demonstration purposes, we will integrate Ray Train with FSDP using a **Vision Transformer (ViT)** trained on the FashionMNIST dataset. We chose ViT because it has clear, repeatable block structures (transformer blocks) that are ideal for demonstrating FSDP's sharding capabilities. \n",
    "\n",
    "While this is a relatively simple example, FSDP's complexity can lead to common challenges during training, such as out-of-memory (OOM) errors. Throughout this guide, we'll address these common issues and provide practical tips for improving performance and reducing memory utilization based on your specific use case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Package Setup\n",
    "\n",
    "Install the required dependencies for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home/ray/anaconda3/lib/python3.11/site-packages (3.10.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ray/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ray/anaconda3/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ray/anaconda3/lib/python3.11/site-packages (from matplotlib) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ray/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/ray/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ray/anaconda3/lib/python3.11/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/ray/anaconda3/lib/python3.11/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ray/anaconda3/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ray/anaconda3/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ray/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[92mSuccessfully registered `matplotlib` package to be installed on all cluster nodes.\u001b[0m\n",
      "\u001b[92mView and update dependencies here: https://console.anyscale.com/cld_kvedZWag2qA8i5BjxUevf5i7/prj_cz951f43jjdybtzkx1s5sjgz99/workspaces/expwrk_nktjw7a3j2l5c7af9rh3n5rskw?workspace-tab=dependencies\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install torch>=2.0.0\n",
    "pip install torchvision>=0.15.0\n",
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Ray Train V2 for the latest train APIs\n",
    "import os\n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"\n",
    "\n",
    "# Ray Train imports\n",
    "import ray\n",
    "import ray.train\n",
    "import ray.train.torch\n",
    "\n",
    "# PyTorch core and FSDP imports\n",
    "import torch\n",
    "from torch.distributed.fsdp import (\n",
    "    fully_shard,\n",
    "    FSDPModule,\n",
    "    CPUOffloadPolicy,\n",
    "    MixedPrecisionPolicy,\n",
    ")\n",
    "\n",
    "# PyTorch Distributed Checkpoint (DCP) imports\n",
    "from torch.distributed.checkpoint.state_dict import (\n",
    "    get_state_dict,\n",
    "    set_state_dict,\n",
    "    get_model_state_dict,\n",
    "    StateDictOptions\n",
    ")\n",
    "from torch.distributed.device_mesh import init_device_mesh \n",
    "from torch.distributed.checkpoint.stateful import Stateful\n",
    "import torch.distributed.checkpoint as dcp\n",
    "\n",
    "# PyTorch training components\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Computer vision components\n",
    "from torchvision.models import VisionTransformer\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "\n",
    "# Profiling and utilities\n",
    "import torch.profiler\n",
    "import tempfile\n",
    "import uuid\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Training Function\n",
    "\n",
    "Below is the main training function that orchestrates the FSDP training process. In the following sections, we'll implement each of the helper functions used within this training loop.\n",
    "\n",
    "### 2a. GPU Memory Profiling\n",
    "\n",
    "GPU memory profiling is an useful tool for monitoring and analyzing memory usage during model training. It helps identify bottlenecks, optimize resource allocation, and prevent out-of-memory errors. We configure PyTorch's GPU memory profiler within the training function.\n",
    "\n",
    "In this demo, we configure the profiler to generate a profiling file for each worker accessible from the Anyscale Files tab under cluster storage. To inspect a worker's memory profile, download the corresponding HTML file and open it in your browser. The profiler configuration and export path can be customized within the training function.  For more details on PyTorch's memory profiler, see the [PyTorch blog](https://pytorch.org/blog/understanding-gpu-memory-1/).\n",
    "\n",
    "<div style=\"display: flex; gap: 40px; align-items: flex-start;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <h3>Example Memory Profile</h3>\n",
    "    <img src=\"images/gpu_memory_profile.png\" width=\"600\"/>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "### 2b. Storage Configuration\n",
    "\n",
    "In this demo, we use cluster storage to allow for quick iteration and development, but this may not be suitable in production environments or at high scale. In such cases, object storage should be used instead. For more information about how to select your storage type, see the [Anyscale storage configuration docs](https://docs.anyscale.com/configuration/storage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(config):\n",
    "    \"\"\"Main training function that integrates FSDP2 with Ray Train.\n",
    "    \n",
    "    Args:\n",
    "        config: Training configuration dictionary containing hyperparameters\n",
    "    \"\"\"\n",
    "    # Step 1: Initialize the model\n",
    "    model = init_model()\n",
    "\n",
    "    # Configure device and move model to GPU\n",
    "    device = ray.train.torch.get_device()\n",
    "    torch.cuda.set_device(device)\n",
    "    model.to(device)\n",
    "\n",
    "    # Step 2: Apply FSDP sharding to the model\n",
    "    shard_model(model)\n",
    "\n",
    "    # Step 3: Initialize loss function and optimizer\n",
    "    criterion = CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=config.get('learning_rate', 0.001))\n",
    "\n",
    "    # Step 4: Load from checkpoint if available (for resuming training)\n",
    "    loaded_checkpoint = ray.train.get_checkpoint()\n",
    "    if loaded_checkpoint:\n",
    "        load_fsdp_checkpoint(model, optimizer, loaded_checkpoint)\n",
    "\n",
    "    # Step 5: Prepare training data\n",
    "    transform = Compose([\n",
    "        ToTensor(), \n",
    "        Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    data_dir = os.path.join(tempfile.gettempdir(), \"data\")\n",
    "    train_data = FashionMNIST(\n",
    "        root=data_dir, train=True, download=True, transform=transform\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_data, \n",
    "        batch_size=config.get('batch_size', 128), \n",
    "        shuffle=True\n",
    "    )\n",
    "    # Prepare data loader for distributed training\n",
    "    train_loader = ray.train.torch.prepare_data_loader(train_loader)\n",
    "\n",
    "    world_rank = ray.train.get_context().get_world_rank()\n",
    "\n",
    "    # Step 6: Setup PyTorch Profiler for memory monitoring\n",
    "    with torch.profiler.profile(\n",
    "       activities=[\n",
    "           torch.profiler.ProfilerActivity.CPU,\n",
    "           torch.profiler.ProfilerActivity.CUDA,\n",
    "       ],\n",
    "       schedule=torch.profiler.schedule(wait=0, warmup=0, active=6, repeat=1),\n",
    "       record_shapes=True,\n",
    "       profile_memory=True,\n",
    "       with_stack=True,\n",
    "   ) as prof:\n",
    "\n",
    "        # Step 7: Main training loop\n",
    "        running_loss = 0.0\n",
    "        num_batches = 0\n",
    "        epochs = config.get('epochs', 5)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Set epoch for distributed sampler to ensure proper shuffling\n",
    "            if ray.train.get_context().get_world_size() > 1:\n",
    "                train_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "            for images, labels in train_loader:\n",
    "                # Note: Data is automatically moved to the correct device by prepare_data_loader\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Standard training step\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update profiler\n",
    "                prof.step()\n",
    "                \n",
    "                # Track metrics\n",
    "                running_loss += loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "            # Step 8: Report metrics and save checkpoint after each epoch\n",
    "            avg_loss = running_loss / num_batches\n",
    "            metrics = {\"loss\": avg_loss, \"epoch\": epoch}\n",
    "            report_metrics_and_save_fsdp_checkpoint(model, optimizer, metrics)\n",
    "\n",
    "            # Log metrics from rank 0 only to avoid duplicate outputs\n",
    "            if world_rank == 0:\n",
    "                print(metrics)\n",
    "    \n",
    "    # Step 9: Export memory profiling results\n",
    "    run_name = ray.train.get_context().get_experiment_name()\n",
    "    prof.export_memory_timeline(\n",
    "        f\"/mnt/cluster_storage/{run_name}/rank{world_rank}_memory_profile.html\"\n",
    "    )\n",
    "\n",
    "    # Step 10: Save the final model for inference\n",
    "    save_model_for_inference(model, world_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Initialization\n",
    "\n",
    "The following function initializes a Vision Transformer (ViT) model configured for the FashionMNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model() -> torch.nn.Module:\n",
    "    \"\"\"Initialize a Vision Transformer model for FashionMNIST classification.\n",
    "    \n",
    "    Returns:\n",
    "        torch.nn.Module: Configured ViT model\n",
    "    \"\"\"\n",
    "    logger.info(\"Initializing Vision Transformer model...\")\n",
    "\n",
    "    # Create a ViT model with architecture suitable for 28x28 images\n",
    "    model = VisionTransformer(\n",
    "        image_size=28,        # FashionMNIST image size\n",
    "        patch_size=7,         # Divide 28x28 into 4x4 patches of 7x7 pixels each\n",
    "        num_layers=4,         # Number of transformer encoder layers\n",
    "        num_heads=2,          # Number of attention heads per layer\n",
    "        hidden_dim=64,        # Hidden dimension size\n",
    "        mlp_dim=128,          # MLP dimension in transformer blocks\n",
    "        num_classes=10,       # FashionMNIST has 10 classes\n",
    "    )\n",
    "\n",
    "    # Modify the patch embedding layer for grayscale images (1 channel instead of 3)\n",
    "    model.conv_proj = torch.nn.Conv2d(\n",
    "        in_channels=1,        # FashionMNIST is grayscale (1 channel)\n",
    "        out_channels=64,      # Match the hidden_dim\n",
    "        kernel_size=7,        # Match patch_size\n",
    "        stride=7,             # Non-overlapping patches\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Sharding with FSDP2\n",
    "\n",
    "PyTorch's `fully_shard` enables sharding at various granularities. At the most granular level, every layer can be sharded, but this increases communication costs between Ray Train workers. Experiment with different sharding granularities to find the optimal balance for your use case. In this demo, we shard only the encoder blocks—the largest layers in the Vision Transformer.\n",
    "\n",
    "Beyond sharding granularity, FSDP2 offers several configuration options to optimize performance and mitigate out-of-memory (OOM) errors:\n",
    "\n",
    "### 4a. CPU Offloading and `reshard_after_forward`\n",
    "\n",
    "CPU offloading reduces GPU memory footprint by storing model components in the CPU. However, this comes with the trade-off of increased data transfer overhead between CPU and GPU during computation.\n",
    "\n",
    "**How CPU offloading works:**\n",
    "- Sharded parameters, gradients, and optimizer states are stored on CPU\n",
    "- Sharded parameters are copied to GPU during forward/backward computation and freed after use\n",
    "- Computed gradients are copied to the CPU where the optimizer step is computed\n",
    "\n",
    "**`reshard_after_forward` optimization:**\n",
    "When enabled, all-gathered model weights are freed immediately after the forward pass. This reduces peak GPU memory usage but increases communication overhead during backward pass as parameters need to be all-gathered again.\n",
    "\n",
    "**When to use CPU offloading:**\n",
    "- When GPU memory is constrained\n",
    "- For very large models that don't fit in GPU memory\n",
    "\n",
    "**When not to use CPU offloading:**\n",
    "- When CPU memory is limited (can cause CPU crashes)\n",
    "- When training speed is more important than memory usage\n",
    "\n",
    "<div style=\"display: flex; gap: 40px; align-items: flex-start;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <h3>Without CPU Offloading</h3>\n",
    "    <img src=\"images/gpu_memory_profile.png\" width=\"600\"/>\n",
    "  </div>\n",
    "  <div style=\"text-align: center;\">\n",
    "    <h3>With CPU Offloading</h3>\n",
    "    <img src=\"images/cpu_offload_profile.png\" width=\"600\"/>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Learn more about CPU offloading on the [PyTorch documentation](https://docs.pytorch.org/docs/stable/distributed.fsdp.fully_shard.html#torch.distributed.fsdp.CPUOffloadPolicy).\n",
    "\n",
    "### 4b. Mixed Precision\n",
    "\n",
    "Enabling mixed precision accelerates training and reduces GPU memory usage with minimal accuracy impact. Unlike other distributed approaches like DDP, FSDP already maintains high-precision copies of sharded parameters, so mixed precision requires no additional memory overhead.\n",
    "\n",
    "**Benefits of mixed precision with FSDP:**\n",
    "- Reduced memory usage for activations and intermediate computations\n",
    "- Faster computation on modern GPUs\n",
    "- Maintained numerical stability through selective precision\n",
    "\n",
    "<div style=\"display: flex; gap: 40px; align-items: flex-start;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <h3>Without Mixed Precision</h3>\n",
    "    <img src=\"images/gpu_memory_profile.png\" width=\"600\"/>\n",
    "  </div>\n",
    "  <div style=\"text-align: center;\">\n",
    "    <h3>With Mixed Precision</h3>\n",
    "    <img src=\"images/mixed_precision_profile.png\" width=\"600\"/>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Learn more about mixed precision configuration on the [PyTorch documentation](https://docs.pytorch.org/docs/stable/distributed.fsdp.fully_shard.html#torch.distributed.fsdp.MixedPrecisionPolicy).\n",
    "\n",
    "### 4c. Device Mesh Configuration\n",
    "\n",
    "`init_device_mesh` configures a `DeviceMesh` that describes the training run's device topology. This demo uses a simple 1D mesh for data parallelism, but `DeviceMesh` also supports multi-dimensional parallelism approaches including tensor parallelism and pipeline parallelism.\n",
    "\n",
    "For advanced multi-dimensional parallelism configurations, see the [PyTorch device mesh documentation](https://docs.pytorch.org/tutorials/recipes/distributed_device_mesh.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shard_model(model: torch.nn.Module): \n",
    "    \"\"\"Apply FSDP2 sharding to the model with optimized configuration.\n",
    "    \n",
    "    Args:\n",
    "        model: The PyTorch model to shard\n",
    "    \"\"\"\n",
    "    logger.info(\"Applying FSDP2 sharding to model...\")\n",
    "\n",
    "    # Step 1: Create 1D device mesh for data parallel sharding\n",
    "    world_size = ray.train.get_context().get_world_size()\n",
    "    mesh = init_device_mesh(\n",
    "        device_type=\"cuda\", \n",
    "        mesh_shape=(world_size,), \n",
    "        mesh_dim_names=(\"data_parallel\",)\n",
    "    )\n",
    "\n",
    "    # Step 2: Configure CPU offloading policy (optional)\n",
    "    offload_policy = CPUOffloadPolicy()\n",
    "\n",
    "    # Step 3: Configure mixed precision policy (optional)\n",
    "    mp_policy = MixedPrecisionPolicy(\n",
    "        param_dtype=torch.float16,    # Store parameters in half precision\n",
    "        reduce_dtype=torch.float16,   # Use half precision for gradient reduction\n",
    "    )\n",
    "\n",
    "    # Step 4: Apply sharding to each transformer encoder block\n",
    "    for encoder_block in model.encoder.layers.children():\n",
    "        fully_shard(\n",
    "            encoder_block, \n",
    "            mesh=mesh, \n",
    "            reshard_after_forward=True,  # Free memory after forward pass\n",
    "            offload_policy=offload_policy, \n",
    "            mp_policy=mp_policy\n",
    "        )\n",
    "\n",
    "    # Step 5: Apply sharding to the root model\n",
    "    # This wraps the entire model and enables top-level FSDP functionality\n",
    "    fully_shard(\n",
    "        model, \n",
    "        mesh=mesh, \n",
    "        reshard_after_forward=True, \n",
    "        offload_policy=offload_policy, \n",
    "        mp_policy=mp_policy\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Distributed Checkpoint Wrapper Setup\n",
    "\n",
    "We create a checkpointing wrapper using PyTorch's `Stateful` API to simplify distributed checkpoint management. This wrapper handles the complexities of saving and loading FSDP model states across multiple workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AppState(Stateful):\n",
    "    \"\"\"This is a useful wrapper for checkpointing the Application State. Since this object is compliant\n",
    "    with the Stateful protocol, PyTorch DCP will automatically call state_dict/load_stat_dict as needed in the\n",
    "    dcp.save/load APIs.\n",
    "\n",
    "    Note: We take advantage of this wrapper to handle calling distributed state dict methods on the model\n",
    "    and optimizer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, optimizer=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def state_dict(self):\n",
    "        # this line automatically manages FSDP FQN's, as well as sets the default state dict type to FSDP.SHARDED_STATE_DICT\n",
    "        model_state_dict, optimizer_state_dict = get_state_dict(self.model, self.optimizer)\n",
    "        return {\n",
    "            \"model\": model_state_dict,\n",
    "            \"optim\": optimizer_state_dict\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        # sets our state dicts on the model and optimizer, now that we've loaded\n",
    "        set_state_dict(\n",
    "            self.model,\n",
    "            self.optimizer,\n",
    "            model_state_dict=state_dict[\"model\"],\n",
    "            optim_state_dict=state_dict[\"optim\"]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Loading FSDP Model from Checkpoint\n",
    "\n",
    "Distributed checkpoints can be loaded using `dcp.load`, which automatically handles resharding when the number of workers changes between training runs. This flexibility allows you to resume training with different resource configurations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fsdp_checkpoint(model: FSDPModule, optimizer: torch.optim.Optimizer, ckpt: ray.train.Checkpoint):\n",
    "    \"\"\"Load an FSDP checkpoint into the model and optimizer.\n",
    "    \n",
    "    This function handles distributed checkpoint loading with automatic resharding\n",
    "    support. It can restore checkpoints even when the number of workers differs\n",
    "    from the original training run.\n",
    "    \n",
    "    Args:\n",
    "        model: The FSDP-wrapped model to load state into\n",
    "        optimizer: The optimizer to load state into\n",
    "        ckpt: Ray Train checkpoint containing the saved state\n",
    "    \"\"\"\n",
    "    logger.info(\"Loading FSDP checkpoint for resuming training...\")\n",
    "    \n",
    "    try:\n",
    "        with ckpt.as_directory() as checkpoint_dir:\n",
    "            # Create state wrapper for DCP loading\n",
    "            state_dict = {\"app\": AppState(model, optimizer)}\n",
    "            \n",
    "            # Load the distributed checkpoint\n",
    "            dcp.load(\n",
    "                state_dict=state_dict,\n",
    "                checkpoint_id=checkpoint_dir\n",
    "            )\n",
    "            \n",
    "        logger.info(\"Successfully loaded FSDP checkpoint\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load checkpoint: {e}\")\n",
    "        raise RuntimeError(f\"Checkpoint loading failed: {e}\") from e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Saving Model Checkpoints\n",
    "\n",
    "The following function handles periodic checkpoint saving during training, combining metrics reporting with distributed checkpoint storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_metrics_and_save_fsdp_checkpoint(\n",
    "    model: FSDPModule, optimizer: torch.optim.Optimizer, metrics: dict\n",
    ") -> None:\n",
    "    \"\"\"Report training metrics and save an FSDP checkpoint.\n",
    "    \n",
    "    This function performs two critical operations:\n",
    "    1. Saves the current model and optimizer state using distributed checkpointing\n",
    "    2. Reports metrics to Ray Train for tracking\n",
    "    \n",
    "    Args:\n",
    "        model: The FSDP-wrapped model to checkpoint\n",
    "        optimizer: The optimizer to checkpoint\n",
    "        metrics: Dictionary of metrics to report (e.g., loss, accuracy)\n",
    "    \"\"\"\n",
    "    logger.info(\"Saving checkpoint and reporting metrics...\")\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "        # Perform a distributed checkpoint with DCP\n",
    "        state_dict = {\"app\": AppState(model, optimizer)}\n",
    "        dcp.save(state_dict=state_dict, checkpoint_id=temp_checkpoint_dir)\n",
    "\n",
    "        # Report each checkpoint shard from all workers\n",
    "        # This saves the checkpoint to shared cluster storage for persistence\n",
    "        checkpoint = ray.train.Checkpoint.from_directory(temp_checkpoint_dir)\n",
    "        ray.train.report(metrics, checkpoint=checkpoint)\n",
    "        \n",
    "    logger.info(f\"Checkpoint saved successfully. Metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Saving Model for Inference\n",
    "\n",
    "After training completes, we need to save the final model in an unsharded format for inference. This differs from regular checkpointing as it consolidates the model checkpoint into one file that is compatible with `torch.load`. The `get_model_state_dict` function performs an all-gather operation to reconstruct the complete model state on rank 0, which then saves and reports the full model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_for_inference(model: FSDPModule, world_rank: int) -> None:\n",
    "    \"\"\"Save the complete unsharded model for inference.\n",
    "    \n",
    "    This function consolidates the distributed model weights into a single\n",
    "    checkpoint file that can be used for inference without FSDP.\n",
    "    \n",
    "    Args:\n",
    "        model: The FSDP-wrapped model to save\n",
    "        world_rank: The rank of the current worker\n",
    "    \"\"\"\n",
    "    logger.info(\"Preparing model for inference...\")\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "        save_file = os.path.join(temp_checkpoint_dir, \"full-model.pt\")\n",
    "\n",
    "        # Step 1: All-gather the model state across all ranks\n",
    "        # This reconstructs the complete model from distributed shards\n",
    "        model_state_dict = get_model_state_dict(\n",
    "            model=model,\n",
    "            options=StateDictOptions(\n",
    "                full_state_dict=True,    # Reconstruct full model\n",
    "                cpu_offload=True,        # Move to CPU to save GPU memory\n",
    "            )\n",
    "        )\n",
    "\n",
    "        logger.info(\"Successfully retrieved complete model state dict\")\n",
    "        checkpoint = None\n",
    "\n",
    "        # Step 2: Save the complete model (rank 0 only)\n",
    "        if world_rank == 0: \n",
    "            torch.save(model_state_dict, save_file)\n",
    "            logger.info(f\"Saved complete model to {save_file}\")\n",
    "\n",
    "            # Create checkpoint for shared storage\n",
    "            checkpoint = ray.train.Checkpoint.from_directory(temp_checkpoint_dir)\n",
    "\n",
    "        # Step 3: Report the final checkpoint to Ray Train\n",
    "        ray.train.report(\n",
    "            {}, \n",
    "            checkpoint=checkpoint, \n",
    "            checkpoint_dir_name=\"full_model\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Launching the Distributed Training Job\n",
    "\n",
    "Now we'll configure and launch the distributed training job using Ray Train's TorchTrainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting FSDP training job...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 12:58:31,501\tINFO worker.py:1768 -- Connecting to existing Ray cluster at address: 10.0.32.71:6379...\n",
      "2025-08-29 12:58:31,513\tINFO worker.py:1939 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-2tq4lu3kdll2ayzdmkgw6lzt3y.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2025-08-29 12:58:31,714\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_c9f407640d38da9de67604a90e9e88004763278a.zip' (82.15MiB) to Ray cluster...\n",
      "2025-08-29 12:58:32,113\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_c9f407640d38da9de67604a90e9e88004763278a.zip'.\n",
      "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py:1987: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "\u001b[36m(TrainController pid=33120)\u001b[0m [State Transition] INITIALIZING -> SCHEDULING.\n",
      "\u001b[36m(TrainController pid=33120)\u001b[0m Attempting to start training worker group of size 2 with the following resources: [{'GPU': 1}] * 2\n",
      "\u001b[36m(TrainController pid=33120)\u001b[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.\n",
      "\u001b[36m(TrainController pid=33120)\u001b[0m [FailurePolicy] Decision: FailureDecision.RETRY, Error source: controller, Error count / maximum errors allowed: 1/inf, Error: Training failed due to controller error:\n",
      "\u001b[36m(TrainController pid=33120)\u001b[0m The worker group startup timed out after 30.0 seconds waiting for 2 workers. Potential causes include: (1) temporary insufficient cluster resources while waiting for autoscaling (ignore this warning in this case), (2) infeasible resource request where the provided `ScalingConfig` cannot be satisfied), and (3) transient network issues. Set the RAY_TRAIN_WORKER_GROUP_START_TIMEOUT_S environment variable to increase the timeout.\n",
      "\u001b[36m(TrainController pid=33120)\u001b[0m [State Transition] SCHEDULING -> RESCHEDULING.\n",
      "\u001b[36m(TrainController pid=33120)\u001b[0m [State Transition] RESCHEDULING -> SCHEDULING.\n",
      "\u001b[36m(TrainController pid=33120)\u001b[0m Attempting to start training worker group of size 2 with the following resources: [{'GPU': 1}] * 2\n",
      "\u001b[36m(TrainController pid=33120)\u001b[0m [FailurePolicy] Decision: FailureDecision.RETRY, Error source: controller, Error count / maximum errors allowed: 2/inf, Error: Training failed due to controller error:\n",
      "\u001b[36m(TrainController pid=33120)\u001b[0m The worker group startup timed out after 30.0 seconds waiting for 2 workers. Potential causes include: (1) temporary insufficient cluster resources while waiting for autoscaling (ignore this warning in this case), (2) infeasible resource request where the provided `ScalingConfig` cannot be satisfied), and (3) transient network issues. Set the RAY_TRAIN_WORKER_GROUP_START_TIMEOUT_S environment variable to increase the timeout.\n",
      "\u001b[36m(TrainController pid=33120)\u001b[0m [State Transition] SCHEDULING -> RESCHEDULING.\n",
      "\u001b[36m(TrainController pid=33120)\u001b[0m [State Transition] RESCHEDULING -> SCHEDULING.\n",
      "\u001b[36m(TrainController pid=33120)\u001b[0m Attempting to start training worker group of size 2 with the following resources: [{'GPU': 1}] * 2\n",
      "\u001b[36m(RayTrainWorker pid=11421, ip=10.0.24.158)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[36m(TrainController pid=33120)\u001b[0m Started training worker group of size 2: \n",
      "\u001b[36m(TrainController pid=33120)\u001b[0m - (ip=10.0.24.158, pid=11421) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TrainController pid=33120)\u001b[0m - (ip=10.0.63.54, pid=10437) world_rank=1, local_rank=0, node_rank=1\n",
      "\u001b[36m(TrainController pid=33120)\u001b[0m [State Transition] SCHEDULING -> RUNNING.\n",
      "\u001b[36m(RayTrainWorker pid=10437, ip=10.0.63.54)\u001b[0m Initializing Vision Transformer model...\n",
      "\u001b[36m(RayTrainWorker pid=10437, ip=10.0.63.54)\u001b[0m Applying FSDP2 sharding to model...\n",
      "  0%|          | 0.00/26.4M [00:00<?, ?B/s]54)\u001b[0m \n",
      "  0%|          | 32.8k/26.4M [00:00<01:51, 238kB/s]\n",
      "  0%|          | 65.5k/26.4M [00:00<01:51, 237kB/s]\n",
      "  0%|          | 98.3k/26.4M [00:00<01:51, 237kB/s]\n",
      "100%|██████████| 26.4M/26.4M [00:02<00:00, 11.9MB/s]\n",
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 214kB/s] \n",
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 214kB/s]\n",
      "\u001b[36m(RayTrainWorker pid=11421, ip=10.0.24.158)\u001b[0m Initializing Vision Transformer model...\n",
      "\u001b[36m(RayTrainWorker pid=11421, ip=10.0.24.158)\u001b[0m Applying FSDP2 sharding to model...\n",
      "  0%|          | 0.00/4.42M [00:00<?, ?B/s]\u001b[32m [repeated 5x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "  2%|▏         | 98.3k/4.42M [00:00<00:18, 234kB/s]\u001b[32m [repeated 41x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=11421, ip=10.0.24.158)\u001b[0m /tmp/ray/session_2025-08-29_11-18-58_966078_2389/runtime_resources/pip/2dc646f4cea92923d9b211dd039da1e14fd3e129/virtualenv/lib/python3.11/site-packages/torch/profiler/profiler.py:509: UserWarning: Profiler won't be using warmup, this can skew profiler results\n",
      "\u001b[36m(RayTrainWorker pid=11421, ip=10.0.24.158)\u001b[0m   warn(\"Profiler won't be using warmup, this can skew profiler results\")\n",
      "\u001b[36m(RayTrainWorker pid=11421, ip=10.0.24.158)\u001b[0m ERROR: External init callback must run in same thread as registerClient (-1282410944 != 1599129408)\n",
      "\u001b[36m(RayTrainWorker pid=11421, ip=10.0.24.158)\u001b[0m Saving checkpoint and reporting metrics...\n",
      "100%|██████████| 5.15k/5.15k [00:00<00:00, 42.6MB/s]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.15MB/s]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=10437, ip=10.0.63.54)\u001b[0m /tmp/ray/session_2025-08-29_11-18-58_966078_2389/runtime_resources/pip/2dc646f4cea92923d9b211dd039da1e14fd3e129/virtualenv/lib/python3.11/site-packages/torch/profiler/profiler.py:509: UserWarning: Profiler won't be using warmup, this can skew profiler results\n",
      "\u001b[36m(RayTrainWorker pid=10437, ip=10.0.63.54)\u001b[0m   warn(\"Profiler won't be using warmup, this can skew profiler results\")\n",
      "\u001b[36m(RayTrainWorker pid=10437, ip=10.0.63.54)\u001b[0m ERROR: External init callback must run in same thread as registerClient (2139088448 != 843249472)\n",
      "\u001b[36m(RayTrainWorker pid=10437, ip=10.0.63.54)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_mnist_ac7573f9/checkpoint_2025-08-29_13-00-50.598902)\n",
      "\u001b[36m(RayTrainWorker pid=10437, ip=10.0.63.54)\u001b[0m Checkpoint saved successfully. Metrics: {'loss': 0.9074613530585106, 'epoch': 0}\n",
      "\u001b[36m(RayTrainWorker pid=11421, ip=10.0.24.158)\u001b[0m {'loss': 0.9220723902925532, 'epoch': 0}\n",
      "\u001b[36m(RayTrainWorker pid=11421, ip=10.0.24.158)\u001b[0m Saving checkpoint and reporting metrics...\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=11421, ip=10.0.24.158)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_mnist_ac7573f9/checkpoint_2025-08-29_13-00-50.598902)\n",
      "\u001b[36m(RayTrainWorker pid=11421, ip=10.0.24.158)\u001b[0m Checkpoint saved successfully. Metrics: {'loss': 0.9220723902925532, 'epoch': 0}\n",
      "\u001b[36m(RayTrainWorker pid=10437, ip=10.0.63.54)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_mnist_ac7573f9/checkpoint_2025-08-29_13-01-13.436430)\n",
      "\u001b[36m(RayTrainWorker pid=10437, ip=10.0.63.54)\u001b[0m Checkpoint saved successfully. Metrics: {'loss': 0.6794438788231383, 'epoch': 1}\n",
      "\u001b[36m(RayTrainWorker pid=11421, ip=10.0.24.158)\u001b[0m {'loss': 0.6911060089760638, 'epoch': 1}\n",
      "\u001b[36m(RayTrainWorker pid=10437, ip=10.0.63.54)\u001b[0m Saving checkpoint and reporting metrics...\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=11421, ip=10.0.24.158)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_mnist_ac7573f9/checkpoint_2025-08-29_13-01-13.436430)\n",
      "\u001b[36m(RayTrainWorker pid=11421, ip=10.0.24.158)\u001b[0m Checkpoint saved successfully. Metrics: {'loss': 0.6911060089760638, 'epoch': 1}\n",
      "\u001b[36m(RayTrainWorker pid=10437, ip=10.0.63.54)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_mnist_ac7573f9/checkpoint_2025-08-29_13-01-37.358961)\n",
      "\u001b[36m(RayTrainWorker pid=10437, ip=10.0.63.54)\u001b[0m Checkpoint saved successfully. Metrics: {'loss': 0.5813783036901595, 'epoch': 2}\n",
      "\u001b[36m(RayTrainWorker pid=11421, ip=10.0.24.158)\u001b[0m {'loss': 0.5903209496897163, 'epoch': 2}\n",
      "\u001b[36m(RayTrainWorker pid=10437, ip=10.0.63.54)\u001b[0m Saving checkpoint and reporting metrics...\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=11421, ip=10.0.24.158)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_mnist_ac7573f9/checkpoint_2025-08-29_13-01-37.358961)\n",
      "\u001b[36m(RayTrainWorker pid=11421, ip=10.0.24.158)\u001b[0m Checkpoint saved successfully. Metrics: {'loss': 0.5903209496897163, 'epoch': 2}\n",
      "\u001b[36m(RayTrainWorker pid=10437, ip=10.0.63.54)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_mnist_ac7573f9/checkpoint_2025-08-29_13-02-00.819530)\n",
      "\u001b[36m(RayTrainWorker pid=10437, ip=10.0.63.54)\u001b[0m Checkpoint saved successfully. Metrics: {'loss': 0.5233754259474734, 'epoch': 3}\n",
      "\u001b[36m(RayTrainWorker pid=11421, ip=10.0.24.158)\u001b[0m {'loss': 0.5298319065824468, 'epoch': 3}\n",
      "\u001b[36m(RayTrainWorker pid=10437, ip=10.0.63.54)\u001b[0m Saving checkpoint and reporting metrics...\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=11421, ip=10.0.24.158)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_mnist_ac7573f9/checkpoint_2025-08-29_13-02-00.819530)\n",
      "\u001b[36m(RayTrainWorker pid=11421, ip=10.0.24.158)\u001b[0m Checkpoint saved successfully. Metrics: {'loss': 0.5298319065824468, 'epoch': 3}\n",
      "\u001b[36m(RayTrainWorker pid=10437, ip=10.0.63.54)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_mnist_ac7573f9/checkpoint_2025-08-29_13-02-23.764725)\n",
      "\u001b[36m(RayTrainWorker pid=10437, ip=10.0.63.54)\u001b[0m Checkpoint saved successfully. Metrics: {'loss': 0.4832368891289894, 'epoch': 4}\n",
      "\u001b[36m(RayTrainWorker pid=11421, ip=10.0.24.158)\u001b[0m {'loss': 0.4901455493683511, 'epoch': 4}\n",
      "\u001b[36m(RayTrainWorker pid=11421, ip=10.0.24.158)\u001b[0m generated new fontManager\n",
      "\u001b[36m(RayTrainWorker pid=11421, ip=10.0.24.158)\u001b[0m Saving checkpoint and reporting metrics...\n",
      "\u001b[36m(RayTrainWorker pid=11421, ip=10.0.24.158)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_mnist_ac7573f9/checkpoint_2025-08-29_13-02-23.764725)\n",
      "\u001b[36m(RayTrainWorker pid=11421, ip=10.0.24.158)\u001b[0m Checkpoint saved successfully. Metrics: {'loss': 0.4901455493683511, 'epoch': 4}\n",
      "\u001b[36m(RayTrainWorker pid=11421, ip=10.0.24.158)\u001b[0m Preparing model for inference...\n",
      "\u001b[36m(RayTrainWorker pid=11421, ip=10.0.24.158)\u001b[0m Successfully retrieved complete model state dict\n",
      "\u001b[36m(RayTrainWorker pid=11421, ip=10.0.24.158)\u001b[0m Saved complete model to /tmp/tmp019hes64/full-model.pt\n",
      "\u001b[36m(RayTrainWorker pid=11421, ip=10.0.24.158)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/fsdp_mnist_ac7573f9/full_model)\n",
      "\u001b[36m(TrainController pid=33120)\u001b[0m [State Transition] RUNNING -> FINISHED.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +1h49m20s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n"
     ]
    }
   ],
   "source": [
    "# Configure distributed training resources\n",
    "scaling_config = ray.train.ScalingConfig(\n",
    "    num_workers=2,      # Number of distributed workers\n",
    "    use_gpu=True        # Enable GPU training\n",
    ")\n",
    "\n",
    "# Configure training parameters\n",
    "train_loop_config = {\n",
    "    \"epochs\": 5,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "\n",
    "# Configure run settings and storage\n",
    "run_config = ray.train.RunConfig(\n",
    "    # Persistent storage path accessible across all worker nodes\n",
    "    storage_path=\"/mnt/cluster_storage/\",\n",
    "    # Unique experiment name (use consistent name to resume from checkpoints)\n",
    "    name=f\"fsdp_mnist_{uuid.uuid4().hex[:8]}\",\n",
    "    # Fault tolerance configuration\n",
    "    failure_config=ray.train.FailureConfig(max_failures=1),\n",
    ")\n",
    "\n",
    "# Initialize and launch the distributed training job\n",
    "trainer = ray.train.torch.TorchTrainer(\n",
    "    train_loop_per_worker=train_func,\n",
    "    scaling_config=scaling_config,\n",
    "    train_loop_config=train_loop_config,\n",
    "    run_config=run_config,\n",
    ")\n",
    "\n",
    "print(\"Starting FSDP training job...\")\n",
    "result = trainer.fit()\n",
    "print(\"Training completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Loading the Trained Model for Inference\n",
    "\n",
    "After training completes, we can load the saved model for inference on new data. The model is loaded in its unsharded form, ready for standard PyTorch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update this path to match your trained model location\n",
    "# The path follows the pattern: /mnt/cluster_storage/{experiment_name}/full_model/full-model.pt\n",
    "PATH_TO_FULL_MODEL = \"/mnt/cluster_storage/fsdp_mnist_16b0e0c2/full_model/full-model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model for inference...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv_proj): Conv2d(1, 64, kernel_size=(7, 7), stride=(7, 7))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads): Sequential(\n",
       "    (head): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the same model architecture for inference\n",
    "print(\"Loading trained model for inference...\")\n",
    "model = init_model()\n",
    "\n",
    "# Load the trained weights \n",
    "state_dict = torch.load(PATH_TO_FULL_MODEL, map_location='cpu')\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: .\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5,), std=(0.5,))\n",
       "           )"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the test data\n",
    "transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "test_data = FashionMNIST(\n",
    "    root=\".\", train=False, download=True, transform=transform\n",
    ")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_label=8 test_label=9\n"
     ]
    }
   ],
   "source": [
    "# Test model inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(test_data.data[0].reshape(1, 1, 28, 28).float())\n",
    "    predicted_label = out.argmax().item()\n",
    "    test_label = test_data.targets[0].item()\n",
    "    print(f\"{predicted_label=} {test_label=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you: \n",
    "\n",
    "- Trained an image classification model using FSDP and Ray Train\n",
    "- Learned how to load and save distributed checkpoints with PyTorch DCP\n",
    "- Gained insight on configuring FSDP to balance training performance and memory usage\n",
    "- Unlocked multi-node GPU memory observability with PyTorch Profiler"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
