{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy a 670 billion parameter reasoning model, DeepSeek R1\n",
    "\n",
    "Deploying a 670B parameter model like DeepSeek R1 presents significant technical challenges. The model is too large to fit in a GPU, or even a single node. This requires distributing the model across multiple GPUs and nodes using *tensor parallelism*, AKA intra-layer parallelism, and *pipeline parallelism*, AKA inter-layer parallelism. The Ray Serve LLM API automates this process.\n",
    "\n",
    "Deploying the model also involves launching multiple nodes manually and configuring them to work together. Anyscale automates this process by autoscaling the cluster with the appropriate number of nodes and GPUs.\n",
    "\n",
    "Beware: this is an expensive deployment. At the time of writing, the deployment cost is around $52.50 USD per hour in the `us-west-2` AWS region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To launch nodes with 1000 GB disk capacity (instead of the default 150 GB), modify the **`Instance config`** field under **`Manage Cluster`** → **`Advanced settings`**.\n",
    "\n",
    "For more information about configuring the disk size of a Google Cloud Platform (GCP) cluster, see [Changing the default disk size for GCP clusters](https://docs.anyscale.com/configuration/compute/gcp/#changing-the-default-disk-size).\n",
    "For more information about configuring the disk size of an Amazon Web Services (AWS) cluster, see [Changing the default disk size for AWS clusters](https://docs.anyscale.com/configuration/compute/aws/#changing-the-default-disk-size).\n",
    "\n",
    "In the case of AWS, the corresponding settings are:\n",
    "\n",
    "```json\n",
    "\n",
    "    {\n",
    "      \"BlockDeviceMappings\": [\n",
    "        {\n",
    "          \"Ebs\": {\n",
    "            \"VolumeSize\": 1000,\n",
    "            \"VolumeType\": \"gp3\",\n",
    "            \"DeleteOnTermination\": true\n",
    "          },\n",
    "          \"DeviceName\": \"/dev/sda1\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "```\n",
    "\n",
    "With this configuration, every launched node has 1000 GB disk capacity. This change may require restarting the cluster with new nodes, which Anyscale automatically handles.\n",
    "\n",
    "## Start the deployment\n",
    "\n",
    "The following code deploys the DeepSeek R1 model using Ray Serve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import serve\n",
    "from ray.serve.llm import LLMConfig, build_openai_app\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    model_loading_config={\n",
    "        \"model_id\": \"big_model\",  # Model ID for Ray Serve\n",
    "        \"model_source\": \"deepseek-ai/DeepSeek-R1\",  # Model ID on Hugging Face\n",
    "    },\n",
    "    deployment_config={\n",
    "        \"autoscaling_config\": {\n",
    "            \"min_replicas\": 1,\n",
    "            \"max_replicas\": 1,\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Accelerator type. With autoscaling enabled, Anyscale automatically\n",
    "    # launches the appropriate instance type.\n",
    "    accelerator_type=\"L40S\",\n",
    "    # Enable the vLLM V1 core engine.\n",
    "    runtime_env={\"env_vars\": {\"VLLM_USE_V1\": \"1\"}},\n",
    "    \n",
    "    # Options passed through to the vLLM engine.\n",
    "    engine_kwargs={\n",
    "        # Automatic model parallelization across GPUs\n",
    "        # Used by the auto-scaler to select the appropriate instance type.\n",
    "        # In this case, it selects machines with 4x L40S GPUs.\n",
    "        \"tensor_parallel_size\": 4,   # Splits model layers across 4 GPUs per node\n",
    "        \"pipeline_parallel_size\": 5,  # Distributes across 5 nodes\n",
    "        # Total: 4 GPUs × 5 nodes = 20 GPUs\n",
    "        \n",
    "        \"gpu_memory_utilization\": 0.92,  # Use 92% of GPU memory\n",
    "        \"dtype\": \"auto\",\n",
    "        \n",
    "        # Performance tuning\n",
    "        \"max_num_seqs\": 20,  # Concurrent requests\n",
    "        \"max_model_len\": 2048,  # Max tokens per pass\n",
    "        \"enable_chunked_prefill\": True,\n",
    "        \"enable_prefix_caching\": True,\n",
    "        \"trust_remote_code\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Serve the model.\n",
    "llm_app = build_openai_app({\"llm_configs\": [llm_config]})\n",
    "serve.run(llm_app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run the preceding code, Anyscale automatically provisions a cluster with the appropriate instance type and number of nodes.\n",
    "After running the preceding code, monitor the progress of the deployment in the Anyscale Console.\n",
    "You might encounter warnings about insufficient capacity in your cloud region. If you experience a GPU shortage, Anyscale continues to poll the cloud provider until enough capacity is available and launches all the nodes at once when possible.\n",
    "\n",
    "Because the model is so large, it takes 15-25 minutes to download the model weights and split the model across the nodes.\n",
    "\n",
    "### Verify deployment\n",
    "\n",
    "The output should look like:\n",
    "```\n",
    "INFO 2025-03-02 17:17:14,162 serve 61769 -- Application 'default' is ready at http://127.0.0.1:8000/\n",
    "INFO 2025-03-02 17:17:14,162 serve 61769 -- Deployed app 'default' successfully.\n",
    "```\n",
    "\n",
    "DeepSeek R1 is running across multiple nodes, ready to serve requests.\n",
    "\n",
    "\n",
    "The deployment provides a standard OpenAI API interface:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Connect to your deployed model\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",  # Ray Serve endpoint\n",
    "    api_key=\"not-needed\"  # No API key required for local deployment\n",
    ")\n",
    "\n",
    "# Use the same model ID from your configuration\n",
    "model_id = \"big_model\"\n",
    "\n",
    "# Basic chat completion with streaming\n",
    "response = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain quantum mechanics in simple terms.\"}\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "# Process streaming response\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observability and monitoring\n",
    "\n",
    "To monitor metrics, access the **`Ray Dashboard`** for the following:\n",
    "- Real-time GPU utilization\n",
    "- Request latency metrics\n",
    "- Queries per second (QPS)\n",
    "- Error rates and logs\n",
    "\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "When finished, gracefully shut down the service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.shutdown()\n",
    "\n",
    "# Or from command line\n",
    "# serve shutdown --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyscale automatically detects idle nodes and scales down the cluster.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Ray Serve LLM API simplifies deploying massive language models using a few lines of Python code. With production-ready scaling and fault tolerance features, you can focus on building applications rather than managing infrastructure.\n",
    "\n",
    "To learn more, see the [Ray Serve LLM documentation](https://docs.ray.io/en/latest/serve/llm/serving-llms.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
