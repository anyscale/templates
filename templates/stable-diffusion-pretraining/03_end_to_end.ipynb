{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalable online preprocessing and model training pipeline for Stable Diffusion\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/stable-diffusion/end_to_end_architecture_v7.png\" width=1000px />\n",
    "\n",
    "The preceding architecture diagram illustrates the online preprocessing and model training pipeline for Stable Diffusion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray Data loads the data from a remote storage system, and then streams the data through the entire processing and training stages:\n",
    "1. **Transformation**\n",
    "   1. **OpenAI CLIP Tokenizer**: Cropping and normalizing images.\n",
    "   2. **Crop+Normalize**: Tokenizing the text captions using a CLIP tokenizer.\n",
    "2. **Encoding**\n",
    "   1. **OpenAI CLIP Text Encoder**: Compressing images into a latent space using a VAE encoder.\n",
    "   2. **VAE Encoder**: Generating text embeddings using a CLIP model.\n",
    "3. **Training**\n",
    "   * **Stable Diffusion Model**: Training a U-Net model on the image latents and text embeddings.\n",
    "   * **Model Checkpoints**: Generating model checkpoints and saving them to a remote storage system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook executes a fully self-contained module, `end_to_end.py`, that performs the online preprocessing and training over a small subset of the full 2 billion dataset to demonstrate the workload. You can parameterize the same module code to process the full dataset. The **Running production-scale model training** section below summarizes the necessary changes to scale the workload.\n",
    "\n",
    "Run the following cell to perform the online preprocessing and model training. The script loads the data, transforms it, encodes it, and runs the model training. After the cell executes, view the generated model checkpoint files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/end_to_end.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Running production-scale model training\n",
    "\n",
    "If you're looking to scale your Stable Diffusion pre-training with custom data, we're here to help ðŸ™Œ !\n",
    "\n",
    "ðŸ‘‰ **[Check out this link](https://forms.gle/9aDkqAqobBctxxMa8) so we can assist you**.\n",
    "\n",
    "\n",
    "In case you would like to get an idea of the changes needed to scale the `end_to_end.py` script to the full dataset, below is a table that provides approximate guidance on the changes you need to make:\n",
    "\n",
    "| Step | Change | Description |\n",
    "| --- | --- | -- | \n",
    "| 1 |  Raw data path | Point to the full dataset |\n",
    "| 2 | Number of data loading workers | Increase to 192 CPUs |\n",
    "| 3 | Number of transformation workers | Increase to 192 CPUs |\n",
    "| 4 | Batch size | Set to 120 for 256x256 images, 40 for 512x512 images |\n",
    "| 5 | Number of encoding workers | Increase to 48 A10G GPUs |\n",
    "| 6 | Batch size | Set to 128 for 256x256 images, 32 for 512x512 images |\n",
    "| 7 | Number of training workers | Increase to 32 A100-80GB GPUs |\n",
    "| 8 | Model config | Use the full U-Net model |\n",
    "| 9 | Distributed training strategy | Set the distributed training strategy to FSDP, configure it to run in `SHARD_GRAD_OP` mode |\n",
    "| 10 | Output path | Change to a permanent path |\n",
    "| 11 | Run the process script | Run as an Anyscale Job |\n",
    "| 12 | Run the first phase of training | Use resolution 256x256 for a total of 550,000 steps |\n",
    "| 13 | Run the second phase of training | Use resolution 512x512 for a total of 850,000 steps loading the checkpoint from the first phase |\n",
    "\n",
    "In terms of infrastructure, you would provision 48 instances of `g5.2xlarge` and 4 instances of `p4de.24xlarge` for the entire process or use Anyscale's autoscaling capabilities to scale up and down as needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
