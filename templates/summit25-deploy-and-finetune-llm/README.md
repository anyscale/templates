# Deploy and Fine-Tune LLMs on Ray

Welcome to the Ray Summit 2025 workshop on deploying and fine-tuning large language models with Ray Serve and Ray Train.

This repository contains two hands-on examples that demonstrate how to build production-ready LLM applications on Ray.

## Examples

### 1. Deploy GPT OSS

Deploy open-source GPT models with Ray Serve LLM for production-ready inference at scale.

**Presenters:** Kunling Geng, Aydin Abiar

Learn how to:
- Configure and deploy GPT OSS models with Ray Serve LLM
- Scale inference with vLLM
- Monitor performance with built-in dashboards
- Deploy to production on Anyscale

**[Get started →](gpt-oss/)**

### 2. Fine-Tune with LLaMA Factory

Fine-tune LLMs on custom datasets using LoRA (Low-Rank Adaptation) with LLaMA Factory and Ray Train.

**Presenter:** Jason Ding

Learn how to:
- Fine-tune models efficiently with LoRA
- Scale training across multiple GPUs with DeepSpeed
- Track experiments with TensorBoard
- Export and serve your fine-tuned models

**[Get started →](sft-lora-fine-tune/)**

## Getting Started

Each example includes comprehensive documentation, code samples, and configuration files. Start with either example based on your goals:

- **Deploy GPT OSS** if you want to serve existing open-source models
- **Fine-Tune with LLaMA Factory** if you want to adapt models to your specific tasks

