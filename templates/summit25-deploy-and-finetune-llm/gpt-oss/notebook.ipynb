{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a51548b",
   "metadata": {},
   "source": [
    "# Deploy gpt-oss\n",
    "\n",
    "<div align=\"left\">\n",
    "<a target=\"_blank\" href=\"https://console.anyscale.com/template-preview/deployment-serve-llm?file=%252Ffiles%252Fgpt-oss\"><img src=\"https://img.shields.io/badge/ðŸš€ Run_on-Anyscale-9hf\"></a>&nbsp;\n",
    "<a href=\"https://github.com/ray-project/ray/tree/master/doc/source/serve/tutorials/deployment-serve-llm/gpt-oss\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n",
    "</div>\n",
    "\n",
    "*gpt-oss* is a family of open-source models designed for general-purpose language understanding and generation. The 20B parameter variant (`gpt-oss-20b`) offers strong reasoning capabilities with lower latency. This makes it well-suited for local or specialized use cases. The larger 120B parameter variant (`gpt-oss-120b`) is designed for production-scale, high-reasoning workloads.\n",
    "\n",
    "For more information, see the [gpt-oss collection](https://huggingface.co/collections/openai/gpt-oss-68911959590a1634ba11c7a4).\n",
    "\n",
    "---\n",
    "\n",
    "## Configure Ray Serve LLM\n",
    "\n",
    "Ray Serve LLM provides multiple [Python APIs](https://docs.ray.io/en/latest/serve/api/index.html#llm-api) for defining your application. Use [`build_openai_app`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.build_openai_app.html#ray.serve.llm.build_openai_app) to build a full application from your [`LLMConfig`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.LLMConfig.html#ray.serve.llm.LLMConfig) object.\n",
    "\n",
    "To deploy a small-sized model such as gpt-oss-20b, a single GPU is sufficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86070ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serve_gpt_oss.py\n",
    "from ray.serve.llm import LLMConfig, build_openai_app\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    model_loading_config=dict(\n",
    "        model_id=\"my-gpt-oss\",\n",
    "        model_source=\"s3://llm-guide/data/ray-serve-llm/hf_repo/gpt-oss-20b\", # also support huggingface repo syntax like openai/gpt-oss-20b\n",
    "    ),\n",
    "    accelerator_type=\"L4\",\n",
    "    deployment_config=dict(\n",
    "        autoscaling_config=dict(\n",
    "            min_replicas=1, # avoid cold starts by keeping at least 1 replica always on\n",
    "            max_replicas=2, # limit max replicas to control cost\n",
    "        )\n",
    "    ),\n",
    "    engine_kwargs=dict(\n",
    "        max_model_len=32768\n",
    "    ),\n",
    "    log_engine_metrics= True,\n",
    ")\n",
    "\n",
    "app = build_openai_app({\"llm_configs\": [llm_config]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17a7140",
   "metadata": {},
   "source": [
    "**Note:** Before moving to a production setup, migrate to using a [Serve config file](https://docs.ray.io/en/latest/serve/production-guide/config.html) to make your deployment version-controlled, reproducible, and easier to maintain for CI/CD pipelines. For an example, see [Serving LLMs - Quickstart Examples: Production Guide](https://docs.ray.io/en/latest/serve/llm/quick-start.html#production-deployment).\n",
    "\n",
    "---\n",
    "\n",
    "## Deploy locally\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "* Access to GPU compute.\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "gpt-oss integration is available starting from `ray[serve,llm]>=2.50.0`.\n",
    "\n",
    "---\n",
    "\n",
    "### Launch the service\n",
    "\n",
    "Follow the instructions in [Configure Ray Serve LLM](#configure-ray-serve-llm), and define your app in a Python module `serve_gpt_oss.py`.\n",
    "\n",
    "In a terminal, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdb0921",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "serve run serve_gpt_oss:app --non-blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df944967",
   "metadata": {},
   "source": [
    "Deployment typically takes a few minutes as Ray provisions the cluster, the vLLM server starts, and Ray Serve downloads the model.\n",
    "\n",
    "---\n",
    "\n",
    "### Send requests\n",
    "\n",
    "Your endpoint is available locally at `http://localhost:8000`. You can use a placeholder authentication token for the OpenAI client, for example `\"FAKE_KEY\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d623a30f",
   "metadata": {},
   "source": [
    "#### Example Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bedc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.py\n",
    "from urllib.parse import urljoin\n",
    "from openai import OpenAI\n",
    "\n",
    "api_key = \"FAKE_KEY\"\n",
    "base_url = \"http://localhost:8000\"\n",
    "\n",
    "client = OpenAI(base_url=urljoin(base_url, \"v1\"), api_key=api_key)\n",
    "\n",
    "# Example query\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-gpt-oss\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"How many r's in strawberry\"}\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "# Stream\n",
    "for chunk in response:\n",
    "    # Stream reasoning content\n",
    "    if hasattr(chunk.choices[0].delta, \"reasoning_content\"):\n",
    "        data_reasoning = chunk.choices[0].delta.reasoning_content\n",
    "        if data_reasoning:\n",
    "            print(data_reasoning, end=\"\", flush=True)\n",
    "    # Later, stream the final answer\n",
    "    if hasattr(chunk.choices[0].delta, \"content\"):\n",
    "        data_content = chunk.choices[0].delta.content\n",
    "        if data_content:\n",
    "            print(data_content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b095ebf3",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Shut down the service\n",
    "\n",
    "To shutdown your LLM service: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd3dacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "serve shutdown -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f67c39",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Enable LLM monitoring\n",
    "\n",
    "The *Serve LLM Dashboard* offers deep visibility into model performance, latency, and system behavior, including:\n",
    "\n",
    "- Token throughput (tokens/sec).\n",
    "- Latency metrics: Time To First Token (TTFT), Time Per Output Token (TPOT).\n",
    "- KV cache utilization.\n",
    "\n",
    "To enable these metrics, go to your LLM config and set `log_engine_metrics: true`:\n",
    "\n",
    "```yaml\n",
    "applications:\n",
    "- ...\n",
    "  args:\n",
    "    llm_configs:\n",
    "      - ...\n",
    "        log_engine_metrics: true\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Improve concurrency\n",
    "\n",
    "Ray Serve LLM uses [vLLM](https://docs.vllm.ai/en/stable/) as its backend engine, which logs the *maximum concurrency* it can support based on your configuration.\n",
    "\n",
    "Example log for gpt-oss-20b with 1xL4:\n",
    "```console\n",
    "INFO 09-08 17:34:28 [kv_cache_utils.py:1017] Maximum concurrency for 32,768 tokens per request: 5.22x\n",
    "```\n",
    "\n",
    "To improve concurrency for gpt-oss models, see [Deploy a small-sized LLM: Improve concurrency](https://docs.ray.io/en/latest/serve/tutorials/deployment-serve-llm/small-size-llm/README.html#improve-concurrency) for small-sized models such as `gpt-oss-20b`, and [Deploy a medium-sized LLM: Improve concurrency](https://docs.ray.io/en/latest/serve/tutorials/deployment-serve-llm/medium-size-llm/README.html#improve-concurrency) for medium-sized models such as `gpt-oss-120b`.\n",
    "\n",
    "**Note:** Some example guides recommend using quantization to boost concurrency. `gpt-oss` weights are already 4-bit by default, so further quantization typically isnâ€™t applicable.  \n",
    "\n",
    "For broader guidance, also see [Choose a GPU for LLM serving](https://docs.anyscale.com/llm/serving/gpu-guidance) and [Optimize performance for Ray Serve LLM](https://docs.anyscale.com/llm/serving/performance-optimization).\n",
    "\n",
    "---\n",
    "\n",
    "## Reasoning configuration (with gpt-oss)\n",
    "\n",
    "You donâ€™t need a custom reasoning parser when deploying `gpt-oss` with Ray Serve LLM, you can access the reasoning content in the model's response directly. You can also control the reasoning effort of the model in the request.\n",
    "\n",
    "---\n",
    "\n",
    "### Access reasoning output\n",
    "\n",
    "The reasoning content is available directly in the `reasoning_content` field of the response:\n",
    "\n",
    "```python\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-gpt-oss\",\n",
    "    messages=[\n",
    "        ...\n",
    "    ]\n",
    ")\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Control reasoning effort\n",
    "\n",
    "`gpt-oss` supports [three reasoning levels](https://huggingface.co/openai/gpt-oss-20b#reasoning-levels): **low**, **medium**, and **high**. The default level is **medium**.\n",
    "\n",
    "You can control reasoning with the `reasoning_effort` request parameter:  \n",
    "```python\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-gpt-oss\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What are the three main touristic spots to see in Paris?\"}\n",
    "    ],\n",
    "    reasoning_effort=\"low\" # Or \"medium\", \"high\"\n",
    ")\n",
    "```\n",
    "\n",
    "You can also set a level explicitly in the system prompt:  \n",
    "```python\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-gpt-oss\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Reasoning: low. You are an AI travel assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What are the three main touristic spots to see in Paris?\"}\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "**Note:** There's no reliable way to completely disable reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial, you learned how to deploy `gpt-oss` models with Ray Serve LLM, from development to production. You learned how to configure Ray Serve LLM, deploy your service on a Ray cluster, send requests, and monitor your service."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repo_ray_docs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
