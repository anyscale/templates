{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80a327f2",
   "metadata": {},
   "source": [
    "# Ray Tasks Fundamentals: Building Distributed Applications\n",
    "\n",
    "¬© 2025, Anyscale. All Rights Reserved\n",
    "\n",
    "This notebook provides a step-by-step introduction to Ray Tasks, the fundamental building block of Ray that enables distributed computing.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b> Here is the roadmap for this notebook </b>\n",
    "\n",
    "<ol>\n",
    "  <li>Overview and setup</li>\n",
    "  <li>Simple task submission (creating, executing, and getting results)</li>\n",
    "  <li>Task options and configuration</li>\n",
    "  <li>Object store and memory model</li>\n",
    "  <li>Chaining tasks and passing data</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd37367",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30318fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ray\n",
    "import requests\n",
    "import ray.runtime_context\n",
    "from ray import tune\n",
    "from ray.util.scheduling_strategies import NodeAffinitySchedulingStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef738c06",
   "metadata": {},
   "source": [
    "## 1. Overview and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c118fc2a",
   "metadata": {},
   "source": [
    "### 1.1. Ray Core at a glance\n",
    "\n",
    "- **Scales your code** across many CPU cores, machines, and accelerators.  \n",
    "- **Schedules arbitrary task graphs** thanks to its distributed scheduler.\n",
    "- **Hides distributed-system overhead** with built-ins for  \n",
    "  - fast data serialization and transfer,  \n",
    "  - smart task placement, \n",
    "  - distributed memory & reference counting.\n",
    "\n",
    "Ray's higher-level libraries build on Ray Core to offer ready-made APIs for common workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4655775",
   "metadata": {},
   "source": [
    "### 1.2. When to use Ray Tasks\n",
    "\n",
    "Ray Tasks are ideal for:\n",
    "- **Parallelizing computationally expensive functions** across multiple cores or machines\n",
    "- **Processing large datasets** by distributing work across workers\n",
    "- **Building complex task dependency graphs** (DAGs) for data pipelines\n",
    "- **Scaling existing Python code** with minimal changes\n",
    "\n",
    "**When NOT to use Ray Tasks:**\n",
    "- Functions that execute in < 1ms (overhead not worth it)\n",
    "- Very fine-grained parallelism (e.g., parallelizing simple arithmetic - use numpy instead)\n",
    "- When you need mutable shared state (use Ray Actors instead)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2677e6",
   "metadata": {},
   "source": [
    "### 1.3. Ray cluster architecture\n",
    "\n",
    "Before diving into tasks, let's understand the key components of a Ray cluster.\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/ray-cluster.svg\" width=\"800\">\n",
    "\n",
    "A Ray cluster consists of:\n",
    "- One or more **worker nodes**, where each worker node consists of the following processes:\n",
    "    - **worker processes** responsible for task submission and execution.\n",
    "    - A **raylet** responsible for:\n",
    "      - resource management and task placement.\n",
    "      - shared memory management through an object store \n",
    "- One of the worker nodes is designated a **head node** and is responsible for running \n",
    "  - A **global control service** responsible for keeping track of the **cluster-level state** that is not supposed to change too frequently.\n",
    "  - An **autoscaler** service responsible for adding and removing worker nodes by integrating with different infrastructure providers (e.g. AWS, GCP, ...) to match the resource requirements of the cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234c092a",
   "metadata": {},
   "source": [
    "### 1.4. Initializing Ray\n",
    "\n",
    "ray.init() is the primary function to connect to an existing Ray cluster or start a new one and connect to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772a9b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df49cd27",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**NOTE** In case you don't manually call ray.init() inside a python script, Ray will automatically call ray.init() for you with default parameters when you define or invoke your first remote function or actor.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b272ae",
   "metadata": {},
   "source": [
    "## 2. Simple task submission (creating, executing, and getting results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4945f344",
   "metadata": {},
   "source": [
    "### 2.1. Creating remote functions\n",
    "\n",
    "The first step in using Ray is to create remote functions. A remote function is a regular Python function that can be executed on any process in your cluster.\n",
    "\n",
    "Given a simple Python function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a78c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddacdfb2",
   "metadata": {},
   "source": [
    "Decorate the function with `@ray.remote` to turn it into a remote function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f2f690",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def remote_add(a, b):\n",
    "    return a + b\n",
    "\n",
    "remote_add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a758026",
   "metadata": {},
   "source": [
    "### 2.2. Executing remote functions (asynchronous by default)\n",
    "\n",
    "Native python functions are invoked by calling them:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c3178",
   "metadata": {},
   "outputs": [],
   "source": [
    "add(1, 2)  # Returns 3 immediately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9f8696",
   "metadata": {},
   "source": [
    "Remote ray functions are executed as tasks by calling them with `.remote()` suffix:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852d40cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_add.remote(1, 2)  # Returns ObjectRef immediately, computation happens async"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf7da6c",
   "metadata": {},
   "source": [
    "Here is what happens when you call `{remote_function}.remote`:\n",
    "1. Ray **immediately** schedules the function execution by submitting a **task** to the cluster\n",
    "2. The submitting process returns an `ObjectRef` (a reference to the future result)\n",
    "3. The cluster begins executing the computation in the background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d45248d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <strong>A <a href=\"https://docs.ray.io/en/latest/ray-core/key-concepts.html#tasks\" target=\"_blank\">task</a></strong> is a remote, stateless Python function invocation.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6bfb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = remote_add.remote(1, 2)\n",
    "ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff9654a",
   "metadata": {},
   "source": [
    "**Think of `ObjectRef` as a future**: it's a placeholder for a value that is being computed on the cluster.\n",
    "\n",
    "Here is a map of how Python code is translated into Ray tasks:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-core/python_to_ray_task_map_v2.png\" alt=\"Python to Ray Task Map\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d3bb71",
   "metadata": {},
   "source": [
    "### 2.3. Getting results\n",
    "\n",
    "If we want to wait (block) and retrieve the corresponding object, we can use `ray.get`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83df25c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.get(ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee36504",
   "metadata": {},
   "source": [
    "`ray.get()` works with single ObjectRefs or lists:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e2475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single result\n",
    "result = ray.get(ref)\n",
    "\n",
    "# Multiple results\n",
    "refs = [remote_add.remote(i, i) for i in range(5)]\n",
    "results = ray.get(refs)  # Wait for all to complete\n",
    "print(results)  # [0, 2, 4, 6, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa80b4b8",
   "metadata": {},
   "source": [
    "### 2.4. Putting it all together\n",
    "\n",
    "Here are the three steps:\n",
    "1. Create the remote function\n",
    "2. Execute it remotely (non-blocking)\n",
    "3. Get the result when needed (blocking)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "__Activity: define and invoke a Ray task__\n",
    "\n",
    "Define a remote function `sqrt_add` that accepts two arguments and performs the following steps:\n",
    "1. computes the square-root of the first\n",
    "2. adds the second\n",
    "3. returns the result\n",
    "\n",
    "Execute it with 2 different sets of parameters and collect the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33033362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: define the below as a remote function\n",
    "def sqrt_add(a, b):\n",
    "    ... \n",
    "\n",
    "# Hint: invoke it as a remote task and collect the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a37e188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e515b5",
   "metadata": {
    "region_name": "markdown"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary> Click to see solution </summary>\n",
    "\n",
    "\n",
    "```python\n",
    "@ray.remote\n",
    "def sqrt_add(a, b):\n",
    "    return math.sqrt(a) + b\n",
    "\n",
    "ray.get([sqrt_add.remote(2, 3), sqrt_add.remote(5, 4)])\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92c3407",
   "metadata": {},
   "source": [
    "### 2.5. Understanding asynchronous execution\n",
    "\n",
    "The key difference between regular Python and Ray is that `.remote()` **does not block**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b41ba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slow_function(x):\n",
    "    time.sleep(3)\n",
    "    return x * x\n",
    "\n",
    "# Sequential Python (blocks for each call)\n",
    "start = time.time()\n",
    "results = [slow_function(i) for i in range(4)]  # Would take 12 seconds!\n",
    "print(f\"Sequential: {time.time() - start:.2f}s\")\n",
    "\n",
    "slow_function = ray.remote(slow_function)\n",
    "\n",
    "# Distributed Ray (non-blocking)\n",
    "start = time.time()\n",
    "refs = [slow_function.remote(i) for i in range(4)]  # Returns immediately!\n",
    "print(f\"Task submission: {time.time() - start:.2f}s\")  # < 0.01s\n",
    "\n",
    "# Now wait for results (blocks until all complete)\n",
    "results = ray.get(refs)\n",
    "print(f\"Total with parallelism: {time.time() - start:.2f}s\")  # ~3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3a52c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "%xdel refs\n",
    "%xdel results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8090139e",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a642e5",
   "metadata": {},
   "source": [
    "### 2.6. What can be passed to Ray tasks? (Serialization)\n",
    "\n",
    "Ray uses **cloudpickle** to serialize code (functions, arguments and return values). Most Python objects work, but there are limitations:\n",
    "\n",
    "**‚úÖ Can serialize:**\n",
    "- Basic types: int, float, str, bool, None\n",
    "- Collections: list, dict, tuple, set\n",
    "- NumPy arrays, Pandas DataFrames\n",
    "- Most custom classes\n",
    "- Nested functions and lambdas\n",
    "\n",
    "**‚ùå Cannot serialize:**\n",
    "- File handles (`open()` objects)\n",
    "- Network sockets\n",
    "- Threading locks\n",
    "\n",
    "**Example of serialization issues:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8410a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå BAD: File handle won't serialize\n",
    "file = open(\"/tmp/data.txt\", \"w\")\n",
    "\n",
    "@ray.remote\n",
    "def read_file(f):\n",
    "    return f.read()\n",
    "\n",
    "# ref = read_file.remote(file)  # Will fail with a PicklingError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3385f8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>üí° Troubleshooting:</b> If you see <code>pickle.PicklingError</code> or <code>TypeError: cannot pickle</code>, check if you're passing non-serializable objects to your task.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8906070",
   "metadata": {},
   "source": [
    "### 2.7 Task submission sequence\n",
    "\n",
    "Here is the sequence of events when you submit a Ray task:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-core/task-submission_old.gif\" alt=\"Task Submission Sequence\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ca3c2e",
   "metadata": {},
   "source": [
    "### 2.8 Task submission under the hood\n",
    "\n",
    "When a task is submitted, here is how resource fulfillment works\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-core/normal-task-resource-fullfilment.svg\" width=\"700\" alt=\"Resource fulfillment and execution of `double(2)` in a Ray cluster.\">\n",
    "\n",
    "The caller must choose **which node (raylet)** should schedule it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2210edb2",
   "metadata": {},
   "source": [
    "#### 1Ô∏è‚É£ Choosing the Preferred Raylet\n",
    "| **Rule**          | **When Used**                    | **How It Works**                                                                                    |\n",
    "| ----------------- | -------------------------------- | --------------------------------------------------------------------------------------------------- |\n",
    "| **Data locality** | Task has data dependencies.      | Pick node holding the most object bytes locally (from the object directory, may be slightly stale). |\n",
    "| **Node affinity** | Task specifies a target node.    | Use the node from `NodeAffinitySchedulingStrategy`.                                                 |\n",
    "| **Default**       | No data or affinity preferences. | Use the **local raylet**.                                                                           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc505a3",
   "metadata": {},
   "source": [
    "#### 2Ô∏è‚É£ Request ‚Üí Lease ‚Üí Worker\n",
    "- Caller sends a **resource request** to the preferred raylet.  \n",
    "- If granted, the raylet **leases a local worker** and returns its address.  \n",
    "- The **lease stays active** while both caller and worker are alive.  \n",
    "- Idle or unused leases are returned after a short timeout (~hundreds of ms).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0bda8e",
   "metadata": {},
   "source": [
    "#### 3Ô∏è‚É£ Task Execution on the Leased Worker\n",
    "The caller can schedule **multiple compatible tasks** on the same worker without re-contacting the scheduler.\n",
    "\n",
    "Compatibility means matching:\n",
    "- **Resource shape**, e.g. `{\"CPU\": 1}`\n",
    "- **Shared-memory arguments** (large objects must be local; small ones are inlined)\n",
    "- **Runtime environment**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c923330",
   "metadata": {},
   "source": [
    "#### 4Ô∏è‚É£ Optimization Insight\n",
    "Worker leases act as a *cache* for scheduling decisions ‚Äî similar tasks can reuse the same worker for lower latency and higher throughput.\n",
    "\n",
    "Note also that the caller can hold multiple worker leases to increase parallelism. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fda9ba",
   "metadata": {},
   "source": [
    "## 3. Task options and configuration\n",
    "\n",
    "You can dynamically configure tasks using the `.options()` method without redefining the function. This is useful for adjusting resources, retries, or other settings per task invocation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077b0f59",
   "metadata": {},
   "source": [
    "### 3.1. Basic usage of .options()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd6b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def flexible_task(x):\n",
    "    return x * 2\n",
    "\n",
    "# Use default configuration (1 CPU)\n",
    "ref1 = flexible_task.remote(5)\n",
    "\n",
    "# Override to use 2 CPUs for this specific invocation\n",
    "ref2 = flexible_task.options(num_cpus=2).remote(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37775635",
   "metadata": {},
   "source": [
    "### 3.2. Common options\n",
    "\n",
    "**Resource options:**\n",
    "- `num_cpus`: Number of CPUs (can be fractional, e.g., 0.5)\n",
    "- `num_gpus`: Number of GPUs (can be fractional)\n",
    "- `memory`: Memory in bytes\n",
    "- `resources`: Dict of custom resources\n",
    "\n",
    "**Fault tolerance options:**\n",
    "- `max_retries`: Max number of retries (default: 3 for system errors)\n",
    "- `retry_exceptions`: List of exception types to retry on\n",
    "\n",
    "**Execution options:**\n",
    "- `runtime_env`: Dict specifying runtime environment\n",
    "- `scheduling_strategy`: Control task placement\n",
    "- `name`: Name for debugging/monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8999b3",
   "metadata": {},
   "source": [
    "### 3.3. Scheduling strategies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6504087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default: Ray decides based on data locality and load\n",
    "flexible_task.remote(4)\n",
    "\n",
    "# SPREAD: Distribute tasks across nodes\n",
    "flexible_task.options(scheduling_strategy=\"SPREAD\").remote(5)\n",
    "\n",
    "# Node affinity: Run on specific node\n",
    "strategy = NodeAffinitySchedulingStrategy(\n",
    "    node_id=ray.get_runtime_context().get_node_id(),\n",
    "    soft=True  # soft=True allows fallback if node unavailable\n",
    ")\n",
    "flexible_task.options(scheduling_strategy=strategy).remote(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233a4098",
   "metadata": {},
   "source": [
    "## 4. Object store and memory model\n",
    "\n",
    "Each worker node has its own object store, and collectively, these form a shared object store across the cluster.\n",
    "\n",
    "Remote objects are immutable. That is, their values cannot be changed after creation. This allows remote objects to be replicated in multiple object stores without needing to synchronize the copies.\n",
    "\n",
    "| <img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/ray-core/ray-cluster.png\" width=\"700px\" loading=\"lazy\"> |\n",
    "| :---------------------------------------------------------------------------------------------------------------------------- |\n",
    "| A Ray cluster with a head node and two worker nodes. Highlighted in orange is distributed object store.                       |\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://docs.ray.io/en/latest/ray-core/key-concepts.html#objects\" target=\"_blank\">Object</a></strong> - tasks and actors create and work with remote objects, which can be stored anywhere in a cluster. These objects are accessed using <strong>ObjectRef</strong> and are cached in a distributed shared-memory <strong>object store</strong>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9429a8",
   "metadata": {},
   "source": [
    "### 4.1. Ray memory model\n",
    "\n",
    "Ray manages memory in several ways to efficiently handle distributed tasks:\n",
    "\n",
    "1. **Heap memory**:\n",
    "   - Used by workers to execute tasks and actors.\n",
    "   - Used to store small objects (less than 100KB) and Ray metadata.\n",
    "   - High memory pressure can cause Ray to terminate some tasks to free up resources.\n",
    "\n",
    "2. **Shared memory (Object Store)**:\n",
    "   - Serves as the medium for passing data between tasks.\n",
    "   - Large objects (greater than 100KB) are stored in a shared memory space, using up to 30% of a node's memory.\n",
    "   - If more space is needed, objects can be spilled to disk or stored on disk in a slower-access format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea75528b",
   "metadata": {},
   "source": [
    "### 4.2. Example: Producer-consumer pattern with numpy arrays\n",
    "\n",
    "This example demonstrates how Ray transfers data in the distributed object store. The `producer_task` creates a 4 GiB numpy array, and the `consumer_task` accesses it with zero-copy deserialization when on the same node:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89785292",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def producer_task(size_mb: int = 4 * 1024) -> np.ndarray:\n",
    "    array = np.random.rand((1024**2 * size_mb // 8)).astype(np.float64)\n",
    "    return array\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def consumer_task(array: np.ndarray) -> None:\n",
    "    assert isinstance(array, np.ndarray)\n",
    "    assert not array.flags.owndata  # Confirms zero-copy\n",
    "\n",
    "# arr_ref = producer_task.remote()  # Produce a 4 GiB array\n",
    "# output_ref = consumer_task.remote(arr_ref)  # Pass ObjectRef to consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1504b0a",
   "metadata": {},
   "source": [
    "**What happens under the hood:**\n",
    "\n",
    "1. **Producer task** creates the array in heap memory, then Ray stores it in the shared object store (large objects > 100KB)\n",
    "2. **Consumer task** receives the `ObjectRef` and directly accesses the array from shared memory with zero-copy deserialization (if on same node)\n",
    "3. If tasks run on different nodes, Ray copies the array across the network only once\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-data-deep-dive/producer-consumer-object-store-v2.png\" width=\"600\">\n",
    "\n",
    "To see memory usage in action, run this inspection script:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee6ff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/memory_inspection.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ce8fb",
   "metadata": {},
   "source": [
    "#### On zero-copy deserialization\n",
    "\n",
    "Ray uses **cloudpickle** for serialization and **pickle 5** for zero-copy deserialization. \n",
    "\n",
    "**How Ray transfers code and data:**\n",
    "\n",
    "1. **Code transfer (functions)**: Functions are pickled and stored in the Global Control Store (GCS), then cached for subsequent calls\n",
    "\n",
    "2. **Data transfer (arguments/return values)**:\n",
    "   - **Small objects (< 100 KB)**: Pickled and transferred inline with the task metadata\n",
    "   - **Large objects (> 100 KB)**: Stored in shared memory (object store), only the `ObjectRef` is transferred\n",
    "\n",
    "**Key performance characteristics:**\n",
    "\n",
    "- **Zero-copy benefits**: Works for contiguous numpy arrays and PyArrow arrays on the same node, enabling efficient read access without data copying. \n",
    "- **Zero-copy limitation**: Does not support PyTorch tensors or other array types\n",
    "- **Immutability**: Objects in the object store are **immutable once sealed**, enabling safe sharing across processes\n",
    "\n",
    "To read more about object serialization in Ray, see [this documentation page here](https://docs.ray.io/en/latest/ray-core/objects/serialization.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c1e79c",
   "metadata": {},
   "source": [
    "### 4.3. Usecase: Hyper-parameter tuning\n",
    "\n",
    "**The Problem**: When running hyperparameter tuning or experimentation, you often need to use the same dataset across dozens or hundreds of trials. If you pass the dataset by value to each training function, Ray will serialize it repeatedly, wasting memory and time.\n",
    "\n",
    "**The Solution**: Store the dataset once in the object store using `ray.put()`, then pass only the lightweight `ObjectRef` to each trial. All workers can access the same data without duplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48488072",
   "metadata": {},
   "source": [
    "#### Real-world scenario: Grid search with shared data\n",
    "\n",
    "Imagine running 20 experiments on a 100MB training dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db38479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a 100MB training dataset\n",
    "df = pd.DataFrame(np.random.rand(100 * 1024 ** 2 // 8))\n",
    "\n",
    "@ray.remote\n",
    "def train_model(data, learning_rate, batch_size):\n",
    "    # Simulate model training\n",
    "    result = data.mean().sum() * learning_rate / batch_size\n",
    "    time.sleep(20)\n",
    "    return {\"lr\": learning_rate, \"batch_size\": batch_size, \"score\": result}\n",
    "\n",
    "# Grid search: 20 different hyperparameter combinations\n",
    "hyperparameters = [\n",
    "    {\"lr\": lr, \"batch_size\": bs}\n",
    "    for lr in [0.001, 0.01, 0.1, 0.5]\n",
    "    for bs in [32, 64, 128, 256, 512]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a8812a",
   "metadata": {},
   "source": [
    "Here is an efficient way to run the experiments by passing the dataset by reference:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dcbbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Memory Efficient: Pass ObjectRef (100 MB total memory)\n",
    "# Ray serializes once, all workers share the same data\n",
    "df_ref = ray.put(df)\n",
    "[\n",
    "    train_model.remote(df_ref, hp[\"lr\"], hp[\"batch_size\"]) \n",
    "    for hp in hyperparameters\n",
    "]\n",
    "print(\"Pass by reference: ~100 MiB memory used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1be68ea",
   "metadata": {},
   "source": [
    "Let's inspect the object store, we should only see the same 100MiB object being used across tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc8019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ray list objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b21909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "%xdel df_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccce33c",
   "metadata": {},
   "source": [
    "Here is the inefficient way by passing the dataset by value:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceea0dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Memory inefficient: Pass dataframe by value (2 GB total memory!)\n",
    "# Ray serializes 100MB √ó 20 times = 2 GB of redundant data\n",
    "[\n",
    "    train_model.remote(df, hp[\"lr\"], hp[\"batch_size\"]) \n",
    "    for hp in hyperparameters\n",
    "]\n",
    "print(\"Pass by value: ~2GiB memory used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e9b533",
   "metadata": {},
   "source": [
    "Let's inspect the object store, we should now see different 100MiB objects being used across tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68775f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ray list objects "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03d4382",
   "metadata": {},
   "source": [
    "**Performance comparison:**\n",
    "- **Pass by value**: 2 GB memory used (20√ó serialization overhead)\n",
    "- **Pass by reference**: 100 MB memory used (1√ó serialization)\n",
    "\n",
    "**Rule of thumb**: Pass by value only for small literals (< 100 KiB); otherwise, pass by reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d818495",
   "metadata": {},
   "source": [
    "#### How Ray Tune leverages this pattern\n",
    "\n",
    "Ray Tune uses `tune.with_parameters()` to automatically pass large constant objects via the object store:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0628610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainable(config, data):\n",
    "    # Each trial receives a reference to the shared data\n",
    "    model = train(data, lr=config[\"lr\"], epochs=config[\"epochs\"])\n",
    "    return {\"accuracy\": model.eval()}\n",
    "\n",
    "# Tune automatically stores train_data in the object store\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_parameters(trainable, data=pd.DataFrame()),  # Passed by reference\n",
    "    param_space={\"lr\": tune.grid_search([0.001, 0.01, 0.1]), \"epochs\": tune.choice([10, 20, 50])},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a371a1a2",
   "metadata": {},
   "source": [
    "Without `tune.with_parameters()`, each trial would receive a separate copy of `train_data`, multiplying memory usage by the number of concurrent trials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dec236",
   "metadata": {},
   "source": [
    "### 4.4. Distributed ownership and fate-sharing\n",
    "\n",
    "Ray uses a **distributed ownership model** to manage objects efficiently across the cluster. Understanding this concept is crucial for building robust distributed applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29c1221",
   "metadata": {},
   "source": [
    "#### How distributed ownership works\n",
    "\n",
    "In Ray, the process that creates or submits a task becomes the **owner** of the task's result. The owner maintains critical metadata about the object, including:\n",
    "- Object location(s) in the cluster\n",
    "- Reference counts\n",
    "- Object size and other properties\n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/distributed_ownership_overview_v4.svg\" width=\"800px\">\n",
    "\n",
    "**Benefits of distributed ownership:**\n",
    "- **Lower latency**: No need to communicate all ownership information back to a central node\n",
    "- **Better scalability**: No single bottleneck as every worker maintains its own ownership information\n",
    "\n",
    "Here is a diagram that explains how distributed ownership works in a Ray cluster:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-core/distributed-ownership.png\" width=\"800\">\n",
    "\n",
    "Below is some code based on the above diagram to illustrate distributed ownership:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafca308",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def b():\n",
    "    size_mib = 19\n",
    "    return np.ones(1024 ** 2 // 8 *  size_mib)\n",
    "\n",
    "@ray.remote\n",
    "def a(dep):\n",
    "    z = b.remote() # z is owned by process running task a\n",
    "\n",
    "    ip = ray.util.get_node_ip_address()\n",
    "    print(f\"{ip=}\")\n",
    "\n",
    "    time.sleep(20)\n",
    "    return dep.sum() / ray.get(z).sum() \n",
    "\n",
    "size_mib = 33\n",
    "arr = np.ones(1024 ** 2 // 8 *  size_mib)\n",
    "x = ray.put(arr)  # x is owned by driver\n",
    "y = a.remote(x)  # y is owned by driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1b9e11",
   "metadata": {},
   "source": [
    "We can verify that the the 19 MB array is owned by the worker that submitted task `b` - i.e. the worker executing task `a` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba1b33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ray list objects --filter TASK_STATUS!=NIL --filter TYPE=WORKER "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b12372",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note: `TASK_STATUS = NIL` matches non-owner processes given only the owner tracks task status\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c2da3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.get(y), 33 / 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4ce36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "%xdel x\n",
    "%xdel y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c30f2e",
   "metadata": {},
   "source": [
    "#### The fate-sharing limitation\n",
    "\n",
    "The main tradeoff of distributed ownership is **fate-sharing**: objects are tied to the lifetime of their owner process.\n",
    "\n",
    "**What this means:**\n",
    "- Even if an object is stored in the object store on a different node, if the owner process dies, the object becomes unreachable\n",
    "- The owner maintains critical metadata (locations, reference counts) that other processes need to access the object\n",
    "- When the owner fails, this metadata is lost, making the object inaccessible even if copies exist elsewhere\n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/distributed_ownership_fate_share_with_owner_v4.svg\" width=\"900px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a42f624",
   "metadata": {},
   "source": [
    "#### Example: Demonstrating fate-sharing\n",
    "\n",
    "This example creates two actors: an **Owner** that creates an object reference, and a **Borrower** that tries to access it. We'll see what happens when the Owner is terminated:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678fcf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def f(data):\n",
    "    return data\n",
    "\n",
    "@ray.remote\n",
    "class Owner:\n",
    "    def __init__(self):\n",
    "        self.ref = None\n",
    "\n",
    "    def set_object_ref(self, data):\n",
    "        self.ref = f.remote(data)\n",
    "        return self.ref\n",
    "    \n",
    "    def is_alive(self): \n",
    "        return True\n",
    "\n",
    "@ray.remote\n",
    "class Borrower:\n",
    "    def get_object(self, ref):\n",
    "        return ray.get(ref)\n",
    "\n",
    "owner = Owner.remote()\n",
    "borrower = Borrower.remote()\n",
    "assert ray.get(owner.is_alive.remote())\n",
    "\n",
    "object_ref = owner.set_object_ref.remote(data=\"test1\")\n",
    "data = ray.get(borrower.get_object.remote(object_ref))\n",
    "assert data == \"test1\"\n",
    "print(f\"‚úì Successfully retrieved data while Owner is alive: {data}\")\n",
    "\n",
    "ray.kill(owner)\n",
    "time.sleep(2)\n",
    "\n",
    "try:\n",
    "    ray.get(borrower.get_object.remote(object_ref))\n",
    "    print(\"‚úó Unexpected: Should have failed!\")\n",
    "except Exception as e:\n",
    "    print(\"‚úì Failed as expected after owner termination:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa9761d",
   "metadata": {},
   "source": [
    "**What happens:**\n",
    "1. While the Owner is alive, the Borrower can successfully retrieve the object using the `ObjectRef`\n",
    "2. After the Owner is killed, the Borrower still has the `ObjectRef`, but attempting to access the object fails\n",
    "3. Even though the object data may still exist in the object store, the ownership metadata is lost\n",
    "\n",
    "**Key takeaway:** In Ray's distributed ownership model, object lifetime is tied to the owner's lifetime. When building fault-tolerant applications:\n",
    "- Keep important owners alive (e.g., use long-running actors or the driver process)\n",
    "- Consider checkpointing critical data outside Ray's object store for durability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a999484",
   "metadata": {
    "region_name": "markdown"
   },
   "source": [
    "### 4.5 Lineage Reconstruction\n",
    "\n",
    "If instead, the owner is still alive, but the object is lost (e.g., due to node failure), Ray can reconstruct the object by either\n",
    "\n",
    "1. Finding any secondary copies in the object store (if they exist) and returning one of those\n",
    "2. Re-executing the task or task chain that created it. This is known as **lineage reconstruction**.\n",
    "\n",
    "See the below test inspired from the [Ray test suite](https://github.com/ray-project/ray/blob/a04cb06bb1a2c09e93b882b611492d62b8d1837a/python/ray/tests/test_reconstruction.py#L126) for an example of lineage reconstruction:\n",
    "\n",
    "```python\n",
    "@pytest.mark.parametrize(\"reconstruction_enabled\", [False, True])\n",
    "def test_basic_reconstruction(config, ray_start_cluster, reconstruction_enabled):\n",
    "    cluster = ray_start_cluster\n",
    "    \n",
    "    # Start head node with reconstruction enabled/disabled\n",
    "    cluster.add_node(num_cpus=0, _system_config=config, \n",
    "                     enable_object_reconstruction=reconstruction_enabled)\n",
    "    ray.init(address=cluster.address)\n",
    "    \n",
    "    # Add worker node to store the object\n",
    "    node_to_kill = cluster.add_node(\n",
    "        num_cpus=1, resources={\"node1\": 1}, object_store_memory=10**8\n",
    "    )\n",
    "    cluster.wait_for_nodes()\n",
    "\n",
    "    @ray.remote(max_retries=1 if reconstruction_enabled else 0)\n",
    "    def create_large_object():\n",
    "        return np.zeros(10**7, dtype=np.uint8)\n",
    "\n",
    "    @ray.remote\n",
    "    def process_large_object(x):\n",
    "        return\n",
    "\n",
    "    # Create object and verify it can be used\n",
    "    # Note: obj_ref owner is the driver (on head node), so lineage is preserved\n",
    "    # even when the worker node storing the object is killed\n",
    "    obj_ref = create_large_object.options(resources={\"node1\": 1}).remote()\n",
    "    ray.get(process_large_object.options(resources={\"node1\": 1}).remote(obj_ref))\n",
    "    \n",
    "    # Simulate node failure and replacement\n",
    "    cluster.remove_node(node_to_kill, allow_graceful=False)\n",
    "    node_to_kill = cluster.add_node(\n",
    "        num_cpus=1, resources={\"node1\": 1}, object_store_memory=10**8\n",
    "    )\n",
    "\n",
    "    # With reconstruction: task re-executes and object is recreated\n",
    "    # Without reconstruction: both task and object are lost\n",
    "    if reconstruction_enabled:\n",
    "        ray.get(process_large_object.remote(obj_ref))\n",
    "    else:\n",
    "        with pytest.raises(ray.exceptions.RayTaskError):\n",
    "            ray.get(process_large_object.remote(obj_ref))\n",
    "        with pytest.raises(ray.exceptions.ObjectLostError):\n",
    "            ray.get(obj_ref)\n",
    "\n",
    "    # Second node failure exceeds max_retries\n",
    "    cluster.remove_node(node_to_kill, allow_graceful=False)\n",
    "    cluster.add_node(num_cpus=1, resources={\"node1\": 1}, object_store_memory=10**8)\n",
    "\n",
    "    expected_error = (\n",
    "        ray.exceptions.ObjectReconstructionFailedMaxAttemptsExceededError\n",
    "        if reconstruction_enabled\n",
    "        else ray.exceptions.ObjectLostError\n",
    "    )\n",
    "    with pytest.raises(expected_error):\n",
    "        ray.get(obj_ref)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981dc45c",
   "metadata": {},
   "source": [
    "### 4.6 ObjectRef lifecycle and garbage collection\n",
    "\n",
    "Objects in the object store are automatically garbage collected when teir distributed reference count drops to zero. This happens when all `ObjectRef`s pointing to the object are deleted or go out of scope.\n",
    "\n",
    "**Example:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb594df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_returns=2)\n",
    "def create_object():\n",
    "    task_id = ray.runtime_context.get_runtime_context().get_task_id()\n",
    "    return np.random.rand(1024 ** 2 // 8 * 20), task_id\n",
    "\n",
    "# Object created and stored\n",
    "ref1, ref2 = create_object.remote()\n",
    "\n",
    "# Object still in memory\n",
    "result = ray.get(ref1)\n",
    "task_id = ray.get(ref2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd782af",
   "metadata": {},
   "source": [
    "Let's inspect the returned objects in the store - note in this case we leverage the object id specification to find the object in the store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c055250",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "!ray list objects --filter TASK_STATUS!=NIL --filter TYPE=DRIVER --filter OBJECT_ID={task_id}01000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596e8b0b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Release reference (allows GC)\n",
    "del ref1\n",
    "del ref2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b10e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ray list objects --filter TASK_STATUS!=NIL --filter TYPE=DRIVER --filter OBJECT_ID={task_id}01000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9376eafb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>‚ö° Performance Tip:</b> For long-running applications, explicitly delete ObjectRefs you no longer need to free up object store memory.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825f7805",
   "metadata": {},
   "source": [
    "## 5. Chaining tasks and passing data\n",
    "\n",
    "Let's say we now want to execute a graph of two tasks:\n",
    "1. Square a value using `expensive_square`\n",
    "2. Add 1 to the `expensive_square` result, by using `remote_add`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85a8c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def expensive_square(x):\n",
    "    time.sleep(1)\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd38804d",
   "metadata": {},
   "source": [
    "This can be achieved without fetching an intermediate result.\n",
    "\n",
    "**‚ùå Anti-pattern:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d23553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st task\n",
    "square_ref = expensive_square.remote(2)\n",
    "square_value = ray.get(square_ref)  # wait to get the value\n",
    "\n",
    "# 2nd task\n",
    "sum_ref = remote_add.remote(1, square_value)  # pass value from 1st task\n",
    "sum_value = ray.get(sum_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1597c4",
   "metadata": {},
   "source": [
    "**‚úÖ Better:** Chain the tasks by passing the `ObjectRef` directly to the second task:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d267b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "square_ref = expensive_square.remote(2)\n",
    "sum_ref = remote_add.remote(1, square_ref)  # Pass ObjectRef, not value!\n",
    "sum_value = ray.get(sum_ref)  # Wait only at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae9e9ff",
   "metadata": {},
   "source": [
    "**Why this is better:**\n",
    "- No unnecessary data transfer (ObjectRef is just an ID)\n",
    "- Ray automatically handles dependencies\n",
    "- Second task waits for first task to complete\n",
    "- More efficient scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4b66e1",
   "metadata": {},
   "source": [
    "### 5.1. Common task graph patterns\n",
    "\n",
    "Ray excels at executing complex directed acyclic graphs (DAGs) of tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee34381",
   "metadata": {},
   "source": [
    "#### Pattern 1: Linear chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddf67d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def step1(data):\n",
    "    return process_a(data)\n",
    "\n",
    "@ray.remote\n",
    "def step2(data):\n",
    "    return process_b(data)\n",
    "\n",
    "@ray.remote\n",
    "def step3(data):\n",
    "    return process_c(data)\n",
    "\n",
    "# Chain tasks\n",
    "# ref1 = step1.remote(input_data)\n",
    "# ref2 = step2.remote(ref1)\n",
    "# ref3 = step3.remote(ref2)\n",
    "# final_result = ray.get(ref3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99085cdb",
   "metadata": {},
   "source": [
    "#### Pattern 2: Fan-out / Fan-in (MapReduce)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438463ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def map_task(chunk):\n",
    "    return process_chunk(chunk)\n",
    "\n",
    "@ray.remote\n",
    "def reduce_task(results):\n",
    "    return aggregate(results)\n",
    "\n",
    "# Map phase (fan-out)\n",
    "# map_refs = [map_task.remote(chunk) for chunk in data_chunks]\n",
    "\n",
    "# Reduce phase (fan-in)\n",
    "# final_result = ray.get(reduce_task.remote(map_refs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d710ca6",
   "metadata": {},
   "source": [
    "#### Pattern 3: Tree reduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3251ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def pairwise_sum(a, b):\n",
    "    return a + b\n",
    "\n",
    "\n",
    "refs = [ray.put(i) for i in range(16)]  # Initial values\n",
    "\n",
    "# Tree reduction (depth = log2(16) = 4)\n",
    "while len(refs) > 1:\n",
    "    refs = [pairwise_sum.remote(refs[i], refs[i + 1]) for i in range(0, len(refs), 2)]\n",
    "\n",
    "result = ray.get(refs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c8637c",
   "metadata": {},
   "source": [
    "### 5.2. Nested tasks\n",
    "\n",
    "Tasks can submit other tasks, enabling dynamic workflows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c1f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def main():\n",
    "    square_ref_1 = expensive_square.remote(1)\n",
    "    square_ref_2 = expensive_square.remote(2)\n",
    "    add_ref = remote_add.remote(square_ref_1, square_ref_2)\n",
    "    return ray.get(add_ref)\n",
    "\n",
    "ray.get(main.remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf44e82",
   "metadata": {},
   "source": [
    "**Avoiding deadlocks:** Ray automatically yields CPU resources when blocked on `ray.get()`, preventing deadlocks when nested tasks need the same resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cde78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster has 2 CPUs total\n",
    "\n",
    "@ray.remote(num_cpus=2)\n",
    "def outer_task():\n",
    "    inner_refs = [inner_task.remote() for _ in range(10)]\n",
    "    return ray.get(inner_refs)  # Ray yields the 2 CPUs while waiting\n",
    "\n",
    "@ray.remote(num_cpus=1)\n",
    "def inner_task():\n",
    "    return \n",
    "\n",
    "ray.get(outer_task.remote())  # Works! No deadlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c37b16e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Read more about <strong><a href=\"https://docs.ray.io/en/latest/ray-core/tasks/nested-tasks.html#yielding-resources-while-blocked\" target=\"_blank\">yielding resources while blocked</a></strong>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bbdbd2",
   "metadata": {},
   "source": [
    "## 6. Error handling and task retries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b6c5d4",
   "metadata": {},
   "source": [
    "### 6.1. Understanding exception types\n",
    "\n",
    "Let's consider two types of exceptions:\n",
    "1. **System errors**: Worker node dies, out of memory, network issues\n",
    "2. **Application-level errors**: Python exceptions in your code (ValueError, TypeError, etc.)\n",
    "\n",
    "Ray will automatically **retry a task up to 3 times** if it fails due to a system error (e.g., a worker node dies)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dffcc12",
   "metadata": {},
   "source": [
    "### 6.2. Handling application exceptions\n",
    "\n",
    "Below task won't be retried by default because it's an application failure:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df95ff3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def incorrect_square(x: int, prob: float) -> int:\n",
    "    if random.random() < prob:\n",
    "        raise ValueError(\"Random failure\")\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5268149",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ray.get([incorrect_square.remote(x=4, prob=0.5) for _ in range(10)])\n",
    "except ray.exceptions.RayTaskError as e:\n",
    "    print(f\"Task failed with: {e}\")\n",
    "    print(f\"Original exception: {e.cause}\")  # Access underlying exception"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565c90a0",
   "metadata": {},
   "source": [
    "**Exception propagation:**\n",
    "- Exceptions in tasks are wrapped in `RayTaskError`\n",
    "- The original exception is available via `.cause` attribute\n",
    "- `ray.get()` will raise the exception\n",
    "- ObjectRefs remain valid, but getting them raises the exception"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1464b5",
   "metadata": {},
   "source": [
    "### 6.3. Configuring retries\n",
    "\n",
    "Ray lets you specify how to handle retries when an exception is encountered:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8013fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(retry_exceptions=[ValueError])\n",
    "def correct_square(x: int, prob: float) -> int:\n",
    "    if random.random() < prob:\n",
    "        raise ValueError(\"Random failure\")\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ac9129",
   "metadata": {},
   "source": [
    "Note we did not have to re-define the remote function, instead we can create an updated version using `.options`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e670d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_square_mod = correct_square.options(\n",
    "    retry_exceptions=[ValueError],\n",
    "    max_retries=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd63de16",
   "metadata": {},
   "source": [
    "Let's try it out:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704a1810",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    outputs = ray.get([correct_square_mod.remote(x=4, prob=0.5) for _ in range(10)])\n",
    "    print(f\"Success! Results: {outputs}\")\n",
    "except ray.exceptions.RayTaskError:\n",
    "    print(\"At least one of the tasks failed after all retries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f71983",
   "metadata": {},
   "source": [
    "### 6.4. Idempotency: Critical for reliable retries\n",
    "\n",
    "**‚ö†Ô∏è WARNING:** Only retry tasks that are **idempotent** (can be safely run multiple times).\n",
    "\n",
    "**‚ùå Non-idempotent (dangerous to retry):**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18729bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(retry_exceptions=[ValueError])\n",
    "def append_to_file(data):\n",
    "    with open(\"/tmp/data.txt\", \"a\") as f:\n",
    "        f.write(data)  # Will duplicate data on retry!\n",
    "    if random.random() < 0.5:\n",
    "        raise ValueError(\"Simulated failure\")\n",
    "    return \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a076f98",
   "metadata": {},
   "source": [
    "**If this task fails and retries:**\n",
    "1. First attempt: Writes \"hello\" ‚Üí fails\n",
    "2. Retry: Writes \"hello\" again ‚Üí file now has \"hellohello\"\n",
    "\n",
    "**‚úÖ Idempotent (safe to retry):**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5fe54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(retry_exceptions=[ValueError])\n",
    "def write_to_file_safe(data, unique_id):\n",
    "    filename = f\"data_{unique_id}.txt\"\n",
    "    with open(filename, \"w\") as f:  # Overwrites on retry\n",
    "        f.write(data)\n",
    "    if random.random() < 0.5:\n",
    "        raise ValueError(\"Simulated failure\")\n",
    "    return \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd11888b",
   "metadata": {},
   "source": [
    "**Other idempotent operations:**\n",
    "- Reading from database\n",
    "- GET requests (not POST/PUT/DELETE)\n",
    "- Mathematical computations\n",
    "- Overwriting files (not appending)\n",
    "\n",
    "**Non-idempotent operations to avoid retrying:**\n",
    "- Appending to files/databases\n",
    "- Sending emails/notifications\n",
    "- Charging credit cards\n",
    "- Incrementing counters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fd0778",
   "metadata": {},
   "source": [
    "### 6.5. Task timeouts and cancellation\n",
    "\n",
    "Sometimes you want to set a maximum execution time or cancel tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02804e3",
   "metadata": {},
   "source": [
    "#### Setting timeouts with ray.get()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406f9258",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def slow_task():\n",
    "    time.sleep(100)\n",
    "    return \"done\"\n",
    "\n",
    "ref = slow_task.remote()\n",
    "\n",
    "try:\n",
    "    result = ray.get(ref, timeout=5)  # Wait max 5 seconds\n",
    "except ray.exceptions.GetTimeoutError:\n",
    "    print(\"Task took too long!\")\n",
    "    ray.cancel(ref)  # Cancel the task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52001d0a",
   "metadata": {},
   "source": [
    "#### Cancelling tasks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1822ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def long_running_task(duration):\n",
    "    time.sleep(duration)\n",
    "    return \"completed\"\n",
    "\n",
    "refs = [long_running_task.remote(10) for _ in range(5)]\n",
    "\n",
    "# Cancel all tasks\n",
    "for ref in refs:\n",
    "    ray.cancel(ref)\n",
    "\n",
    "# Check if cancelled\n",
    "try:\n",
    "    ray.get(refs[0])\n",
    "except ray.exceptions.TaskCancelledError:\n",
    "    print(\"Task was cancelled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c2ac4b",
   "metadata": {},
   "source": [
    "**Important notes about cancellation:**\n",
    "- Cancellation is best-effort, not guaranteed\n",
    "- Task might complete before cancellation takes effect\n",
    "- Dependent tasks are also cancelled\n",
    "- Use for cleanup, not critical functionality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b698c1",
   "metadata": {},
   "source": [
    "## 7. Task runtime environments\n",
    "\n",
    "A runtime environment defines dependencies such as files, packages, and environment variables needed for a Python script to run.\n",
    "\n",
    "- **Runtime Environment Management**:\n",
    "  - Managed by a `RuntimeEnvAgent` gRPC server on each node.\n",
    "  - The `RuntimeEnvAgent` fate-shares with the raylet, simplifying the failure model and ensuring it is a core component for task and actor scheduling.\n",
    "\n",
    "- **Environment Creation**:\n",
    "  - Triggered by the raylet via a gRPC request to the `RuntimeEnvAgent` when a task or actor requires a runtime environment.\n",
    "  - May involve:\n",
    "    - Installing packages using `pip install`.\n",
    "    - Setting environment variables for Ray worker processes.\n",
    "    - Activating conda environments with `conda activate`.\n",
    "    - Downloading files from remote cloud storage.\n",
    "\n",
    "- **Resource Caching**:\n",
    "  - Runtime environment resources, such as downloaded files and installed conda environments, are cached on each node.\n",
    "  - The cache allows sharing of resources between different tasks, actors, and jobs.\n",
    "  - When the cache size limit is exceeded, resources not currently in use are deleted to free up space.\n",
    "\n",
    "Here is a diagram showcasing the above concepts:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-core/runtime_env.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2fb2ce",
   "metadata": {},
   "source": [
    "### 7.1. Setting environment variables\n",
    "\n",
    "For example, we can set an environment variable:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c90c4a8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "@ray.remote(runtime_env={\"env_vars\": {\"my_custom_env\": \"prod\"}})\n",
    "def f():\n",
    "    env = os.environ[\"my_custom_env\"]\n",
    "    return f\"My custom env is {env}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b86c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.get(f.remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8f62fb",
   "metadata": {},
   "source": [
    "### 7.2. Installing pip dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f506cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(runtime_env={\"pip\": [\"requests\", \"pandas==1.5.0\"]})\n",
    "def fetch_data(url):\n",
    "    return requests.get(url).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c3e2ab",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "pip dependencies add overhead to first task startup but are cached afterwards; for frequently used dependencies, bake them into your cluster image instead.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c298a63",
   "metadata": {},
   "source": [
    "### 7.3. Working directory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995d0f7e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "@ray.remote(runtime_env={\"working_dir\": \"s3://my-bucket/project/my_directory.zip\"})\n",
    "def load_config():\n",
    "    with open(\"config.yaml\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1eeaed",
   "metadata": {},
   "source": [
    "## 8. Resource allocation and management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5202394c",
   "metadata": {},
   "source": [
    "### 8.1. Understanding logical vs physical resources\n",
    "\n",
    "Ray resource specifications are **logical**, not physical:\n",
    "\n",
    "- **Logical resources**: Used by Ray scheduler for placement decisions (default: `num_cpus=1`)\n",
    "- **Physical resources**: Actual CPU/GPU/memory usage by your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b6da18",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_cpus=1)  # Ray reserves 1 CPU slot for scheduling\n",
    "def cpu_intensive_task():\n",
    "    # Ray sets OMP_NUM_THREADS=1 to match num_cpus\n",
    "    return np.dot(large_matrix_a, large_matrix_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5209db04",
   "metadata": {},
   "source": [
    "**Key points:**\n",
    "- `num_cpus` is a scheduling hint, not a hard limit\n",
    "- Ray automatically sets `OMP_NUM_THREADS` to match `num_cpus` to prevent oversubscription\n",
    "\n",
    "You can override this if needed (may cause oversubscription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838c1700",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_cpus=1)\n",
    "def mm(n: int = 4000):\n",
    "    return np.dot(np.random.rand(n, n), np.random.rand(n, n))\n",
    "\n",
    "# Override to use 8 threads (caution: may oversubscribe)\n",
    "ray.get(mm.options(runtime_env={\"env_vars\": {\"OMP_NUM_THREADS\": \"8\"}}).remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fdcd2c",
   "metadata": {},
   "source": [
    "Note assigning \"GPU\" resources to a task, Ray will automatically set the `CUDA_VISIBLE_DEVICES` env var within the worker to limit it to specific GPU ids.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Learn more about <strong><a href=\"https://docs.ray.io/en/latest/ray-core/scheduling/resources.html#physical-resources-and-logical-resources\" target=\"_blank\">physical resources and logical resources</a></strong>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028f1541",
   "metadata": {},
   "source": [
    "### 8.2. Fractional resources for I/O-bound tasks\n",
    "\n",
    "Ray supports **fractional CPU requests** to enable efficient oversubscription of I/O-bound tasks.\n",
    "\n",
    "**When to use fractional CPUs:**\n",
    "\n",
    "Tasks that spend most of their time waiting (not computing) can share CPU slots:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af5ff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moderately I/O-bound: Some computation, some I/O\n",
    "@ray.remote(num_cpus=0.5)  # Allow 2 tasks per CPU core\n",
    "def download_and_parse(url):\n",
    "    data = requests.get(url).text\n",
    "    return process_file(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3017ee",
   "metadata": {},
   "source": [
    "**Benefits:**\n",
    "- **Higher throughput**: Run more tasks concurrently when they're waiting on I/O\n",
    "- **Better resource utilization**: Don't waste CPU cores on tasks that are mostly idle\n",
    "- **Cost efficiency**: Process more work on the same hardware\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Note:** Don't abuse fractional resources and fall into the anti-pattern of launching too many small tasks. Instead, batch work and leverage multi-threading within tasks when possible.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370ea732",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Fractional resources include support for <strong><a href=\"https://docs.ray.io/en/latest/ray-core/scheduling/accelerators.html#fractional-accelerators\" target=\"_blank\">multiple accelerators</a></strong>, allowing users to load multiple smaller models onto a single GPU. Learn more about <strong><a href=\"https://docs.ray.io/en/latest/ray-core/scheduling/resources.html#fractional-resource-requirements\" target=\"_blank\">fractional resource requirements</a></strong>.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e47ccf",
   "metadata": {},
   "source": [
    "### 8.3. Resource availability and cluster inspection\n",
    "\n",
    "Ray's scheduler matches tasks to nodes based on **resource requirements** like CPUs, GPUs, memory, or custom resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe87c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_cpus=2, num_gpus=1)\n",
    "def train_model(data):\n",
    "    return model.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe404fcd",
   "metadata": {},
   "source": [
    "**Inspecting cluster resources** returns total resources across the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f39196",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.cluster_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383f5696",
   "metadata": {},
   "source": [
    "**Inspecting available resources** returns currently unreserved resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632ff517",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.available_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31271485",
   "metadata": {},
   "source": [
    "### 8.4. Resource management and autoscaling\n",
    "\n",
    "Ray's **Global Control Service (GCS)** orchestrates cluster-wide resource management and autoscaling:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-core/resource_management_autoscaling.svg\" width=\"700\">\n",
    "\n",
    "**The resource synchronization loop:**\n",
    "\n",
    "1. **Raylets report usage**: Each raylet sends its resource usage to the GCS every ~100ms\n",
    "2. **GCS broadcasts state**: GCS pushes the global resource view back to all raylets every ~100ms  \n",
    "3. **Autoscaler reconciles**: The autoscaler queries the GCS for cluster load and:\n",
    "   - Adds nodes when demand exceeds available capacity\n",
    "   - Removes idle nodes to reduce costs\n",
    "\n",
    "**Why this matters:** Distributed scheduling requires fresh resource data. Stale views cause raylets to send tasks to overloaded nodes, creating scheduling delays.\n",
    "\n",
    "**Example:** If 10 tasks need `num_gpus=1` but only 4 GPUs exist, the autoscaler provisions additional GPU nodes to meet demand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a3a7df",
   "metadata": {},
   "source": [
    "## 9. Pipeline data processing and waiting for results\n",
    "\n",
    "After launching a number of tasks, you may want to know which ones have finished executing without blocking on all of them. This could be achieved by `ray.wait()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75d930b",
   "metadata": {},
   "source": [
    "### 9.1. Understanding ray.wait()\n",
    "\n",
    "`ray.wait()` is a powerful primitive for building pipelines and managing task completion.\n",
    "\n",
    "Given a sample remote function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ee259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def remote_fn(x):\n",
    "    time.sleep(random.uniform(2, 10))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf43add",
   "metadata": {},
   "source": [
    "Unlike `ray.get`, which blocks until all tasks are complete, `ray.wait` allows you to wait for a specified number of tasks to finish and returns two lists: one with the completed tasks and another with the pending tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417405ea",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "refs = [remote_fn.remote(i) for i in range(10)]\n",
    "\n",
    "ready, not_ready = ray.wait(\n",
    "    refs,\n",
    "    num_returns=1,      # Number of references to wait for\n",
    "    timeout=None,       # Max time to wait for (seconds)\n",
    "    fetch_local=True    # Whether to fetch objects to the local node or not\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a687a1f4",
   "metadata": {},
   "source": [
    "Let's inspect the ready refs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b0bd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1590395b",
   "metadata": {},
   "source": [
    "**Returns:**\n",
    "- `ready`: List of ObjectRefs that are ready\n",
    "- `not_ready`: List of ObjectRefs still pending"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a616d86",
   "metadata": {},
   "source": [
    "### 9.2. Pipeline pattern with ray.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c19e01e",
   "metadata": {},
   "source": [
    "| <img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/ray-core/pipeline-data-processing.png\" width=\"400px\" loading=\"lazy\">                                                                               |\n",
    "| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| (top panel) Execution timeline when using ray.get() to wait for all results before calling process results. (bottom panel) Execution timeline when using ray.wait() to process results as soon as they become available. |\n",
    "\n",
    "Here are functions to match the above diagram:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abf7578",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def do_some_work(x: int) -> int:\n",
    "    time.sleep(x)  # varying execution time based on input\n",
    "    return x\n",
    "\n",
    "@ray.remote\n",
    "def process_incremental(result: int) -> int:\n",
    "    time.sleep(1)\n",
    "    return result * 2\n",
    "\n",
    "@ray.remote\n",
    "def process_results(result_refs: list) -> list:\n",
    "    results = ray.get(result_refs)  # need to call ray.get explicitly for containers\n",
    "    out = []\n",
    "    for result in results:\n",
    "        time.sleep(1)\n",
    "        out.append(result * 2)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe14b78",
   "metadata": {},
   "source": [
    "This is the **naive approach:** block until all tasks are complete and then process the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0e3f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [2, 3, 1, 4]\n",
    "start = time.time()\n",
    "data_list = [do_some_work.remote(x) for x in inputs]\n",
    "output = ray.get(process_results.remote(data_list))\n",
    "print(\"duration =\", time.time() - start, \"\\nresult = \", output)\n",
    "# Duration: ~8 seconds (4s max task + 4s processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0938ae2d",
   "metadata": {},
   "source": [
    "This is the **pipelined** approach: process items as soon as they become available\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f715141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "result_ids = [do_some_work.remote(x) for x in inputs]\n",
    "refs = []\n",
    "while len(result_ids):\n",
    "    done_id, result_ids = ray.wait(result_ids, num_returns=1)\n",
    "    print(done_id)\n",
    "    refs.append(process_incremental.remote(done_id[0]))\n",
    "output = ray.get(refs)\n",
    "print(\"duration =\", time.time() - start, \"\\nresult = \", output)\n",
    "# Duration: ~5 seconds (overlapping ~4s computation and 1s processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77db1c9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Read more about the <strong><a href=\"https://docs.ray.io/en/latest/ray-core/tips-for-first-time.html#tip-4-pipeline-data-processing\" target=\"_blank\">pipeline data processing</a></strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bac785",
   "metadata": {},
   "source": [
    "## 10. Ray generators\n",
    "\n",
    "[Ray Generators](https://docs.ray.io/en/latest/ray-core/ray-generator.html) are a way to make use of the python generator pattern to generate data.\n",
    "\n",
    "They are useful for:\n",
    "- Reducing worker heap memory usage **by** avoiding building up a large in-memory collection\n",
    "- Reducing object store memory usage **by** allowing for garbage collection of objects that are processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9556cf",
   "metadata": {},
   "source": [
    "### 10.1. Why use Ray Generators?\n",
    "\n",
    "**Problem with regular tasks:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee679686",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def produce_large_dataset():\n",
    "    # Creates all data in memory at once\n",
    "    results = []\n",
    "    for i in range(100):\n",
    "        results.append(np.random.rand(1000, 1000))  # Each object is ~8MB\n",
    "    return results  # ~800MB in memory!\n",
    "\n",
    "# High memory pressure\n",
    "ref = produce_large_dataset.remote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5a648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ray list objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399c2f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "%xdel ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7b8148",
   "metadata": {},
   "source": [
    "**Solution with generators:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3646e7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def produce_large_dataset():\n",
    "    # Yields one object at a time\n",
    "    for i in range(100):\n",
    "        yield np.random.rand(1000, 1000)  # Only ~8MB at a time\n",
    "\n",
    "@ray.remote\n",
    "def process(result):\n",
    "    time.sleep(0.1)\n",
    "\n",
    "# Process streaming\n",
    "for obj_ref in produce_large_dataset.remote():\n",
    "    process.remote(result)\n",
    "    # Previous objects can be garbage collected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cc87a6",
   "metadata": {},
   "source": [
    "### 10.2. Python generator recap\n",
    "\n",
    "Let's start with a sample python generator function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adda18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_function():\n",
    "    for i in range(10):\n",
    "        time.sleep(1)\n",
    "        yield i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292c23cb",
   "metadata": {},
   "source": [
    "Here is how we can iterate over the generator function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f0037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in generator_function():\n",
    "    print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9405619",
   "metadata": {},
   "source": [
    "### 10.3. Converting to Ray generator\n",
    "\n",
    "Converting into a Ray generator function is straightforward:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482659e6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def generator_function():\n",
    "    for i in range(10):\n",
    "        time.sleep(1)\n",
    "        yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2b1e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj_ref in generator_function.remote():\n",
    "    print(obj_ref)  # Prints ObjectRef\n",
    "\n",
    "result = ray.get(obj_ref)\n",
    "print(result)  # Prints actual value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3589d54d",
   "metadata": {},
   "source": [
    "### 10.4. Memory usage comparison\n",
    "\n",
    "See the below script which shows the memory consumption when running with and without a generator:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c728b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "!RAY_DEDUP_LOGS=0 python scripts/ray_generator_object_store_diff.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7016245",
   "metadata": {},
   "source": [
    "### 10.5. Key differences from Python generators\n",
    "\n",
    "Unlike python generators, Ray generators:\n",
    "- **Don't pause execution** - i.e. they don't require `__next__` to be called to yield the next element\n",
    "- **Don't support all APIs** like send and throw\n",
    "- **Execute eagerly** - the task runs to completion regardless of consumer speed\n",
    "\n",
    "Given that Ray eagerly executes a generator task to completion regardless of whether the caller is polling the partial results or not, it might lead to object store spilling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbc3075",
   "metadata": {},
   "source": [
    "### 10.6. Backpressure control\n",
    "\n",
    "To backpressure the generator, we can specify the `_generator_backpressure_num_objects` argument in the `@ray.remote` decorator:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fec0917",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_returns=\"streaming\", _generator_backpressure_num_objects=10)\n",
    "def generate_data():\n",
    "    for i in range(1000):\n",
    "        yield expensive_square(i)\n",
    "        # Will pause if consumer is slow and 10 objects are buffered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014815fc",
   "metadata": {},
   "source": [
    "Below is a script that shows execution with and without backpressure:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7620a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/ray_generator_backpressure_diff.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def455cb",
   "metadata": {},
   "source": [
    "### 10.7. When to use Ray Generators\n",
    "\n",
    "**‚úÖ Use Ray Generators when:**\n",
    "- Processing large datasets that don't fit in memory\n",
    "- Streaming results from long-running computations\n",
    "- Building data pipelines with multiple stages\n",
    "- You want incremental results (don't wait for everything)\n",
    "\n",
    "**‚ùå Don't use Ray Generators when:**\n",
    "- You need random access to results\n",
    "- You need the full result set at once"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
