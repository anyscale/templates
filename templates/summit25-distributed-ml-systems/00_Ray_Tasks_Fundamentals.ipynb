{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80a327f2",
   "metadata": {},
   "source": [
    "# Ray Tasks Fundamentals: Building Distributed Applications\n",
    "\n",
    "Â© 2025, Anyscale. All Rights Reserved\n",
    "\n",
    "This notebook provides a step-by-step introduction to Ray Tasks, the fundamental building block of Ray that enables distributed computing.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b> Here is the roadmap for this notebook </b>\n",
    "\n",
    "<ol>\n",
    "  <li>Overview and setup</li>\n",
    "  <li>Simple task submission (creating, executing, and getting results)</li>\n",
    "  <li>Task options and configuration</li>\n",
    "  <li>Object store and memory model</li>\n",
    "  <li>Chaining tasks and passing data</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd37367",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30318fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ray\n",
    "import requests\n",
    "import ray.runtime_context\n",
    "from ray import tune\n",
    "from ray.util.scheduling_strategies import NodeAffinitySchedulingStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef738c06",
   "metadata": {},
   "source": [
    "## 1. Overview and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c118fc2a",
   "metadata": {},
   "source": [
    "### 1.1. Ray Core at a glance\n",
    "\n",
    "- **Scales your code** across many CPU cores, machines, and accelerators.  \n",
    "- **Schedules arbitrary task graphs** thanks to its distributed scheduler.\n",
    "- **Hides distributed-system overhead** with built-ins for  \n",
    "  - fast data serialization and transfer,  \n",
    "  - smart task placement, \n",
    "  - distributed memory & reference counting.\n",
    "\n",
    "Ray's higher-level libraries build on Ray Core to offer ready-made APIs for common workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4655775",
   "metadata": {},
   "source": [
    "### 1.2. When to use Ray Tasks\n",
    "\n",
    "Ray Tasks are ideal for:\n",
    "- **Parallelizing computationally expensive functions** across multiple cores or machines\n",
    "- **Processing large datasets** by distributing work across workers\n",
    "- **Building complex task dependency graphs** (DAGs) for data pipelines\n",
    "- **Scaling existing Python code** with minimal changes\n",
    "\n",
    "**When NOT to use Ray Tasks:**\n",
    "- Functions that execute in < 1ms (overhead not worth it)\n",
    "- Very fine-grained parallelism (e.g., parallelizing simple arithmetic - use numpy instead)\n",
    "- When you need mutable shared state (use Ray Actors instead)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2677e6",
   "metadata": {},
   "source": [
    "### 1.3. Ray cluster architecture\n",
    "\n",
    "Before diving into tasks, let's understand the key components of a Ray cluster.\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/ray-cluster.svg\" width=\"800\">\n",
    "\n",
    "A Ray cluster consists of:\n",
    "- One or more **worker nodes**, where each worker node consists of the following processes:\n",
    "    - **worker processes** responsible for task submission and execution.\n",
    "    - A **raylet** responsible for:\n",
    "      - resource management and task placement.\n",
    "      - shared memory management through an object store \n",
    "- One of the worker nodes is designated a **head node** and is responsible for running \n",
    "  - A **global control service** responsible for keeping track of the **cluster-level state** that is not supposed to change too frequently.\n",
    "  - An **autoscaler** service responsible for adding and removing worker nodes by integrating with different infrastructure providers (e.g. AWS, GCP, ...) to match the resource requirements of the cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234c092a",
   "metadata": {},
   "source": [
    "### 1.4. Initializing Ray\n",
    "\n",
    "ray.init() is the primary function to connect to an existing Ray cluster or start a new one and connect to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772a9b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df49cd27",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**NOTE** In case you don't manually call ray.init() inside a python script, Ray will automatically call ray.init() for you with default parameters when you define or invoke your first remote function or actor.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b272ae",
   "metadata": {},
   "source": [
    "## 2. Simple task submission (creating, executing, and getting results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4945f344",
   "metadata": {},
   "source": [
    "### 2.1. Creating remote functions\n",
    "\n",
    "The first step in using Ray is to create remote functions. A remote function is a regular Python function that can be executed on any process in your cluster.\n",
    "\n",
    "Given a simple Python function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a78c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddacdfb2",
   "metadata": {},
   "source": [
    "Decorate the function with `@ray.remote` to turn it into a remote function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f2f690",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def remote_add(a, b):\n",
    "    return a + b\n",
    "\n",
    "remote_add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a758026",
   "metadata": {},
   "source": [
    "### 2.2. Executing remote functions (asynchronous by default)\n",
    "\n",
    "Native python functions are invoked by calling them:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c3178",
   "metadata": {},
   "outputs": [],
   "source": [
    "add(1, 2)  # Returns 3 immediately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9f8696",
   "metadata": {},
   "source": [
    "Remote ray functions are executed as tasks by calling them with `.remote()` suffix:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852d40cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_add.remote(1, 2)  # Returns ObjectRef immediately, computation happens async"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf7da6c",
   "metadata": {},
   "source": [
    "Here is what happens when you call `{remote_function}.remote`:\n",
    "1. Ray **immediately** schedules the function execution by submitting a **task** to the cluster\n",
    "2. The submitting process returns an `ObjectRef` (a reference to the future result)\n",
    "3. The cluster begins executing the computation in the background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d45248d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <strong>A <a href=\"https://docs.ray.io/en/latest/ray-core/key-concepts.html#tasks\" target=\"_blank\">task</a></strong> is a remote, stateless Python function invocation.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6bfb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = remote_add.remote(1, 2)\n",
    "ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff9654a",
   "metadata": {},
   "source": [
    "**Think of `ObjectRef` as a future**: it's a placeholder for a value that is being computed on the cluster.\n",
    "\n",
    "Here is a map of how Python code is translated into Ray tasks:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-core/python_to_ray_task_map_v2.png\" alt=\"Python to Ray Task Map\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d3bb71",
   "metadata": {},
   "source": [
    "### 2.3. Getting results\n",
    "\n",
    "If we want to wait (block) and retrieve the corresponding object, we can use `ray.get`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83df25c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.get(ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa80b4b8",
   "metadata": {},
   "source": [
    "### 2.4. Putting it all together\n",
    "\n",
    "Here are the three steps:\n",
    "1. Create the remote function\n",
    "2. Execute it remotely (non-blocking)\n",
    "3. Get the result when needed (blocking)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "__Activity: define and invoke a Ray task__\n",
    "\n",
    "Define a remote function `sqrt_add` that accepts two arguments and performs the following steps:\n",
    "1. computes the square-root of the first\n",
    "2. adds the second\n",
    "3. returns the result\n",
    "\n",
    "Execute it with 2 different sets of parameters and collect the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33033362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: define the below as a remote function\n",
    "def sqrt_add(a, b):\n",
    "    ... \n",
    "\n",
    "# Hint: invoke it as a remote task and collect the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a37e188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e515b5",
   "metadata": {
    "region_name": "markdown"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary> Click to see solution </summary>\n",
    "\n",
    "\n",
    "```python\n",
    "@ray.remote\n",
    "def sqrt_add(a, b):\n",
    "    return math.sqrt(a) + b\n",
    "\n",
    "ray.get([sqrt_add.remote(2, 3), sqrt_add.remote(5, 4)])\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92c3407",
   "metadata": {},
   "source": [
    "### 2.5. Understanding asynchronous execution\n",
    "\n",
    "The key difference between regular Python and Ray is that `.remote()` **does not block**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b41ba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slow_function(x):\n",
    "    time.sleep(3)\n",
    "    return x * x\n",
    "\n",
    "# Sequential Python (blocks for each call)\n",
    "start = time.time()\n",
    "results = [slow_function(i) for i in range(4)]  # Would take 12 seconds!\n",
    "print(f\"Wall time sequential: {time.time() - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e955a92",
   "metadata": {},
   "source": [
    "`.remote()` will **immediately submit** the task and return, `.get` **will block** until the reference(s) is/are resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16498471",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def slow_function(x):\n",
    "    time.sleep(3)\n",
    "    return x * x\n",
    "\n",
    "# Distributed Ray (non-blocking)\n",
    "start = time.time()\n",
    "refs = [slow_function.remote(i) for i in range(4)]  # Returns immediately!\n",
    "print(f\"Task submission: {time.time() - start:.2f}s\")  # < 0.01s\n",
    "\n",
    "# Now wait for results (blocks until all complete)\n",
    "results = ray.get(refs)\n",
    "print(f\"Wall time with ray: {time.time() - start:.2f}s\")  # ~3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3a52c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "%xdel refs\n",
    "%xdel results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8090139e",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a642e5",
   "metadata": {},
   "source": [
    "### 2.6. What can be passed to Ray tasks? (Serialization)\n",
    "\n",
    "Ray uses **cloudpickle** to serialize code (functions, arguments and return values). Most Python objects work, but there are limitations:\n",
    "\n",
    "**â Can serialize:**\n",
    "- Basic types: int, float, str, bool, None\n",
    "- Collections: list, dict, tuple, set\n",
    "- NumPy arrays, Pandas DataFrames\n",
    "- Most custom classes\n",
    "- Nested functions and lambdas\n",
    "\n",
    "**â Cannot serialize:**\n",
    "- File handles (`open()` objects)\n",
    "- Network sockets\n",
    "- Threading locks\n",
    "\n",
    "**Example of serialization issues:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8410a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â BAD: File handle won't serialize\n",
    "file = open(\"/tmp/data.txt\", \"w\")\n",
    "\n",
    "@ray.remote\n",
    "def read_file(f):\n",
    "    return f.read()\n",
    "\n",
    "# ref = read_file.remote(file)  # Will fail with a PicklingError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3385f8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>ð¡ Troubleshooting:</b> If you see <code>pickle.PicklingError</code> or <code>TypeError: cannot pickle</code>, check if you're passing non-serializable objects to your task.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8906070",
   "metadata": {},
   "source": [
    "### 2.7 Task submission sequence\n",
    "\n",
    "Here is the sequence of events when you submit a Ray task:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-core/task-submission_old.gif\" alt=\"Task Submission Sequence\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ca3c2e",
   "metadata": {},
   "source": [
    "### 2.8 Task submission under the hood\n",
    "\n",
    "When a task is submitted, here is how resource fulfillment works\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-core/normal-task-resource-fullfilment.svg\" width=\"700\" alt=\"Resource fulfillment and execution of `double(2)` in a Ray cluster.\">\n",
    "\n",
    "The caller must choose **which node (raylet)** should schedule it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2210edb2",
   "metadata": {},
   "source": [
    "#### 1ï¸â£ Choosing the Preferred Raylet\n",
    "| **Rule**          | **When Used**                    | **How It Works**                                                                                    |\n",
    "| ----------------- | -------------------------------- | --------------------------------------------------------------------------------------------------- |\n",
    "| **Data locality** | Task has data dependencies.      | Pick node holding the most object bytes locally (from the object directory, may be slightly stale). |\n",
    "| **Node affinity** | Task specifies a target node.    | Use the node from `NodeAffinitySchedulingStrategy`.                                                 |\n",
    "| **Default**       | No data or affinity preferences. | Use the **local raylet**.                                                                           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc505a3",
   "metadata": {},
   "source": [
    "#### 2ï¸â£ Request â Lease â Worker\n",
    "- Caller sends a **resource request** to the preferred raylet.  \n",
    "- If granted, the raylet **leases a local worker** and returns its address.  \n",
    "- The **lease stays active** while both caller and worker are alive.  \n",
    "- Idle or unused leases are returned after a short timeout (~hundreds of ms).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0bda8e",
   "metadata": {},
   "source": [
    "#### 3ï¸â£ Task Execution on the Leased Worker\n",
    "The caller can schedule **multiple compatible tasks** on the same worker without re-contacting the scheduler.\n",
    "\n",
    "Compatibility means matching:\n",
    "- **Resource shape**, e.g. `{\"CPU\": 1}`\n",
    "- **Shared-memory arguments** (large objects must be local; small ones are inlined)\n",
    "- **Runtime environment**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c923330",
   "metadata": {},
   "source": [
    "#### 4ï¸â£ Optimization Insight\n",
    "Worker leases act as a *cache* for scheduling decisions â similar tasks can reuse the same worker for lower latency and higher throughput.\n",
    "\n",
    "Note also that the caller can hold multiple worker leases to increase parallelism. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fda9ba",
   "metadata": {},
   "source": [
    "## 3. Task options and configuration\n",
    "\n",
    "You can dynamically configure tasks using the `.options()` method without redefining the function. This is useful for adjusting resources, retries, or other settings per task invocation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077b0f59",
   "metadata": {},
   "source": [
    "### 3.1. Basic usage of .options()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd6b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def flexible_task(x):\n",
    "    return x * 2\n",
    "\n",
    "# Use default configuration (1 CPU)\n",
    "ref1 = flexible_task.remote(5)\n",
    "\n",
    "# Override to use 2 CPUs for this specific invocation\n",
    "ref2 = flexible_task.options(num_cpus=2).remote(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37775635",
   "metadata": {},
   "source": [
    "### 3.2. Common options\n",
    "\n",
    "**Resource options:**\n",
    "- `num_cpus`: Number of CPUs (can be fractional, e.g., 0.5)\n",
    "- `num_gpus`: Number of GPUs (can be fractional)\n",
    "- `memory`: Memory in bytes\n",
    "- `resources`: Dict of custom resources\n",
    "\n",
    "**Fault tolerance options:**\n",
    "- `max_retries`: Max number of retries (default: 3 for system errors)\n",
    "- `retry_exceptions`: List of exception types to retry on\n",
    "\n",
    "**Execution options:**\n",
    "- `runtime_env`: Dict specifying runtime environment\n",
    "- `scheduling_strategy`: Control task placement\n",
    "- `name`: Name for debugging/monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8999b3",
   "metadata": {},
   "source": [
    "### 3.3. Scheduling strategies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6504087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default: Ray decides based on data locality and load\n",
    "flexible_task.remote(4)\n",
    "\n",
    "# SPREAD: Distribute tasks across nodes\n",
    "flexible_task.options(scheduling_strategy=\"SPREAD\").remote(5)\n",
    "\n",
    "# Node affinity: Run on specific node\n",
    "strategy = NodeAffinitySchedulingStrategy(\n",
    "    node_id=ray.get_runtime_context().get_node_id(),\n",
    "    soft=True  # soft=True allows fallback if node unavailable\n",
    ")\n",
    "flexible_task.options(scheduling_strategy=strategy).remote(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233a4098",
   "metadata": {},
   "source": [
    "## 4. Object store and memory model\n",
    "\n",
    "Each worker node has its own object store, and collectively, these form a shared object store across the cluster.\n",
    "\n",
    "Remote objects are immutable. That is, their values cannot be changed after creation. This allows remote objects to be replicated in multiple object stores without needing to synchronize the copies.\n",
    "\n",
    "| <img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/ray-core/ray-cluster.png\" width=\"700px\" loading=\"lazy\"> |\n",
    "| :---------------------------------------------------------------------------------------------------------------------------- |\n",
    "| A Ray cluster with a head node and two worker nodes. Highlighted in orange is distributed object store.                       |\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://docs.ray.io/en/latest/ray-core/key-concepts.html#objects\" target=\"_blank\">Object</a></strong> - tasks and actors create and work with remote objects, which can be stored anywhere in a cluster. These objects are accessed using <strong>ObjectRef</strong> and are cached in a distributed shared-memory <strong>object store</strong>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9429a8",
   "metadata": {},
   "source": [
    "### 4.1. Ray memory model\n",
    "\n",
    "Ray manages memory in several ways to efficiently handle distributed tasks:\n",
    "\n",
    "1. **Heap memory**:\n",
    "   - Used by workers to execute tasks and actors.\n",
    "   - Used to store small objects (less than 100KB) and Ray metadata.\n",
    "   - High memory pressure can cause Ray to terminate some tasks to free up resources.\n",
    "\n",
    "2. **Shared memory (Object Store)**:\n",
    "   - Serves as the medium for passing data between tasks.\n",
    "   - Large objects (greater than 100KB) are stored in a shared memory space, using up to 30% of a node's memory.\n",
    "   - If more space is needed, objects can be spilled to disk or stored on disk in a slower-access format.\n",
    "\n",
    "Here is a diagram showing a horizontal slicing of a node's memory.\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/memory.svg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea75528b",
   "metadata": {},
   "source": [
    "### 4.2. Example: Producer-consumer pattern with numpy arrays\n",
    "\n",
    "This example demonstrates how Ray transfers data in the distributed object store. The `producer_task` creates a 4 GiB numpy array, and the `consumer_task` accesses it with zero-copy deserialization when on the same node:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89785292",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def producer_task(size_mb: int = 4 * 1024) -> np.ndarray:\n",
    "    array = np.random.rand((1024**2 * size_mb // 8)).astype(np.float64)\n",
    "    return array\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def consumer_task(array: np.ndarray) -> None:\n",
    "    assert isinstance(array, np.ndarray)\n",
    "    assert not array.flags.owndata  # Confirms zero-copy\n",
    "\n",
    "# arr_ref = producer_task.remote()  # Produce a 4 GiB array\n",
    "# output_ref = consumer_task.remote(arr_ref)  # Pass ObjectRef to consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1504b0a",
   "metadata": {},
   "source": [
    "**What happens under the hood:**\n",
    "\n",
    "1. **Producer task** creates the array in heap memory, then Ray stores it in the shared object store (large objects > 100KB)\n",
    "2. **Consumer task** receives the `ObjectRef` and directly accesses the array from shared memory with zero-copy deserialization (if on same node)\n",
    "3. If tasks run on different nodes, Ray copies the array across the network only once\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-data-deep-dive/producer-consumer-object-store-v2.png\" width=\"600\">\n",
    "\n",
    "To see memory usage in action, run this inspection script:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee6ff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/memory_inspection.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ce8fb",
   "metadata": {},
   "source": [
    "#### On zero-copy deserialization\n",
    "\n",
    "Ray uses **cloudpickle** for serialization and **pickle 5** for zero-copy deserialization. \n",
    "\n",
    "**How Ray transfers code and data:**\n",
    "\n",
    "1. **Code transfer (functions)**: Functions are pickled and stored in the Global Control Store (GCS), then cached for subsequent calls\n",
    "\n",
    "2. **Data transfer (arguments/return values)**:\n",
    "   - **Small objects (< 100 KB)**: Pickled and transferred inline with the task metadata\n",
    "   - **Large objects (> 100 KB)**: Stored in shared memory (object store), only the `ObjectRef` is transferred\n",
    "\n",
    "**Key performance characteristics:**\n",
    "\n",
    "- **Zero-copy benefits**: Works for contiguous numpy arrays and PyArrow arrays on the same node, enabling efficient read access without data copying. \n",
    "- **Zero-copy limitation**: Does not support PyTorch tensors or other array types\n",
    "- **Immutability**: Objects in the object store are **immutable once sealed**, enabling safe sharing across processes\n",
    "\n",
    "To read more about object serialization in Ray, see [this documentation page here](https://docs.ray.io/en/latest/ray-core/objects/serialization.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c1e79c",
   "metadata": {},
   "source": [
    "### 4.3. Usecase: Hyper-parameter tuning\n",
    "\n",
    "**The Problem**: When running hyperparameter tuning or experimentation, you often need to use the same dataset across dozens or hundreds of trials. If you pass the dataset by value to each training function, Ray will serialize it repeatedly, wasting memory and time.\n",
    "\n",
    "**The Solution**: Store the dataset once in the object store using `ray.put()`, then pass only the lightweight `ObjectRef` to each trial. All workers can access the same data without duplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48488072",
   "metadata": {},
   "source": [
    "#### Real-world scenario: Grid search with shared data\n",
    "\n",
    "Imagine running 20 experiments on a 100MB training dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db38479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a 100MB training dataset\n",
    "df = pd.DataFrame(np.random.rand(100 * 1024 ** 2 // 8))\n",
    "\n",
    "@ray.remote\n",
    "def train_model(data, learning_rate, batch_size):\n",
    "    # Simulate model training\n",
    "    result = data.mean().sum() * learning_rate / batch_size\n",
    "    time.sleep(20)\n",
    "    return {\"lr\": learning_rate, \"batch_size\": batch_size, \"score\": result}\n",
    "\n",
    "# Grid search: 20 different hyperparameter combinations\n",
    "hyperparameters = [\n",
    "    {\"lr\": lr, \"batch_size\": bs}\n",
    "    for lr in [0.001, 0.01, 0.1, 0.5]\n",
    "    for bs in [32, 64, 128, 256, 512]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a8812a",
   "metadata": {},
   "source": [
    "Here is an efficient way to run the experiments by passing the dataset **once by reference**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dcbbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â Memory Efficient: Pass ObjectRef (100 MB total memory)\n",
    "# Ray serializes once, all workers share the same data\n",
    "df_ref = ray.put(df)\n",
    "[\n",
    "    train_model.remote(df_ref, hp[\"lr\"], hp[\"batch_size\"]) \n",
    "    for hp in hyperparameters\n",
    "]\n",
    "print(\"Pass once by reference: ~100 MiB memory used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1be68ea",
   "metadata": {},
   "source": [
    "Let's inspect the object store, we should only see the same 100MiB object being used across tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc8019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ray list objects --filter TASK_STATUS!=NIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b21909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "%xdel df_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccce33c",
   "metadata": {},
   "source": [
    "Here is the inefficient way by passing the dataset by value:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceea0dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â Memory inefficient: Pass dataframe by value (2 GB total memory!)\n",
    "# Ray serializes 100MB Ã 20 times = 2 GB of redundant data\n",
    "[\n",
    "    train_model.remote(df, hp[\"lr\"], hp[\"batch_size\"]) \n",
    "    for hp in hyperparameters\n",
    "]\n",
    "print(\"Pass by value: ~2GiB memory used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e9b533",
   "metadata": {},
   "source": [
    "Let's inspect the object store, we should now see different 100MiB objects being used across tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68775f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ray list objects --filter TASK_STATUS!=NIL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03d4382",
   "metadata": {},
   "source": [
    "**Performance comparison:**\n",
    "- **Pass by value**: 2 GB memory used (20Ã serialization overhead)\n",
    "- **Pass by reference**: 100 MB memory used (1Ã serialization)\n",
    "\n",
    "**Rule of thumb**: Pass by value only for small literals (< 100 KiB); otherwise, pass by reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d818495",
   "metadata": {},
   "source": [
    "#### How Ray Tune leverages this pattern\n",
    "\n",
    "Ray Tune uses `tune.with_parameters()` to automatically pass large constant objects via the object store:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0628610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainable(config, data):\n",
    "    # Each trial receives a reference to the shared data\n",
    "    model = train(data, lr=config[\"lr\"], epochs=config[\"epochs\"])\n",
    "    return {\"accuracy\": model.eval()}\n",
    "\n",
    "# Tune automatically stores train_data in the object store\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_parameters(trainable, data=pd.DataFrame()),  # Passed by reference\n",
    "    param_space={\"lr\": tune.grid_search([0.001, 0.01, 0.1]), \"epochs\": tune.choice([10, 20, 50])},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a371a1a2",
   "metadata": {},
   "source": [
    "Without `tune.with_parameters()`, each trial would receive a separate copy of `train_data`, multiplying memory usage by the number of concurrent trials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dec236",
   "metadata": {},
   "source": [
    "### 4.4. Distributed ownership and fate-sharing\n",
    "\n",
    "Ray uses a **distributed ownership model** to manage objects efficiently across the cluster. Understanding this concept is crucial for building robust distributed applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29c1221",
   "metadata": {},
   "source": [
    "#### How distributed ownership works\n",
    "\n",
    "In Ray, the process that creates or submits a task becomes the **owner** of the task's result. The owner maintains critical metadata about the object, including:\n",
    "- Object location(s) in the cluster\n",
    "- Reference counts\n",
    "- Object size and other properties\n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/distributed_ownership_overview_v4.svg\" width=\"800px\">\n",
    "\n",
    "**Benefits of distributed ownership:**\n",
    "- **Lower latency**: No need to communicate all ownership information back to a central node\n",
    "- **Better scalability**: No single bottleneck as every worker maintains its own ownership information\n",
    "\n",
    "Here is a diagram that explains how distributed ownership works in a Ray cluster:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-core/distributed-ownership.png\" width=\"800\">\n",
    "\n",
    "Below is some code based on the above diagram to illustrate distributed ownership:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafca308",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def b():\n",
    "    size_mib = 19\n",
    "    return np.ones(1024 ** 2 // 8 *  size_mib)\n",
    "\n",
    "@ray.remote\n",
    "def a(dep):\n",
    "    z = b.remote() # z is owned by worker process running task a\n",
    "\n",
    "    ip = ray.util.get_node_ip_address()\n",
    "    print(f\"{ip=}\")\n",
    "\n",
    "    time.sleep(20)\n",
    "    return dep.sum() / ray.get(z).sum() \n",
    "\n",
    "size_mib = 33\n",
    "arr = np.ones(1024 ** 2 // 8 *  size_mib)\n",
    "x = ray.put(arr)  # x is owned by driver process\n",
    "y = a.remote(x)  # y is owned by driver process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1b9e11",
   "metadata": {},
   "source": [
    "We can verify that the the 19 MB array is owned by the worker that submitted task `b` - i.e. the worker executing task `a` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba1b33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ray list objects --filter TASK_STATUS!=NIL --filter TYPE=WORKER "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b12372",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note: `TASK_STATUS = NIL` matches non-owner processes given only the owner tracks task status\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c2da3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.get(y), 33 / 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4ce36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "%xdel x\n",
    "%xdel y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c30f2e",
   "metadata": {},
   "source": [
    "#### The fate-sharing limitation\n",
    "\n",
    "The main tradeoff of distributed ownership is **fate-sharing**: objects are tied to the lifetime of their owner process.\n",
    "\n",
    "**What this means:**\n",
    "- Even if an object is stored in the object store on a different node, if the owner process dies, the object becomes unreachable\n",
    "- The owner maintains critical metadata (locations, reference counts) that other processes need to access the object\n",
    "- When the owner fails, this metadata is lost, making the object inaccessible even if copies exist elsewhere\n",
    "\n",
    "<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-core/task-actor-lifecycle/v2/scheduling/distributed_ownership_fate_share_with_owner_v4.svg\" width=\"900px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a42f624",
   "metadata": {},
   "source": [
    "#### Example: Demonstrating fate-sharing\n",
    "\n",
    "This example creates two actors: an **Owner** that creates an object reference, and a **Borrower** that tries to access it. We'll see what happens when the Owner is terminated:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678fcf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def f(data):\n",
    "    return data\n",
    "\n",
    "@ray.remote\n",
    "class Owner:\n",
    "    def __init__(self):\n",
    "        self.ref = None\n",
    "\n",
    "    def set_object_ref(self, data):\n",
    "        self.ref = f.remote(data)\n",
    "        return self.ref\n",
    "    \n",
    "    def is_alive(self): \n",
    "        return True\n",
    "\n",
    "@ray.remote\n",
    "class Borrower:\n",
    "    def get_object(self, ref):\n",
    "        return ray.get(ref)\n",
    "\n",
    "owner = Owner.remote()\n",
    "borrower = Borrower.remote()\n",
    "assert ray.get(owner.is_alive.remote())\n",
    "\n",
    "object_ref = owner.set_object_ref.remote(data=\"test1\")\n",
    "# Since owner is alive we can resolve the object reference\n",
    "data = ray.get(borrower.get_object.remote(object_ref))\n",
    "assert data == \"test1\"\n",
    "print(f\"â Successfully retrieved data while Owner is alive: {data}\")\n",
    "\n",
    "ray.kill(owner)\n",
    "time.sleep(2)\n",
    "\n",
    "# After killing the owner we can no longer resolve the object reference\n",
    "try:\n",
    "    ray.get(borrower.get_object.remote(object_ref))\n",
    "    print(\"â Unexpected: Should have failed!\")\n",
    "except Exception as e:\n",
    "    print(\"â Failed as expected after owner termination:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa9761d",
   "metadata": {},
   "source": [
    "**What happens:**\n",
    "1. While the Owner is alive, the Borrower can successfully retrieve the object using the `ObjectRef`\n",
    "2. After the Owner is killed, the Borrower still has the `ObjectRef`, but attempting to access the object fails\n",
    "3. Even though the object data may still exist in the object store, the ownership metadata is lost\n",
    "\n",
    "**Key takeaway:** In Ray's distributed ownership model, object lifetime is tied to the owner's lifetime. When building fault-tolerant applications:\n",
    "- Keep important owners alive (e.g., use long-running actors or the driver process)\n",
    "- Consider checkpointing critical data outside Ray's object store for durability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a999484",
   "metadata": {
    "region_name": "markdown"
   },
   "source": [
    "### 4.5 Lineage Reconstruction\n",
    "\n",
    "If instead, the **owner is still alive**, but **the object is lost** (e.g., due to node failure), Ray can reconstruct the object by either\n",
    "\n",
    "1. Finding any secondary copies in the object store (if they exist) and returning one of those\n",
    "2. Re-executing the task or task chain that created it. This is known as **lineage reconstruction**.\n",
    "\n",
    "See the below test inspired from the [Ray test suite](https://github.com/ray-project/ray/blob/a04cb06bb1a2c09e93b882b611492d62b8d1837a/python/ray/tests/test_reconstruction.py#L126) for an example of lineage reconstruction:\n",
    "\n",
    "```python\n",
    "@pytest.mark.parametrize(\"reconstruction_enabled\", [False, True])\n",
    "def test_basic_reconstruction(config, ray_start_cluster, reconstruction_enabled):\n",
    "    cluster = ray_start_cluster\n",
    "    \n",
    "    # Start head node with reconstruction enabled/disabled\n",
    "    cluster.add_node(num_cpus=0, _system_config=config, \n",
    "                     enable_object_reconstruction=reconstruction_enabled)\n",
    "    ray.init(address=cluster.address)\n",
    "    \n",
    "    # Add worker node to store the object\n",
    "    node_to_kill = cluster.add_node(\n",
    "        num_cpus=1, resources={\"node1\": 1}, object_store_memory=10**8\n",
    "    )\n",
    "    cluster.wait_for_nodes()\n",
    "\n",
    "    @ray.remote(max_retries=1 if reconstruction_enabled else 0)\n",
    "    def create_large_object():\n",
    "        return np.zeros(10**7, dtype=np.uint8)\n",
    "\n",
    "    @ray.remote\n",
    "    def process_large_object(x):\n",
    "        return\n",
    "\n",
    "    # Create object and verify it can be used\n",
    "    # Note: obj_ref owner is the driver (on head node), so lineage is preserved\n",
    "    # even when the worker node storing the object is killed\n",
    "    obj_ref = create_large_object.options(resources={\"node1\": 1}).remote()\n",
    "    ray.get(process_large_object.options(resources={\"node1\": 1}).remote(obj_ref))\n",
    "    \n",
    "    # Simulate node failure and replacement\n",
    "    cluster.remove_node(node_to_kill, allow_graceful=False)\n",
    "    node_to_kill = cluster.add_node(\n",
    "        num_cpus=1, resources={\"node1\": 1}, object_store_memory=10**8\n",
    "    )\n",
    "\n",
    "    # With reconstruction: task re-executes and object is recreated\n",
    "    # Without reconstruction: both task and object are lost\n",
    "    if reconstruction_enabled:\n",
    "        ray.get(process_large_object.remote(obj_ref))\n",
    "    else:\n",
    "        with pytest.raises(ray.exceptions.RayTaskError):\n",
    "            ray.get(process_large_object.remote(obj_ref))\n",
    "        with pytest.raises(ray.exceptions.ObjectLostError):\n",
    "            ray.get(obj_ref)\n",
    "\n",
    "    # Second node failure exceeds max_retries\n",
    "    cluster.remove_node(node_to_kill, allow_graceful=False)\n",
    "    cluster.add_node(num_cpus=1, resources={\"node1\": 1}, object_store_memory=10**8)\n",
    "\n",
    "    expected_error = (\n",
    "        ray.exceptions.ObjectReconstructionFailedMaxAttemptsExceededError\n",
    "        if reconstruction_enabled\n",
    "        else ray.exceptions.ObjectLostError\n",
    "    )\n",
    "    with pytest.raises(expected_error):\n",
    "        ray.get(obj_ref)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981dc45c",
   "metadata": {},
   "source": [
    "### 4.6 ObjectRef lifecycle and garbage collection\n",
    "\n",
    "Objects in the object store are automatically garbage collected when their distributed reference count drops to zero. This happens when all `ObjectRef`s pointing to the object are deleted or go out of scope.\n",
    "\n",
    "**Example:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb594df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_returns=2)\n",
    "def create_object():\n",
    "    task_id = ray.runtime_context.get_runtime_context().get_task_id()\n",
    "    return np.random.rand(1024 ** 2 // 8 * 20), task_id\n",
    "\n",
    "# Object created and stored\n",
    "ref1, ref2 = create_object.remote()\n",
    "\n",
    "# Object still in memory\n",
    "result = ray.get(ref1)\n",
    "task_id = ray.get(ref2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd782af",
   "metadata": {},
   "source": [
    "Let's inspect the returned objects in the store - note in this case we leverage the object id specification to find the object in the store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c055250",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "!ray list objects --filter TASK_STATUS!=NIL --filter TYPE=DRIVER --filter OBJECT_ID={task_id}01000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596e8b0b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Release reference (allows GC)\n",
    "del ref1\n",
    "del ref2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b10e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ray list objects --filter TASK_STATUS!=NIL --filter TYPE=DRIVER --filter OBJECT_ID={task_id}01000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9376eafb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>â¡ Performance Tip:</b> For long-running applications, explicitly delete ObjectRefs you no longer need to free up object store memory.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825f7805",
   "metadata": {},
   "source": [
    "## 5. Chaining tasks and passing data\n",
    "\n",
    "Let's say we now want to execute a graph of two tasks:\n",
    "1. Square a value using `expensive_square`\n",
    "2. Add 1 to the `expensive_square` result, by using `remote_add`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85a8c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def expensive_square(x):\n",
    "    time.sleep(1)\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd38804d",
   "metadata": {},
   "source": [
    "This can be achieved without fetching an intermediate result.\n",
    "\n",
    "**â Anti-pattern:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d23553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st task\n",
    "square_ref = expensive_square.remote(2)\n",
    "square_value = ray.get(square_ref)  # wait to get the value\n",
    "\n",
    "# 2nd task\n",
    "sum_ref = remote_add.remote(1, square_value)  # pass value from 1st task\n",
    "sum_value = ray.get(sum_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1597c4",
   "metadata": {},
   "source": [
    "**â Better:** Chain the tasks by passing the `ObjectRef` directly to the second task:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d267b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "square_ref = expensive_square.remote(2)\n",
    "sum_ref = remote_add.remote(1, square_ref)  # Pass ObjectRef, not value!\n",
    "sum_value = ray.get(sum_ref)  # Wait only at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae9e9ff",
   "metadata": {},
   "source": [
    "**Why this is better:**\n",
    "- No unnecessary data transfer (ObjectRef is just an ID)\n",
    "- Ray automatically handles dependencies\n",
    "- Second task waits for first task to complete\n",
    "- More efficient scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4b66e1",
   "metadata": {},
   "source": [
    "### 5.1. Common task graph patterns\n",
    "\n",
    "Ray excels at executing complex directed acyclic graphs (DAGs) of tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee34381",
   "metadata": {},
   "source": [
    "#### Pattern 1: Linear chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddf67d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def step1(data):\n",
    "    return process_a(data)\n",
    "\n",
    "@ray.remote\n",
    "def step2(data):\n",
    "    return process_b(data)\n",
    "\n",
    "@ray.remote\n",
    "def step3(data):\n",
    "    return process_c(data)\n",
    "\n",
    "# Chain tasks\n",
    "# ref1 = step1.remote(input_data)\n",
    "# ref2 = step2.remote(ref1)\n",
    "# ref3 = step3.remote(ref2)\n",
    "# final_result = ray.get(ref3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99085cdb",
   "metadata": {},
   "source": [
    "#### Pattern 2: Fan-out / Fan-in (MapReduce)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438463ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def map_task(chunk):\n",
    "    return process_chunk(chunk)\n",
    "\n",
    "@ray.remote\n",
    "def reduce_task(results):\n",
    "    return aggregate(results)\n",
    "\n",
    "# Map phase (fan-out)\n",
    "# map_refs = [map_task.remote(chunk) for chunk in data_chunks]\n",
    "\n",
    "# Reduce phase (fan-in)\n",
    "# final_result = ray.get(reduce_task.remote(map_refs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d710ca6",
   "metadata": {},
   "source": [
    "#### Pattern 3: Tree reduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3251ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def pairwise_sum(a, b):\n",
    "    return a + b\n",
    "\n",
    "\n",
    "refs = [ray.put(i) for i in range(16)]  # Initial values\n",
    "\n",
    "# Tree reduction (depth = log2(16) = 4)\n",
    "while len(refs) > 1:\n",
    "    refs = [pairwise_sum.remote(refs[i], refs[i + 1]) for i in range(0, len(refs), 2)]\n",
    "\n",
    "result = ray.get(refs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c8637c",
   "metadata": {},
   "source": [
    "### 5.2. Nested tasks\n",
    "\n",
    "Tasks can submit other tasks, enabling dynamic workflows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c1f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def main():\n",
    "    square_ref_1 = expensive_square.remote(1)\n",
    "    square_ref_2 = expensive_square.remote(2)\n",
    "    add_ref = remote_add.remote(square_ref_1, square_ref_2)\n",
    "    return ray.get(add_ref)\n",
    "\n",
    "ray.get(main.remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf44e82",
   "metadata": {},
   "source": [
    "**Avoiding deadlocks:** Ray automatically yields CPU resources when blocked on `ray.get()`, preventing deadlocks when nested tasks need the same resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cde78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Imagine if the cluster has 2 CPUs total\n",
    "\n",
    "@ray.remote(num_cpus=2)\n",
    "def outer_task():\n",
    "    inner_refs = [inner_task.remote() for _ in range(10)]\n",
    "    return ray.get(inner_refs)  # Ray yields the 2 CPUs while waiting\n",
    "\n",
    "@ray.remote(num_cpus=1)\n",
    "def inner_task():\n",
    "    return \n",
    "\n",
    "ray.get(outer_task.remote())  # Works! No deadlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c37b16e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Read more about <strong><a href=\"https://docs.ray.io/en/latest/ray-core/tasks/nested-tasks.html#yielding-resources-while-blocked\" target=\"_blank\">yielding resources while blocked</a></strong>.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
