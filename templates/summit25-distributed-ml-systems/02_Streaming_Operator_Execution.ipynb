{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86c00629",
   "metadata": {},
   "source": [
    "## Streaming Operator Execution with Ray: End-to-End Guide\n",
    "\n",
    "This notebook explain how to build a simple streaming execution engine using Ray tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f16f530",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b> Here is the roadmap for this notebook </b>\n",
    "\n",
    "<ol>\n",
    "  <li>Architecture Overview</li>\n",
    "  <li>Key Components</li>\n",
    "  <li>Part 1: Data structures and streaming task</li>\n",
    "  <li>Part 2: Operators built on queues</li>\n",
    "  <li>Part 3: Backpressure via a Resource Policy</li>\n",
    "  <li>Part 4: A streaming executor scheduling loop</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc4c662",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af26940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "import ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8530000",
   "metadata": {},
   "source": [
    "## Architecture Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d113e1",
   "metadata": {},
   "source": [
    "### Key Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec56051b",
   "metadata": {},
   "source": [
    "Here are the key principles of the streamig execution design:\n",
    "\n",
    "- **Stream with generators**: Start producing outputs immediately without waiting for all work to finish\n",
    "- Build a streaming topology using **operators with queues**\n",
    "- **Pass data between queues as references**: Move `ObjectRef`s between operators without materializing data\n",
    "- **Schedule operators to optimize throughput**: Select which operators to run based on resource and readiness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b04e54",
   "metadata": {},
   "source": [
    "### High-Level Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5bf37c",
   "metadata": {},
   "source": [
    "At a high-level, a pipeline is a sequence of operators.\n",
    "\n",
    "Each operator runs Ray tasks that stream output blocks as they are produced. Outputs flow downstream as `ObjectRef`s are moved between operators.\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-core/streaming-execution.png\" alt=\"streaming-execution\" width=\"700\"/>\n",
    "\n",
    "Here is a sample executor loop that schedules operators:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-core/streaming-scheduling-loop.png\" alt=\"streaming-execution\" width=\"700\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeac7c3",
   "metadata": {},
   "source": [
    "## Key Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec3f97c",
   "metadata": {},
   "source": [
    "Here are the key components of the streaming operator execution pipeline:\n",
    "\n",
    "- **Block**: data plus metadata\n",
    "- **Resources**: simple CPU/memory accounting per task\n",
    "- **OperatorState**: queues and task tracking\n",
    "- **Operator**: stage with transform, dispatch, and resource usage\n",
    "- **ResourcePolicy**: available vs. needed resources to apply backpressure\n",
    "- A **custom scheduling loop** coordinating the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436e420e",
   "metadata": {},
   "source": [
    "## Part 1: Data Structures and Streaming Task\n",
    "\n",
    "The `Block` and `Resources` types keep things explicit and simple.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84e20b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Block:\n",
    "    data: List[int]\n",
    "    block_id: int\n",
    "\n",
    "    def size_bytes(self) -> int:\n",
    "        return len(self.data) * 8\n",
    "\n",
    "    def num_rows(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "@dataclass\n",
    "class Resources:\n",
    "    cpu: float = 0.0\n",
    "    memory: int = 0\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return Resources(cpu=self.cpu + other.cpu, memory=self.memory + other.memory)\n",
    "\n",
    "    def __le__(self, other):\n",
    "        return self.cpu <= other.cpu and self.memory <= other.memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1852bb2c",
   "metadata": {},
   "source": [
    "The core idea: use a Ray task that returns a streaming generator. Each `yield` emits an `ObjectRef`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a6a050",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_cpus=1, num_returns=\"streaming\")\n",
    "def map_task_generator(*blocks: Block, transform_fn, task_id: int):\n",
    "    for block in blocks:\n",
    "        transformed_data = transform_fn(block.data)\n",
    "        output_block = Block(data=transformed_data, block_id=block.block_id)\n",
    "        yield output_block  # streaming output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d42f2f",
   "metadata": {},
   "source": [
    "**Note on block sizing**: You can adjust `map_task_generator` to emit uniformly sized blocks (by rows or approximate bytes). This keeps object store load predictable, bounds data transfer time per block, and limits scheduling overheads, improving end-to-end stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4864d97c",
   "metadata": {},
   "source": [
    "## Part 2: Operators Built on Queues\n",
    "\n",
    "Each operator tracks input/output queues and active streaming tasks. The state is simple and explicit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce43c02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OperatorState:\n",
    "    def __init__(self, operator: \"Operator\"):\n",
    "        self.operator = operator\n",
    "        self.input_queue = deque()\n",
    "        self.output_queue = deque()\n",
    "        self.active_tasks = {}\n",
    "        self.next_task_id = 0\n",
    "        self.completed_tasks = 0\n",
    "        self.inputs_done = False\n",
    "\n",
    "    def has_pending_input(self) -> bool:\n",
    "        return len(self.input_queue) > 0\n",
    "\n",
    "    def has_output(self) -> bool:\n",
    "        return len(self.output_queue) > 0\n",
    "\n",
    "    def num_active_tasks(self) -> int:\n",
    "        return len(self.active_tasks)\n",
    "\n",
    "    def is_completed(self) -> bool:\n",
    "        return (\n",
    "            self.inputs_done\n",
    "            and len(self.input_queue) == 0\n",
    "            and len(self.active_tasks) == 0\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dfb8be",
   "metadata": {},
   "source": [
    "The operator encapsulates a transform, resource need per task, and optional concurrency cap. It dispatches tasks that produce streaming outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66756bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operator:\n",
    "    def __init__(self, name, transform_fn, resource_per_task: Resources):\n",
    "        self.name = name\n",
    "        self.transform_fn = transform_fn\n",
    "        self.resource_per_task = resource_per_task\n",
    "        self.state = OperatorState(self)\n",
    "\n",
    "    def add_input(self, block_refs: List[ray.ObjectRef]):\n",
    "        self.state.input_queue.append(block_refs)\n",
    "\n",
    "    def should_dispatch(self) -> bool:\n",
    "        return self.state.has_pending_input()\n",
    "\n",
    "    def dispatch_task(self):\n",
    "        block_refs = self.state.input_queue.popleft()\n",
    "        task_id = self.state.next_task_id\n",
    "        self.state.next_task_id += 1\n",
    "        gen = map_task_generator.remote(*block_refs, transform_fn=self.transform_fn, task_id=task_id)\n",
    "        self.state.active_tasks[task_id] = gen\n",
    "\n",
    "    def current_resource_usage(self) -> Resources:\n",
    "        num_tasks = self.state.num_active_tasks()\n",
    "        return Resources(\n",
    "            cpu=self.resource_per_task.cpu * num_tasks,\n",
    "            memory=self.resource_per_task.memory * num_tasks,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3b82b9",
   "metadata": {},
   "source": [
    "**Queues expose references**: \n",
    "- `add_input` enqueues `ObjectRef`s\n",
    "- `dispatch_task` consumes input refs and starts a streaming Ray task\n",
    "- outputs re-enter the graph as `ObjectRef`s in `output_queue`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f17ea3",
   "metadata": {},
   "source": [
    "## Part 3: Backpressure via a Resource Policy\n",
    "\n",
    "The resource policy computes available vs. needed resources across all operators, enabling the scheduler to apply backpressure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068bdf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticResourcePolicy:\n",
    "    def __init__(self, total_resources: Resources):\n",
    "        self.total_resources = total_resources\n",
    "\n",
    "    def get_available_resources(self, operators: List[Operator]) -> Resources:\n",
    "        used = Resources()\n",
    "        for op in operators:\n",
    "            used = used + op.current_resource_usage()\n",
    "        return Resources(\n",
    "            cpu=self.total_resources.cpu - used.cpu,\n",
    "            memory=self.total_resources.memory - used.memory,\n",
    "        )\n",
    "\n",
    "    def can_dispatch(self, operator: Operator, operators: List[Operator]) -> bool:\n",
    "        available = self.get_available_resources(operators)\n",
    "        needed = operator.resource_per_task\n",
    "        return needed <= available\n",
    "\n",
    "ResourcePolicy = StaticResourcePolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c7fe3",
   "metadata": {},
   "source": [
    "**Effect**: When resources are tight, `can_dispatch` returns false for some operators, throttling concurrency and preventing overload."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fd59d0",
   "metadata": {},
   "source": [
    "## Part 4: The Streaming Executor\n",
    "\n",
    "The executor coordinates three phases per scheduling step: process ready outputs, transfer downstream, then dispatch new work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaa615c",
   "metadata": {},
   "source": [
    "### Control flow: `run()` and the scheduling loop\n",
    "\n",
    "Before diving into each phase, here is the high-level control flow that drives execution and returns final `ObjectRef`s.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a585ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scheduling_loop(operators: list[Operator], resource_policy: ResourcePolicy) -> List[ray.ObjectRef]:\n",
    "    while True:\n",
    "        if all(op.state.is_completed() for op in operators):\n",
    "            break\n",
    "\n",
    "        # Phase 1: Process completed tasks\n",
    "        # 1) Process ready outputs from active streaming tasks\n",
    "        process_completed_tasks(operators)\n",
    "        \n",
    "        # 2) Transfer produced ObjectRefs to output queues\n",
    "        transfer_outputs(operators)\n",
    "        \n",
    "        # 3) Update downstream input completion\n",
    "        update_operator_states(operators)\n",
    "\n",
    "        # 4) Dispatch as many new tasks as possible under constraints\n",
    "        while True:\n",
    "            op = select_operator_to_run(operators, resource_policy)\n",
    "            if op is None:\n",
    "                break\n",
    "            op.dispatch_task()\n",
    "            print(f\"Dispatch: {op.name} +1\")\n",
    "\n",
    "        status = \" | \".join(\n",
    "            f\"{op.name}[in={len(op.state.input_queue)} act={op.state.num_active_tasks()} out={len(op.state.output_queue)} done={op.state.completed_tasks}]\"\n",
    "            for op in operators\n",
    "        )\n",
    "        print(f\"Status: {status}\")\n",
    "\n",
    "    # Return final ObjectRefs\n",
    "    final_op = operators[-1]\n",
    "    result_refs = list(final_op.state.output_queue)\n",
    "    final_op.state.output_queue.clear()\n",
    "    return result_refs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a52eee8",
   "metadata": {},
   "source": [
    "Next, we detail each helper invoked by the scheduling loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5e2c25",
   "metadata": {},
   "source": [
    "### Processing outputs from streaming tasks\n",
    "\n",
    "This step polls active streaming tasks to fetch the next available output. It turns \"work in progress\" into `ObjectRef`s in the operator's `output_queue`.\n",
    "\n",
    "- Purpose: move ready results from task generators into the graph as references\n",
    "- Inputs: each operator's `active_tasks` (streaming generators)\n",
    "- Outputs: enqueued `ObjectRef`s in `output_queue`; completed tasks removed\n",
    "- Considerations: use a `ray.wait` with timeout; never `ray.get` here; handle `StopIteration`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a771eb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_completed_tasks(operators):\n",
    "    for op in operators:\n",
    "        if not op.state.active_tasks:\n",
    "            continue\n",
    "        task_refs = list(op.state.active_tasks.values())\n",
    "        # Wait up to 100ms for generator tasks to yield outputs\n",
    "        ready, _ = ray.wait(task_refs, num_returns=len(task_refs), fetch_local=False, timeout=0.1)\n",
    "        for ref in ready:\n",
    "            task_id = next(tid for tid, task_ref in op.state.active_tasks.items() if task_ref == ref)\n",
    "            try:\n",
    "                block_ref = next(ref)  # next streaming result as ObjectRef\n",
    "                op.state.output_queue.append(block_ref)\n",
    "                print(f\"[{op.name}] #{task_id} yield\")\n",
    "            except StopIteration:\n",
    "                # Task is completed given no longer yielding block references\n",
    "                del op.state.active_tasks[task_id]\n",
    "                op.state.completed_tasks += 1\n",
    "                print(f\"[{op.name}] #{task_id} done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac23e85",
   "metadata": {},
   "source": [
    "**Key points**:\n",
    "- `next(ref)` does not materialize data; it retrieves the next `ObjectRef` produced by the remote generator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec556fe",
   "metadata": {},
   "source": [
    "### Transferring outputs downstream as references\n",
    "\n",
    "Once outputs exist, we push them to the next operator's `input_queue`. This keeps data as `ObjectRef`s (no materialization) and enables immediate downstream work.\n",
    "\n",
    "- Purpose: propagate produced refs to the next stage\n",
    "- Inputs: upstream `output_queue`\n",
    "- Outputs: downstream `input_queue`\n",
    "- Considerations: transfer all available outputs; backpressure is enforced by dispatch/resource checks in later steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16195df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_outputs(operators):\n",
    "    for upstream_op, downstream_op in zip(operators, operators[1:]):\n",
    "        while upstream_op.state.has_output():\n",
    "            block_ref = upstream_op.state.output_queue.popleft()\n",
    "            downstream_op.add_input([block_ref])  # pass ObjectRef (object store), not the block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6426335",
   "metadata": {},
   "source": [
    "### Updating downstream input completion\n",
    "\n",
    "When an upstream operator is finished and has no more outputs, we signal the downstream operator that no more inputs will arrive. This allows downstream stages to cleanly finalize.\n",
    "\n",
    "- Purpose: propagate completion to unlock downstream termination\n",
    "- Inputs: upstream `is_completed()` and `output_queue` emptiness\n",
    "- Outputs: `mark_inputs_done()` on downstream operator\n",
    "- Considerations: only mark once; do not prematurely mark while outputs remain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424e1019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_operator_states(operators):\n",
    "    for upstream_op, downstream_op in zip(operators, operators[1:]):\n",
    "        if upstream_op.state.is_completed() and not upstream_op.state.has_output():\n",
    "            downstream_op.state.inputs_done = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d89a7d",
   "metadata": {},
   "source": [
    "### Selecting which operator to run\n",
    "\n",
    "Ideally, you want to pick the bottleneck operator. In Ray Data, this is done by estimating each operator's throughput as the number of bytes produced to the object store per unit time and prioritizing the bottleneck. In our simplified example we don't track object store throughput, so we use a lightweight heuristic instead.\n",
    "\n",
    "- Purpose: choose the next eligible operator to dispatch a task for\n",
    "- Inputs: `should_dispatch()` status and resource policy `can_dispatch()`\n",
    "- Output: one operator (or `None` if none are eligible)\n",
    "- Considerations: the ranking policy is pluggable; different heuristics to optimize throughput.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948c580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_operator_to_run(operators: list[Operator], resource_policy: ResourcePolicy) -> Operator | None:\n",
    "    eligible_ops = []\n",
    "    for op in operators:\n",
    "        if not op.should_dispatch():\n",
    "            continue\n",
    "        if not resource_policy.can_dispatch(op, operators):\n",
    "            continue\n",
    "        eligible_ops.append(op)\n",
    "    if not eligible_ops:\n",
    "        return None\n",
    "    # simple ranking heuristic: prefer lower memory usage\n",
    "    ranked = sorted(eligible_ops, key=lambda op: op.current_resource_usage().memory)\n",
    "    return ranked[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5e07d9",
   "metadata": {},
   "source": [
    "## Part 5: End-to-End Demo Pipeline\n",
    "\n",
    "The demo builds a two-stage pipeline where the first multiplies by 2 and the second adds 10. It then runs the executor until completion and only materializes final results for printing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3adcded",
   "metadata": {},
   "source": [
    "Here is quick context before we define the transform functions:\n",
    "- **Transform functions**: Stateless functions applied to each `Block`'s `data` list.\n",
    "- **Artificial delay**: `time.sleep(0.1)` simulates work so you can observe streaming and scheduling interleave.\n",
    "- **Block identity**: We preserve `block_id` across stages to track provenance of outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b22d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_by_2(data: List[int]) -> List[int]:\n",
    "    time.sleep(0.1)\n",
    "    return [x * 2 for x in data]\n",
    "\n",
    "def add_10(data: List[int]) -> List[int]:\n",
    "    time.sleep(0.1)\n",
    "    return [x + 10 for x in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036c08d0",
   "metadata": {},
   "source": [
    "let's setup the operators:\n",
    "- **Operator configuration**: Each `Operator` declares a `transform_fn`, `resource_per_task`, and an optional `max_concurrency` that caps concurrent streaming tasks for that operator.\n",
    "- **Throughput vs. resources**: With `num_cpus=1` per task and `max_concurrency=2`, each operator can run up to two tasks if the global budget allows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34abb872",
   "metadata": {},
   "outputs": [],
   "source": [
    "op1 = Operator(\n",
    "    name=\"MultiplyOperator\",\n",
    "    transform_fn=multiply_by_2,\n",
    "    resource_per_task=Resources(cpu=1.0, memory=1000),\n",
    ")\n",
    "\n",
    "op2 = Operator(\n",
    "    name=\"AddOperator\",\n",
    "    transform_fn=add_10,\n",
    "    resource_per_task=Resources(cpu=1.0, memory=1000),\n",
    ")\n",
    "\n",
    "resource_policy = StaticResourcePolicy(total_resources=Resources(cpu=3.0, memory=3000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cef3f1",
   "metadata": {},
   "source": [
    "Let's queue up some input data:\n",
    "- **Inputs as references**: We `ray.put` each `Block` to pass `ObjectRef`s through queues without materializing data in the driver.\n",
    "- **Completion signal**: `mark_inputs_done()` tells the upstream operator no more inputs will arrive; downstream completion is inferred by the executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97048792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place 4 blocks in the input queue\n",
    "input_blocks = [\n",
    "    Block(data=[1, 2, 3], block_id=0),\n",
    "    Block(data=[4, 5, 6], block_id=1),\n",
    "    Block(data=[7, 8, 9], block_id=2),\n",
    "    Block(data=[10, 11, 12], block_id=3),\n",
    "]\n",
    "\n",
    "for block in input_blocks:\n",
    "    block_ref = ray.put(block)\n",
    "    op1.add_input([block_ref])\n",
    "\n",
    "op1.state.inputs_done = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aada69",
   "metadata": {},
   "source": [
    "A few quick points about before running the streaming executor:\n",
    "- **Global resource budget**: `Resources(cpu=3.0, memory=3000)` means at most **three tasks** across the whole pipeline can run concurrently.\n",
    "- **Scheduling behavior**: Each iteration polls streaming outputs, transfers refs downstream, updates completion, then dispatches more work under the policy.\n",
    "- **Materialize only at the end**: We call `ray.get` on final `result_refs`; intermediate blocks remain as `ObjectRef`s in the object store.\n",
    "- **Streaming effect**: `op2` starts as soon as `op1` yields its first output block, overlapping stages for better latency/throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c273264",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "result_refs = run_scheduling_loop([op1, op2], resource_policy)\n",
    "blocks = ray.get(result_refs)\n",
    "\n",
    "for block in blocks:\n",
    "    print(f\"Block {block.block_id}: {block.data}\")\n",
    "    assert block.data == [2 * x + 10 for x in input_blocks[block.block_id].data]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "ray-core-deep-dive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
