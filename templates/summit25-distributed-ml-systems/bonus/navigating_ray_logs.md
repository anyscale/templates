# Ray Log Files - Complete Guide

## Overview

Ray generates multiple log files to track system operations and application execution. This guide provides a comprehensive overview of all log files, organized by node type and purpose.

## Log Directory Structure

```
/tmp/ray/
â”œâ”€â”€ session_latest/  (symlink to latest session)
â”‚   â””â”€â”€ logs/
â”‚       â”œâ”€â”€ [application logs]
â”‚       â”œâ”€â”€ [system logs]
â”‚       â””â”€â”€ events/
â”‚           â””â”€â”€ [event logs]
â”œâ”€â”€ session_2023-05-14_21-19-58_128000_45083/
â”‚   â””â”€â”€ logs/
â””â”€â”€ session_2023-05-15_21-54-19_361265_24281/
    â””â”€â”€ logs/
```

> **Note:** Default location is `/tmp/ray/session_*/logs`. Customize with `ray.init()` or `ray start`.



## Ray Cluster Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           HEAD NODE                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ GCS Server   â”‚  â”‚  Dashboard   â”‚  â”‚   Autoscaler    â”‚            â”‚
â”‚  â”‚ (Metadata)   â”‚  â”‚   (Web UI)   â”‚  â”‚   (Scaling)     â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Raylet (Scheduler + Object Store)              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚   Workers    â”‚  â”‚  IO Workers  â”‚  â”‚ Runtime Env  â”‚               â”‚
â”‚  â”‚(Tasks/Actors)â”‚  â”‚  (Spill/     â”‚  â”‚    Agent     â”‚               â”‚
â”‚  â”‚              â”‚  â”‚   Restore)   â”‚  â”‚              â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         WORKER NODE(S)                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                   â”‚
â”‚  â”‚ Dashboard    â”‚                                                   â”‚
â”‚  â”‚   Agent      â”‚                                                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                   â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Raylet (Scheduler + Object Store)              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚   Workers    â”‚  â”‚  IO Workers  â”‚  â”‚ Runtime Env  â”‚               â”‚
â”‚  â”‚(Tasks/Actors)â”‚  â”‚  (Spill/     â”‚  â”‚    Agent     â”‚               â”‚
â”‚  â”‚              â”‚  â”‚   Restore)   â”‚  â”‚              â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```



## Log Files by Node Type

### ğŸ¯ HEAD NODE ONLY

These logs only exist on the head node:

| Log File | Purpose | File Types |
|----------|---------|------------|
| `gcs_server.[out\|err]` | Global Control Service - manages cluster metadata | stdout/stderr |
| `dashboard.[log\|out\|err]` | Dashboard web UI server | logger + stdout/stderr |
| `monitor.[log\|out\|err]` | Autoscaler managing cluster scaling | logger + stdout/stderr |
| `dashboard_[module_name].[log\|out\|err]` | Dashboard child processes (per module) | logger + stdout/stderr |

### ğŸ”„ EVERY NODE (Head + Workers)

These logs exist on every node in the cluster:

| Log File | Purpose | File Types |
|----------|---------|------------|
| `raylet.[out\|err]` | Local scheduler and object store manager | stdout/stderr |
| `dashboard_agent.[log\|out\|err]` | Dashboard agent (one per node) | logger + stdout/stderr |
| `log_monitor.[log\|out\|err]` | Streams logs to driver | logger + stdout/stderr |
| `runtime_env_agent.[log\|out\|err]` | Manages runtime environments | logger + stdout/stderr |
| `worker-[worker_id]-[job_id]-[pid].[out\|err]` | Python/Java task and actor output | stdout/stderr |
| `java-worker*.log` | Java worker logs (if using Java) | logger |
| `python-core-driver-[worker_id]_[pid].log` | C++ core for Python/Java drivers | logger |
| `python-core-worker-[worker_id]_[pid].log` | C++ core for Python/Java workers | logger |
| `io-worker-[worker_id]-[pid].[out\|err]` | Object spill/restore workers | stdout/stderr |
| `runtime_env_setup-[job_id].log` | Runtime environment installation | logger |


## Application Logs

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Your Ray Application                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Job Submission (Ray Jobs API)                              â”‚
â”‚       â†“                                                     â”‚
â”‚  job-driver-[submission_id].log â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚                                             â”‚               â”‚
â”‚  Driver Process                             â”‚               â”‚
â”‚       â†“                                     â”‚               â”‚
â”‚  worker-[id]-[job_id]-[pid].out â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤               â”‚
â”‚  worker-[id]-[job_id]-[pid].err â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤               â”‚
â”‚                                             â”‚               â”‚
â”‚  Task/Actor Processes                       â”‚               â”‚
â”‚       â†“                                     â”‚               â”‚
â”‚  worker-[id]-[job_id]-[pid].out â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤               â”‚
â”‚  worker-[id]-[job_id]-[pid].err â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤               â”‚
â”‚                                             â”‚               â”‚
â”‚  Runtime Environment Setup                  â”‚               â”‚
â”‚       â†“                                     â”‚               â”‚
â”‚  runtime_env_setup-[job_id].log â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Application Log Files

1. **`job-driver-[submission_id].log`**
   - Stdout of jobs submitted via Ray Jobs API
   - **Location:** Node where job was submitted

2. **`worker-[worker_id]-[job_id]-[pid].[out|err]`**
   - Python/Java drivers and workers
   - Captures all stdout/stderr from tasks and actors
   - `.out` = stdout + stderr
   - `.err` = stderr only
   - **Location:** All nodes running tasks/actors

3. **`runtime_env_setup-[job_id].log`**
   - Runtime environment installation logs (pip install, conda, etc.)
   - Only created when runtime environments are used
   - **Location:** Nodes where runtime environment is installed

4. **`runtime_env_setup-ray_client_server_[port].log`**
   - Runtime environment setup logs when using Ray Client
   - **Location:** Head node (Ray Client server)



## System Component Logs

### Core System Components

#### **`raylet.[out|err]`**
- Local scheduler managing task execution and object store
- **Location:** EVERY NODE
- **Rotation:** No

#### **`gcs_server.[out|err]`** ğŸ¯
- Global Control Service managing cluster metadata
- **Location:** HEAD NODE ONLY
- **Rotation:** No

#### **`python-core-driver-[worker_id]_[pid].log`**
- C++ core logs for Ray drivers
- Ray drivers = Python/Java frontend + C++ core
- **Location:** Nodes running drivers
- **Rotation:** Yes

#### **`python-core-worker-[worker_id]_[pid].log`**
- C++ core logs for Ray workers
- **Location:** All nodes with workers
- **Rotation:** Yes


### Dashboard & Monitoring

```
Head Node                          Worker Nodes
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Dashboard   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  Dashboard  â”‚
â”‚   Server    â”‚      Reports      â”‚    Agent    â”‚
â”‚             â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                                  â”‚
      â”œâ”€â–º dashboard.log                  â”œâ”€â–º dashboard_agent.log
      â”œâ”€â–º dashboard.out                  â”œâ”€â–º dashboard_agent.out
      â””â”€â–º dashboard.err                  â””â”€â–º dashboard_agent.err
```

#### **`dashboard.[log|out|err]`** ğŸ¯
- Ray Dashboard web UI server
- `.log` = structured logger output
- `.out/.err` = stdout/stderr (usually empty unless crashes)
- **Location:** HEAD NODE ONLY

#### **`dashboard_agent.[log|out|err]`**
- Dashboard agent collecting metrics/logs from each node
- One agent per node
- **Location:** EVERY NODE

#### **`dashboard_[module_name].[log|out|err]`** ğŸ¯
- Dashboard child process logs (one per module)
- Examples: `dashboard_job.log`, `dashboard_reporter.log`
- **Location:** HEAD NODE ONLY (typically)

#### **`log_monitor.[log|out|err]`**
- Streams logs from workers to driver
- **Location:** EVERY NODE

#### **`monitor.[log|out|err]`** ğŸ¯
- Autoscaler logs for cluster scaling decisions
- **Location:** HEAD NODE ONLY


### Runtime Environment

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Runtime Environment Lifecycle             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  1. Request runtime env (pip, conda, containers)        â”‚
â”‚          â†“                                              â”‚
â”‚  2. runtime_env_agent.log â”€â”€â”€â”€â”€â–º Agent handles request  â”‚
â”‚          â†“                                              â”‚
â”‚  3. runtime_env_setup-[job_id].log â”€â–º Installation      â”‚
â”‚          â†“                             logs (pip,       â”‚
â”‚  4. Environment ready                  conda, etc.)     â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **`runtime_env_agent.[log|out|err]`**
- Manages runtime environment creation, deletion, caching
- One agent per node
- **Location:** EVERY NODE

#### **`runtime_env_setup-[job_id].log`**
- Detailed installation logs (pip output, conda output, etc.)
- Only present when runtime environments are used
- **Location:** Nodes where environment is installed



### I/O Workers

#### **`io-worker-[worker_id]-[pid].[out|err]`**
- IO workers for spilling/restoring objects to external storage
- Created automatically from Ray 1.3+
- **Location:** Nodes performing object spilling


## Event Logs

Event logs are stored in the `events/` subdirectory and contain structured event data.

```
logs/
â””â”€â”€ events/
    â”œâ”€â”€ event_GCS.log
    â”œâ”€â”€ event_RAYLET.log
    â”œâ”€â”€ event_CORE_WORKER_[pid].log
    â”œâ”€â”€ event_AUTOSCALER.log
    â”œâ”€â”€ event_EXPORT_DRIVER_JOB.log
    â”œâ”€â”€ event_EXPORT_ACTOR.log
    â””â”€â”€ event_EXPORT_TASK_[pid].log
```

### Event Log Files

| Log File | Source | Location |
|----------|--------|----------|
| `event_GCS.log` | GCS server events | HEAD NODE |
| `event_RAYLET.log` | Raylet events | EVERY NODE |
| `event_CORE_WORKER_[pid].log` | Core worker events (per process) | EVERY NODE |
| `event_AUTOSCALER.log` | Autoscaler events | HEAD NODE |
| `event_EXPORT_DRIVER_JOB.log` | Export events for driver jobs | Varies |
| `event_EXPORT_ACTOR.log` | Export events for actors | Varies |
| `event_EXPORT_TASK_[pid].log` | Export events for tasks | Varies |


## Log Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Your Ray Application                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”œâ”€â–º print(), logging â”€â”€â”€â”€â”€â”
             â”‚                          â”‚
             â”œâ”€â–º Task execution â”€â”€â”€â”€â”€â”€â”€â”€â”¤
             â”‚                          â”‚
             â””â”€â–º Actor methods â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                                        â†“
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚    Worker Process                     â”‚
                     â”‚    worker-[id]-[job].out              â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â†“              â†“              â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Local    â”‚ â”‚    Log    â”‚ â”‚   Driver   â”‚
              â”‚    File    â”‚ â”‚  Monitor  â”‚ â”‚  (stdout)  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â†“
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚  External Logger â”‚
                          â”‚  (FluentBit,     â”‚
                          â”‚   Vector, etc.)  â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Reference: Finding Logs by Use Case

Use this section to quickly locate the right logs and messages. Each item lists where to look and what to search for.

### My task is failing
- **Where to look**: `worker-[worker_id]-[job_id]-[pid].out` (stdout), `worker-[worker_id]-[job_id]-[pid].err` (errors)
- **Also check**: `python-core-worker-[worker_id]_[pid].log` (C++ core crashes)
- **Search for**: Tracebacks, import errors, segmentation faults, connection errors

### My job won't start
- **Where to look**: `job-driver-[submission_id].log`
- **Also check**: `runtime_env_setup-[job_id].log` (env install), `raylet.out` (scheduling)
- **Search for**: Dependency install failures, â€œFailed to put objectâ€¦â€, scheduling/backpressure warnings

### Cluster isnâ€™t scaling
- **Where to look**: `monitor.[log|out|err]` (head node)
- **Also check**: `events/event_AUTOSCALER.log` (head node)
- **Search for**: â€œScaling up/downâ€¦â€, node launch failures, cloud quota limits

### Dashboard isnâ€™t working
- **Where to look**: `dashboard.[log|out|err]` (head node)
- **Also check**: `dashboard_agent.[log|out|err]` (every node), `dashboard_[module].log`
- **Search for**: Port binding errors, HTTP 5xx, module crashes

### A node crashed
- **Where to look**: `raylet.[out|err]` (on that node), `gcs_server.[out|err]` (head node)
- **Also check**: `events/event_RAYLET.log`, `events/event_GCS.log`
- **Search for**: Process crashes, heartbeat timeouts, disconnections

### Object store is full or spilling to disk
- **Where to look**: `raylet.out` (spilling), `io-worker-[worker_id]-[pid].out` (I/O workers)
- **Also check**: `worker-*.err` or `python-core-worker-*.log` (ObjectStoreFullError)
- **Search for**:
  - â€œ:info_message:Spilled â€¦ MiBâ€¦â€ (INFO) â€” normal spill progress
  - â€œShared memory store full, falling back to allocating from filesystemâ€
  - â€œOut-of-disk: Failed to create object â€¦â€ (critical)

### Workers are being killed due to memory (OOM)
- **Where to look**: `raylet.out` (Ray memory monitor), system logs via `dmesg`/`journalctl` (Linux OOM killer)
- **Also check**: `worker-*.err` for â€œUNEXPECTED_SYSTEM_EXITâ€ or â€œOutOfMemoryErrorâ€ on `ray.get`
- **Search for**:
  - Ray memory monitor: â€œKilling worker with task â€¦ Memory on the node â€¦ exceeds the thresholdâ€
  - Periodic summary: â€œWorkers â€¦ killed due to memory pressure (OOM)â€
  - Linux OOM: â€œkilled processâ€ in `dmesg`
  - See detailed OOM guidance below for remediation and configuration

