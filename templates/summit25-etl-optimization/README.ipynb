{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ETL Processing and Optimization With Ray Data\n",
        "\n",
        "**Time to complete**: 40 min | **Difficulty**: Intermediate | **Prerequisites**: ETL concepts, basic SQL knowledge, data processing experience\n",
        "\n",
        "## What you'll build\n",
        "\n",
        "Build comprehensive ETL pipelines using Ray Data's distributed processing capabilities, from foundational concepts with TPC-H benchmark to production-scale optimization techniques for enterprise data processing.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [ETL Fundamentals with TPC-H](#step-1-etl-fundamentals-with-tpc-h) (10 min)\n",
        "2. [Data Transformations and Processing](#step-2-data-transformations-and-processing) (12 min)\n",
        "3. [Performance Optimization Techniques](#step-3-performance-optimization-techniques) (10 min)\n",
        "4. [Large-Scale ETL Patterns](#step-4-large-scale-etl-patterns) (8 min)\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "**Why ETL optimization matters**: The difference between fast and slow data pipelines directly impacts business agility and operational costs. Understanding optimization techniques enables data teams to deliver insights faster while reducing infrastructure costs.\n",
        "\n",
        "**Ray Data's ETL capabilities**: Native operations for distributed processing that automatically optimize memory, CPU, and I/O utilization. You'll learn how Ray Data's architecture enables efficient processing of large datasets.\n",
        "\n",
        "**TPC-H benchmark patterns**: Learn ETL fundamentals using the TPC-H benchmark that simulates complex business environments with customers, orders, suppliers, and products.\n",
        "\n",
        "**Production optimization strategies**: Memory management, parallel processing, and resource configuration patterns for production ETL workloads that scale from gigabytes to petabytes.\n",
        "\n",
        "**Enterprise ETL patterns**: Techniques used by data engineering teams to process large datasets efficiently while maintaining data quality and performance.\n",
        "\n",
        "## Prerequisites Checklist\n",
        "\n",
        "Before starting, ensure you have:\n",
        "- Understanding of ETL (Extract, Transform, Load) concepts\n",
        "- Basic SQL knowledge for data transformations\n",
        "- Python experience with data processing\n",
        "- Familiarity with distributed computing concepts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick start (3 minutes)\n",
        "\n",
        "This section demonstrates ETL processing concepts using Ray Data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Dict, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import ray\n",
        "from ray.data.expressions import col, lit\n",
        "\n",
        "from typing import Dict, Any, List\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import ray\n",
        "from ray.data.aggregate import Count, Mean, Sum, Max\n",
        "from ray.data.expressions import col, lit\n",
        "\n",
        "\n",
        "# Configure Ray Data for optimal performance monitoring\n",
        "ctx = ray.data.DataContext.get_current()\n",
        "ctx.enable_progress_bars = False\n",
        "ctx.enable_operator_progress_bars = False\n",
        "\n",
        "# Initialize Ray\n",
        "ray.init(ignore_reinit_error=True)\n",
        "\n",
        "# Load sample dataset for ETL demonstration\n",
        "sample_data = ray.data.read_parquet(\n",
        "    \"s3://ray-benchmark-data/tpch/parquet/sf1/customer\",\n",
        ")\n",
        "\n",
        "sample_data = sample_data.drop_columns([\"column8\"])\n",
        "sample_data = sample_data.rename_columns([\n",
        "    \"c_custkey\",\n",
        "    \"c_name\",\n",
        "    \"c_address\",\n",
        "    \"c_nationkey\",\n",
        "    \"c_phone\",\n",
        "    \"c_acctbal\",\n",
        "    \"c_mktsegment\",\n",
        "    \"c_comment\",\n",
        "    ])\n",
        "\n",
        "print(f\"Loaded ETL sample dataset: {sample_data.count()} records\")\n",
        "print(f\"Schema: {sample_data.schema()}\")\n",
        "print(\"\\nSample records:\")\n",
        "for i, record in enumerate(sample_data.take(3)):\n",
        "    print(f\"  {i+1}. Customer {record['c_custkey']}: {record['c_name']} from {record['c_mktsegment']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "**Challenge**: Traditional ETL tools struggle with modern data volumes and complexity. Processing large datasets can take significant time, creating bottlenecks in data-driven organizations.\n",
        "\n",
        "**Solution**: Ray Data's distributed architecture and optimized operations enable efficient processing of large datasets through parallel computation and native operations.\n",
        "\n",
        "**Impact**: Data engineering teams process terabytes of data daily using Ray Data's ETL capabilities. Companies transform raw data into analytics-ready datasets efficiently while maintaining data quality and performance.\n",
        "\n",
        "### ETL pipeline architecture\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────────────────────────────────┐\n",
        "│                    Ray Data ETL Pipeline                        │\n",
        "├─────────────────────────────────────────────────────────────────┤\n",
        "│                                                                 │\n",
        "│  Extract              Transform              Load               │\n",
        "│  ────────            ──────────            ──────              │\n",
        "│                                                                 │\n",
        "│  read_parquet()  →   map_batches()    →   write_parquet()     │\n",
        "│  (TPC-H Data)        (Business Logic)     (Data Warehouse)     │\n",
        "│                                                                 │\n",
        "│  ↓ Column Pruning    ↓ Filter/Join       ↓ Partitioning       │\n",
        "│  ↓ Parallel I/O      ↓ Aggregations      ↓ Compression        │\n",
        "│  ↓ High Concurrency  ↓ Enrichment        ↓ Schema Optimization│\n",
        "│                                                                 │\n",
        "└─────────────────────────────────────────────────────────────────┘\n",
        "\n",
        "Data Flow:\n",
        "  TPC-H Customer (150K) ─┐\n",
        "  TPC-H Orders (1.5M)   ─┼→ Join → Enrich → Aggregate → Warehouse\n",
        "  TPC-H LineItems (6M)  ─┘      ↓         ↓            ↓\n",
        "                            Filter    Transform    Partition\n",
        "```\n",
        "\n",
        "### ETL performance comparison\n",
        "\n",
        "| Approach | Data Loading | Transformations | Joins | Output | Use Case |\n",
        "|-----------|--------------|------------------|--------|----------|-----------|\n",
        "| **Traditional** | Sequential | Single-threaded | Memory-limited | Slow writes | Small datasets |\n",
        "| **Ray Data** | Parallel I/O | Distributed | Scalable | Optimized writes | Production scale |\n",
        "\n",
        "**Key advantages**:\n",
        "- **Parallel processing**: Distribute transformations across cluster nodes\n",
        "- **Memory efficiency**: Stream processing without materializing full datasets\n",
        "- **Native operations**: Optimized filter, join, and aggregate functions\n",
        "- **Scalability**: Handle datasets from gigabytes to petabytes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: ETL Fundamentals with TPC-H\n",
        "\n",
        "### Understanding TPC-H benchmark\n",
        "\n",
        "**What is TPC-H?**\n",
        "\n",
        "The TPC-H benchmark is used for testing database and data processing performance. It simulates a business environment with data relationships that represent business scenarios.\n",
        "\n",
        "**TPC-H Business Context**: The benchmark models a wholesale supplier managing customer orders, inventory, and supplier relationships - representing business data systems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TPC-H schema overview\n",
        "\n",
        "The TPC-H benchmark provides realistic business data for learning ETL patterns. Understanding the schema helps you apply these techniques to your own data.\n",
        "\n",
        "| Table | Description | Typical Size (SF10) | Primary Use |\n",
        "|-----------|------------------|--------------------------|------------------|\n",
        "| **CUSTOMER** | Customer master data | 1.5M rows | Dimensional analysis |\n",
        "| **ORDERS** | Order transactions | 15M rows | Fact table, time series |\n",
        "| **LINEITEM** | Order line items | 60M rows | Largest fact table |\n",
        "| **PART** | Product catalog | 2M rows | Product dimensions |\n",
        "| **SUPPLIER** | Supplier information | 100K rows | Supplier analytics |\n",
        "| **PARTSUPP** | Part-supplier links | 8M rows | Supply chain |\n",
        "| **NATION** | Geographic data | 25 rows | Geographic grouping |\n",
        "| **REGION** | Regional groups | 5 rows | High-level geography |\n",
        "\n",
        "**Schema relationships**:\n",
        "\n",
        "```\n",
        "CUSTOMER ──one-to-many──→ ORDERS ──one-to-many──→ LINEITEM\n",
        "                                                      ↓\n",
        "NATION ──one-to-many──→ SUPPLIER                   PART\n",
        "   ↓                        ↓                         ↓\n",
        "REGION                  PARTSUPP ←────many-to-one────┘\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TPC-H Schema Overview for ETL Processing\n",
        "tpch_tables = {\n",
        "    \"customer\": \"Customer master data with demographics and market segments\",\n",
        "    \"orders\": \"Order header information with dates, priorities, and status\",\n",
        "    \"lineitem\": \"Detailed line items for each order (largest table)\",\n",
        "    \"part\": \"Parts catalog with specifications and retail prices\", \n",
        "    \"supplier\": \"Supplier information including contact details\",\n",
        "    \"partsupp\": \"Part-supplier relationships with costs\",\n",
        "    \"nation\": \"Nation reference data with geographic regions\",\n",
        "    \"region\": \"Regional groupings for geographic analysis\"\n",
        "}\n",
        "\n",
        "print(\"TPC-H Schema (8 Tables):\")\n",
        "for table, description in tpch_tables.items():\n",
        "    print(f\"  {table.upper()}: {description}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading TPC-H data with Ray Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TPC-H benchmark data location\n",
        "TPCH_S3_PATH = \"s3://ray-benchmark-data/tpch/parquet/sf10\"\n",
        "\n",
        "print(\"Loading TPC-H benchmark data for distributed processing...\")\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    # Read TPC-H Customer Master Data\n",
        "    customers_ds = ray.data.read_parquet(\n",
        "        f\"{TPCH_S3_PATH}/customer\",\n",
        "        ray_remote_args={\"num_cpus\":0.25}\n",
        "    )\n",
        "    customers_ds = customers_ds.drop_columns([\"column8\"])\n",
        "    customers_ds = customers_ds.rename_columns([\n",
        "        \"c_custkey\",\n",
        "        \"c_name\",\n",
        "        \"c_address\",\n",
        "        \"c_nationkey\",\n",
        "        \"c_phone\",\n",
        "        \"c_acctbal\",\n",
        "        \"c_mktsegment\",\n",
        "        \"c_comment\",\n",
        "        ])\n",
        "    \n",
        "    # Read TPC-H Orders Data\n",
        "    orders_ds = ray.data.read_parquet(\n",
        "        f\"{TPCH_S3_PATH}/orders\", \n",
        "        ray_remote_args={\"num_cpus\":0.25}\n",
        "    )\n",
        "    orders_ds = (orders_ds\n",
        "        .select_columns([f\"column{i}\" for i in range(9)])\n",
        "        .rename_columns([\n",
        "            \"o_orderkey\",\n",
        "            \"o_custkey\",\n",
        "            \"o_orderstatus\",\n",
        "            \"o_totalprice\",\n",
        "            \"o_orderdate\",\n",
        "            \"o_orderpriority\",\n",
        "            \"o_clerk\",\n",
        "            \"o_shippriority\",\n",
        "            \"o_comment\",\n",
        "        ])\n",
        "    )\n",
        "    \n",
        "    # Read TPC-H Line Items (largest table)\n",
        "    lineitems_ds = ray.data.read_parquet(\n",
        "        f\"{TPCH_S3_PATH}/lineitem\",\n",
        "        ray_remote_args={\"num_cpus\":0.25}\n",
        "    )\n",
        "    lineitem_cols = [f\"column{str(i).zfill(2)}\" for i in range(16)]\n",
        "    lineitems_ds = (lineitems_ds\n",
        "        .select_columns(lineitem_cols)\n",
        "        .rename_columns([\n",
        "            \"l_orderkey\",\n",
        "            \"l_partkey\",\n",
        "            \"l_suppkey\",\n",
        "            \"l_linenumber\",\n",
        "            \"l_quantity\",\n",
        "            \"l_extendedprice\",\n",
        "            \"l_discount\",\n",
        "            \"l_tax\",\n",
        "            \"l_returnflag\",\n",
        "            \"l_linestatus\",\n",
        "            \"l_shipdate\",\n",
        "            \"l_commitdate\",\n",
        "            \"l_receiptdate\",\n",
        "            \"l_shipinstruct\",\n",
        "            \"l_shipmode\",\n",
        "            \"l_comment\",\n",
        "        ])\n",
        "    )\n",
        "    \n",
        "    load_time = time.time() - start_time\n",
        "    \n",
        "    # Count records in parallel\n",
        "    customer_count = customers_ds.count()\n",
        "    orders_count = orders_ds.count()\n",
        "    lineitems_count = lineitems_ds.count()\n",
        "    \n",
        "    print(f\"TPC-H data loaded successfully in {load_time:.2f} seconds\")\n",
        "    print(f\"   Customers: {customer_count:,}\")\n",
        "    print(f\"   Orders: {orders_count:,}\")\n",
        "    print(f\"   Line items: {lineitems_count:,}\")\n",
        "    print(f\"   Total records: {customer_count + orders_count + lineitems_count:,}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Failed to load TPC-H data: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Basic ETL transformations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ETL Transform: Customer segmentation using Ray Data native operations\n",
        "def segment_customers(batch: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Apply business rules for customer segmentation.\n",
        "    \n",
        "    This demonstrates common ETL pattern of adding derived business attributes\n",
        "    based on rules and thresholds.\n",
        "    \n",
        "    Args:\n",
        "        batch: Pandas DataFrame with customer records\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with added customer_segment column\n",
        "    \"\"\"\n",
        "    # Business logic for customer segmentation based on account balance\n",
        "    batch['customer_segment'] = 'standard'\n",
        "    batch.loc[batch['c_acctbal'] > 5000, 'customer_segment'] = 'premium'\n",
        "    batch.loc[batch['c_acctbal'] > 10000, 'customer_segment'] = 'enterprise'\n",
        "    \n",
        "    return batch\n",
        "\n",
        "# Apply customer segmentation transformation\n",
        "print(\"Applying customer segmentation...\")\n",
        "\n",
        "try:\n",
        "    segmented_customers = customers_ds.map_batches(\n",
        "        segment_customers,\n",
        "        num_cpus=0.5,  # Medium complexity transformation\n",
        "        batch_format=\"pandas\"\n",
        "    )\n",
        "    \n",
        "    segment_count = segmented_customers.count()\n",
        "    print(f\"Customer segmentation completed: {segment_count:,} customers segmented\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Segmentation failed: {e}\")\n",
        "    raise\n",
        "\n",
        "# ETL Filter: High-value customers using expressions API\n",
        "print(\"Filtering high-value customers...\")\n",
        "\n",
        "try:\n",
        "    high_value_customers = segmented_customers.filter(\n",
        "        expr=\"c_acctbal > 1000\",\n",
        "        num_cpus=0.1\n",
        "    )\n",
        "    \n",
        "    high_value_count = high_value_customers.count()\n",
        "    total_count = segmented_customers.count()\n",
        "    percentage = (high_value_count / total_count) * 100 if total_count > 0 else 0\n",
        "    \n",
        "    print(f\"High-value customers: {high_value_count:,} ({percentage:.1f}% of total)\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error during filtering: {e}\")\n",
        "    raise\n",
        "\n",
        "# ETL Aggregation: Customer statistics by market segment\n",
        "customer_stats = segmented_customers.groupby(\"c_mktsegment\").aggregate(\n",
        "    Count(),\n",
        "    Mean(\"c_acctbal\"),\n",
        "    Sum(\"c_acctbal\"),\n",
        "    Max(\"c_acctbal\")\n",
        ")\n",
        "\n",
        "print(\"Customer Statistics by Market Segment:\")\n",
        "print(\"=\" * 70)\n",
        "# Display customer statistics\n",
        "stats_df = customer_stats.limit(10).to_pandas()\n",
        "print(stats_df.to_string(index=False))\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Data Transformations and Processing\n",
        "\n",
        "This section demonstrates how Ray Data handles common ETL transformation patterns including data enrichment, filtering, and complex business logic. You'll learn to build production-grade transformations that scale efficiently.\n",
        "\n",
        "### Why transformations are critical\n",
        "\n",
        "Data transformations convert raw data into business-valuable information. Common transformation patterns include:\n",
        "\n",
        "- **Enrichment**: Adding calculated fields and derived metrics\n",
        "- **Filtering**: Removing irrelevant or invalid records  \n",
        "- **Joins**: Combining data from multiple sources\n",
        "- **Aggregations**: Computing summary statistics and rollups\n",
        "- **Type conversions**: Ensuring correct data types for analytics\n",
        "\n",
        "### Transformation performance comparison\n",
        "\n",
        "| Transformation Type | Traditional Approach | Ray Data Approach | Scalability |\n",
        "|-------------------|---------------------|-------------------|--------------|\n",
        "| **Column calculations** | Row-by-row processing | Vectorized batches | Linear scaling |\n",
        "| **Date parsing** | Sequential parsing | Parallel batch parsing | High throughput |\n",
        "| **Categorization** | Conditional logic loops | Pandas vectorization | Efficient |\n",
        "| **Business rules** | Single-threaded | Distributed map_batches | Scales to cluster |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Complex data transformations\n",
        "\n",
        "\n",
        "<div style=\"margin:1em 0; padding:12px 16px; border-left:4px solid #2e7d32; background:#f1f8e9; border-radius:4px;\">\n",
        "\n",
        "  **GPU Acceleration for Pandas ETL Operations**: For complex pandas transformations in your ETL pipeline, you can use **NVIDIA RAPIDS cuDF** to accelerate DataFrame operations on GPUs. \n",
        "  \n",
        "  Replace `import pandas as pd` with `import cudf as pd` in your `map_batches` functions to use GPU acceleration for operations like datetime parsing, groupby, joins, and aggregations.\n",
        "\n",
        "**When to use cuDF**:\n",
        "- Complex datetime operations (parsing, extracting components)\n",
        "- Large aggregations and groupby operations\n",
        "- String operations on millions of rows\n",
        "- Join operations on large datasets\n",
        "- Statistical calculations across many columns\n",
        "\n",
        "**Performance benefit**: GPU-accelerated pandas operations can be 10-50x faster for large batches (1000+ rows) with complex transformations.\n",
        "\n",
        "**Requirements**: Add `cudf` to your dependencies and ensure GPU-enabled cluster nodes.\n",
        "\n",
        "**Before**\n",
        "\n",
        "```python\n",
        "def my_fnc(batch):\n",
        "    # Process batch with pandas operations here\n",
        "    res = ...\n",
        "    return res\n",
        "\n",
        "ds = ds.map_batches(my_fnc, format=\"pandas\")\n",
        "```\n",
        "\n",
        "**After**\n",
        "\n",
        "```python\n",
        "def my_fnc(batch):\n",
        "    batch = cudf.from_pandas(batch)\n",
        "    res = ...\n",
        "    return res\n",
        "\n",
        "ds = ds.map_batches(my_fnc, format=\"pandas\", num_gpus=1)\n",
        "```\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ETL Transform: Order enrichment with business metrics\n",
        "def enrich_orders_with_metrics(batch):\n",
        "    \"\"\"Enrich orders with calculated business metrics.\n",
        "    \n",
        "    For GPU acceleration, replace 'import pandas as pd' with 'import cudf as pd'\n",
        "    to speed up complex DataFrame operations like datetime parsing and categorization.\n",
        "    \"\"\"\n",
        "    import pandas as pd  # or 'import cudf as pd' for GPU acceleration\n",
        "    df = pd.DataFrame(batch)\n",
        "    \n",
        "    # Parse order date and create time dimensions\n",
        "    # This datetime parsing is GPU-accelerated with cuDF\n",
        "    df['o_orderdate'] = pd.to_datetime(df['o_orderdate'])\n",
        "    df['order_year'] = df['o_orderdate'].dt.year\n",
        "    df['order_quarter'] = df['o_orderdate'].dt.quarter\n",
        "    df['order_month'] = df['o_orderdate'].dt.month\n",
        "    \n",
        "    # Business classifications\n",
        "    # These conditional operations are GPU-accelerated with cuDF\n",
        "    df['is_large_order'] = df['o_totalprice'] > 200000\n",
        "    df['is_urgent'] = df['o_orderpriority'].isin(['1-URGENT', '2-HIGH'])\n",
        "    df['revenue_tier'] = pd.cut(\n",
        "        df['o_totalprice'],\n",
        "        bins=[0, 50000, 150000, 300000, float('inf')],\n",
        "        labels=['Small', 'Medium', 'Large', 'Enterprise']\n",
        "    ).astype(str)  # Convert categorical to string for Ray Data compatibility\n",
        "    \n",
        "    return df\n",
        "    \n",
        "# Apply order enrichment\n",
        "print(\"\\nEnriching orders with business metrics...\")\n",
        "\n",
        "try:\n",
        "    enriched_orders = orders_ds.map_batches(\n",
        "        enrich_orders_with_metrics,\n",
        "        num_cpus=0.5,  # Medium complexity transformation\n",
        "        batch_format=\"pandas\"\n",
        "    )\n",
        "    \n",
        "    enriched_count = enriched_orders.count()\n",
        "    print(f\"Order enrichment completed: {enriched_count:,} orders processed\")\n",
        "    \n",
        "    # Show sample enriched record\n",
        "    sample = enriched_orders.take(1)[0]\n",
        "    print(f\"\\nSample enriched order:\")\n",
        "    print(f\"   Order ID: {sample.get('o_orderkey')}\")\n",
        "    print(f\"   Year: {sample.get('order_year')}, Quarter: {sample.get('order_quarter')}\")\n",
        "    print(f\"   Revenue Tier: {sample.get('revenue_tier')}\")\n",
        "    print(f\"   Is Large Order: {sample.get('is_large_order')}\")\n",
        "    print(f\"   Is Urgent: {sample.get('is_urgent')}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error during enrichment: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Advanced filtering and selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced filtering using Ray Data expressions API\n",
        "print(\"Applying advanced filtering techniques...\")\n",
        "\n",
        "recent_high_value_orders = enriched_orders.filter(\n",
        "expr=\"order_year >= 1995 and o_totalprice > 100000 and is_urgent\",\n",
        "num_cpus=0.1\n",
        ")\n",
        "\n",
        "enterprise_orders = enriched_orders.filter(\n",
        "expr=\"revenue_tier == 'Enterprise'\",\n",
        "num_cpus=0.1\n",
        ")\n",
        "\n",
        "complex_filtered_orders = enriched_orders.filter(\n",
        "expr=\"order_quarter == 4 and o_orderstatus == 'F' and o_totalprice > 50000\",\n",
        "num_cpus=0.1\n",
        ")\n",
        "\n",
        "print(\"Advanced filtering results:\")\n",
        "print(f\"  Recent high-value orders: {recent_high_value_orders.count():,}\")\n",
        "print(f\"  Enterprise orders: {enterprise_orders.count():,}\")\n",
        "print(f\"  Complex filtered orders: {complex_filtered_orders.count():,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "recent_high_value_orders.limit(5).to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "enterprise_orders.limit(5).to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "complex_filtered_orders.limit(5).to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data joins and relationships\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ETL Join: Customer-Order analysis using Ray Data joins\n",
        "print(\"\\nPerforming distributed joins for customer-order analysis...\")\n",
        "\n",
        "try:\n",
        "    # Join customers with their orders for comprehensive analysis\n",
        "    # Ray Data optimizes join execution across distributed nodes\n",
        "    customer_order_analysis = customers_ds.join(\n",
        "        enriched_orders,\n",
        "        on=(\"c_custkey\",),\n",
        "        right_on=(\"o_custkey\",),\n",
        "        join_type=\"inner\",\n",
        "        num_partitions=100\n",
        "    )\n",
        "    \n",
        "    join_count = customer_order_analysis.count()\n",
        "    print(f\"Customer-order join completed: {join_count:,} records\")\n",
        "    \n",
        "    # Calculate join statistics\n",
        "    customer_count = customers_ds.count()\n",
        "    orders_count = enriched_orders.count()\n",
        "    join_ratio = (join_count / orders_count) * 100 if orders_count > 0 else 0\n",
        "    \n",
        "    print(f\"   Input: {customer_count:,} customers, {orders_count:,} orders\")\n",
        "    print(f\"   Join ratio: {join_ratio:.1f}% of orders matched\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error during join: {e}\")\n",
        "    raise\n",
        "\n",
        "# Aggregate customer order metrics\n",
        "customer_order_metrics = customer_order_analysis.groupby(\"c_mktsegment\").aggregate(\n",
        "    Count(),\n",
        "    Mean(\"o_totalprice\"),\n",
        "    Sum(\"o_totalprice\"),\n",
        "    Count(\"o_orderkey\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "customer_order_metrics.limit(5).to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Performance Optimization Techniques\n",
        "\n",
        "This section covers advanced optimization techniques for production ETL workloads.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure Ray Data for optimal ETL performance\n",
        "print(\"Configuring Ray Data for ETL optimization...\")\n",
        "\n",
        "# Memory optimization for large datasets\n",
        "ctx.target_max_block_size = 128 * 1024 * 1024  # 128 MB blocks\n",
        "ctx.eager_free = True  # Aggressive memory cleanup\n",
        "\n",
        "# Enable performance monitoring\n",
        "ctx.enable_auto_log_stats = True\n",
        "ctx.memory_usage_poll_interval_s = 5.0\n",
        "\n",
        "print(\"Ray Data configured for optimal ETL performance\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Batch size and concurrency optimization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import uuid\n",
        "from datetime import datetime\n",
        "\n",
        "# Demonstrate different batch size strategies for ETL operations\n",
        "print(\"Testing ETL batch size optimization...\")\n",
        "\n",
        "# Small batch processing for memory-constrained operations\n",
        "def memory_intensive_etl(batch):\n",
        "    \"\"\"Memory-intensive ETL transformation.\"\"\"\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    df = pd.DataFrame(batch)\n",
        "    \n",
        "    # Simulate memory-intensive operations\n",
        "    df['complex_metric'] = df['o_totalprice'] * np.log(df['o_totalprice'] + 1)\n",
        "    df['percentile_rank'] = df['o_totalprice'].rank(pct=True)\n",
        "    \n",
        "    return df \n",
        "\n",
        "# Apply with optimized batch size for memory management\n",
        "memory_optimized_orders = enriched_orders.map_batches(\n",
        "    memory_intensive_etl,\n",
        "    num_cpus=1.0,  # Fewer concurrent tasks for memory management\n",
        "    batch_size=500,  # Smaller batches for memory efficiency\n",
        "    batch_format=\"pandas\"\n",
        ")\n",
        "\n",
        "print(f\"Memory-optimized processing: {memory_optimized_orders.count():,} records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "memory_optimized_orders.limit(5).to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "output_dir = \"/mnt/cluster_storage/temp_etl_batches\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "def io_intensive_etl(batch):\n",
        "    \"\"\"I/O-intensive ETL transformation with actual disk writes.\"\"\"\n",
        "    import pandas as pd\n",
        "    from datetime import datetime\n",
        "    import uuid\n",
        "    \n",
        "    df = pd.DataFrame(batch)\n",
        "    \n",
        "    # Add processing metadata\n",
        "    df['processing_timestamp'] = datetime.now().isoformat()\n",
        "    batch_id = str(uuid.uuid4())[:8]\n",
        "    df['batch_id'] = batch_id\n",
        "    \n",
        "    # Actual I/O operation: write batch to disk\n",
        "    output_path = f\"{output_dir}/batch_{batch_id}.parquet\"\n",
        "    df.to_parquet(output_path, index=False)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Apply with optimized batch size for I/O efficiency\n",
        "io_optimized_orders = enriched_orders.map_batches(\n",
        "    io_intensive_etl,\n",
        "    num_cpus=0.25,  # Higher concurrency for I/O operations\n",
        "    batch_size=2000,  # Larger batches for I/O efficiency\n",
        "    batch_format=\"pandas\"\n",
        ")\n",
        "\n",
        "print(f\"I/O-optimized processing: {io_optimized_orders.count():,} records\")\n",
        "print(f\"Batch files written to: /mnt/cluster_storage/temp_etl_batches/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "io_optimized_orders.limit(5).to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Column selection and schema optimization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ETL Optimization: Column pruning for performance\n",
        "print(\"Applying column selection optimization...\")\n",
        "\n",
        "# Select only essential columns for downstream processing\n",
        "essential_customer_columns = customers_ds.select_columns([\n",
        "    \"c_custkey\", \"c_name\", \"c_mktsegment\", \"c_acctbal\", \"c_nationkey\"\n",
        "])\n",
        "\n",
        "essential_order_columns = enriched_orders.select_columns([\n",
        "    \"o_orderkey\", \"o_custkey\", \"o_totalprice\", \"o_orderdate\", \n",
        "    \"order_year\", \"revenue_tier\", \"is_large_order\"\n",
        "])\n",
        "\n",
        "print(f\"Column optimization:\")\n",
        "print(f\"  Customer columns: {len(essential_customer_columns.schema().names)}\")\n",
        "print(f\"  Order columns: {len(essential_order_columns.schema().names)}\")\n",
        "\n",
        "# Optimized join with selected columns\n",
        "optimized_join = essential_customer_columns.join(\n",
        "    essential_order_columns,\n",
        "    on=(\"c_custkey\",),\n",
        "    right_on=(\"o_custkey\",),\n",
        "    num_partitions=100,\n",
        "    join_type=\"inner\",\n",
        ")\n",
        "\n",
        "print(f\"Optimized join completed: {optimized_join.count():,} records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimized_join.limit(5).to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Large-Scale ETL Patterns\n",
        "\n",
        "Production ETL systems must handle billions of records efficiently. This section demonstrates Ray Data patterns for large-scale data processing including distributed aggregations, multi-dimensional analysis, and data warehouse integration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Large-scale aggregations using Ray Data \n",
        "print(\"Performing large-scale distributed aggregations...\")\n",
        "\n",
        "# Multi-dimensional aggregations for business intelligence\n",
        "comprehensive_metrics = optimized_join.groupby([\"c_mktsegment\", \"order_year\", \"revenue_tier\"]).aggregate(\n",
        "    Count(),\n",
        "    Sum(\"o_totalprice\"),\n",
        "    Mean(\"o_totalprice\"),\n",
        "    Max(\"o_totalprice\"),\n",
        "    Mean(\"c_acctbal\")\n",
        ")\n",
        "\n",
        "print(\"Comprehensive Business Metrics:\")\n",
        "print(comprehensive_metrics.limit(5).to_pandas())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time-series aggregations for trend analysis\n",
        "yearly_trends = optimized_join.groupby(\"order_year\").aggregate(\n",
        "    Count(),\n",
        "    Sum(\"o_totalprice\"),\n",
        "    Mean(\"o_totalprice\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yearly_trends.limit(5).to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Customer segment performance analysis\n",
        "segment_performance = optimized_join.groupby([\"c_mktsegment\", \"revenue_tier\"]).aggregate(\n",
        "    Count(),\n",
        "    Sum(\"o_totalprice\"),\n",
        "    Mean(\"c_acctbal\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "segment_performance.limit(5).to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ETL output and data warehouse integration\n",
        "\n",
        "Ray Data provides native write functions for various data warehouses and file formats, enabling you to export processed datasets directly to your target storage systems. You can write to Snowflake using `write_snowflake()`, which handles authentication and schema management automatically. \n",
        "\n",
        "\n",
        "For other data warehouses, Ray Data supports writing to BigQuery with `write_bigquery()`, SQL databases with `write_sql()`, and modern table formats like Delta Lake (`write_delta()` and `write_unity_catalog()`, *coming soon*) and Apache Iceberg (`write_iceberg()`). Additionally, you can write to file-based formats such as Parquet using `write_parquet(),` which offers efficient columnar storage with compression options. \n",
        "\n",
        "\n",
        "These native write functions integrate seamlessly with Ray Data's distributed processing, allowing you to scale data export operations across your cluster while maintaining data consistency and optimizing write performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write ETL results to data warehouse formats\n",
        "print(\"Writing ETL results to data warehouse...\")\n",
        "\n",
        "# Replace with S3 or other cloud storage in a real production use case\n",
        "BASE_DIRECTORY = \"/mnt/cluster_storage/\"\n",
        "\n",
        "# Write customer analytics with partitioning\n",
        "enriched_customers = segmented_customers\n",
        "enriched_customers.write_parquet(\n",
        "    f\"{BASE_DIRECTORY}/etl_warehouse/customers/\",\n",
        "    partition_cols=[\"customer_segment\"],\n",
        "    compression=\"snappy\",\n",
        "    ray_remote_args={\"num_cpus\": 0.1}\n",
        ")\n",
        "\n",
        "# Write order analytics with time-based partitioning\n",
        "enriched_orders.write_parquet(\n",
        "    f\"{BASE_DIRECTORY}/etl_warehouse/orders/\",\n",
        "    partition_cols=[\"order_year\"],\n",
        "    compression=\"snappy\",\n",
        "    ray_remote_args={\"num_cpus\": 0.1}\n",
        ")\n",
        "\n",
        "# Write aggregated analytics for BI tools\n",
        "final_analytics = optimized_join.groupby([\"c_mktsegment\", \"revenue_tier\", \"order_year\"]).aggregate(\n",
        "    Count(),\n",
        "    Sum(\"o_totalprice\"),\n",
        "    Mean(\"o_totalprice\"),\n",
        "    Mean(\"c_acctbal\")\n",
        ")\n",
        "\n",
        "final_analytics.write_parquet(\n",
        "    f\"{BASE_DIRECTORY}/etl_warehouse/analytics/\",\n",
        "    partition_cols=[\"order_year\"],\n",
        "    compression=\"snappy\",\n",
        "    ray_remote_args={\"num_cpus\": 0.1}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Output Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate ETL pipeline performance\n",
        "print(\"Validating ETL output...\")\n",
        "\n",
        "BASE_DIRECTORY = \"/mnt/cluster_storage/\"\n",
        "\n",
        "# Read back and verify outputs\n",
        "customer_verification = ray.data.read_parquet(\n",
        "    f\"{BASE_DIRECTORY}/etl_warehouse/customers/\",\n",
        "    ray_remote_args={\"num_cpus\":0.025}\n",
        ")\n",
        "\n",
        "order_verification = ray.data.read_parquet(\n",
        "    f\"{BASE_DIRECTORY}/etl_warehouse/orders/\",\n",
        "    ray_remote_args={\"num_cpus\":0.025}\n",
        ")\n",
        "\n",
        "analytics_verification = ray.data.read_parquet(\n",
        "    f\"{BASE_DIRECTORY}/etl_warehouse/analytics/\",\n",
        "    ray_remote_args={\"num_cpus\":0.025}\n",
        ")\n",
        "\n",
        "print(f\"ETL Pipeline Verification:\")\n",
        "print(f\"  Customer records: {customer_verification.count():,}\")\n",
        "print(f\"  Order records: {order_verification.count():,}\")\n",
        "print(f\"  Analytics records: {analytics_verification.count():,}\")\n",
        "\n",
        "# Display sample results\n",
        "sample_analytics = analytics_verification.take(25)\n",
        "print(\"\\nSample ETL Analytics Results:\")\n",
        "for i, record in enumerate(sample_analytics):\n",
        "    print(f\"  {i+1}. Segment: {record['c_mktsegment']}, Tier: {record['revenue_tier']}, \"\n",
        "          f\"Year: {record['order_year']}, Orders: {record['count()']}, Revenue: ${record['sum(o_totalprice)']:,.0f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
