{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "920e1a85",
   "metadata": {},
   "source": [
    "# Intro to Ray Serve\n",
    "\n",
    "This notebook will introduce you to Ray Serve, a framework for building and deploying scalable ML applications.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b>Here is the roadmap for this notebook:</b>\n",
    "\n",
    "<ul>\n",
    "    <li><b>1.</b> When to consider Ray Serve</li>\n",
    "    <li><b>2.</b> Implement an image classification service</li>\n",
    "    <li><b>3.</b> Key concepts in Ray Serve</li>\n",
    "    <li><b>4.</b> Model composition with Ray Serve</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f195b29c",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007cd0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import logging\n",
    "from typing import Any\n",
    "from langdetect import detect\n",
    "\n",
    "import json\n",
    "import fastapi\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pydantic import BaseModel\n",
    "from ray import serve\n",
    "from ray.serve.handle import DeploymentHandle\n",
    "from starlette.requests import Request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf1c875",
   "metadata": {},
   "source": [
    "## 1. When to Consider Ray Serve\n",
    "\n",
    "Consider using Ray Serve for your project if it meets one or more of the following criteria:\n",
    "\n",
    "| **Challenge** | **Details** | **Ray Serve Solution** |\n",
    "|---------------|------------------|--------------------------|\n",
    "| **Slow iteration speed for ML engineers** | - Developers need to containerize and rollout components on Kubernetes to test changes<br>- Developers need to use complex protocols (e.g. gRPC) to achieve acceptable performance | - Provides a Python-first API to develop lightweight services<br>- Services are lightweight [Ray actors](https://docs.ray.io/en/latest/ray-core/actors.html)<br>- Ray Serve can be run locally for development |\n",
    "| **Need to efficiently compose multiple components** | - Requiring efficient data sharing between components<br>- Implementing performant streaming protocols (e.g. gRPC) is a complex task | - Relies on [Ray's object store](https://docs.ray.io/en/latest/ray-core/objects.html) to share data optimally<br>- Avoids the need to implement gRPC streaming |\n",
    "| **Poor utilization of expensive hardware** | Naive request handling e.g. by passing requests one a time to GPUs or accelerators | - Offers [dynamic batching of requests](https://docs.ray.io/en/latest/serve/advanced-guides/dyn-req-batch.html) to improve hardware utilization<br>- Leverages Ray Core's support for accelerators and custom resources:<br>&nbsp;&nbsp;&nbsp;&nbsp;• [Multi-node/multi-GPU serving](https://docs.ray.io/en/latest/serve/tutorials/vllm-example.html)<br>&nbsp;&nbsp;&nbsp;&nbsp;• [Fractional compute resource usage](https://docs.ray.io/en/latest/serve/configure-serve-deployment.html)<br>- RayTurbo Serve offers [replica compaction](https://www.anyscale.com/blog/new-feature-replica-compaction?_gl=1*lrhlou*_gcl_au*OTY4NjkwODIzLjE3Mzg1Mjc2MzA.) |\n",
    "| **High-latency outliers when juggling many models** | Stuck with naive load balancing and expensive state loading (e.g. ML models) | - Provides [model multiplexing](https://docs.ray.io/en/latest/serve/model-multiplexing.html) to avoid unnecessary load times<br>- Routes to replicas that already have a model loaded |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e946b37",
   "metadata": {},
   "source": [
    "### Ray vs K8s \n",
    "\n",
    "Here are some key points to keep in mind when comparing Ray to Kubernetes:\n",
    "\n",
    "<table style=\"border-collapse: collapse; width: 100%; font-family: sans-serif; font-size: 15px;\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: left; border-bottom: 1px solid #ddd;\">\n",
    "      <th style=\"padding: 6px; color: #444;\">Category</th>\n",
    "      <th style=\"padding: 6px; color: #444;\">Kubernetes (Traditional Microservices)</th>\n",
    "      <th style=\"padding: 6px; color: #444;\">Ray on Kubernetes (AI/ML-Native Runtime)</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr style=\"border-bottom: 1px solid #f0f0f0;\">\n",
    "      <td style=\"padding: 6px;\"></td>\n",
    "      <td style=\"padding: 6px;\"><img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-serve-deep-dive/k8s.png\" width=\"500\"></td>\n",
    "      <td style=\"padding: 6px;\"><img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-serve-deep-dive/ray_on_k8s.png\" width=\"500\"></td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid #f0f0f0;\">\n",
    "      <td style=\"padding: 6px;\">Definition of Work</td>\n",
    "      <td style=\"padding: 6px;\">Microservices defined by <strong>Pods</strong></td>\n",
    "      <td style=\"padding: 6px;\">Work defined by <strong>Tasks/Actors</strong></td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid #f0f0f0;\">\n",
    "      <td style=\"padding: 6px;\">Interface</td>\n",
    "      <td style=\"padding: 6px;\">Declarative configs (YAML)</td>\n",
    "      <td style=\"padding: 6px;\">Programmatic API (Python-native)</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid #f0f0f0;\">\n",
    "      <td style=\"padding: 6px;\">Orchestration</td>\n",
    "      <td style=\"padding: 6px;\">Pods orchestrated on shared compute</td>\n",
    "      <td style=\"padding: 6px;\">Tasks/Actors orchestrated on any substrate (k8s shown)</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid #f0f0f0;\">\n",
    "      <td style=\"padding: 6px;\">State</td>\n",
    "      <td style=\"padding: 6px;\">Hard separation of stateless/stateful pods</td>\n",
    "      <td style=\"padding: 6px;\">Built-in object store and stateful actor model</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid #f0f0f0;\">\n",
    "      <td style=\"padding: 6px;\">Scaling</td>\n",
    "      <td style=\"padding: 6px;\">Pods scaled independently</td>\n",
    "      <td style=\"padding: 6px;\">Dynamic scheduling + autoscaling built into programming model</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid #f0f0f0;\">\n",
    "      <td style=\"padding: 6px;\">AI/ML Fit</td>\n",
    "      <td style=\"padding: 6px;\">General-purpose; evolving to meet AI/ML needs</td>\n",
    "      <td style=\"padding: 6px;\">Optimized for AI/ML workloads, deeply integrated with GPUs/accelerators</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"padding: 6px;\">Granularity</td>\n",
    "      <td style=\"padding: 6px;\">Coarse-grained (~seconds per container, ~500ms startup)</td>\n",
    "      <td style=\"padding: 6px;\">Fine-grained (~milliseconds per task, <5ms startup)</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7f8558",
   "metadata": {},
   "source": [
    "## 2. Implement an image classification service\n",
    "\n",
    "Let’s jump right in and get a simple ML service up and running on Ray Serve. \n",
    "\n",
    "Here is an image classification service that performs inference on a batch of handwritten digits using an `MNISTClassifier` model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594b6cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier:\n",
    "    def __init__(self, remote_path: str, local_path: str, device: str):\n",
    "        subprocess.run(f\"aws s3 cp {remote_path} {local_path}\", shell=True, check=True)\n",
    "        \n",
    "        self.device = device\n",
    "        self.model = torch.jit.load(local_path).to(device).eval()\n",
    "\n",
    "    def __call__(self, batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "        return self.predict(batch)\n",
    "    \n",
    "    def predict(self, batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "        images = torch.tensor(batch[\"image\"]).float().to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(images).cpu().numpy()\n",
    "\n",
    "        batch[\"predicted_label\"] = np.argmax(logits, axis=1)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904f95fa",
   "metadata": {},
   "source": [
    "First we need to load the classifier model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94033104",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MNISTClassifier(remote_path=\"s3://anyscale-public-materials/ray-ai-libraries/mnist/model/model.pt\", local_path=\"/mnt/cluster_storage/model.pt\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d861b66",
   "metadata": {},
   "source": [
    "Then we can run inference to generate predicted labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f37ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = classifier({\"image\": np.random.rand(1, 1, 28, 28).astype(np.float32)})  # Example input (B, C, H, W)\n",
    "output[\"predicted_label\"]  # Should be a numpy array with the predicted label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063850a2",
   "metadata": {},
   "source": [
    "Now, if want to migrate to an online inference setting, we can transform this into a Ray Serve Deployment by applying the `@serve.deployment` decorator \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a45646",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment() # this is the decorator to add\n",
    "class OnlineMNISTClassifier:\n",
    "    # same code as MNISTClassifier.__init__\n",
    "    def __init__(self, remote_path: str, local_path: str, device: str):\n",
    "        subprocess.run(f\"aws s3 cp {remote_path} {local_path}\", shell=True, check=True)\n",
    "        \n",
    "        self.device = device\n",
    "        self.model = torch.jit.load(local_path).to(device).eval()\n",
    "\n",
    "    async def __call__(self, request: Request) -> dict[str, Any]:  # __call__ now takes a Request object\n",
    "        batch = json.loads(await request.json()) # we will need to parse the JSON body of the request\n",
    "        return await self.predict(batch)\n",
    "    \n",
    "    # same code as MNISTClassifier.predict\n",
    "    async def predict(self, batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "        images = torch.tensor(batch[\"image\"]).float().to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(images).cpu().numpy()\n",
    "\n",
    "        batch[\"predicted_label\"] = np.argmax(logits, axis=1)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33f1043",
   "metadata": {},
   "source": [
    "We have now defined our Ray Serve deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028e98fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "OnlineMNISTClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7516ef57",
   "metadata": {},
   "source": [
    "We can now build an Application using `OnlineMNISTClassifier` deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef8ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_app = OnlineMNISTClassifier.bind(remote_path=\"s3://anyscale-public-materials/ray-ai-libraries/mnist/model/model.pt\", local_path=\"/mnt/cluster_storage/model.pt\", device=\"cpu\")\n",
    "mnist_app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8c0b99",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Note:** `.bind` is a method that takes in the arguments to pass to the Deployment constructor.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcd2570",
   "metadata": {},
   "source": [
    "We can then run the application \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d654bbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_app_handle = serve.run(mnist_app, name='mnist_classifier', blocking=False)\n",
    "mnist_app_handle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0982c42c",
   "metadata": {},
   "source": [
    "We can test it as an HTTP endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84202cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.random.rand(2, 1, 28, 28).tolist()\n",
    "json_request = json.dumps({\"image\": images})\n",
    "response = requests.post(\"http://localhost:8000/\", json=json_request)\n",
    "response.json()[\"predicted_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c50b4",
   "metadata": {},
   "source": [
    "We can also test it as a gRPC endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ef4950",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {\"image\": np.random.rand(10, 1, 28, 28)}\n",
    "response = await mnist_app_handle.predict.remote(batch)\n",
    "response[\"predicted_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bc0431",
   "metadata": {},
   "source": [
    "For more details on the recommended development workflow, read the [docs here](https://docs.ray.io/en/latest/serve/advanced-guides/dev-workflow.html#development-workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d789dd",
   "metadata": {},
   "source": [
    "For unit testing and debugging, Ray Serve provides a local testing mode. For more details, see the [docs here](https://docs.ray.io/en/latest/serve/advanced-guides/dev-workflow.html#local-testing-mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807ed413",
   "metadata": {},
   "source": [
    "## 3. Key concepts in Ray Serve\n",
    "\n",
    "Serve is a framework for serving ML applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1545ef03",
   "metadata": {},
   "source": [
    "### Applications\n",
    "\n",
    "Here is a high-level overview of the architecture of a Ray Serve Application.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/serve_architecture.png' width=700/>\n",
    "\n",
    "A Ray Serve cluster is made up of one or more Applications.\n",
    "\n",
    "An Application is composed of one or more Deployments that work together. Key characteristics:\n",
    "- Applications are coarse-grained units of functionality\n",
    "- They can be **independently upgraded** without affecting other applications running on the same cluster\n",
    "- They provide isolation and separate deployment lifecycles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5348a05e",
   "metadata": {},
   "source": [
    "### Deployments\n",
    "\n",
    "A Deployment is the fundamental building block in Ray Serve's architecture.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/deployment.png' width=600/>\n",
    "\n",
    "Deployments enable:\n",
    "- Separation of concerns (e.g., different models, business logic, data transformations)\n",
    "- **Independent scaling**, including autoscaling capabilities\n",
    "- Multiple replicas for handling concurrent requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f465fe7f",
   "metadata": {},
   "source": [
    "### Replicas\n",
    "Each Replica is a worker process (Ray actor) with its own request processing queue. Replicas offer flexible configuration options:\n",
    "\n",
    "- Specify its own hardware and resource requirements (e.g., GPUs)\n",
    "- Specify its own runtime environments (e.g., libraries)\n",
    "- Maintain state (e.g., models)\n",
    "\n",
    "This architecture provides a clean separation of concerns while enabling high scalability and efficient resource utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae1123e",
   "metadata": {},
   "source": [
    "## 4. Model composition with Ray Serve\n",
    "\n",
    "Below is a sample Serve instance that we will build to better understand Ray Serve and its architecture\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-serve-deep-dive/Serve+architecture+-+instance.png\" width=\"800px\" loading=\"lazy\">\n",
    "\n",
    "We can break down the above diagram into the following steps:\n",
    "1. HTTP or GRPC requests come in \n",
    "2. The load balancer routes the request to one of the cluster nodes\n",
    "3. The request is handled by a proxy\n",
    "4. The proxy routes the request to the relevant deployment replica\n",
    "5. The replica processes the request and returns the response\n",
    "6. The proxy returns the response to the client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66afa4c",
   "metadata": {},
   "source": [
    "### 4.1 Building out an ingress deployment\n",
    "\n",
    "Let's first build out the ingress deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9bf543",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class DeploymentA:\n",
    "    def __init__(\n",
    "        self, deployment_b: DeploymentHandle, deployment_c: DeploymentHandle\n",
    "    ) -> None:\n",
    "        self.deployment_b = self.deployment_b\n",
    "        self.deployment_c = self.deployment_c\n",
    "\n",
    "    # __call__ corresponds to post(\"/\") endpoint\n",
    "    async def __call__(self, request: Request):\n",
    "        payload = await request.json()  # parse starlette Request\n",
    "        payload_language = detect(payload[\"text\"])\n",
    "\n",
    "        if payload_language == \"en\":\n",
    "            out = await self.deployment_b.run(payload)\n",
    "            return \"English embedding done\"\n",
    "        elif payload_language == \"de\":\n",
    "            out = await self.deployment_b.run(payload)\n",
    "            return \"German embedding done\"\n",
    "        else:\n",
    "            return \"Not supported language\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8eadc4",
   "metadata": {},
   "source": [
    "### 4.2 Integrating with FastAPI\n",
    "\n",
    "Ray Serve can be integrated with FastAPI to provide:\n",
    "- HTTP routing\n",
    "- Pydantic model validation\n",
    "- OpenAPI documentation\n",
    "\n",
    "To integrate a Deployment with FastAPI, we can use the `@serve.ingress` decorator to designate a FastAPI app as the entrypoint for HTTP requests to our Serve application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12728763",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastapi_app = fastapi.FastAPI()\n",
    "\n",
    "\n",
    "class Payload(BaseModel):\n",
    "    text: str\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "@serve.ingress(fastapi_app)\n",
    "class DeploymentA:\n",
    "    def __init__(self, deployment_b: DeploymentHandle, deployment_c: DeploymentHandle) -> None:\n",
    "        self.deployment_b = deployment_b\n",
    "        self.deployment_c = deployment_c\n",
    "\n",
    "    @fastapi_app.post(\"/predict\")\n",
    "    async def run(self, payload: Payload):\n",
    "        logger = logging.getLogger(\"ray.serve\")\n",
    "        logger.info(f\"{payload=}\")\n",
    "        payload_language = detect(payload.text)\n",
    "        logger.info(f\"Detected language: {payload_language}\")\n",
    "\n",
    "        if payload_language == \"en\":\n",
    "            await self.deployment_b.run.remote(payload.text)\n",
    "            return \"English embedding done\"\n",
    "\n",
    "        elif payload_language == \"de\":\n",
    "            await self.deployment_c.run.remote(payload.text)\n",
    "            return \"German embedding done\"\n",
    "\n",
    "        else:\n",
    "            raise fastapi.HTTPException(\n",
    "                status_code=400,\n",
    "                detail=\"Unsupported language detected. Only English and German are supported.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd466538",
   "metadata": {},
   "source": [
    "### 4.3 Resource specification\n",
    "\n",
    "Then we can build out `DeploymentB` and `DeploymentC`. In this example, the deployments are using different models and different hardware.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcfbc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={\"num_gpus\": 1})\n",
    "class DeploymentB:\n",
    "    def __init__(self) -> None:\n",
    "        self.model = SentenceTransformer(\"intfloat/multilingual-e5-large\", trust_remote_code=True, device=\"cuda\")\n",
    "\n",
    "    def run(self, input: str) -> list[float]:\n",
    "        return self.model.encode(input)\n",
    "\n",
    "\n",
    "@serve.deployment(ray_actor_options={\"num_gpus\": 1})\n",
    "class DeploymentC:\n",
    "    def __init__(self) -> None:\n",
    "        self.model = SentenceTransformer(\"intfloat/multilingual-e5-small\", trust_remote_code=True, device=\"cuda\")\n",
    "\n",
    "    def run(self, input: str) -> list[float]:\n",
    "        return self.model.encode(input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf92e80",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Deployment boundaries allow for independent scaling but introduce latency overhead (serde + data transfer). If both deployments require similar resources, it may be better to fuse them into a single deployment. \n",
    "\n",
    "</div>\n",
    "\n",
    "Continue to build out the application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6278a82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve_app = DeploymentA.bind(\n",
    "    deployment_b=DeploymentB.bind(),\n",
    "    deployment_c=DeploymentC.bind()\n",
    ")\n",
    "serve_app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47db089",
   "metadata": {},
   "source": [
    "#### 4.3.1 Fractional GPU Usage\n",
    "\n",
    "Fractional GPU usage allows for more efficient use of GPU resources by allowing multiple replicas to share a single GPU.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade91c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={\"num_gpus\": 0.25})\n",
    "class DeploymentB:\n",
    "    def __init__(self) -> None:\n",
    "        self.model = SentenceTransformer(\"intfloat/multilingual-e5-large\", trust_remote_code=True, device=\"cuda\")\n",
    "\n",
    "    def run(self, input: str) -> list[float]:\n",
    "        return self.model.encode(input)\n",
    "\n",
    "\n",
    "@serve.deployment(ray_actor_options={\"num_gpus\": 0.25})\n",
    "class DeploymentC:\n",
    "    def __init__(self) -> None:\n",
    "        self.model = SentenceTransformer(\"intfloat/multilingual-e5-small\", trust_remote_code=True, device=\"cuda\")\n",
    "\n",
    "    def run(self, input: str) -> list[float]:\n",
    "        return self.model.encode(input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410717f2",
   "metadata": {},
   "source": [
    "Let's rebuild the application \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0812ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve_app = DeploymentA.bind(\n",
    "    deployment_b=DeploymentB.bind(),\n",
    "    deployment_c=DeploymentC.bind()\n",
    ")\n",
    "serve_app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd3fecd",
   "metadata": {},
   "source": [
    "Finally, let's run the application:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab34cf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve_app_handle = serve.run(serve_app, route_prefix=\"/composed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c76ad9f",
   "metadata": {},
   "source": [
    "We can test the running application via HTTP requests:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643f6a08",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "response = requests.post(\"http://localhost:8000/composed/predict\", json={\"text\": \"hello there\"})\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453469c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\"http://localhost:8000/composed/predict\", json={\"text\": \"Ein, zwei, drei, vier\"})\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d022257f",
   "metadata": {},
   "source": [
    "### 4.4 Activity: Extend the model composition to other languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91db1585",
   "metadata": {},
   "source": [
    "Here is what you need to do:\n",
    "\n",
    "1. Create a new model deployment DeploymentD to handle french text\n",
    "    1. DeploymentD should use the multilingual-e5-small-model and require 0.25 of a GPU\n",
    "2. Define a new `DeploymentAV2` which forwards french text to `DeploymentD`\n",
    "3. Run the application (use route_prefix=\"/new\")\n",
    "4. Test it by sending a request with french text (e.g. \"Quelle est la capitale de la France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cbd7e2",
   "metadata": {
    "region_name": "markdown"
   },
   "source": [
    "<details>\n",
    "<summary>Click to view hints</summary>\n",
    "\n",
    "```python\n",
    "new_fastapi_app = fastapi.FastAPI()\n",
    "\n",
    "\n",
    "# Hint: Create a new model deployment DeploymentD\n",
    "@serve.deployment(...)\n",
    "class DeploymentD:\n",
    "    def __init__(self):\n",
    "        self.model = ...\n",
    "\n",
    "    def run(self, input: str) -> list[float]:\n",
    "        ...\n",
    "\n",
    "# Hint: Define a new `DeploymentAV2` which forwards french text to `DeploymentD`\n",
    "@serve.deployment(...)\n",
    "@serve.ingress(new_fastapi_app)\n",
    "class DeploymentAV2:\n",
    "    def __init__(self, ...):\n",
    "        ...\n",
    "\n",
    "    @new_fastapi_app.post(\"/predict\")\n",
    "    async def run(self, payload: Payload):\n",
    "        ...\n",
    "\n",
    "# Hint: Run the application\n",
    "serve_app = DeploymentAV2.bind(...)\n",
    "serve.run(serve_app, route_prefix=\"/new\")\n",
    "\n",
    "# Hint: Test the application with french text\n",
    "requests.post(\"http://localhost:8000/new/predict\", json=...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f9485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e4c819",
   "metadata": {
    "lines_to_next_cell": 2,
    "region_name": "markdown"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<details>\n",
    "<summary>Click to view the solution</summary>\n",
    "\n",
    "```python\n",
    "new_fastapi_app = fastapi.FastAPI()\n",
    "\n",
    "# Create a new model deployment DeploymentD\n",
    "@serve.deployment(ray_actor_options={\"num_gpus\": 0.5})\n",
    "class DeploymentD:\n",
    "    def __init__(self) -> None:\n",
    "        self.model = SentenceTransformer(\"intfloat/multilingual-e5-small\", trust_remote_code=True, device=\"cuda\")\n",
    "\n",
    "    def run(self, input: str) -> list[float]:\n",
    "        return self.model.encode(input)\n",
    "\n",
    "# Define a new DeploymentAV2 so it forwards french text to DeploymentD\n",
    "@serve.deployment\n",
    "@serve.ingress(new_fastapi_app)\n",
    "class DeploymentAV2:\n",
    "    def __init__(self, deployment_b: DeploymentHandle, deployment_c: DeploymentHandle, deployment_d: DeploymentHandle) -> None:\n",
    "        self.deployment_b = deployment_b\n",
    "        self.deployment_c = deployment_c\n",
    "        self.deployment_d = deployment_d\n",
    "\n",
    "    @new_fastapi_app.post(\"/predict\")\n",
    "    async def run(self, payload: Payload):\n",
    "        logger = logging.getLogger(\"ray.serve\")\n",
    "        logger.info(f\"{payload=}\")\n",
    "        payload_language = detect(payload.text)\n",
    "        \n",
    "        if payload_language == \"en\":\n",
    "            await self.deployment_b.run.remote(payload.text)\n",
    "            return \"English embedding done\"\n",
    "\n",
    "        elif payload_language == \"de\":\n",
    "            await self.deployment_c.run.remote(payload.text)\n",
    "            return \"German embedding done\"\n",
    "\n",
    "        elif payload_language == \"fr\":\n",
    "            await self.deployment_d.run.remote(payload.text)\n",
    "            return \"French embedding done\"\n",
    "\n",
    "# Run the application\n",
    "serve_app = DeploymentAV2.bind(\n",
    "    deployment_b=DeploymentB.bind(),\n",
    "    deployment_c=DeploymentC.bind(),\n",
    "    deployment_d=DeploymentD.bind(),\n",
    ")\n",
    "serve.run(serve_app, route_prefix=\"/new\")\n",
    "\n",
    "# Test the application with french text\n",
    "print(requests.post(\"http://localhost:8000/new/predict\", json={\"text\": \"Quelle est la capitale de la France ?\"}).json())\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bce0e31",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# cleanup \n",
    "!serve shutdown -y"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "region_name,-all",
   "main_language": "python",
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
