{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b918fb4d",
   "metadata": {},
   "source": [
    "# Architecture \n",
    "\n",
    "This notebook provides an overview of the architecture of Ray Serve\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Here is the roadmap for this notebook: </b>\n",
    "<ol>\n",
    "    <li>Components of Ray Serve</li>\n",
    "    <li>Lifetime of a Request</li>\n",
    "    <li>Request Routing Process</li>\n",
    "    <li>Fault Tolerance</li>\n",
    "    <li>Load Shedding and Backpressure</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90966f1",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a1d232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import backoff\n",
    "import time\n",
    "import requests\n",
    "import threading\n",
    "\n",
    "import ray\n",
    "import ray.util.state\n",
    "from ray import serve\n",
    "from starlette.requests import Request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0500b05c",
   "metadata": {},
   "source": [
    "## 1. Components of Ray Serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d043c6",
   "metadata": {},
   "source": [
    "### Sample Serve Instance\n",
    "\n",
    "Below is a sample Serve instance that we will use to illustrate the architecture.\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-serve/sample_sercve_instance.png\" width=\"800\">\n",
    "\n",
    "We can break down the above diagram into the following steps:\n",
    "1. HTTP or GRPC requests come in \n",
    "2. The load balancer routes the request to one of the cluster nodes\n",
    "3. The request is handled by a proxy\n",
    "4. The proxy routes the request to the relevant deployment replica\n",
    "5. The replica processes the request and returns the response\n",
    "6. The proxy returns the response to the client\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdea7fa",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "Serve runs on Ray and utilizes Ray actors.\n",
    "\n",
    "There are three kinds of actors that are created to make up a Serve instance:\n",
    "\n",
    " * **Controller Actor**\n",
    "    * Global actor unique to each Serve instance\n",
    "    * Manages the control plane\n",
    "    * Handles creating, updating, and destroying other actors\n",
    "    * Runs the Serve Autoscaler\n",
    "    * Processes Serve API calls for deployment management\n",
    "\n",
    "  * **Proxy Actor**\n",
    "    * One HTTP proxy actor by default on head node\n",
    "    * Runs a Uvicorn HTTP server\n",
    "    * Accepts incoming requests\n",
    "    * Forwards requests to replicas\n",
    "    * Returns responses when completed\n",
    "    * Can be scaled across cluster nodes using `proxy_location` setting\n",
    "\n",
    "  * **Replica Actors**\n",
    "    * Execute the actual request processing code\n",
    "    * Can host ML models or other business logic\n",
    "    * Process individual requests from the proxy\n",
    "    * Support dynamic request batching via `@serve.batch`\n",
    "\n",
    "Here is a diagram of the Serve architecture:\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/architecture-2.0.svg\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a8379c",
   "metadata": {},
   "source": [
    "The system tab of the Serve dashboard shows the controller and proxy actors.\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-serve-deep-dive/serve-dashboard-system-tab.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a454aef0",
   "metadata": {},
   "source": [
    "The applications tab shows the applications and their deployments.\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-serve-deep-dive/serve-dashboard-applications-tab.png\" width=\"600\">\n",
    "\n",
    "And clicking on a deployment shows the replicas and their status."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e40ec1",
   "metadata": {},
   "source": [
    "## 2. Lifetime of a request\n",
    "\n",
    "When an HTTP or gRPC request is sent to the corresponding HTTP or gRPC proxy, the following happens:\n",
    "\n",
    "Requests flow through the system as follows:\n",
    "1. Request is received and parsed\n",
    "2. Deployment is looked up based on HTTP URL path\n",
    "3. Request enters deployment queue\n",
    "4. Available replica is identified for processing\n",
    "5. Request is either immediately processed by replica or stored in its queue\n",
    "6. Response is returned through proxy\n",
    "\n",
    "Queueing in Ray Serve occurs in two places:\n",
    "- On the caller side (DeploymentHandle or Proxy)\n",
    "- On the receiver side (Replica)\n",
    "\n",
    "Each replica maintains a queue of requests and executes requests one at a time, possibly using asyncio to process them concurrently.\n",
    "\n",
    "When making a request via a `DeploymentHandle` (e.g. instead of HTTP), the request is placed on a queue in the `DeploymentHandle`, and we skip to step 3 above.\n",
    "\n",
    "Here is a diagram of the request lifecycle:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/geotab/request_lifecycle.jpg\" width=\"800\">\n",
    "\n",
    "<div class=\"alert alert-block alert-secondary\">\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Click to view implementation details</summary>\n",
    "\n",
    "- [`HTTPProxy.__call__` will delegate to `HTTPProxy.proxy_request`](https://github.com/ray-project/ray/blob/78bb1f0fe5dbc3d15f94953593c3751c17e2097c/python/ray/serve/_private/proxy.py#L854)\n",
    "- [`HTTPProxy.proxy_request` will invoke a response handler method](https://github.com/ray-project/ray/blob/78bb1f0fe5dbc3d15f94953593c3751c17e2097c/python/ray/serve/_private/proxy.py#L429)\n",
    "- [The response handler will delegate to `HTTPProxy.match_route` if an HTTP request is received](https://github.com/ray-project/ray/blob/78bb1f0fe5dbc3d15f94953593c3751c17e2097c/python/ray/serve/_private/proxy.py#L351)\n",
    "- [The response handler will then call `HTTPProxy.send_request_to_replica`](https://github.com/ray-project/ray/blob/78bb1f0fe5dbc3d15f94953593c3751c17e2097c/python/ray/serve/_private/proxy.py#L401C1-L402C1)\n",
    "- [`HTTPProxy.send_request_to_replica` will call the `DeploymentHandle.remote` method](https://github.com/ray-project/ray/blob/78bb1f0fe5dbc3d15f94953593c3751c17e2097c/python/ray/serve/_private/proxy.py#L966)\n",
    "- [`DeploymentHandle.remote` will call the `AsyncioRouter.assign_request` method](https://github.com/ray-project/ray/blob/master/python/ray/serve/handle.py#L206)\n",
    "- [`AsyncioRouter.assign_request` will call `AsyncioRouter.schedule_and_send_request` method](https://github.com/ray-project/ray/blob/master/python/ray/serve/_private/router.py#L613)\n",
    "- [`AsyncioRouter.schedule_and_send_request` will first call `ReplicaScheduler.choose_replica_for_request` to choose a replica for the request](https://github.com/ray-project/ray/blob/master/python/ray/serve/_private/router.py#L574)\n",
    "    - [`ReplicaScheduler.choose_replica_for_request` will call `ReplicaScheduler.choose_replicas` to try to submit a scheduling task to find a replica for the request](https://github.com/ray-project/ray/blob/master/python/ray/serve/_private/replica_scheduler/pow_2_scheduler.py#L811)\n",
    "- [`AsyncioRouter.schedule_and_send_request` will then call `ReplicaWrapper.send_request`](https://github.com/ray-project/ray/blob/master/python/ray/serve/_private/router.py#L540C46-L540C58)\n",
    "- [`ReplicaWrapper.send_request` will call `ReplicaWrapper.send_request_python`](https://github.com/ray-project/ray/blob/master/python/ray/serve/_private/replica_scheduler/replica_wrapper.py#L188)\n",
    "- [`ReplicaWrapper.send_request_python` will then execute `Replica.handle_request` Actor tasks and get back the replica result](https://github.com/ray-project/ray/blob/master/python/ray/serve/_private/replica_scheduler/replica_wrapper.py#L92)\n",
    "- the ReplicaResult will be sent back to the response handler\n",
    "- the response handler will then send the replica result back to the client\n",
    "\n",
    "</details>\n",
    "\n",
    "If you have tracing enabled, here are the three main spans as seen in the traces:\n",
    "\n",
    "- `proxy_http_request` (`HTTPProxy.send_request_to_replica` scope)\n",
    "    - `proxy_route_to_replica` (`AsyncioRouter.assign_request` scope)\n",
    "        - `replica_handle_request` (`Replica.handle_request or Replica.handle_request_streaming` scope)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24063dac",
   "metadata": {},
   "source": [
    "## 3. Request Routing Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9ad35d",
   "metadata": {},
   "source": [
    "### Power of Two Choices Replica Routing Algorithm\n",
    "* Randomly samples two replicas for each request\n",
    "* Selects replica with shorter queue length if below `max_ongoing_requests`\n",
    "* Maintains strict FIFO ordering of requests\n",
    "\n",
    "see the below diagram which visualizes the routing/scheduling process:\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-serve/power_of_two_choices.png\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de0c034",
   "metadata": {},
   "source": [
    "### Routing Priority\n",
    "1. Local node replicas (if enabled - default True for proxy actors and False for replicas)\n",
    "2. Same availability zone replicas (if enabled - default True)\n",
    "3. Any available replica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d71b211",
   "metadata": {},
   "source": [
    "#### Model Multiplexing (if enabled)\n",
    "* First attempts to route to replicas with requested model loaded\n",
    "* Falls back to replicas with fewest loaded models\n",
    "* Finally considers all replicas after configurable timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7256fc05",
   "metadata": {},
   "source": [
    "### Fault Handling\n",
    "* Implements exponential backoff on failures\n",
    "* Removes dead replicas from selection pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3747f3",
   "metadata": {},
   "source": [
    "### Implementation Details\n",
    "* Caches replica queue lengths with configurable TTL\n",
    "* Uses active probing with deadlines for queue length updates\n",
    "* Limits concurrent routing tasks to `(2 * num_replicas)`\n",
    "* Single routing task handles multiple requests to maintain FIFO\n",
    "\n",
    "This scheduler balances efficient request distribution with predictable ordering, while handling replica failures and model multiplexing requirements.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "To view implementation details, check out the `PowerOfTwoChoicesRequestRouter` [class](https://github.com/ray-project/ray/blob/ray-2.50.0/python/ray/serve/_private/request_router/pow_2_router.py)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c4fd09",
   "metadata": {},
   "source": [
    "### New (alpha): Custom request routing\n",
    "\n",
    "Ray Serve now lets you plug in a custom <code>RequestRouter</code> to decide which replica handles a request (beyond the default power-of-two-choices).\n",
    "\n",
    "- <b>What</b>: Implement your own policy (e.g., random, throughput-aware, KV-cache-aware), optionally using <code>FIFOMixin</code>, <code>LocalityMixin</code>, and <code>MultiplexMixin</code>.\n",
    "- <b>How</b>: Set in <code>@serve.deployment</code> via <code>request_router_config=RequestRouterConfig(request_router_class=\"{module}:{Class}\")</code>. Routers can read per-replica stats you expose via <code>record_routing_stats</code>.\n",
    "- <b>Note</b>: Alpha API; configure at deploy time (not swappable on existing handles).\n",
    "\n",
    "Read more in the [custom request routing docs](https://docs.ray.io/en/latest/serve/advanced-guides/custom-request-router.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c766f5",
   "metadata": {},
   "source": [
    "## 4. Fault tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ba32be",
   "metadata": {},
   "source": [
    "### Application errors\n",
    "- Application-level errors like exceptions in your model evaluation code are caught and wrapped.\n",
    "- A 500 status code will be returned with the traceback information. \n",
    "- The replica will be able to continue to handle requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4156f825",
   "metadata": {},
   "source": [
    "### Machine errors and faults\n",
    "Machine errors and faults are handled by Ray Serve as follows:\n",
    "\n",
    "- When **Replica** Actors fail, the **Controller** Actor replaces them with new ones.\n",
    "- When the **Proxy** Actor fails, the **Controller** Actor restarts it.\n",
    "- When the **Controller** Actor fails, Ray restarts it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a58b8c5",
   "metadata": {},
   "source": [
    "#### Transient Data Loss\n",
    "- When a machine hosting any of the actors crashes, those actors are automatically restarted on another available machine.\n",
    "- Transient data in the router and the replica (like network connections and internal request queues) will be lost for this kind of failure. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Best practice** implement client side retries and backoff to handle transient data loss.\n",
    "\n",
    "</div>\n",
    "\n",
    "Let's create a deployment that simulates a spot instance interruption.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332a9d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class SpotInstanceReplica:\n",
    "    def __init__(self):\n",
    "        self.state = \"my_model\"\n",
    "    async def __call__(self, request: Request):\n",
    "        await asyncio.sleep(60) # Simulate long computation\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699799c9",
   "metadata": {},
   "source": [
    "We run the application\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd45bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = SpotInstanceReplica.bind()\n",
    "app_handle = serve.run(app, name=\"spot-instance\", blocking=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef43de18",
   "metadata": {},
   "source": [
    "We define some helper functions and class to:\n",
    "- make a request from a thread\n",
    "- simulate an instance preemption by killing the replica actor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaafb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_request():\n",
    "    response = requests.get(\"http://localhost:8000/\")\n",
    "    return response\n",
    "\n",
    "\n",
    "class RequestThread(threading.Thread):\n",
    "    def run(self):\n",
    "        self.result = make_request()\n",
    "\n",
    "\n",
    "def simulate_instance_preemption():\n",
    "    actor_name = ray.util.state.list_actors(filters=[(\"state\", \"=\", \"ALIVE\"), (\"class_name\", \"=\", 'ServeReplica:spot-instance:SpotInstanceReplica'),])[0][\"name\"]\n",
    "    actor_handle = ray.get_actor(name=actor_name, namespace=\"serve\") \n",
    "    ray.kill(actor_handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c015acbd",
   "metadata": {},
   "source": [
    "We now launch two threads:\n",
    "- one to make a request\n",
    "- one to simulate an instance preemption\n",
    "\n",
    "Given we don't have client-side retries, the request will fail immediately after the replica is killed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3b76db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create threads\n",
    "t1 = RequestThread()\n",
    "t1.start()\n",
    "time.sleep(20)\n",
    "t2 = threading.Thread(target=simulate_instance_preemption)\n",
    "t2.start()\n",
    "\n",
    "# Wait for threads to complete\n",
    "t1.join()\n",
    "t2.join()\n",
    "\n",
    "# Get the response object\n",
    "t1_result = t1.result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b06d99",
   "metadata": {},
   "source": [
    "Serve will replace the replica but all transient requests will be lost.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ad1de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_result.status_code, t1_result.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229f6f85",
   "metadata": {},
   "source": [
    "To resolve this, it is best to implement client-side retries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30ee689",
   "metadata": {},
   "outputs": [],
   "source": [
    "@backoff.on_exception(backoff.expo, requests.exceptions.RequestException, max_tries=5)\n",
    "def make_request_robust():\n",
    "    response = requests.get(\"http://localhost:8000/\")\n",
    "    response.raise_for_status()\n",
    "    return response\n",
    "\n",
    "\n",
    "class RequestThreadWithRetries(threading.Thread):\n",
    "    def run(self):\n",
    "        self.result = make_request_robust()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedc1bd0",
   "metadata": {},
   "source": [
    "We now launch the same two threads:\n",
    "- one to make a request\n",
    "- one to simulate an instance preemption\n",
    "\n",
    "Given we have client-side retries, the request will be retried and eventually succeed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8359f175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create threads\n",
    "t1 = RequestThreadWithRetries()\n",
    "t1.start()\n",
    "time.sleep(20)\n",
    "t2 = threading.Thread(target=simulate_instance_preemption)\n",
    "t2.start()\n",
    "\n",
    "# Wait for threads to complete\n",
    "t1.join()\n",
    "t2.join()\n",
    "\n",
    "# Get the response object\n",
    "t1_result = t1.result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ed7c06",
   "metadata": {},
   "source": [
    "We verify the status code is 200 and the text is \"my_model\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c4b3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_result.status_code, t1_result.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eac779",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Best practice** If client retries are not an option, you will need to introduce request persistence (e.g. using a queueing system like kafka, sqs, etc.)\n",
    "\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd95c32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc4219e",
   "metadata": {},
   "source": [
    "### Ray Head Node Failure\n",
    "\n",
    "If Ray's global control service (GCS) fails, Ray Serve will continue serving traffic. However, no cluster autoscaling will be triggered until the GCS is restored.\n",
    "\n",
    "For more details on fault-tolerance, check out the [End-to-End Fault Tolerance](https://docs.ray.io/en/latest/serve/production-guide/fault-tolerance.html) documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321b9cc5",
   "metadata": {},
   "source": [
    "## 5. Load shedding and backpressure\n",
    "\n",
    "Here is a reminder of **request handling**:\n",
    "- When a request is sent to a cluster, it is first received by the **Serve proxy**.\n",
    "- The proxy forwards the request to a **replica** for handling using a `DeploymentHandle`.\n",
    "- Replicas can handle a configurable number of requests at a time, set using the **`max_ongoing_requests`** option.\n",
    "- If all replicas are busy, the request is queued in the `DeploymentHandle` until a replica becomes available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d04464",
   "metadata": {},
   "source": [
    "### Implementing Load Shedding\n",
    "\n",
    "Under heavy load, `DeploymentHandle` queues can grow, causing high tail latency and excessive load on the system.\n",
    "\n",
    "To avoid instability, it is often preferable to intentionally reject some requests to prevent queues from growing indefinitely. This technique is called **\"load shedding,\"** allowing the system to handle excessive load gracefully without spiking tail latencies or overloading components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e733d1",
   "metadata": {},
   "source": [
    "#### Configuration\n",
    "\n",
    "Configure load shedding for your Serve deployments using the **`max_queued_requests`** parameter in the `@serve.deployment` decorator.\n",
    "\n",
    "- This parameter controls the maximum number of requests that each `DeploymentHandle`, including the Serve proxy, will queue.\n",
    "- Once the limit is reached, any new requests will immediately raise a **`BackPressureError`**.\n",
    "- HTTP requests will return a **503 status code** (service unavailable).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd01c9b",
   "metadata": {},
   "source": [
    "#### Example\n",
    "The following example defines a deployment that emulates slow request handling and has `max_ongoing_requests` and `max_queued_requests` configured.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ce1120",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(\n",
    "    # Each replica will be allowed to handle 2 requests at a time.\n",
    "    max_ongoing_requests=2,\n",
    "    # Each caller will be allowed to queue up to 2 requests at a time.\n",
    "    # (beyond those that are sent to replicas).\n",
    "    max_queued_requests=2,\n",
    ")\n",
    "class SlowDeployment:\n",
    "    def __call__(self, request: Request) -> str:\n",
    "        # Emulate a long-running request, such as ML inference.\n",
    "        time.sleep(2)\n",
    "        return \"Hello!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bc615a",
   "metadata": {},
   "source": [
    "We define a requester actor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ec23cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class Requester:\n",
    "    async def do_request(self) -> str:\n",
    "        handle = serve.get_app_handle(\"slow-deployment\")\n",
    "        return await handle.remote({\"sample\": \"input\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbae0dc",
   "metadata": {},
   "source": [
    "Let's schedule the actor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19243df",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Requester.remote()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a4ae6e",
   "metadata": {},
   "source": [
    "Let's run the serve application\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25fa4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.run(SlowDeployment.bind(), name=\"slow-deployment\", blocking=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923a580e",
   "metadata": {},
   "source": [
    "Observe this sequence of requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef6e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send 2 requests.\n",
    "# These will be sent to the replica. Requests take two seconds to execute.\n",
    "first_refs = [r.do_request.remote() for _ in range(2)]\n",
    "available, pending = ray.wait(first_refs, timeout=1)\n",
    "assert len(pending) == 2\n",
    "assert len(available) == 0\n",
    "\n",
    "# Send another 2 requests\n",
    "# These will get queued in the proxy because we have exceeed max_ongoing_requests\n",
    "queued_refs = [r.do_request.remote() for _ in range(2)]\n",
    "available, pending = ray.wait(queued_refs, timeout=0.1)\n",
    "assert len(pending) == 2\n",
    "\n",
    "# Send another 2 requests.\n",
    "# These should be **rejected** immediately because the replica and the proxy queue are already full.\n",
    "# The replica has 2 ongoing, and proxy has 2 queued = max_queued_requests\n",
    "try:\n",
    "    ray.get([r.do_request.remote() for _ in range(2)], timeout=5)\n",
    "except ray.serve.exceptions.BackPressureError as e:\n",
    "    print(\"Received expected BackPressureError as expected\")\n",
    "\n",
    "# The initial requests will finish successfully.\n",
    "for ref in first_refs:\n",
    "    print(f\"Request finished with status code {ray.get(ref)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd9c675",
   "metadata": {},
   "source": [
    "Run clean up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17636e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "!serve shutdown -y"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "split_at_heading": true
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
