{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a344e32",
   "metadata": {},
   "source": [
    "# Developing Serve Applications\n",
    "\n",
    "This notebook covers best practices for designing, testing and developing Ray Serve applications.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b>Here is the roadmap for this notebook:</b>\n",
    "\n",
    "<ol>\n",
    "    <li>Structuring Serve Code</li>\n",
    "    <li>Testing Serve Code</li>\n",
    "    <li>Patterns of Integrating with FastAPI</li>\n",
    "    <li>Debugging Serve Applications</li>\n",
    "    <li>Configuration in Ray Serve</li>\n",
    "    <li>Running Serve Locally</li>\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3e61fc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "import requests\n",
    "from fastapi import FastAPI, APIRouter, Depends\n",
    "from fastapi import HTTPException\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel\n",
    "from ray import serve\n",
    "from ray.serve import Application\n",
    "from ray.serve.handle import DeploymentHandle\n",
    "from starlette.requests import Request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed36c08",
   "metadata": {},
   "source": [
    "## 1. Structuring Serve Code\n",
    "\n",
    "Structuring your Ray Serve applications effectively requires separating business logic from deployment concerns. This section provides guidance on how to organize your code for better testability and maintainability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa8f817",
   "metadata": {},
   "source": [
    "### Code Structure for Testability\n",
    "\n",
    "The key to effective testing with Ray Serve is to separate your business logic from the deployment wrapper. Here's the recommended structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ce3faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel:\n",
    "    \"\"\"Core business logic - easily unit testable\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str):\n",
    "        self.model_path = model_path\n",
    "        self.model = self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Model loading logic\"\"\"\n",
    "    \n",
    "    def predict(self, input_data: dict) -> dict:\n",
    "        \"\"\"Core prediction logic\"\"\"    \n",
    "\n",
    "\n",
    "@serve.deployment(\n",
    "    ray_actor_options={\"num_cpus\": 1},\n",
    "    max_ongoing_requests=5\n",
    ")\n",
    "class MyModelDeployment(MyModel):\n",
    "    \"\"\"Ray Serve deployment wrapper\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str):\n",
    "        super().__init__(model_path)\n",
    "    \n",
    "    async def __call__(self, request: Request) -> dict:\n",
    "        \"\"\"HTTP endpoint handler\"\"\"\n",
    "        input_data = await request.json()\n",
    "        \n",
    "        # Add basic validation\n",
    "        if \"test\" not in input_data:\n",
    "            raise HTTPException(status_code=400, detail=\"Missing required field 'test'\")\n",
    "        \n",
    "        return self.predict(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7eb435",
   "metadata": {},
   "source": [
    "#### Alternative Patterns for Creating Deployments\n",
    "\n",
    "Instead of using inheritance, you can use alternative patterns to create deployments from your business logic classes:\n",
    "\n",
    "**Pattern 1: Using an `as_deployment()` class method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16218ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel:\n",
    "    \"\"\"Core business logic with deployment factory method\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str):\n",
    "        self.model_path = model_path\n",
    "        self.model = self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Model loading logic\"\"\"\n",
    "    \n",
    "    def predict(self, input_data: dict) -> dict:\n",
    "        \"\"\"Core prediction logic\"\"\"\n",
    "    \n",
    "    @classmethod\n",
    "    def as_deployment(cls, **deployment_options):\n",
    "        \"\"\"Factory method to create a Ray Serve deployment\"\"\"\n",
    "        default_options = {\n",
    "            \"ray_actor_options\": {\"num_cpus\": 1},\n",
    "            \"max_ongoing_requests\": 5\n",
    "        }\n",
    "        default_options.update(deployment_options)\n",
    "        return serve.deployment(**default_options)(cls)\n",
    "\n",
    "# Usage\n",
    "app = MyModel.as_deployment(name=\"my_model\").bind(\"path/to/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2facb03",
   "metadata": {},
   "source": [
    "**Pattern 2: Using a factory function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca14d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_deployment(cls: type, **custom_options) -> serve.Deployment:\n",
    "    \"\"\"Factory function to create deployments with custom options\"\"\"\n",
    "    default_options = {\n",
    "        \"ray_actor_options\": {\"num_cpus\": 1},\n",
    "        \"max_ongoing_requests\": 5\n",
    "    }\n",
    "    default_options.update(custom_options)\n",
    "    return serve.deployment(**default_options)(cls)\n",
    "\n",
    "class MyModel:\n",
    "    \"\"\"Core business logic\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str):\n",
    "        self.model_path = model_path\n",
    "        self.model = self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Model loading logic\"\"\"\n",
    "    \n",
    "    def predict(self, input_data: dict) -> dict:\n",
    "        \"\"\"Core prediction logic\"\"\"\n",
    "\n",
    "# Usage\n",
    "MyModelDeployment = make_deployment(MyModel, name=\"my_model\", num_replicas=2)\n",
    "app = MyModelDeployment.bind(\"path/to/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71d002d",
   "metadata": {},
   "source": [
    "**Comparison of Patterns:**\n",
    "\n",
    "- **Inheritance Pattern**: Good when you need to add deployment-specific logic \n",
    "- **`as_deployment()` Method**: Keeps deployment configuration close to the business logic class\n",
    "- **Factory Function**: Provides centralized deployment creation logic, useful when applying consistent configurations across multiple classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee6d780",
   "metadata": {},
   "source": [
    "## 2. Testing Serve Code\n",
    "\n",
    "Testing Ray Serve applications requires a structured approach with different testing strategies for different layers of your application. This section covers unit testing, integration testing with deployment handles, HTTP integration testing, and testing deployment composition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35af7830",
   "metadata": {},
   "source": [
    "### Unit Testing Business Logic\n",
    "\n",
    "With this structure, you can write comprehensive unit tests for your business logic without any Ray Serve dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91007d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestMyModel:\n",
    "    \"\"\"Unit tests for core business logic\"\"\"\n",
    "    \n",
    "    def test_model_initialization(self):\n",
    "        \"\"\"Test model loading and initialization\"\"\"\n",
    "        model = MyModel(\"test_model_path\")\n",
    "        assert model.model_path == \"test_model_path\"\n",
    "        # Add assertions for model state\n",
    "    \n",
    "    def test_end_to_end_prediction(self):\n",
    "        \"\"\"Test complete prediction pipeline\"\"\"\n",
    "        model = MyModel(\"test_model_path\")            \n",
    "        input_data = {\"test\": \"input\"}\n",
    "        result = model.predict(input_data)        \n",
    "        assert result == {\"final\": \"result\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d393a37f",
   "metadata": {},
   "source": [
    "### Integration Testing with DeploymentHandle\n",
    "\n",
    "For integration testing, use Ray Serve's DeploymentHandle to test your deployment without HTTP overhead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbea396",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestMyModelDeployment:\n",
    "    \"\"\"Integration tests using DeploymentHandle\"\"\"\n",
    "    \n",
    "    @pytest.fixture\n",
    "    def deployment_handle(self):\n",
    "        \"\"\"Setup deployment for testing\"\"\"\n",
    "        app = MyModelDeployment.bind(\"test_model_path\")\n",
    "        handle = serve.run(app, name=\"test_model\", blocking=False)\n",
    "        yield handle\n",
    "        serve.shutdown()\n",
    "    \n",
    "    def test_deployment_prediction(self, deployment_handle):\n",
    "        \"\"\"Test prediction through deployment handle\"\"\"\n",
    "        input_data = {\"input\": \"data\"}\n",
    "        result = deployment_handle.predict.remote(input_data).result()\n",
    "        assert result == {\"prediction\": \"test_result\", \"confidence\": 0.95}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e891a9",
   "metadata": {},
   "source": [
    "### Integration Testing with HTTP Requests\n",
    "\n",
    "For full HTTP integration testing, use a library like `requests` after starting the Serve application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95b29a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestMyModelHTTP:\n",
    "    \"\"\"HTTP integration tests\"\"\"\n",
    "    \n",
    "    @pytest.fixture\n",
    "    def serve_app(self):\n",
    "        \"\"\"Setup HTTP server for testing\"\"\"\n",
    "        app = MyModelDeployment.bind(\"test_model_path\")\n",
    "        serve.run(app, name=\"test_model\", blocking=False)\n",
    "        yield\n",
    "        serve.shutdown()\n",
    "    \n",
    "    def test_http_prediction_endpoint(self, serve_app):\n",
    "        \"\"\"Test HTTP prediction endpoint\"\"\"\n",
    "        input_data = {\"input\": \"data\"}\n",
    "        response = requests.post(\n",
    "            \"http://localhost:8000/\",\n",
    "            json=input_data,\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        assert response.status_code == 200\n",
    "        result = response.json()\n",
    "        # Add assertions for result\n",
    "    \n",
    "    def test_http_error_handling(self, serve_app):\n",
    "        \"\"\"Test HTTP error handling\"\"\"\n",
    "        invalid_data = {\"invalid\": \"data\"}\n",
    "        response = requests.post(\n",
    "            \"http://localhost:8000/\",\n",
    "            json=invalid_data,\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        # Test appropriate error response\n",
    "        assert response.status_code in [400, 422, 500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf240bf",
   "metadata": {},
   "source": [
    "### Advanced: Testing Composition of Deployments\n",
    "\n",
    "When testing applications with multiple deployments that work together, you need to test both individual components and their interactions. Here's how to structure tests for deployment composition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5eac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Multi-deployment application\n",
    "class TextPreprocessor:\n",
    "    \"\"\"Text preprocessing logic\"\"\"\n",
    "    \n",
    "    def preprocess(self, text: str) -> dict:\n",
    "        \"\"\"Clean and tokenize text\"\"\"\n",
    "        cleaned_text = text.strip().lower()\n",
    "        tokens = cleaned_text.split()\n",
    "        return {\"tokens\": tokens, \"length\": len(tokens)}\n",
    "\n",
    "class TextEmbedder:\n",
    "    \"\"\"Text embedding logic\"\"\"\n",
    "    \n",
    "    def embed(self, tokens: dict) -> list:\n",
    "        \"\"\"Convert tokens to embeddings\"\"\"\n",
    "        # Simulate embedding generation\n",
    "        return [0.1] * tokens[\"length\"]\n",
    "\n",
    "class TextClassifier:\n",
    "    \"\"\"Text classification logic\"\"\"\n",
    "    \n",
    "    def classify(self, embeddings: list) -> dict:\n",
    "        \"\"\"Classify based on embeddings\"\"\"\n",
    "        # Simulate classification\n",
    "        return {\"label\": \"positive\", \"confidence\": 0.85}\n",
    "\n",
    "@serve.deployment\n",
    "class TextPreprocessorDeployment(TextPreprocessor):\n",
    "    \"\"\"Deployment for text preprocessing\"\"\"\n",
    "\n",
    "@serve.deployment\n",
    "class TextEmbedderDeployment(TextEmbedder):\n",
    "    \"\"\"Deployment for text embedding\"\"\"\n",
    "\n",
    "@serve.deployment\n",
    "class TextClassifierDeployment(TextClassifier):\n",
    "    \"\"\"Deployment for text classification\"\"\"\n",
    "\n",
    "@serve.deployment\n",
    "class TextPipelineDeployment:\n",
    "    \"\"\"Composed pipeline deployment\"\"\"\n",
    "    \n",
    "    def __init__(self, preprocessor: DeploymentHandle, embedder: DeploymentHandle, classifier: DeploymentHandle):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.embedder = embedder\n",
    "        self.classifier = classifier\n",
    "    \n",
    "    async def __call__(self, request: Request) -> dict:\n",
    "        data = await request.json()\n",
    "        self.run(data)\n",
    "\n",
    "    async def run(self, data: dict) -> dict:\n",
    "        text = data[\"text\"]\n",
    "        \n",
    "        # Step 1: Preprocess\n",
    "        preprocessed = await self.preprocessor.preprocess.remote(text)\n",
    "        \n",
    "        # Step 2: Embed\n",
    "        embeddings = await self.embedder.embed.remote(preprocessed)\n",
    "        \n",
    "        # Step 3: Classify\n",
    "        result = await self.classifier.classify.remote({\"embeddings\": embeddings})\n",
    "        \n",
    "        return {\n",
    "            \"preprocessed\": preprocessed,\n",
    "            \"embeddings\": embeddings,\n",
    "            \"classification\": result\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035e32f7",
   "metadata": {},
   "source": [
    "#### Integration Testing Deployment Composition\n",
    "\n",
    "Test how deployments work together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd57efdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestTextPipelineComposition:\n",
    "    \"\"\"Integration tests for deployment composition\"\"\"\n",
    "    \n",
    "    @pytest.fixture\n",
    "    def pipeline_handle(self):\n",
    "        \"\"\"Setup composed pipeline for testing\"\"\"\n",
    "        # Create individual deployments\n",
    "        preprocessor_app = TextPreprocessorDeployment.bind()\n",
    "        embedder_app = TextEmbedderDeployment.bind()\n",
    "        classifier_app = TextClassifierDeployment.bind()\n",
    "        \n",
    "        # Create composed pipeline\n",
    "        pipeline_app = TextPipelineDeployment.bind(\n",
    "            preprocessor=preprocessor_app,\n",
    "            embedder=embedder_app,\n",
    "            classifier=classifier_app\n",
    "        )\n",
    "        \n",
    "        handle = serve.run(pipeline_app, name=\"text_pipeline\", blocking=False)\n",
    "        yield handle\n",
    "        serve.shutdown()\n",
    "    \n",
    "    def test_end_to_end_pipeline(self, pipeline_handle):\n",
    "        \"\"\"Test complete pipeline execution\"\"\"\n",
    "        input_text = \"This is a test message\"\n",
    "        \n",
    "        # Execute pipeline\n",
    "        result = pipeline_handle.remote({\"text\": input_text}).result()\n",
    "        \n",
    "        # Verify result\n",
    "        assert \"preprocessed\" in result\n",
    "        assert \"embeddings\" in result\n",
    "        assert \"classification\" in result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a85863",
   "metadata": {},
   "source": [
    "You can also test each deployment individually using their respective handles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8c5f54",
   "metadata": {},
   "source": [
    "To execute the full test suite follow this command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ebaa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to run the test suite\n",
    "# !cd examples && pytest tests/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd6d946",
   "metadata": {},
   "source": [
    "### Best Practices Summary\n",
    "\n",
    "1. **Separate Concerns**: Keep business logic in plain Python classes separate from Ray Serve deployment wrappers\n",
    "2. **Unit Test Business Logic**: Test core functionality without Ray Serve dependencies\n",
    "3. **Integration Test with Handles**: Use DeploymentHandle for testing deployment behavior\n",
    "4. **Integration Testing with HTTP**: Perform full HTTP stack testing if needed\n",
    "\n",
    "This testing approach ensures your Ray Serve applications are robust, maintainable, and production-ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca2908d",
   "metadata": {},
   "source": [
    "## 3. Patterns of Integrating with FastAPI\n",
    "\n",
    "Ray Serve provides flexible ways to integrate with FastAPI applications. This section covers the basic pattern and an advanced builder pattern recommended for large applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd831c7e",
   "metadata": {},
   "source": [
    "### Basic Pattern\n",
    "\n",
    "The most straightforward way to integrate FastAPI with Ray Serve is to directly decorate the FastAPI app object using `@serve.ingress(app)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d407f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/\")\n",
    "def endpoint():\n",
    "    return \"hello\"\n",
    "\n",
    "@serve.deployment\n",
    "@serve.ingress(app)\n",
    "class MyDeployment:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6135ff0d",
   "metadata": {},
   "source": [
    "This approach works well for simple FastAPI applications with minimal state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e68242",
   "metadata": {},
   "source": [
    "### Factory Pattern for Large Applications\n",
    "\n",
    "For large FastAPI applications, the factory/builder pattern is recommended. Instead of decorating the app object directly, use a builder function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c00824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastapi_builder():\n",
    "    app = FastAPI()\n",
    "    \n",
    "    @app.get(\"/\")\n",
    "    def endpoint():\n",
    "        return \"hello\"\n",
    "    \n",
    "    return app\n",
    "\n",
    "deployment = serve.deployment(serve.ingress(fastapi_builder)())\n",
    "serve.run(deployment.bind(), blocking=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28761b7f",
   "metadata": {},
   "source": [
    "**Why Use the Factory Pattern?**\n",
    "\n",
    "The builder pattern provides several advantages for large applications:\n",
    "\n",
    "1. **Avoids Serialization Issues**: Directly decorating a FastAPI ASGI app object requires serialization, which can cause memory spikes when the app contains many large objects or complex state. The builder pattern defers app construction until after deployment initialization.\n",
    "\n",
    "2. **Better Dependency Injection**: Enables FastAPI's dependency injection system to reference Ray Serve deployment handles, making it easier to test FastAPI endpoints in isolation.\n",
    "\n",
    "3. **Cleaner Separation**: Keeps FastAPI app construction logic separate from Ray Serve deployment concerns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03d36cd",
   "metadata": {},
   "source": [
    "#### Complete Example with Deployment Composition\n",
    "\n",
    "Here's a comprehensive example showing the builder pattern with middleware, error handling, and sub-deployment composition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b279e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubModel:\n",
    "    def run(self, a: int):\n",
    "        return a + 1\n",
    "\n",
    "def sub_deployment():\n",
    "    \"\"\"Create a sub-deployment for processing\"\"\"\n",
    "    return serve.deployment(name=\"sub_deployment\")(SubModel)\n",
    "\n",
    "\n",
    "def fastapi_builder_with_composition():\n",
    "    \"\"\"Build a FastAPI app with all features\"\"\"\n",
    "    app = FastAPI(docs_url=\"/custom-docs\")\n",
    "    \n",
    "    # Basic route\n",
    "    @app.get(\"/\")\n",
    "    def root():\n",
    "        return \"hello\"\n",
    "    \n",
    "    # Router for organizing endpoints\n",
    "    router = APIRouter()\n",
    "    \n",
    "    @router.get(\"/f2\")\n",
    "    def f2():\n",
    "        return \"hello f2\"\n",
    "    \n",
    "    @router.get(\"/error\")\n",
    "    def error():\n",
    "        raise ValueError(\"some error\")\n",
    "    \n",
    "    app.include_router(router)\n",
    "    \n",
    "    # Middleware\n",
    "    @app.middleware(\"http\")\n",
    "    async def add_process_time_header(request: Request, call_next):\n",
    "        response = await call_next(request)\n",
    "        response.headers[\"X-Custom-Middleware\"] = \"fake-middleware\"\n",
    "        return response\n",
    "    \n",
    "    # Custom exception handler\n",
    "    @app.exception_handler(ValueError)\n",
    "    async def custom_exception_handler(request: Request, exc: ValueError):\n",
    "        return JSONResponse(status_code=500, content={\"error\": \"fake-error\"})\n",
    "    \n",
    "    # Dependency injection for sub-deployment\n",
    "    def get_sub_deployment_handle():\n",
    "        return serve.get_deployment_handle(\"sub_deployment\", \"default\")\n",
    "    \n",
    "    class Data(BaseModel):\n",
    "        a: int\n",
    "    \n",
    "    @app.get(\"/sub_deployment\", response_model=Data)\n",
    "    async def call_sub_deployment(\n",
    "        request: Request,\n",
    "        handle: DeploymentHandle = Depends(get_sub_deployment_handle)\n",
    "    ):\n",
    "        a = int(request.query_params.get(\"a\", 1))\n",
    "        result = await handle.run.remote(a)\n",
    "        return Data(a=result)\n",
    "    \n",
    "    return app\n",
    "\n",
    "\n",
    "# Deploy the application\n",
    "ingress_deployment = serve.deployment(serve.ingress(fastapi_builder_with_composition)())\n",
    "app = ingress_deployment.bind(sub_deployment().bind())\n",
    "serve.run(app, blocking=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3043c39e",
   "metadata": {},
   "source": [
    "#### Testing the Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b712dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic endpoint\n",
    "resp = requests.get(\"http://localhost:8000/\")\n",
    "assert resp.json() == \"hello\"\n",
    "assert resp.headers[\"X-Custom-Middleware\"] == \"fake-middleware\"\n",
    "\n",
    "# Test router endpoint\n",
    "resp = requests.get(\"http://localhost:8000/f2\")\n",
    "assert resp.json() == \"hello f2\"\n",
    "\n",
    "# Test error handling\n",
    "resp = requests.get(\"http://localhost:8000/error\")\n",
    "assert resp.status_code == 500\n",
    "assert resp.json() == {\"error\": \"fake-error\"}\n",
    "\n",
    "# Test sub-deployment composition\n",
    "resp = requests.get(\"http://localhost:8000/sub_deployment?a=2\")\n",
    "assert resp.json() == {\"a\": 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da478ca",
   "metadata": {},
   "source": [
    "#### When to Use This Pattern\n",
    "\n",
    "Use the builder pattern when:\n",
    "- Your FastAPI app has many dependencies or objects that are **expensive/impossible to serialize**\n",
    "- You need to reference Ray Serve deployment handles from FastAPI endpoints\n",
    "- You want to test FastAPI endpoints with mocked deployment handles\n",
    "\n",
    "For simple FastAPI applications with minimal state, the traditional decorator approach remains a valid option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73af3c59",
   "metadata": {},
   "source": [
    "## 4. Debugging Serve Applications\n",
    "\n",
    "Ray Serve provides a local testing mode that enables running deployments locally in a single process, making it easier to debug your applications without the overhead of a full Ray cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffc34d6",
   "metadata": {},
   "source": [
    "### Using Local Testing Mode\n",
    "\n",
    "To enable local testing mode, use the `_local_testing_mode` flag when calling `serve.run()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9ba02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = MyDeployment.bind()\n",
    "handle = serve.run(app, _local_testing_mode=True, blocking=False) # deployment will now run in a background thread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9076e423",
   "metadata": {},
   "source": [
    "Local testing mode offers several advantages for development and debugging:\n",
    "\n",
    "- **Simplified Debugging**: Each deployment runs in a background thread, making it easier to use debuggers and step through code\n",
    "- **Faster Iteration**: No need to start a Ray cluster, reducing startup time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296df514",
   "metadata": {},
   "source": [
    "#### Example: Debugging \n",
    "\n",
    "Let's examine the script file `examples/debugging/debug.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402afcfb",
   "metadata": {},
   "source": [
    "With this setup, you can:\n",
    "- Run the script directly: `python debug.py`\n",
    "- Set breakpoints in your IDE\n",
    "- Step through the code with a debugger\n",
    "- Inspect variables and state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ca2923",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "\n",
    "While local testing mode supports most features, some limitations exist:\n",
    "- Cannot convert `DeploymentResponse` to `ObjectRef`\n",
    "- Not suitable for testing multi-node deployments\n",
    "\n",
    "For production deployments or testing distributed features, use a full Ray cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78927b7d",
   "metadata": {},
   "source": [
    "## 5. Configuration in Ray Serve\n",
    "\n",
    "Ray Serve provides flexible configuration options that allow you to separate deployment settings and application parameters from your code. This section covers two complementary approaches that work together:\n",
    "\n",
    "1. **YAML Configuration Files**: Define deployment settings (replicas, resources, routing) and application parameters\n",
    "2. **Application Builders**: Functions that accept parameters and return your application, enabling dynamic configuration\n",
    "\n",
    "These approaches work together seamlessly - you can use YAML config files to pass arguments to application builders, combining the benefits of both: version-controlled configuration with parameterized application logic.\n",
    "\n",
    "This is useful for:\n",
    "- Managing different configurations for different environments (dev, staging, production)\n",
    "- Passing parameters without modifying code (model paths, hyperparameters, etc.)\n",
    "- Running multiple instances of the same application with different parameters\n",
    "- Version controlling both deployment settings and application arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470c9fde",
   "metadata": {},
   "source": [
    "### Generating a Config File\n",
    "\n",
    "Use the `serve build` command to generate a configuration file from your Python application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f14738",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd examples/intro && serve build -o config.yaml main:mnist_app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2addcc2b",
   "metadata": {},
   "source": [
    "This command:\n",
    "- Reads your application definition from `main.py` (the `mnist_app` variable)\n",
    "- Generates a `config.yaml` file with all deployment settings\n",
    "- Includes default values for HTTP options, proxy location, and logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd99ed6",
   "metadata": {},
   "source": [
    "### Config File Structure\n",
    "\n",
    "A generated config file looks like this:\n",
    "\n",
    "```yaml\n",
    "proxy_location: EveryNode\n",
    "\n",
    "http_options:\n",
    "  host: 0.0.0.0\n",
    "  port: 8000\n",
    "\n",
    "grpc_options:\n",
    "  port: 9000\n",
    "  grpc_servicer_functions: []\n",
    "\n",
    "logging_config:\n",
    "  encoding: JSON\n",
    "  log_level: INFO\n",
    "  logs_dir: null\n",
    "  enable_access_log: true\n",
    "\n",
    "applications:\n",
    "- name: app1\n",
    "  route_prefix: /\n",
    "  import_path: main:mnist_app\n",
    "  runtime_env: {}\n",
    "  deployments:\n",
    "  - name: OnlineMNISTClassifier\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efaf98b",
   "metadata": {},
   "source": [
    "### Key Configuration Sections\n",
    "\n",
    "Here are the main configurations to set\n",
    "\n",
    "- **proxy_location**: Where to run HTTP proxies (`EveryNode`, `HeadOnly`, or `Disabled`)\n",
    "- **http_options**: HTTP server configuration (host, port)\n",
    "- **logging_config**: Logging settings (level, format, access logs)\n",
    "- **applications**: List of applications to deploy\n",
    "  - **name**: Application name\n",
    "  - **route_prefix**: HTTP route prefix\n",
    "  - **import_path**: Python import path to your application\n",
    "  - **deployments**: List of deployments in the application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da3582e",
   "metadata": {},
   "source": [
    "### Customizing the Config\n",
    "\n",
    "After generating the config, you can customize it:\n",
    "\n",
    "```yaml\n",
    "applications:\n",
    "- name: production_app\n",
    "  route_prefix: /api/v1\n",
    "  import_path: main:mnist_app\n",
    "  runtime_env:\n",
    "    pip:\n",
    "      - torch==2.0.0\n",
    "      - numpy==1.24.0\n",
    "  deployments:\n",
    "  - name: OnlineMNISTClassifier\n",
    "    num_replicas: 4\n",
    "    ray_actor_options:\n",
    "      num_cpus: 2\n",
    "      num_gpus: 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8d6084",
   "metadata": {},
   "source": [
    "### Application Builders\n",
    "\n",
    "When writing an application, you often have parameters that need to change **between environments** or experiments. \n",
    "\n",
    "For example, you might want to deploy different model weights or adjust hyperparameters without modifying your code.\n",
    "\n",
    "**Application builders** solve this by defining a function that accepts parameters and returns a built application. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293cbad8",
   "metadata": {},
   "source": [
    "#### Defining an Application Builder\n",
    "\n",
    "An application builder is a function that takes an arguments dictionary (or Pydantic object) and returns the application to be run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3661e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class HelloWorld:\n",
    "    def __init__(self, message: str):\n",
    "        self._message = message\n",
    "        print(\"Message:\", self._message)\n",
    "\n",
    "    def __call__(self, request):\n",
    "        return self._message\n",
    "\n",
    "class HelloWorldArgs(BaseModel):\n",
    "    message: str\n",
    "\n",
    "def app_builder(args: HelloWorldArgs) -> Application:\n",
    "    return HelloWorld.bind(args.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a708fa10",
   "metadata": {},
   "source": [
    "This `app_builder` function can be used as the import path in `serve run` commands or config files, with arguments passed separately from the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f135fe",
   "metadata": {},
   "source": [
    "### Passing Arguments\n",
    "\n",
    "Pass arguments to the application builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be47dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.run(app_builder(HelloWorldArgs(message=\"Hello World\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72308fb2",
   "metadata": {},
   "source": [
    "The arguments can also be passed to the `serve run` CLI using key=value syntax (not pydantic validation is performed automatically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a05629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to run\n",
    "!cd examples/app_builder && serve run main:app_builder message=\"Hello from CLI\" --non-blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33cef04",
   "metadata": {},
   "source": [
    "Notice that \"Hello from CLI\" is printed from within the deployment constructor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69f57f6",
   "metadata": {},
   "source": [
    "### Passing Arguments via Config File\n",
    "\n",
    "You can also pass arguments to the application builder through the config file's `args` field. This combines the benefits of both approaches: version-controlled YAML configs that parameterize your application builders.\n",
    "\n",
    "```yaml\n",
    "applications:\n",
    "  - name: MyApp\n",
    "    import_path: hello:app_builder\n",
    "    args:\n",
    "      message: \"Hello from config\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9027841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd examples/app_builder && serve run config.yaml --non-blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418e2d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "!serve shutdown -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8e01c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
