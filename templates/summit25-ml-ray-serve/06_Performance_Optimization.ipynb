{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c7c62f",
   "metadata": {},
   "source": [
    "# Performance Optimization in Ray Serve\n",
    "\n",
    "This guide covers three key performance optimization techniques in Ray Serve that can significantly improve throughput, latency, and resource utilization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d787c798",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b> Here is the roadmap of this notebook:</b>\n",
    "\n",
    "<ol>\n",
    "    <li> Dynamic Request Batching</li>\n",
    "    <li> Multiplexing for Multi-Model Serving</li>\n",
    "    <li> Request Pipelining with Streaming</li>\n",
    "    <li> Combining Optimization Techniques</li>\n",
    "</ol>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1b91a2",
   "metadata": {},
   "source": [
    "**Imports**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c46ecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import List\n",
    "\n",
    "import ray\n",
    "import requests\n",
    "from fastapi import FastAPI\n",
    "from ray import serve\n",
    "from ray.serve.handle import DeploymentHandle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b5f3fb",
   "metadata": {},
   "source": [
    "## 1. Dynamic Request Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7e781c",
   "metadata": {},
   "source": [
    "### What is Dynamic Batching?\n",
    "\n",
    "Dynamic batching groups incoming requests that arrive within a short window into batches, up to a maximum size.\n",
    "\n",
    "Here is a diagram illustrating dynamic batching:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-serve-deep-dive/dynamic_request_batching.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415b176c",
   "metadata": {},
   "source": [
    "### When to Use Dynamic Batching\n",
    "\n",
    "Use dynamic batching:\n",
    "- To enable efficient GPU utilization and vectorized CPU operations.\n",
    "- For high-throughput request patterns.\n",
    "- When request sizes and processing times are relatively uniform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ba7c41",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Without batching, each request is processed individually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5018480",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class SimpleModel:\n",
    "    def run(self, single_sample: int) -> int:\n",
    "        return single_sample * 2\n",
    "\n",
    "handle = serve.run(SimpleModel.bind())\n",
    "await handle.run.remote(1)  # Returns 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28905cde",
   "metadata": {},
   "source": [
    "With batching enabled using `@serve.batch`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a894c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class SimpleBatchedModel:\n",
    "    @serve.batch(batch_wait_timeout_s=0.1, max_batch_size=4)\n",
    "    async def run(self, multiple_samples: list[int]) -> list[int]:\n",
    "        print(f\"{multiple_samples=}\")\n",
    "        return [sample * 2 for sample in multiple_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55e8848",
   "metadata": {},
   "source": [
    "Now batching occurs on the replica:\n",
    "- A batch forms when the maximum size is reached (4) or the timeout elapses (0.1 s).\n",
    "- Clients still send individual requests with single samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684eab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = serve.run(SimpleBatchedModel.bind())\n",
    "responses = [handle.run.remote(i) for i in range(8)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c75b4af",
   "metadata": {},
   "source": [
    "Responses arrive individually and in order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b365974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, resp in enumerate(responses):\n",
    "    resp_val = await resp\n",
    "    assert resp_val == i * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b8ab7b",
   "metadata": {},
   "source": [
    "### How to tune batching configurations \n",
    "\n",
    "Here are some guidelines for tuning the `max_batch_size` parameter:\n",
    "\n",
    "- Start small and double until throughput gains flatten (2 → 4 → 8 → 16 ...), watching latency and memory.\n",
    "- Keep within memory limits; larger batches increase peak memory usage.\n",
    "- Match downstream expectations in pipelines (prefer consistent multiples to avoid partial batches: 8→8→4 over 8→6→4).\n",
    "\n",
    "Here are some guidelines for tuning the `batch_wait_timeout_s` parameter:\n",
    "- Set based on latency SLO minus average compute time. Example: if SLO=150ms and compute≈100ms, use 10–20ms.\n",
    "- Lower timeout = lower latency, smaller batches; higher timeout = bigger batches, higher tail latency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187ffb9c",
   "metadata": {},
   "source": [
    "## 2. Multiplexing for Multi-Model Serving\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f554381",
   "metadata": {},
   "source": [
    "### What is Multiplexing?\n",
    "\n",
    "Multiplexing enables a **single deployment** to serve **multiple models** by **dynamically loading and unloading** them as needed. This is ideal when serving **many models** with **sparse or unpredictable traffic patterns**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b629b13",
   "metadata": {},
   "source": [
    "#### The Problem Multiplexing Solves\n",
    "\n",
    "Without multiplexing, serving **100 models** with sparse traffic patterns requires **100 separate deployments**:\n",
    "\n",
    "❌ Without multiplexing:\n",
    "- 100 deployments × 1 replica each = 100 replicas\n",
    "- Either slow start up time: need to scale up replicas from 0\n",
    "- Or inefficient resource usage: need to have all 100 replicas up and running\n",
    "\n",
    "With multiplexing, a **single deployment** handles all models:\n",
    "\n",
    "✅ With multiplexing:\n",
    "- 1 deployment with N replicas\n",
    "  - e.g. 20 replicas each caching up to 5 models can handle all 100 models\n",
    "- Each replica keeps only **K most-used** models **cached in memory**\n",
    "- Automatic **loading/eviction of cache** based on usage\n",
    "- Intelligent routing to replicas with requested models already loaded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3303ee10",
   "metadata": {},
   "source": [
    "### When to Use Multiplexing\n",
    "\n",
    "Use multiplexing when you have:\n",
    "\n",
    "1. **Per-user personalized models**: Serve a unique model for each user\n",
    "2. **Multi-tenancy**: Isolate models per customer/tenant\n",
    "3. **Sparse traffic patterns**: Many models with infrequent requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181c727c",
   "metadata": {},
   "source": [
    "#### Common Use Cases\n",
    "\n",
    "1. Serving base models (e.g LLMs) with fine-tuned variants (e.g. LoRa adapters).\n",
    "   1. Fine-tuned adapters can be loaded/unloaded dynamically on top of a shared base model.\n",
    "2. Personalized recommendation models per user\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b637b56",
   "metadata": {},
   "source": [
    "### How It Works\n",
    "\n",
    "Here is a diagram illustrating how model multiplexing works:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-serve-deep-dive/model_multiplexing.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f0f4d8",
   "metadata": {},
   "source": [
    "**Request Execution Flow:**\n",
    "\n",
    "1. **Client specifies model**: Requests include a model ID via HTTP header (`serve_multiplexed_model_id`) or handle options\n",
    "2. **Intelligent routing**: The controller finds candidate replicas using a three-tier fallback strategy:\n",
    "   - **First priority**: Replicas that already have the requested model loaded (cache hit)\n",
    "   - **Second priority**: Replicas with the fewest models loaded (most capacity)\n",
    "   - **Fallback**: After timeout (~1 second), choose available replicas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbb6409",
   "metadata": {},
   "source": [
    "#### Performance Tradeoffs\n",
    "\n",
    "**Pros:**\n",
    "- Reduced memory footprint (only active models loaded)\n",
    "- Efficient resource utilization\n",
    "- Fewer deployments to manage\n",
    "\n",
    "**Cons:**\n",
    "- Model loading latency on first request (cold start)\n",
    "- Complexity in routing and model ID management\n",
    "- Potential thrashing if too many models accessed frequently\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a64cbfa",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Let's take a look at a basic example of serve multiplexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dafd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class ModelServer:\n",
    "    # specify the size of the cache\n",
    "    @serve.multiplexed(max_num_models_per_replica=2)\n",
    "    async def load_model(self, model_id: str):\n",
    "        return await self.load_model_from_storage(model_id)\n",
    "\n",
    "    async def __call__(self, request):\n",
    "        # fetch the model id associated with this request\n",
    "        model_id = serve.get_multiplexed_model_id()\n",
    "        model = await self.load_model(model_id)\n",
    "        return self.predict(model, request)\n",
    "\n",
    "    async def load_model_from_storage(self, model_id):\n",
    "        print(f\"loading {model_id=} from blob store\")\n",
    "        await asyncio.sleep(0.1) # simulate download weights from S3/GCS\n",
    "        return model_id\n",
    "\n",
    "    def predict(self, model, request):\n",
    "        await asyncio.sleep(0.05)  # simulate loading data onto device and running inference\n",
    "        return f\"prediction from {model}\"\n",
    "\n",
    "\n",
    "handle = serve.run(ModelServer.bind())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfcfc93",
   "metadata": {},
   "source": [
    "We can now send requests specifying which model to use. \n",
    "\n",
    "We can either specify a `multiplexed_model_id` via the `handle.options` if performing a direct rpc call:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836737bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "await handle.options(multiplexed_model_id=\"model_v1\").remote({\"simple\": \"request\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a8b540",
   "metadata": {},
   "source": [
    "Or we can use the `serve_multiplexed_model_id` **HTTP Header** if sending HTTP requests:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a23e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    \"http://localhost:8000/\",\n",
    "    json={\"data\": [1, 2, 3]},\n",
    "    headers={\"serve_multiplexed_model_id\": \"model_v2\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86da789a",
   "metadata": {},
   "source": [
    "By now, we have loaded both `model_v1` and `model_v2` (2 models per replica) which is the configured limit. \n",
    "\n",
    "If we send a request for `model_v3` then `model_v1` will be evicted (unloaded) to make room\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7110ea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    \"http://localhost:8000/\",\n",
    "    json={\"data\": [1, 2, 3]},\n",
    "    headers={\"serve_multiplexed_model_id\": \"model_v3\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cd9bf4",
   "metadata": {},
   "source": [
    "Note `model_v1` was chosen given the implementation uses a Least Recently Used (LRU) cache."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e63404d",
   "metadata": {},
   "source": [
    "### Key knobs and best practices\n",
    "\n",
    "Here are the key configuration options for multiplexing:\n",
    "\n",
    "- **`max_num_models_per_replica`**: Cache size per replica (default 3).\n",
    "- **Router timeout**: `RAY_SERVE_MULTIPLEXED_MODEL_ID_MATCHING_TIMEOUT_S` (~1s) before falling back.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99b6299",
   "metadata": {},
   "source": [
    "Key best practices for multiplexing:\n",
    "\n",
    "- **Tune cache size** to balance memory vs. cold starts.\n",
    "- **Monitor cache hits** to spot thrashing; increase cache or shard if needed.\n",
    "- **Pre-warm critical models** if you must avoid cold-start latency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1deb5c",
   "metadata": {},
   "source": [
    "### Observability\n",
    "\n",
    "Ray Serve provides metrics and the ray serve deployment dashboard has default panels showcasing multiplexed deployment performance.\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-serve-deep-dive/multiplexing-dashboard.png\" width=\"900\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf5b14c",
   "metadata": {},
   "source": [
    "## 3. Request Pipelining with Streaming\n",
    "\n",
    "Pipeline parallelism with streaming allows you to process data as soon as it becomes available, rather than waiting for an entire stage to complete. This is particularly effective for large inputs like videos or long sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93d1892",
   "metadata": {},
   "source": [
    "### Key Concept\n",
    "\n",
    "Instead of waiting for each stage to complete before starting the next:\n",
    "\n",
    "* Decode entire video → Predict on all frames (Perform object detection + add bounding boxes) → Encode entire video\n",
    "\n",
    "```\n",
    "❌ Naive approach (66s total):\n",
    "Decode (4s) → Predict (38s) → Encode (24s)\n",
    "```\n",
    "\n",
    "Process chunks in parallel across stages:\n",
    "\n",
    "```\n",
    "✅ Streaming approach (7s total - 10x faster):\n",
    "Chunk 1: Decode (0.1s) → Predict (0.5s) → Concat (0.1s)\n",
    "Chunk 2: Decode (0.1s) → Predict (0.5s) → Concat (0.1s)\n",
    "...\n",
    "Chunk N: Decode (0.1s) → Predict (0.5s) → Concat (0.1s)\n",
    "```\n",
    "\n",
    "Here is a diagram illustrating request pipelining with streaming:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-serve-deep-dive/video_processing_sequence.png\" width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99072007",
   "metadata": {},
   "source": [
    "### Implementation Pattern\n",
    "\n",
    "Let's look at examples of both naive and streaming orchestration patterns for a video processing pipeline that performs face detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fe5713",
   "metadata": {},
   "source": [
    "#### NaiveOrchestration (no streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966bd01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastapi_app = FastAPI()\n",
    "\n",
    "@serve.deployment\n",
    "@serve.ingress(fastapi_app)\n",
    "class NaiveOrchestration:\n",
    "    def __init__(\n",
    "        self,\n",
    "        decode_video: DeploymentHandle,\n",
    "        fused_detect_encode: DeploymentHandle,\n",
    "    ) -> None:\n",
    "        self.decode_video = decode_video\n",
    "        self.fused_detect_encode = fused_detect_encode\n",
    "\n",
    "    @fastapi_app.post(\"/detect_faces_naive\")\n",
    "    async def run(\n",
    "        self,\n",
    "        video_url: str,\n",
    "        output_path: str,\n",
    "        decode_batch_size: int = 20,\n",
    "    ) -> str:\n",
    "        # Decode entire video first (no streaming)\n",
    "        frames: List[ray.ObjectRef] = await self.decode_video.decode.remote(\n",
    "            video_url, decode_batch_size\n",
    "        )\n",
    "\n",
    "        # Process frames (run detection + encoding) and store video on blob store\n",
    "        return await self.fused_detect_encode.run.remote(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44172d3d",
   "metadata": {},
   "source": [
    "#### StreamingOrchestration (streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1593b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastapi_app = FastAPI()\n",
    "\n",
    "@serve.deployment\n",
    "@serve.ingress(fastapi_app)\n",
    "class StreamingOrchestration:\n",
    "    def __init__(\n",
    "        self,\n",
    "        decode_video: DeploymentHandle,\n",
    "        fused_detect_encode: DeploymentHandle,\n",
    "        concat_video: DeploymentHandle,\n",
    "    ) -> None:\n",
    "        self.decode_video = decode_video.options(stream=True) # Enable streaming\n",
    "        self.fused_detect_encode = fused_detect_encode\n",
    "        self.concat_video = concat_video\n",
    "\n",
    "    @fastapi_app.post(\"/detect_faces_streaming\")\n",
    "    async def run(\n",
    "        self,\n",
    "        video_url: str,\n",
    "        output_path: str,\n",
    "        decode_batch_size: int = 20,\n",
    "    ) -> str:\n",
    "        # Decode yields batches of frames incrementally (streaming)\n",
    "        frames_iter = self.decode_video.decode.remote(video_url, decode_batch_size)\n",
    "        encoded_video_refs: List[ray.ObjectRef] = []\n",
    "        obj_ref_gen = await frames_iter._to_object_ref_gen()\n",
    "        \n",
    "        # Process frames as they arrive (detection + encoding)\n",
    "        async for frame_ref in obj_ref_gen:\n",
    "            encoded_video_refs.append(\n",
    "                self.fused_detect_encode.run.remote(frame_ref)\n",
    "            )\n",
    "\n",
    "        # Concatenate encoded video chunks and store video on blob store\n",
    "        return await self.concat_video.concat.remote(\n",
    "            output_path,\n",
    "            *encoded_video_refs\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ff1bc5",
   "metadata": {},
   "source": [
    "### Key Implementation Details\n",
    "\n",
    "1. **Enable streaming**: Use `.options(stream=True)` on the deployment handle\n",
    "2. **Convert to ObjectRef generator**: Use `await frames_iter._to_object_ref_gen()` to avoid unnecessary data transfer\n",
    "3. **Async iteration**: Use `async for` to process chunks as they arrive\n",
    "4. **Prefer local routing**: Set `_prefer_local_routing=True` for better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad7ecd",
   "metadata": {},
   "source": [
    "### When to Use Streaming\n",
    "\n",
    "- Large input data (videos, long documents, audio files)\n",
    "- Multi-stage processing pipelines\n",
    "- Need to minimize time-to-first-output\n",
    "- Want to overlap computation across pipeline stages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a952b9eb",
   "metadata": {},
   "source": [
    "## Combining Optimization Techniques\n",
    "\n",
    "These techniques can be combined for maximum performance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c56290",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class OptimizedPipeline:\n",
    "    @serve.multiplexed(max_num_models_per_replica=5)\n",
    "    async def load_model(self, model_id: str):\n",
    "        return load_model_from_storage(model_id)\n",
    "    \n",
    "    @serve.batch(max_batch_size=8, batch_wait_timeout_s=0.1)\n",
    "    async def __call__(self, requests: list) -> list:\n",
    "        model_id = serve.get_multiplexed_model_id()\n",
    "        model = await self.load_model(model_id)\n",
    "        results = model.predict_batch(requests)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58b8add",
   "metadata": {},
   "source": [
    "This combines:\n",
    "- **Multiplexing** for serving multiple models\n",
    "- **Dynamic batching** for efficient GPU utilization\n",
    "- Could also add **streaming** if processing large inputs"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "split_at_heading": true
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
