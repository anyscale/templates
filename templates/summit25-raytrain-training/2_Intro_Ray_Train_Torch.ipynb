{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Ray Train + PyTorch\n",
    "Â© 2025, Anyscale. All Rights Reserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will walk you through the basics of distributed training with Ray Train and PyTorch.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b> Here is the roadmap for this notebook </b>\n",
    "\n",
    "<ol>\n",
    "  <li>When to use Ray Train</li>\n",
    "  <li>Single GPU Training with PyTorch</li>\n",
    "  <li>Distributed Training with Ray Train and PyTorch</li>\n",
    "  <li>Ray Train in Production</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Install Dependencies__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "uv pip install -r python_depset.lock --no-cache-dir --no-deps --system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import os   \n",
    "import tempfile\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "\n",
    "import ray\n",
    "from ray.train import ScalingConfig, RunConfig\n",
    "from ray.train.torch import TorchTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. When to use Ray Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Use Ray Train when you face one of the following challenges:\n",
    "\n",
    "|Challenge|Detail|Solution|\n",
    "|---|---|---|\n",
    "|**Need to speed up or scale up training**| Training jobs might take a long time to complete, or require a lot of compute | Ray Train provides a **distributed training** framework that allows engineers to scale training jobs to multiple GPUs |\n",
    "|**Minimize overhead of setting up clusters**| Engineers need to manage the underlying infrastructure | Ray Train **provisions the underlying infrastructure** via Ray's cluster autoscaler. |\n",
    "|**Achieve observability**| Engineers need to connect to different nodes and GPUs to find the root cause of failures, fetch logs, traces, etc | Ray Train **provides observability** via Ray's dashboard, metrics, and traces that allow engineers to monitor the training job |\n",
    "|**Ensure reliable training**| Training jobs can fail due to hardware failures, network issues, or other unexpected events | Ray Train **ensures fault tolerance** via checkpointing, automatic retries, and the ability to resume training from the last checkpoint |\n",
    "|**Avoid significant code rewrite**| Engineers might need to fully rewrite their training loop to support distributed training | Ray Train has **built-in integrations** with the PyTorch ecosystem (Torch, Lightning, Huggingface), Tree-based methods (XGB, LGBM), and more to minimize the amount of code changes needed |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Single GPU Training with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by fitting a `ResNet18` model to an `MNIST` dataset. Conceptually we will follow the below recipe presented below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-ai-libraries/diagrams/single_gpu_pytorch_v3.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|An overview of the single GPU training process. At a high level, here is how training loop in PyTorch looks like. The key stages include loading the dataset; run the training on mini-batches on a single GPU; saving the model checkpoint to the persistent storage.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop_torch(num_epochs: int = 2, batch_size: int = 128, local_path: str = \"./checkpoints\"):\n",
    "\n",
    "    # Model, Loss, Optimizer\n",
    "    criterion = CrossEntropyLoss()\n",
    "    model = load_model_torch()\n",
    "    optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "    # Load the data loader\n",
    "    data_loader = build_data_loader_torch(batch_size=batch_size)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for images, labels in data_loader:\n",
    "\n",
    "            # Move the data to the GPU\n",
    "            images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "        # Report the metrics\n",
    "        metrics = report_metrics_torch(loss=loss, epoch=epoch)\n",
    "        \n",
    "        # Save the checkpoint and metrics\n",
    "        Path(local_path).mkdir(parents=True, exist_ok=True)\n",
    "        save_checkpoint_and_metrics_torch(metrics=metrics, model=model, local_path=local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Quick notes:\n",
    "\n",
    "<ul>\n",
    "    <li><code>report_metrics_torch</code> and <code>save_checkpoint_and_metrics_torch</code> are defined below,</li>\n",
    "    <li><code>local_path</code> is used for checkpointing. (default) Current working directory simply points to the notebook location (check <code>pwd</code> below).</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Build model and load it on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build [Resnet18](https://pytorch.org/vision/main/models/resnet.html#resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_resnet18():\n",
    "    model = resnet18(num_classes=10)\n",
    "    model.conv1 = torch.nn.Conv2d(\n",
    "        in_channels=1, # grayscale MNIST images\n",
    "        out_channels=64,\n",
    "        kernel_size=(7, 7),\n",
    "        stride=(2, 2),\n",
    "        padding=(3, 3),\n",
    "        bias=False,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "resnet18's <code>model.conv1</code> has <code>in_channels=3</code> by default. Here, we work with the <a href=\"https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html#mnist\" target=\"_blank\">MNIST</a> grayscale images, thus <code>in_channels=1</code>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Load the model on a single GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model_torch() -> torch.nn.Module:\n",
    "    model = build_resnet18()\n",
    "\n",
    "    # move to the GPU device\n",
    "    model.to(\"cuda\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = MNIST(root=\"./data\", train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tree ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display 9 example (image, target) pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = np.random.randint(0, len(dataset.data))\n",
    "    img, label = dataset[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a DataLoader to apply transformations and load data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_data_loader_torch(batch_size: int) -> torch.utils.data.DataLoader:\n",
    "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "    dataset = MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Create metrics and checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and report the metrics using a simple print statement, and also save them to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def report_metrics_torch(loss: torch.Tensor, epoch: int) -> None:\n",
    "    metrics = {\"loss\": loss.item(), \"epoch\": epoch}\n",
    "    print(metrics)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the checkpoint in a previously defined local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_checkpoint_and_metrics_torch(metrics: dict[str, float], model: torch.nn.Module, local_path: str) -> None:\n",
    "\n",
    "    # Save the metrics\n",
    "    with open(os.path.join(local_path, \"metrics.csv\"), \"a\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(metrics.values())\n",
    "\n",
    "    # Save the model\n",
    "    checkpoint_path = os.path.join(local_path, \"model.pt\")\n",
    "    torch.save(model.state_dict(), checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Run the training loop\n",
    "Schedule the training loop on a single GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now(datetime.UTC).strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "local_path = f\"/mnt/local_storage/single_gpu_mnist/torch_{timestamp}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Note about Anyscale storage options</b>\n",
    "\n",
    "In this example <code>local_path</code> points to the Anyscale's <a href=\"https://docs.anyscale.com/configuration/storage/#local-storage-for-a-node\" target=\"_blank\">local storage</a>. It's a convenient and quick access location for this basic example.\n",
    "\n",
    "* Anyscale provides each node with its own volume and disk and doesnât share them with other nodes.\n",
    "* Local storage is very fast - Anyscale supports the Non-Volatile Memory Express (NVMe) interface.\n",
    "* This is not a persisent storage, Anyscale deletes data in the local storage after instances are terminated. \n",
    "\n",
    "Read more about available <a href=\"https://docs.anyscale.com/configuration/storage\" target=\"_blank\">storage</a> options.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loop_torch(\n",
    "    num_epochs=1,\n",
    "    local_path=local_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the produced checkpoints and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -l {local_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = pd.read_csv(\n",
    "    os.path.join(local_path, \"metrics.csv\"),\n",
    "    header=None,\n",
    "    names=[\"loss\", \"epoch\"],\n",
    ")\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Use checkpointed model to generate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model checkpoint to the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "loaded_model = build_resnet18()\n",
    "loaded_model.load_state_dict(torch.load(os.path.join(local_path, \"model.pt\")))\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate predictions on randomly selected 9 images rom the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = np.random.randint(0, len(dataset.data))\n",
    "    img, label = dataset[sample_idx]\n",
    "    normalized_img = Normalize((0.5,), (0.5,))(ToTensor()(img))\n",
    "    normalized_img = normalized_img.to(device)\n",
    "\n",
    "    # use loaded model to generate preds\n",
    "    with torch.no_grad():        \n",
    "        prediction = loaded_model(normalized_img.unsqueeze(0)).argmax().cpu()\n",
    "\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(f\"label: {label}; pred: {int(prediction)}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Distributed Data Parallel Training with Ray Train and PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the case where we have a very large dataset of images that would take a long time to train on a single GPU. We would now like to scale this training job to run on multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "|<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-ai-libraries/diagrams/multi_gpu_pytorch_v4.png\" width=\"900px\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Schematic overview of DistributedDataParallel (DDP) training: (1) the model is replicated from the <code>GPU rank 0</code> to all other workers; (2) each worker receives a shard of the dataset and processes a mini-batch; (3) during the backward pass, gradients are averaged across GPUs; (4) checkpoint and metrics from rank 0 GPU are saved to the persistent storage.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Here is a migration roadmap: from PyTorch DDP to PyTorch with Ray Train</b>\n",
    "\n",
    "<ol>\n",
    "    <li>Configure scale and GPUs</li>\n",
    "    <li>Migrate the model to Ray Train</li>\n",
    "    <li>Migrate the dataset to Ray Train</li>\n",
    "    <li>Build checkpoints and metrics reporting</li>\n",
    "    <li>Configure persistent storage</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Overview of the training loop in Ray Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this data-parallel training loop will look like with Ray Train and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop_ray_train(config: dict):  # pass in hyperparameters in config\n",
    "\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    # Use Ray Train to wrap the model with DistributedDataParallel\n",
    "    model = load_model_ray_train()\n",
    "    optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "    # Calculate the batch size for each worker\n",
    "    global_batch_size = config[\"global_batch_size\"]\n",
    "    world_size = ray.train.get_context().get_world_size()\n",
    "    batch_size = global_batch_size // world_size\n",
    "    print(f\"{world_size=}\\n{batch_size=}\")\n",
    "\n",
    "    # Use Ray Train to wrap the data loader as a DistributedSampler\n",
    "    data_loader = build_data_loader_ray_train(batch_size=batch_size)\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "\n",
    "        # Ensure data is on the correct device\n",
    "        data_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        # images, labels are now sharded across the workers\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # gradients are now accumulated across the workers\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Use Ray Train to report metrics\n",
    "        metrics = print_metrics_ray_train(loss, epoch)\n",
    "\n",
    "        # Use Ray Train to save checkpoint and metrics\n",
    "        save_checkpoint_and_metrics_ray_train(model, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>Main training loop</b>\n",
    "<ul>\n",
    "  <li><strong>global_batch_size</strong>: the total number of samples processed in a single training step of the entire training job.\n",
    "    <ul>\n",
    "      <li>It's estimated like this: <code>batch size * DDP workers * gradient accumulation steps</code>.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>Notice that images and labels are no longer manually moved to device (<code>images.to(\"cuda\")</code>). This is done by \n",
    "    <a href=\"https://docs.ray.io/en/latest/train/api/doc/ray.train.torch.prepare_data_loader.html#ray-train-torch-prepare-data-loader\" target=\"_blank\">\n",
    "      prepare_data_loader()\n",
    "    </a>.\n",
    "  </li>\n",
    "  <li>Config that will be passed here, is defined below. It will be passed to the Ray Train's <a href=\"https://docs.ray.io/en/latest/train/api/doc/ray.train.torch.TorchTrainer.html#ray-train-torch-torchtrainer\" target=\"_blank\">TorchTrainer</a>.</li>\n",
    "  <li>\n",
    "    <a href=\"https://docs.ray.io/en/latest/train/api/doc/ray.train.v2.api.context.TrainContext.html#ray-train-v2-api-context-traincontext\" target=\"_blank\">\n",
    "      TrainContext\n",
    "    </a> lets users get useful information about the training i.e. node rank, world size, world rank, experiment name.\n",
    "  </li>\n",
    "\n",
    "  <li><code>load_model_ray_train</code> and <code>build_data_loader_ray_train</code> are implemented below.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loop_config = {\n",
    "    \"num_epochs\": 1, \n",
    "    \"global_batch_size\": 128\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Configure scale and GPUs\n",
    "Outside of our training function, we create a `ScalingConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaling_config = ScalingConfig(num_workers=2, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<a href=\"https://docs.ray.io/en/latest/train/api/doc/ray.train.ScalingConfig.html#ray-train-scalingconfig\" target=\"_blank\">ScalingConfig</a> configures:\n",
    "\n",
    "<ul>\n",
    "  <li><code>num_workers</code>: The number of distributed training worker processes.</li>\n",
    "  <li><code>use_gpu</code>: Whether each worker should use a GPU (or CPU).</li>\n",
    "</ul>\n",
    "\n",
    "See docs on configuring <a href=\"https://docs.ray.io/en/latest/train/user-guides/using-gpus.html\" target=\"_blank\">scale and GPUs</a> for more details.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1. Note on Ray Train key concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray Train is built around [four key concepts](https://docs.ray.io/en/latest/train/overview.html):\n",
    "1. **Training function**: (implemented above `train_loop_ray_train`): A Python function that contains your model training logic.\n",
    "1. **Worker**: A process that runs the training function.\n",
    "1. **Scaling config**: specifices number of workers and compute resources (CPUs or GPUs, TPUs).\n",
    "1. **Trainer**: A Python class (Ray Actor) that ties together the training function, workers, and scaling configuration to execute a distributed training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<img src=\"https://docs.ray.io/en/latest/_images/overview.png\" width=\"700px\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|High-level architecture of how Ray Train|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Migrating the model to Ray Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [`prepare_model()`](https://docs.ray.io/en/latest/train/api/doc/ray.train.torch.prepare_model.html#ray-train-torch-prepare-model) utility function to:\n",
    "\n",
    "* automatically move your model to the correct device,\n",
    "* wrap the model in PyTorch's DDP or FSDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model_ray_train() -> torch.nn.Module:\n",
    "    model = build_resnet18()\n",
    "    model = ray.train.torch.prepare_model(model) # Instead of model = model.to(\"cuda\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "  <a href=\"https://docs.ray.io/en/latest/train/api/doc/ray.train.torch.prepare_model.html#ray-train-torch-prepare-model\" target=\"_blank\">\n",
    "    prepare_model()\n",
    "  </a> allows users to specify additional parameters:\n",
    "  <ul>\n",
    "    <li><code>parallel_strategy</code>: \"ddp\", \"fsdp\" â wrap models in <code>DistributedDataParallel</code> or <code>FullyShardedDataParallel</code></li>\n",
    "    <li><code>parallel_strategy_kwargs</code>: pass additional arguments to \"ddp\" or \"fsdp\"</li>\n",
    "  </ul>\n",
    "  <p>\n",
    "    With <a href=\"https://docs.ray.io/en/latest/train/api/doc/ray.train.torch.prepare_model.html#ray-train-torch-prepare-model\" target=\"_blank\">\n",
    "      prepare_model()\n",
    "    </a> you can use the same code regardless of number of workers or the device type being used (CPU, GPU).\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Migrating the dataset to Ray Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [`prepare_data_loader()`](https://docs.ray.io/en/latest/train/api/doc/ray.train.torch.prepare_data_loader.html#ray-train-torch-prepare-data-loader) utility function, to automatically:\n",
    "\n",
    "* move the batches to the right device,\n",
    "* copy data from host (CPU) memory to device (GPU) memory,\n",
    "* pass PyTorch's [`DistributedSampler`](https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler) to the DataLoader, if using more than 1 worker. Each worker will load a subset of the original dataset that is exclusive to it.\n",
    "\n",
    "[`prepare_data_loader()`](https://docs.ray.io/en/latest/train/api/doc/ray.train.torch.prepare_data_loader.html#ray-train-torch-prepare-data-loader) allows users to use the same code regardless of number of workers or the device type being used (CPU, GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_data_loader_ray_train(batch_size: int) -> torch.utils.data.DataLoader:\n",
    "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "    train_data = MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    # Automatically pass a DistributedSampler instance as a DataLoader sampler\n",
    "    train_loader = ray.train.torch.prepare_data_loader(train_loader)\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<b>Ray Data integration</b>\n",
    "\n",
    "This step isn't necessary if you are integrating your Ray Train workload with Ray Data. It's especially useful if preprocessing is CPU-heavly and user wants to run preprocessing and training of separate instances.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Reporting checkpoints and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "To monitor progress, we can continue to print/log metrics as before. This time we chose to log from all workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_metrics_ray_train(loss: torch.Tensor, epoch: int) -> None:\n",
    "    metrics = {\"loss\": loss.item(), \"epoch\": epoch}\n",
    "    world_rank = ray.train.get_context().get_world_rank() # report from all workers\n",
    "    print(f\"{metrics=} {world_rank=}\")\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "If you want to log only from the rank 0 worker, use this code:\n",
    "\n",
    "```python\n",
    "def print_metrics_ray_train(loss: torch.Tensor, epoch: int) -> None:\n",
    "    metrics = {\"loss\": loss.item(), \"epoch\": epoch}\n",
    "    if ray.train.get_context().get_world_rank() == 0:  # report only from the rank 0 worker\n",
    "        print(f\"{metrics=} {world_rank=}\")\n",
    "    return metrics\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will report intermediate metrics and checkpoints using the [`ray.train.report`](https://docs.ray.io/en/latest/train/api/doc/ray.train.report.html#ray.train.report) utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_checkpoint_and_metrics_ray_train(\n",
    "    model: torch.nn.Module, metrics: dict[str, float]\n",
    ") -> None:\n",
    "    with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "        torch.save(\n",
    "            model.module.state_dict(),  # note the `.module` to unwrap the DistributedDataParallel\n",
    "            os.path.join(temp_checkpoint_dir, \"model.pt\"),\n",
    "        )\n",
    "\n",
    "        ray.train.report(\n",
    "            metrics,\n",
    "            checkpoint=ray.train.Checkpoint.from_directory(temp_checkpoint_dir),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "  <p><strong>Quick notes:</strong></p>\n",
    "  <ul>\n",
    "    <li>\n",
    "      Use \n",
    "      <a href=\"https://docs.ray.io/en/latest/train/api/doc/ray.train.report.html#ray.train.report\" target=\"_blank\">\n",
    "        ray.train.report\n",
    "      </a> to save the metrics and checkpoint.\n",
    "    </li>\n",
    "    <li>Only metrics from the rank 0 worker are reported.</li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.1. Note on the checkpoint lifecycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the lifecycle of a checkpoint from being created using a local path to being uploaded to persistent storage.\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/checkpoint_lifecycle.png\" width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "  <p><strong>Notes:</strong></p>\n",
    "  <ul>\n",
    "    <li>\n",
    "      Given it is the same model across all workers, we can instead only build the checkpoint on the worker of rank 0.\n",
    "      Note that we will still need to call \n",
    "      <a href=\"https://docs.ray.io/en/latest/train/api/doc/ray.train.report.html#ray.train.report\" target=\"_blank\">\n",
    "        ray.train.report\n",
    "      </a> on all workers to ensure that the training loop is synchronized.\n",
    "    </li>\n",
    "    <li>Ray Train expects all workers to be able to write files to the same persistent storage location.</li>\n",
    "    <li>Cloud storage is the recommended persistent storage location.</li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_checkpoint_and_metrics_ray_train(\n",
    "    model: torch.nn.Module, metrics: dict[str, float]\n",
    ") -> None:\n",
    "    with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "        checkpoint = None\n",
    "\n",
    "        # checkpoint only from rank 0 worker\n",
    "        if ray.train.get_context().get_world_rank() == 0:\n",
    "            torch.save(\n",
    "                model.module.state_dict(), os.path.join(temp_checkpoint_dir, \"model.pt\")\n",
    "            )\n",
    "            checkpoint = ray.train.Checkpoint.from_directory(temp_checkpoint_dir)\n",
    "\n",
    "        ray.train.report(\n",
    "            metrics,\n",
    "            checkpoint=checkpoint,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check our guide on [saving and loading checkpoints](https://docs.ray.io/en/latest/train/user-guides/checkpoints.html) for more details and best practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Configure remote storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `RunConfig` object to specify the path where results (including checkpoints and artifacts) will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "storage_path = \"/mnt/cluster_storage/training/\"\n",
    "run_config = RunConfig(storage_path=storage_path, name=\"distributed-mnist-resnet18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. Launching the distributed training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributed data-parallel training, but now using Ray Train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-ai-libraries/diagrams/multi_gpu_pytorch_annotated_v5.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now launch a distributed training job with a [`TorchTrainer`](https://docs.ray.io/en/latest/train/api/doc/ray.train.torch.TorchTrainer.html#ray.train.torch.TorchTrainer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = TorchTrainer(\n",
    "    train_loop_ray_train,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    "    train_loop_config=train_loop_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `trainer.fit()` will start the run and block until it completes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be able to observe relevant logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-intro/ray-train-intro-logs.png\" width=\"80%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8. Access the training results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training completes, a `Result` object is returned which contains information about the training run, including the metrics and checkpoints reported during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls /mnt/cluster_storage/training/distributed-mnist-resnet18/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the metrics produced by the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result.metrics_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9. Use checkpointed model to generate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take the latest checkpoint and load it to inspect the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ckpt = result.checkpoint\n",
    "with ckpt.as_directory() as ckpt_dir:\n",
    "    model_path = os.path.join(ckpt_dir, \"model.pt\")\n",
    "    loaded_model_ray_train = build_resnet18()\n",
    "    state_dict = torch.load(model_path, map_location=torch.device('cpu'), weights_only=True)\n",
    "    loaded_model_ray_train.load_state_dict(state_dict)\n",
    "    loaded_model_ray_train.to(\"cuda\")\n",
    "    loaded_model_ray_train.eval()\n",
    "\n",
    "loaded_model_ray_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "  <p>\n",
    "    To learn more about the training results, see this \n",
    "    <a href=\"https://docs.ray.io/en/latest/train/user-guides/results.html\" target=\"_blank\">\n",
    "      docs\n",
    "    </a> on inspecting the training results.\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate predictions on randomly selected 9 images rom the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +7m30s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n"
     ]
    }
   ],
   "source": [
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = np.random.randint(0, len(dataset.data))\n",
    "    img, label = dataset[sample_idx]\n",
    "    normalized_img = Normalize((0.5,), (0.5,))(ToTensor()(img))\n",
    "    normalized_img = normalized_img.to(\"cuda\")\n",
    "\n",
    "    # use loaded model to generate preds\n",
    "    with torch.no_grad():        \n",
    "        prediction = loaded_model_ray_train(normalized_img.unsqueeze(0)).argmax().cpu()\n",
    "\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(f\"label: {label}; pred: {int(prediction)}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ray Train in Production\n",
    "\n",
    "Here are some use-cases of using Ray Train in production:\n",
    "1. Canva uses Ray Train + Ray Data to cut down Stable Diffusion training costs by 3.7x. Read this [Anyscale blog post here](https://www.anyscale.com/blog/scalable-and-cost-efficient-stable-diffusion-pre-training-with-ray) and the [Canva  case study here](https://www.anyscale.com/resources/case-study/how-canva-built-a-modern-ai-platform-using-anyscale)\n",
    "2. Anyscale uses Ray Train + Deepspeed to finetune language models. Read more [here](https://github.com/ray-project/ray/tree/master/doc/source/templates/04_finetuning_llms_with_deepspeed).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell for file cleanup \n",
    "!rm -rf /mnt/cluster_storage/single_gpu_mnist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orphan": true,
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
