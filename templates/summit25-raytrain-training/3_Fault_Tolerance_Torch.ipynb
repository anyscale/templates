{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fault Tolerance in Ray Train + PyTorch\n",
    "Â© 2025, Anyscale. All Rights Reserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will walk you through ensuring fault tolerance in Ray Train + PyTorch.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b> Here is the roadmap for this notebook </b>\n",
    "\n",
    "<ol>\n",
    "  <li>Overview of fault tolerance in Ray Train</li>\n",
    "  <li>Automatic retries</li>\n",
    "  <li>Manual restoration</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os   \n",
    "import tempfile\n",
    "\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "\n",
    "import ray\n",
    "from ray.train import ScalingConfig, RunConfig\n",
    "from ray.train.torch import TorchTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet18():\n",
    "    model = resnet18(num_classes=10)\n",
    "    model.conv1 = torch.nn.Conv2d(\n",
    "        in_channels=1,  # grayscale MNIST images\n",
    "        out_channels=64,\n",
    "        kernel_size=(7, 7),\n",
    "        stride=(2, 2),\n",
    "        padding=(3, 3),\n",
    "        bias=False,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_model_ray_train() -> torch.nn.Module:\n",
    "    model = build_resnet18()\n",
    "    model = ray.train.torch.prepare_model(model)  # Instead of model = model.to(\"cuda\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_data_loader_ray_train(batch_size: int) -> torch.utils.data.DataLoader:\n",
    "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "    train_data = MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_data, batch_size=batch_size, shuffle=True, drop_last=True\n",
    "    )\n",
    "\n",
    "    # Automatically pass a DistributedSampler instance as a DataLoader sampler\n",
    "    train_loader = ray.train.torch.prepare_data_loader(train_loader)\n",
    "    return train_loader\n",
    "\n",
    "def print_metrics_ray_train(loss: torch.Tensor, epoch: int) -> None:\n",
    "    metrics = {\"loss\": loss.item(), \"epoch\": epoch}\n",
    "    world_rank = ray.train.get_context().get_world_rank() # report from all workers\n",
    "    print(f\"{metrics=} {world_rank=}\")\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview of fault tolerance in Ray Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray Train provides two main mechanisms to handle failures:\n",
    "\n",
    "- Automatic retries\n",
    "- Manual restoration\n",
    "\n",
    "Here is a diagram showing these two primary mechanisms:\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-summit/stable-diffusion/diagrams/fault_tolerant_cropped_v2.png\" width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Automatic retries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Modifying the Training Loop to Enable Checkpoint Loading\n",
    "\n",
    "We need to make use of `get_checkpoint()` in the training loop to enable checkpoint loading for fault tolerance.\n",
    "\n",
    "Here is how the modified training loop looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_ray_train_with_checkpoint_loading(config: dict):\n",
    "    # Same initialization of loss, model, optimizer as before\n",
    "    criterion = CrossEntropyLoss()\n",
    "    model = load_model_ray_train()\n",
    "    optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Same initialization of the data loader as before\n",
    "    global_batch_size = config[\"global_batch_size\"]\n",
    "    batch_size = global_batch_size // ray.train.get_context().get_world_size()\n",
    "    data_loader = build_data_loader_ray_train(batch_size=batch_size)\n",
    "\n",
    "    # Assume we start from epoch 0 unless we find a checkpoint\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Load the latest checkpoint if it exists\n",
    "    checkpoint = ray.train.get_checkpoint()\n",
    "    if checkpoint:\n",
    "        # Continue training from a previous checkpoint\n",
    "        with checkpoint.as_directory() as ckpt_dir:\n",
    "            model_state_dict = torch.load(\n",
    "                os.path.join(ckpt_dir, \"model.pt\"),\n",
    "            )\n",
    "            # Load the model and optimizer state\n",
    "            model.module.load_state_dict(model_state_dict)\n",
    "            optimizer.load_state_dict(\n",
    "                torch.load(os.path.join(ckpt_dir, \"optimizer.pt\"))\n",
    "            )\n",
    "\n",
    "            # Load the last epoch from the extra state\n",
    "            start_epoch = (\n",
    "                torch.load(os.path.join(ckpt_dir, \"extra_state.pt\"))[\"epoch\"] + 1\n",
    "            )\n",
    "\n",
    "    # Same loop as before except it starts at a parameterized start_epoch\n",
    "    for epoch in range(start_epoch, config[\"num_epochs\"]):\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        metrics = print_metrics_ray_train(loss,  epoch)\n",
    "        # We now save the optimizer and epoch state in addition to the model\n",
    "        save_checkpoint_and_metrics_ray_train_with_extra_state(\n",
    "            model, metrics, optimizer, epoch\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also to update the checkpoint saving function to save the optimizer and epoch state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint_and_metrics_ray_train_with_extra_state(\n",
    "    model: torch.nn.Module,\n",
    "    metrics: dict[str, float],\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    ") -> None:\n",
    "    with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "        checkpoint = None\n",
    "        if ray.train.get_context().get_world_rank() == 0:\n",
    "            # === Make sure to save all state needed for resuming training ===\n",
    "            torch.save(\n",
    "                model.module.state_dict(),  # NOTE: Unwrap the model.\n",
    "                os.path.join(temp_checkpoint_dir, \"model.pt\"),\n",
    "            )\n",
    "            torch.save(\n",
    "                optimizer.state_dict(),\n",
    "                os.path.join(temp_checkpoint_dir, \"optimizer.pt\"),\n",
    "            )\n",
    "            torch.save(\n",
    "                {\"epoch\": epoch},\n",
    "                os.path.join(temp_checkpoint_dir, \"extra_state.pt\"),\n",
    "            )\n",
    "            # ================================================================\n",
    "            checkpoint = ray.train.Checkpoint.from_directory(temp_checkpoint_dir)\n",
    "    \n",
    "        ray.train.report(  # use ray.train.report to save the metrics and checkpoint\n",
    "            metrics,  # train.report will only save worker rank 0's metrics\n",
    "            checkpoint=checkpoint,\n",
    "            )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Configuring Automatic Retries\n",
    "Now that we have enabled checkpoint loading, we can configure a failure config which sets the maximum number of retries for a training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_config = ScalingConfig(num_workers=2, use_gpu=True)\n",
    "\n",
    "experiment_name = \"fault-tolerant-cifar-vit\"\n",
    "storage_path = \"/mnt/cluster_storage/training/\"\n",
    "failure_config = ray.train.FailureConfig(max_failures=3) \n",
    "run_name = \"distributed-mnist-resnet18-auto-retry\"\n",
    "run_config = RunConfig(\n",
    "    storage_path=storage_path,\n",
    "    name=run_name,\n",
    "    failure_config=failure_config,\n",
    ")\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_ray_train_with_checkpoint_loading,\n",
    "    train_loop_config={\"num_epochs\": 1, \"global_batch_size\": 512},\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can proceed to run the training job as before. \n",
    "\n",
    "This time, if any worker fails, Ray Train will create a new attempt, restart the worker group and resume training from the last checkpoint up to the specified maximum number of failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Manual restoration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case the retries are exhausted, we can perform a manual restoration by re-initializing the TorchTrainer with the same `run_config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_ray_train_with_checkpoint_loading,\n",
    "    train_loop_config={\"num_epochs\": 1, \"global_batch_size\": 512},\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=ray.train.RunConfig(\n",
    "        name=run_name,\n",
    "        storage_path=storage_path, \n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the fit method will resume training from the last checkpoint.\n",
    "\n",
    "Given we already have completed all epochs, we expect the training to terminate immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = restored_trainer.fit()\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orphan": true,
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
