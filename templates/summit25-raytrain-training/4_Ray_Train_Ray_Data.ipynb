{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Train + Ray Data\n",
    "Â© 2025, Anyscale. All Rights Reserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will walk you through integrating Ray Train with Ray Data.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b> Here is the roadmap for this notebook </b>\n",
    "\n",
    "<ol>\n",
    "  <li>When to integrate Ray Train with Ray Data</li>\n",
    "  <li>Architecture</li>\n",
    "  <li>Integrating Ray Train with Ray Data</li>\n",
    "  <li>Training with Ray Train and Ray Data</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os   \n",
    "import tempfile\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "\n",
    "import ray\n",
    "from ray.train import ScalingConfig, RunConfig\n",
    "from ray.train.torch import TorchTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet18():\n",
    "    model = resnet18(num_classes=10)\n",
    "    model.conv1 = torch.nn.Conv2d(\n",
    "        in_channels=1,  # grayscale MNIST images\n",
    "        out_channels=64,\n",
    "        kernel_size=(7, 7),\n",
    "        stride=(2, 2),\n",
    "        padding=(3, 3),\n",
    "        bias=False,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_model_ray_train() -> torch.nn.Module:\n",
    "    model = build_resnet18()\n",
    "    model = ray.train.torch.prepare_model(model)  # Instead of model = model.to(\"cuda\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def print_metrics_ray_train(loss: torch.Tensor, epoch: int) -> None:\n",
    "    metrics = {\"loss\": loss.item(), \"epoch\": epoch}\n",
    "    world_rank = ray.train.get_context().get_world_rank()  # report from all workers\n",
    "    print(f\"{metrics=} {world_rank=}\")\n",
    "    return metrics\n",
    "\n",
    "def save_checkpoint_and_metrics_ray_train(\n",
    "    model: torch.nn.Module, metrics: dict[str, float]\n",
    ") -> None:\n",
    "    with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "        torch.save(\n",
    "            model.module.state_dict(),  # note the `.module` to unwrap the DistributedDataParallel\n",
    "            os.path.join(temp_checkpoint_dir, \"model.pt\"),\n",
    "        )\n",
    "\n",
    "        ray.train.report(\n",
    "            metrics,\n",
    "            checkpoint=ray.train.Checkpoint.from_directory(temp_checkpoint_dir),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. When to integrate Ray Train with Ray Data\n",
    "Use both Ray Train and Ray Data when you face one of the following challenges:\n",
    "| Challenge | Detail | Solution |\n",
    "| --- | --- | --- |\n",
    "| Need to perform online or just-in-time data processing | The training pipeline requires processing data on the fly, such as data augmentation, normalization, or other transformations that may differ for each training epoch. | Ray Train's integration with Ray Data makes it easy to implement just-in-time data processing. |\n",
    "| Need to improve hardware utilization | Training and data processing need to be scaled independently to keep GPUs fully utilized, especially when preprocessing is CPU-intensive. | Ray Data can distribute data processing across multiple CPU nodes, while Ray Train runs the training loop on GPUs. |\n",
    "| Need a consistent interface for loading data | The training process may need to load data from various sources, such as Parquet, CSV, or lakehouses. | Ray Data provides a consistent interface for loading, shuffling, sharding, and batching data for training loops. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Architecture\n",
    "\n",
    "Here is a diagram showing the a sample Ray Data and Ray Train integration\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-train-deep-dive/ray_train_v2_architecture.png\" width=\"900\" loading=\"lazy\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Integrating Ray Train with Ray Data\n",
    "\n",
    "Here is how our training loop will look like using **Ray Data** instead of the **PyTorch DataLoader**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_ray_train_ray_data(config: dict):\n",
    "    # Same initialization as before\n",
    "    criterion = CrossEntropyLoss()\n",
    "    model = load_model_ray_train()\n",
    "    optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    # This time we use Ray Train's integration with Ray Data to load the data\n",
    "    global_batch_size = config[\"global_batch_size\"]\n",
    "    batch_size = global_batch_size // ray.train.get_context().get_world_size()\n",
    "    data_loader = build_data_loader_ray_train_ray_data(batch_size=batch_size) \n",
    "    \n",
    "\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        # No longer need to ensure data is on the correct device\n",
    "        # data_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        # Note our batches are now dictionaries instead of tuples\n",
    "        for batch in data_loader: \n",
    "            outputs = model(batch[\"image\"])\n",
    "            loss = criterion(outputs, batch[\"label\"])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        metrics = print_metrics_ray_train(loss, epoch)\n",
    "        save_checkpoint_and_metrics_ray_train(model, metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the updated `build_data_loader_ray_train_ray_data` function that uses Ray Data to load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_loader_ray_train_ray_data(\n",
    "    batch_size: int, prefetch_batches: int = 2\n",
    "):\n",
    "    dataset_iterator = ray.train.get_dataset_shard(\"train\")\n",
    "    data_loader = dataset_iterator.iter_torch_batches(\n",
    "        batch_size=batch_size, prefetch_batches=prefetch_batches\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Note** Use the `iter_torch_batches` function to build a torch compatible data loader.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preparing the data\n",
    "Let's store the training data in a format that Ray Data can easily read. \n",
    "\n",
    "Let's use the Parquet format, which is a columnar storage format that is efficient for reading and writing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dataset = MNIST(root=\"./data\", train=True, download=True)\n",
    "df = pd.DataFrame({\"image\": torch_dataset.data.tolist(), \"label\": torch_dataset.targets})\n",
    "df.to_parquet(\"/mnt/cluster_storage/mnist.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, construct a Ray Data Dataset from the Parquet source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ray.data.read_parquet(\"/mnt/cluster_storage/mnist.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the same preprocessing steps that pytorch data loader does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_images(row: dict):\n",
    "    # Define the torchvision transform.\n",
    "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "    image_arr = np.array(row[\"image\"], dtype=np.uint8)\n",
    "    row[\"image\"] = transform(Image.fromarray(image_arr))\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Note** Unlike the PyTorch DataLoader, the preprocessing can now occur on any node in the cluster.\n",
    "\n",
    "The data will be passed to training workers via the ray object store (a distributed in-memory object store).\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(transform_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training with Ray Train and Ray Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the constructed `train_ds` to the `TorchTrainer` via the `datasets` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\"train\": train_ds}\n",
    "scaling_config = ScalingConfig(num_workers=2, use_gpu=True)\n",
    "storage_path = \"/mnt/cluster_storage/training/\"\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_ray_train_ray_data,\n",
    "    train_loop_config={\"num_epochs\": 1, \"global_batch_size\": 512},\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=RunConfig(storage_path=storage_path, name=\"dist-mnist-res18-ray-data\"),\n",
    "    datasets=datasets,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `trainer.fit()` will now use Ray Data to load and shard the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orphan": true,
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
