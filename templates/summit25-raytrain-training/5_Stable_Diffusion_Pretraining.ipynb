{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7078ab58-6ca4-4255-8050-b7c5fe7eae1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Distributed Training for Stable Diffusion\n",
    "\n",
    "This notebook demonstrates how to train a Stable Diffusion model using PyTorch Lightning and Ray Train. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>Here is the roadmap for this notebook:</b>\n",
    "\n",
    "<ul>\n",
    "    <li>Part 1: Load the preprocessed data into a Ray Dataset</li>\n",
    "    <li>Part 2: Define a stable diffusion model</li>\n",
    "    <li>Part 3: Define a PyTorch Lightning training loop</li>\n",
    "    <li>Part 4: Migrate the training loop to Ray Train</li>\n",
    "    <li>Part 5: Create and fit a Ray Train TorchTrainer</li>\n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765f5851",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ecea11-fc44-4bc2-af6d-09db4753d78e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from diffusers import DDPMScheduler, UNet2DConditionModel\n",
    "from pytorch_lightning.utilities.types import OptimizerLRScheduler\n",
    "from transformers import PretrainedConfig, get_linear_schedule_with_warmup\n",
    "\n",
    "import ray.train\n",
    "from ray.train.lightning import (\n",
    "    RayDDPStrategy,\n",
    "    RayLightningEnvironment,\n",
    "    RayTrainReportCallback,\n",
    ")\n",
    "from ray.train.torch import TorchTrainer, get_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1537cf",
   "metadata": {},
   "source": [
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/stable-diffusion/training_architecture_v3.jpeg\" width=\"700px\">\n",
    "\n",
    "The preceding architecture diagram illustrates the training pipeline for Stable Diffusion. \n",
    "\n",
    "It is primarily composed of three main stages:\n",
    "1. **Streaming data from the preprocessing stage**\n",
    "2. **Training the model**\n",
    "3. **Storing the model checkpoints**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7c50ba",
   "metadata": {},
   "source": [
    "## 1. Load the preprocessed data into a Ray Dataset\n",
    "\n",
    "Let's start by specifying the datasets we want to use. We'll use `parquet` data that was generated using the same preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40392506-334f-4b05-9bb0-f2815daff428",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns = [\"image_latents_256\", \"caption_latents\"]\n",
    "\n",
    "train_data_uri = (\n",
    "    \"s3://anyscale-public-materials/ray-summit/stable-diffusion/data/preprocessed/256/\"\n",
    ")\n",
    "train_ds = ray.data.read_parquet(train_data_uri, columns=columns, shuffle=\"files\")\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e9f3a7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<b>NOTE:</b> We make use of column pruning by setting `columns=columns` in `read_parquet` to only load the columns we need. Column pruning is a good practice to follow when working with large datasets to reduce memory usage.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fec75d",
   "metadata": {},
   "source": [
    "Given pyarrow and in turn parquet does not support saving float16, we need to add a step to convert the float32 columns to float16. \n",
    "\n",
    "Halving the precision of the data helps us reduce the memory usage and speed up the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bd6050",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_precision(batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "    for k, v in batch.items():\n",
    "        batch[k] = v.astype(np.float16)\n",
    "    return batch\n",
    "\n",
    "train_ds = train_ds.map_batches(convert_precision, batch_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686c3c91",
   "metadata": {},
   "source": [
    "We form a dictionary of the datasets to eventually pass to the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6962ef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray_datasets = {\"train\": train_ds}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f8be2a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<b>NOTE:</b> We did not create a validation dataset in the preprocessing step. Validation can consume valuable GPU hours and resources that could be better utilized for training, especially on high-performance GPUs like the A100. Thoughtful scheduling of validation can help optimize resource usage.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0d8b57-ded8-42c2-84a1-60e8102d17ba",
   "metadata": {},
   "source": [
    "## 2. Define a stable diffusion model\n",
    "\n",
    "This \"standard\" LightningModule does not explicitly refer to Ray or Ray Train, which makes migrating workloads easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b7e93d-4e8f-4053-86d0-0fd5f44b5f86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StableDiffusion(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr: float,\n",
    "        resolution: int,\n",
    "        weight_decay: float,\n",
    "        num_warmup_steps: int,\n",
    "        model_name: str,\n",
    "    ) -> None:\n",
    "        self.lr = lr\n",
    "        self.resolution = resolution\n",
    "        self.weight_decay = weight_decay\n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # Initialize U-Net.\n",
    "        model_config = PretrainedConfig.get_config_dict(model_name, subfolder=\"unet\")[0]\n",
    "        self.unet = UNet2DConditionModel(**model_config)\n",
    "        # Define the training noise scheduler.\n",
    "        self.noise_scheduler = DDPMScheduler.from_pretrained(\n",
    "            model_name, subfolder=\"scheduler\"\n",
    "        )\n",
    "        # Setup loss function.\n",
    "        self.loss_fn = F.mse_loss\n",
    "        self.current_training_steps = 0\n",
    "\n",
    "    def on_fit_start(self) -> None:\n",
    "        \"\"\"Move cumprod tensor to GPU in advance to avoid data movement on each step.\"\"\"\n",
    "        self.noise_scheduler.alphas_cumprod = self.noise_scheduler.alphas_cumprod.to(\n",
    "            get_device()\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, batch: dict[str, torch.Tensor]\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward pass of the model.\"\"\"\n",
    "        # Extract inputs.\n",
    "        latents = batch[\"image_latents_256\"]\n",
    "        conditioning = batch[\"caption_latents\"]\n",
    "        # Sample the diffusion timesteps.\n",
    "        timesteps = self._sample_timesteps(latents)\n",
    "        # Add noise to the inputs (forward diffusion).\n",
    "        noise = torch.randn_like(latents)\n",
    "        noised_latents = self.noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "        # Forward through the model.\n",
    "        outputs = self.unet(noised_latents, timesteps, conditioning)[\"sample\"]\n",
    "        return outputs, noise\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: dict[str, torch.Tensor], batch_idx: int\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Training step of the model.\"\"\"\n",
    "        outputs, targets = self.forward(batch)\n",
    "        loss = self.loss_fn(outputs, targets)\n",
    "        self.log(\n",
    "            \"train/loss_mse\", loss.item(), prog_bar=False, on_step=True, sync_dist=False\n",
    "        )\n",
    "        self.current_training_steps += 1\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self) -> OptimizerLRScheduler:\n",
    "        \"\"\"Configure the optimizer and learning rate scheduler.\"\"\"\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.trainer.model.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "        # Set a large training step here to keep lr constant after warm-up.\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.num_warmup_steps,\n",
    "            num_training_steps=100000000000,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def _sample_timesteps(self, latents: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.randint(\n",
    "            0, len(self.noise_scheduler), (latents.shape[0],), device=latents.device\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822926ee-bd2b-4977-8ffd-725881160da3",
   "metadata": {},
   "source": [
    "## 3. Define a PyTorch Lightning training loop\n",
    "\n",
    "Here is a training loop that is specific to PyTorch Lightning.\n",
    "\n",
    "It performs the following steps:\n",
    "1. **Model Initialization:**\n",
    "   - Instantiate the diffusion model.\n",
    "2. **Trainer Setup:**\n",
    "   - Instantiate the Lightning Trainer with a `DDPStrategy` to perform data parallel training.\n",
    "3. **Training Execution:**\n",
    "   - Run the trainer using the `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503c914e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lightning_training_loop(\n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    storage_path: str,\n",
    "    model_name: str = \"stabilityai/stable-diffusion-2-base\",\n",
    "    resolution: int = 256,\n",
    "    lr: float = 1e-4,\n",
    "    max_epochs: int = 1,\n",
    "    num_warmup_steps: int = 10_000,\n",
    "    weight_decay: float = 1e-2,\n",
    ") -> None:\n",
    "    # 1. Initialize the model\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "    model = StableDiffusion(\n",
    "        model_name=model_name,\n",
    "        resolution=resolution,\n",
    "        lr=lr,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "\n",
    "    # 2. Initialize the Lightning Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        devices=\"auto\",\n",
    "        precision=\"bf16-mixed\",\n",
    "        max_epochs=max_epochs,\n",
    "        default_root_dir=storage_path\n",
    "    )\n",
    "\n",
    "    # 3. Run the trainer\n",
    "    trainer.fit(model=model, train_dataloaders=train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6423b358",
   "metadata": {},
   "source": [
    "Here is how we would run the lightning training loop on a single GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d227b70-cc09-4e40-b43e-c66841b03d93",
   "metadata": {
    "tags": []
   },
   "source": [
    "```python\n",
    "pl_compatible_data_loader = train_ds.limit(128).iter_torch_batches(batch_size=8)\n",
    "storage_path = \"/mnt/local_storage/lightning/stable-diffusion-pretraining/\"\n",
    "lightning_training_loop(train_loader=pl_compatible_data_loader, storage_path=storage_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba4283a-bf1e-4337-b642-20634d5aa49c",
   "metadata": {},
   "source": [
    "We can run this on a worker with a GPU using Ray Core, just to check it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f673dd66-98e4-47b6-9293-f24a19cd899d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus=1)\n",
    "def demo_train():\n",
    "    pl_compatible_data_loader = train_ds.limit(128).iter_torch_batches(batch_size=8)\n",
    "    return lightning_training_loop(train_loader=pl_compatible_data_loader, storage_path='/mnt/cluster_storage/demo_single_node_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0835d2b-c840-46d9-b6c3-1105ac77b8d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.get(demo_train.remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770a09b6",
   "metadata": {},
   "source": [
    "Let's inspect the storage path to see what files were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70a3ac9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls /mnt/cluster_storage/demo_single_node_train --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2c2063-8988-4687-b1d4-b24e0a5a3d66",
   "metadata": {},
   "source": [
    "# 4. Migrate the training loop to Ray Train\n",
    "\n",
    "Let's start by migrating the training loop to Ray Train to achieve distributed data parallel training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49711c5",
   "metadata": {},
   "source": [
    "### Distributed Data Parallel Training\n",
    "Here is a diagram showing the standard distributed data parallel training loop.\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-ai-libraries/diagrams/multi_gpu_pytorch_v4.png\" width=800>\n",
    "\n",
    "Note how the model state is initially synchronized across all the GPUs before the training loop begins.\n",
    "\n",
    "Then after each backward pass, the gradients are synchronized across all the GPUs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9f8204",
   "metadata": {},
   "source": [
    "### Ray Train Migration\n",
    "\n",
    "Here are the changes we need to make to the training loop to migrate it to Ray Train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b42be9f-fad5-4f3f-8a1c-5812c5573eca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop_per_worker(\n",
    "    config: dict, # Update the function signature to comply with Ray Train\n",
    "):  \n",
    "    # Prepare data loaders using Ray\n",
    "    train_ds = ray.train.get_dataset_shard(\"train\")\n",
    "    train_dataloader = train_ds.iter_torch_batches(\n",
    "        batch_size=config[\"batch_size_per_worker\"],\n",
    "        drop_last=True,\n",
    "        prefetch_batches=config[\"prefetch_batches\"],\n",
    "    )\n",
    "\n",
    "    # Same model initialization as vanilla lightning\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "    model = StableDiffusion(\n",
    "        lr=config[\"lr\"],\n",
    "        resolution=config[\"resolution\"],\n",
    "        weight_decay=config[\"weight_decay\"],\n",
    "        num_warmup_steps=config[\"num_warmup_steps\"],\n",
    "        model_name=config[\"model_name\"],\n",
    "    )\n",
    "\n",
    "    # Same trainer setup as vanilla lightning except we add Ray Train specific arguments\n",
    "    trainer = pl.Trainer(\n",
    "        max_steps=config[\"max_steps\"],\n",
    "        max_epochs=config[\"max_epochs\"],\n",
    "        accelerator=\"gpu\",\n",
    "        precision=\"bf16-mixed\",\n",
    "        devices=\"auto\",  # Set devices to \"auto\" to use all available GPUs\n",
    "        strategy=RayDDPStrategy(),  # Use RayDDPStrategy for distributed data parallel training\n",
    "        plugins=[\n",
    "            RayLightningEnvironment()\n",
    "        ],  # Use RayLightningEnvironment to run the Lightning Trainer\n",
    "        callbacks=[\n",
    "            RayTrainReportCallback()\n",
    "        ],  # Use RayTrainReportCallback to report metrics and checkpoints\n",
    "        enable_checkpointing=False,  # Disable lightning checkpointing\n",
    "    )\n",
    "\n",
    "    # 4. Same as vanilla lightning\n",
    "    trainer.fit(model, train_dataloaders=train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5328b5",
   "metadata": {},
   "source": [
    "Here is the same diagram as before but with the Ray Train specific components highlighted.\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-ai-libraries/diagrams/multi_gpu_lightning_annotated_v5.png\" width=800>\n",
    "\n",
    "We made use of:\n",
    "- `ray.train.get_dataset_shard(\"train\")` to get the training dataset shard.\n",
    "- `RayDDPStrategy` to perform distributed data parallel training.\n",
    "- `RayLightningEnvironment` to run the Lightning Trainer.\n",
    "- `RayTrainReportCallback` to report metrics and checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242d8ccb-30c2-4491-a381-63ac3330bc2e",
   "metadata": {},
   "source": [
    "## 5. Create and fit a Ray Train TorchTrainer\n",
    "\n",
    "Let's first specify the scaling configuration to tell Ray Train to use 2 GPU training workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86459a71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaling_config = ray.train.ScalingConfig(num_workers=2, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b32a89",
   "metadata": {},
   "source": [
    "We then specify the run configuration to tell Ray Train where to store the checkpoints and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f441b156",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "storage_path = \"/mnt/cluster_storage/\"\n",
    "experiment_name = \"stable-diffusion-pretraining\"\n",
    "\n",
    "run_config = ray.train.RunConfig(name=experiment_name, storage_path=storage_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1d2f69",
   "metadata": {},
   "source": [
    "Now we can create our Ray Train `TorchTrainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052fa684",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loop_config = {\n",
    "    \"batch_size_per_worker\": 8,\n",
    "    \"prefetch_batches\": 2,\n",
    "    \"lr\": 0.0001,\n",
    "    \"num_warmup_steps\": 10_000,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"max_steps\": 550_000,\n",
    "    \"max_epochs\": 1,\n",
    "    \"resolution\": 256,\n",
    "    \"model_name\": \"stabilityai/stable-diffusion-2-base\",\n",
    "}\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker,\n",
    "    train_loop_config=train_loop_config,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    "    datasets=ray_datasets,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dac0bcd",
   "metadata": {},
   "source": [
    "Here is a high-level architecture of how Ray Train works:\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/overview.png\" width=600>\n",
    "\n",
    "Here are some key points:\n",
    "- The scaling config specifies the number of training workers.\n",
    "- A trainer actor process is launched that oversees the training workers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81045b35",
   "metadata": {},
   "source": [
    "We call `.fit()` to start the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc0bc5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = trainer.fit()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396caa4a",
   "metadata": {},
   "source": [
    "## Clean up \n",
    "\n",
    "Let's clean up the storage path to remove the checkpoints and artifacts we created during this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373ad2bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf /mnt/cluster_storage/stable-diffusion-pretraining"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
