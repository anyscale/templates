{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Train Observability\n",
    "\n",
    "This notebook will walk you through the different observability features in Ray Train. We will cover the following topics:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b> Here is the roadmap for this notebook:</b>\n",
    "\n",
    "<ul>\n",
    "    <li><b>Part 1:</b> Starting with a sample distributed training loop</li>\n",
    "    <li><b>Part 2:</b> Using the Ray dashboard</li>\n",
    "    <li><b>Part 3:</b> Monitoring throughput</li>\n",
    "    <li><b>Part 4:</b> Profiling the training loop \n",
    "        <ul>\n",
    "            <li><b>Part 4.1:</b> Operator view</li>\n",
    "            <li><b>Part 4.2:</b> Trace view</li>\n",
    "            <li><b>Part 4.2:</b> Memory view</li>\n",
    "            <li><b>Part 4.2:</b> Kernel view</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Part 5:</b> Adding Ray Data to the mix</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os   \n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchmetrics\n",
    "from PIL import Image\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import VisionTransformer\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "\n",
    "import ray\n",
    "from ray.train import ScalingConfig, RunConfig\n",
    "from ray.train.torch import TorchTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Starting with a sample distributed training loop\n",
    "\n",
    "Below is a sample distributed training loop of Ray Train and PyTorch. We will use this training loop to demonstrate the different observability features in Ray Train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop_ray_train(config: dict):  # pass in hyperparameters in config\n",
    "    criterion = CrossEntropyLoss()\n",
    "    # Use Ray Train to wrap the model with DistributedDataParallel\n",
    "    model = load_model_ray_train()\n",
    "    optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    # Calculate the batch size for each worker\n",
    "    global_batch_size = config[\"global_batch_size\"]\n",
    "    batch_size = global_batch_size // ray.train.get_context().get_world_size()\n",
    "    # Use Ray Train to wrap the data loader as a DistributedSampler\n",
    "    data_loader = build_data_loader_ray_train(batch_size=batch_size) \n",
    "    \n",
    "    acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(model.device)\n",
    "\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        # Ensure data is on the correct device\n",
    "        data_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        for images, labels in data_loader: # images, labels are now sharded across the workers\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() # gradients are now accumulated across the workers\n",
    "            optimizer.step()\n",
    "            acc(outputs, labels)\n",
    "\n",
    "        accuracy = acc.compute() # accuracy is now aggregated across the workers\n",
    "\n",
    "        # Use Ray Train to report metrics\n",
    "        metrics = print_metrics_ray_train(loss, accuracy, epoch)\n",
    "\n",
    "        # Use Ray Train to save checkpoint and metrics\n",
    "        save_checkpoint_and_metrics_ray_train(model, metrics)\n",
    "        acc.reset() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**On aggregating evaluation metrics from different workers**\n",
    "\n",
    "Ray Train natively supports [TorchMetrics](https://lightning.ai/docs/torchmetrics/stable/), which provides a collection of machine learning metrics for distributed, scalable PyTorch models.\n",
    "\n",
    "Torchmetrics follows these three steps:\n",
    "1. Initialize\n",
    "2. Compute\n",
    "3. Reset\n",
    "\n",
    "Where Compute performs a distributed gathering of individual metrics from the training workers.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to build and prepare the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_visual_transformer():\n",
    "    model = VisionTransformer(\n",
    "        image_size=32,   # CIFAR-10 image size is 32x32\n",
    "        patch_size=4,    # Patch size is 4x4\n",
    "        num_layers=12,   # Number of transformer layers\n",
    "        num_heads=8,     # Number of attention heads\n",
    "        hidden_dim=384,  # Hidden size (can be adjusted)\n",
    "        mlp_dim=768,     # MLP dimension (can be adjusted)\n",
    "        num_classes=10   # CIFAR-10 has 10 classes\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def load_model_ray_train() -> torch.nn.Module:\n",
    "    model = build_visual_transformer()\n",
    "    model = ray.train.torch.prepare_model(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to build and prepare the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_data_loader_ray_train(batch_size: int) -> DataLoader:\n",
    "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "    train_data = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=2)\n",
    "\n",
    "    # Add DistributedSampler to the DataLoader\n",
    "    train_loader = ray.train.torch.prepare_data_loader(train_loader)\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple function to print the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_metrics_ray_train(\n",
    "    loss: torch.Tensor, accuracy: torch.Tensor, epoch: int\n",
    ") -> None:\n",
    "    metrics = {\"loss\": loss.item(), \"accuracy\": accuracy.item(), \"epoch\": epoch}\n",
    "    if ray.train.get_context().get_world_rank() == 0:\n",
    "        print(metrics)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing metrics and checkpoints using Ray Train's reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_checkpoint_and_metrics_ray_train(\n",
    "    model: torch.nn.Module, metrics: dict[str, float]\n",
    ") -> None:\n",
    "    with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "        checkpoint = None\n",
    "        if ray.train.get_context().get_world_rank() == 0:\n",
    "            torch.save(\n",
    "                model.module.state_dict(), os.path.join(temp_checkpoint_dir, \"model.pt\")\n",
    "            )\n",
    "            checkpoint = ray.train.Checkpoint.from_directory(temp_checkpoint_dir)\n",
    "\n",
    "        ray.train.report(\n",
    "            metrics,\n",
    "            checkpoint=checkpoint,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the scaling config and run config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaling_config = ScalingConfig(num_workers=2, use_gpu=True)\n",
    "\n",
    "storage_path = \"/mnt/cluster_storage/distributed-training/\"\n",
    "run_config = RunConfig(storage_path=storage_path, name=\"distributed-cifar-vit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now launch a distributed training job with a `TorchTrainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = TorchTrainer(\n",
    "    train_loop_ray_train,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    "    train_loop_config={\"num_epochs\": 3, \"global_batch_size\": 512},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `trainer.fit()` will start the run and block until it completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using the Ray Dashboard\n",
    "\n",
    "We can use the Ray Dashboard to monitor the performance of the training job. The Ray Dashboard provides a lot of useful information about the cluster and the running tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray Dashboard - metrics based monitoring\n",
    "\n",
    "1. Look at overall metrics for the cluster\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/apple/cluster_util.png\" width=400>\n",
    "\n",
    "2. Inspect GPU utilization\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/apple/gpu_util_and_disk.png\" width=400>\n",
    "\n",
    "Note: we also show the disk IO where the data is being downloaded locally to the worker nodes prior to training.\n",
    "\n",
    "3. Inspect Cluster Network IO \n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/apple/network_io.png\" width=400>\n",
    "\n",
    "4. Inspection of  GPU memory usage and auxiliary resoures (CPU and Memory usage of the cluster)\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/apple/gpu_gram.png\" width=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Ray Train Dashboard - training-specific monitoring and debugging\n",
    "\n",
    "Here is the Train Dashboard overview page:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/apple/train_dashboard.png\" width=800>\n",
    "\n",
    "Here is a usual workflow for debugging a training job:\n",
    "\n",
    "1. View the Training worker Ray actor\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/apple/train_dashboard_worker.png\" width=700>\n",
    "\n",
    "2. Inspect the Stack trace of each training worker\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/apple/train_dashboard_stack_trace.png\" width=700>\n",
    "\n",
    "3. Perform CPU profiling of the training worker\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/apple/train_dashboard_cpu_profile.png\" width=700>\n",
    "\n",
    "4. Perform memory profiling of the training worker\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/apple/train_dashboard_memory_profile.png\" width=700>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Monitoring throughput\n",
    "\n",
    "It is important to monitor the throughput of the training job to ensure that the model is training at the desired speed.\n",
    "\n",
    "You can do so either:\n",
    "- By counting the number of rows and dividing by the total time of a training step or epoch.\n",
    "- By using a higher-level API like Pytorch Lightning's [`ThroughputMonitor`](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.ThroughputMonitor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop_ray_train_monitored(config: dict):\n",
    "    criterion = CrossEntropyLoss()\n",
    "    model = load_model_ray_train()\n",
    "    optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    global_batch_size = config[\"global_batch_size\"]\n",
    "    batch_size = global_batch_size // ray.train.get_context().get_world_size()\n",
    "\n",
    "    data_loader = build_data_loader_ray_train(batch_size=batch_size)\n",
    "\n",
    "    acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(model.device)\n",
    "\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        epoch_start_time = time.perf_counter()\n",
    "        num_rows = 0\n",
    "        num_steps = 0\n",
    "        data_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            acc(outputs, labels)\n",
    "            num_rows += images.size(0)\n",
    "            num_steps += 1\n",
    "\n",
    "        accuracy = acc.compute()\n",
    "\n",
    "        # ensure all relevant CUDA operations are complete before measuring time\n",
    "        torch.cuda.synchronize()\n",
    "        epoch_end_time = time.perf_counter()\n",
    "        epoch_duration = epoch_end_time - epoch_start_time\n",
    "\n",
    "        print(f\"Epoch {epoch} completed in {epoch_duration:.2f} seconds and {num_steps} steps\")\n",
    "\n",
    "        worker_throughput = num_rows / epoch_duration\n",
    "        print(f\"Worker throughput: {worker_throughput:.2f} rows/sec\")\n",
    "\n",
    "        num_workers = ray.train.get_context().get_world_size()\n",
    "        global_throughput = worker_throughput * num_workers\n",
    "        print(f\"Global throughput: {global_throughput:.2f} rows/sec\")\n",
    "\n",
    "        metrics = print_metrics_ray_train(loss, accuracy, epoch)\n",
    "\n",
    "        save_checkpoint_and_metrics_ray_train(model, metrics)\n",
    "        acc.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can proceed to run the training loop with throughput monitoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TorchTrainer(\n",
    "    train_loop_ray_train_monitored,\n",
    "    scaling_config=ScalingConfig(num_workers=2, use_gpu=True),\n",
    "    run_config=RunConfig(storage_path=storage_path, name=\"distributed-cifar-vit-monitored\"),\n",
    "    train_loop_config={\n",
    "        \"num_epochs\": 3,\n",
    "        \"global_batch_size\": 512,\n",
    "    },\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-run with four workers and check how throughput is scaling"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "TorchTrainer(\n",
    "    train_loop_ray_train_monitored,\n",
    "    scaling_config=ScalingConfig(num_workers=4, use_gpu=True),\n",
    "    run_config=RunConfig(storage_path=storage_path, name=\"distributed-cifar-vit-monitored-scaled\"),\n",
    "    train_loop_config={\n",
    "        \"num_epochs\": 3,\n",
    "        \"global_batch_size\": 512,\n",
    "    },\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Important Note on Distributed Training Performance:**\n",
    "\n",
    "1. **Throughput Scaling:**\n",
    "   - We observed that the global throughput scales linearly with the number of workers\n",
    "   - For example, doubling the number of workers (from 2 to 4) resulted in approximately 2x throughput\n",
    "   - This linear scaling is the ideal behavior for distributed training\n",
    "\n",
    "2. **Hyperparameter Considerations:**\n",
    "   - When increasing the number of workers, one options is to increase the effective global batch size. This helps maximize the utilization of the GPU.\n",
    "   - However, larger batch sizes typically require adjustments to hyperparameters, particularly:\n",
    "     - Learning rate\n",
    "     - Other training parameters\n",
    "   - This is because the optimal hyperparameter values are sensitive to batch size changes\n",
    "\n",
    "For detailed guidance on batch size selection and hyperparameter tuning, refer to the [Google Research Tuning Playbook](https://github.com/google-research/tuning_playbook?tab=readme-ov-file#choosing-the-batch-size).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Profiling the training loop with torch.profiler\n",
    "\n",
    "PyTorch includes a simple profiler API that is useful when user needs to determine the most expensive operators in the model.\n",
    "\n",
    "Here is an example of how to use the profiler to profile your Ray Train training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop_ray_train_profiled(config: dict):\n",
    "    criterion = CrossEntropyLoss()\n",
    "    model = load_model_ray_train()\n",
    "    optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    global_batch_size = config[\"global_batch_size\"]\n",
    "    batch_size = global_batch_size // ray.train.get_context().get_world_size()\n",
    "    data_loader = build_data_loader_ray_train(batch_size=batch_size)\n",
    "\n",
    "    acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(model.device)\n",
    "    world_rank = ray.train.get_context().get_world_rank()\n",
    "\n",
    "    wait = 10\n",
    "    warmup = 1\n",
    "    active = 2\n",
    "    repeat = 1\n",
    "\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        data_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        with torch.profiler.profile(\n",
    "            activities=[\n",
    "                torch.profiler.ProfilerActivity.CPU,\n",
    "                torch.profiler.ProfilerActivity.CUDA,\n",
    "            ],\n",
    "            schedule=torch.profiler.schedule(\n",
    "                wait=wait, warmup=warmup, active=active, repeat=repeat\n",
    "            ),\n",
    "            on_trace_ready=torch.profiler.tensorboard_trace_handler(\n",
    "                \"/mnt/cluster_storage/vit/distributed-cifar-vit-profiled\",\n",
    "                worker_name=f\"rank={world_rank}\",\n",
    "            ),\n",
    "            record_shapes=True,\n",
    "            with_stack=True,\n",
    "            profile_memory=True,\n",
    "        ) as profiler:\n",
    "            for step, (images, labels) in enumerate(data_loader):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                profiler.step()  # Add this line to profile the training loop\n",
    "                acc(outputs, labels)\n",
    "\n",
    "                if step >= wait + warmup + active:\n",
    "                    # no need to profile further\n",
    "                    break\n",
    "\n",
    "        # in case we want the memory timeline as well\n",
    "        profiler.export_memory_timeline(\n",
    "            f\"/mnt/cluster_storage/vit/distributed-cifar-vit-profiled/memory_{world_rank}.html\"\n",
    "        )\n",
    "\n",
    "        accuracy = acc.compute()\n",
    "        metrics = print_metrics_ray_train(loss, accuracy, epoch)\n",
    "        save_checkpoint_and_metrics_ray_train(model, metrics)\n",
    "        acc.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-run the training loop with the profiler and inspect the generated traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = TorchTrainer(\n",
    "    train_loop_ray_train_profiled,\n",
    "    scaling_config=ScalingConfig(num_workers=2, use_gpu=True),\n",
    "    run_config=RunConfig(\n",
    "        storage_path=storage_path, name=\"distributed-cifar-vit-profiled\"\n",
    "    ),\n",
    "    train_loop_config={\"num_epochs\": 1, \"global_batch_size\": 512},\n",
    ")\n",
    "\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls /mnt/cluster_storage/vit/distributed-cifar-vit-profiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and paste the following command in a terminal to start TensorBoard\n",
    "# tensorboard --logdir /mnt/cluster_storage/vit/distributed-cifar-vit-profiled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch-Profiler Output\n",
    "\n",
    "Let's take a look at the different views available in the Pytorch tensorboard profile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High-level overview\n",
    "You should be able to view a high-level overview like this:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/apple/tensorboard_summary.png\" width=900>\n",
    "\n",
    "- The \"GPU Summary\" panel shows the GPU configuration and GPU usage metrics (Utilization, SM Efficiency, and Achieved Occupancy).\n",
    "- The \"Step breakdown\" shows the distribution of time spent in each step over different categories of execution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operator view\n",
    "\n",
    "The Operator view shows the time spent in each operator:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-train-deep-dive/operator-view-v2.png\" width=900>\n",
    "\n",
    "Note: The \"Self\" duration does not include child operators' time. Whereas the \"Total\" duration includes child operators' time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trace view\n",
    "The Trace view shows graphs of time spent in both CPU threads and GPU streams:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/apple/tensorboard_trace_torch_loader.png\" width=900>\n",
    "\n",
    "Note in the above sample trace, we can see the GPU idling while waiting for the data to be loaded.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel view\n",
    "\n",
    "The GPU kernel view shows all kernels time spent on GPU.\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-train-deep-dive/kernel-view.png\" width=900>\n",
    "\n",
    "- Tensor Cores Used: Whether this kernel uses Tensor Cores.\n",
    "- \"Mean Blocks per SM\" = `Blocks of this kernel / SM number of this GPU`. If this number is less than 1, it indicates the GPU multiprocessors are not fully utilized. \"Mean Blocks per SM\" is weighted average of all runs of this kernel name, using each run's duration as weight.\n",
    "- \"Mean Est. Achieved Occupancy\": For most cases such as memory bandwidth bounded kernels, the higher the better. \"Mean Est. Achieved Occupancy\" is a weighted average of all runs of a given kernel, using each run's duration as weight.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory views\n",
    "\n",
    "You can also view the memory timeline of the training job either as a standalone HTML file or through the PyTorch tensorboard profile.\n",
    "\n",
    "Below is an example memory timeline as a standalone HTML file:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-train-deep-dive/torch-profile-memory-html-view.png\" width=900>\n",
    "\n",
    "It shows the memory usage of different components:\n",
    "- Parameters\n",
    "- Gradients\n",
    "- Optimizer states\n",
    "- Activations\n",
    "- Other\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adding Ray Data to the mix\n",
    "\n",
    "Instead of using torch data loaders, we can use Ray Data to load and preprocess the data in a distributed manner. This can be done by using the `ray.data` API to load the data and then use the `iter_torch_batches` function to build a torch compatible data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = CIFAR10(root=\"./data\", train=True, download=True)\n",
    "df = pd.DataFrame({\"image\": dataset.data.tolist(), \"label\": dataset.targets})\n",
    "df.to_parquet(\"/mnt/cluster_storage/cifar10.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code to define the Ray Data pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = ray.data.read_parquet(\"/mnt/cluster_storage/cifar10.parquet\")\n",
    "\n",
    "def transform_images(row: dict):\n",
    "    # Define the torchvision transform.\n",
    "    transform = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    image_arr = np.array(row[\"image\"], dtype=np.uint8)\n",
    "    row[\"image\"] = transform(Image.fromarray(image_arr))\n",
    "    return row\n",
    "\n",
    "train_ds = train_ds.map(transform_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the updated training loop with Ray Data and the torch profiler: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop_ray_train_ray_data(config: dict):\n",
    "    # Same initialization as before\n",
    "    criterion = CrossEntropyLoss()\n",
    "    model = load_model_ray_train()\n",
    "    optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # This time we use Ray Train's integration with Ray Data to load the data\n",
    "    global_batch_size = config[\"global_batch_size\"]\n",
    "    batch_size = global_batch_size // ray.train.get_context().get_world_size()\n",
    "    data_loader = build_data_loader_ray_train_ray_data(batch_size=batch_size)\n",
    "\n",
    "    acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(model.device)\n",
    "\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        with torch.profiler.profile(\n",
    "            activities=[\n",
    "                torch.profiler.ProfilerActivity.CPU,\n",
    "                torch.profiler.ProfilerActivity.CUDA,\n",
    "            ],\n",
    "            schedule=torch.profiler.schedule(wait=10, warmup=1, active=2, repeat=1),\n",
    "            on_trace_ready=torch.profiler.tensorboard_trace_handler(\n",
    "                \"/mnt/cluster_storage/vit/ray_train_and_data/\",\n",
    "                worker_name=f\"rank={ray.train.get_context().get_world_rank()}\",\n",
    "            ),\n",
    "            with_stack=False,\n",
    "        ) as profiler:\n",
    "            for batch in data_loader:\n",
    "                outputs = model(batch[\"image\"])\n",
    "                loss = criterion(outputs, batch[\"label\"])\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                profiler.step()\n",
    "                acc(outputs, batch[\"label\"])\n",
    "\n",
    "        accuracy = acc.compute()\n",
    "\n",
    "        metrics = print_metrics_ray_train(loss, accuracy, epoch)\n",
    "        save_checkpoint_and_metrics_ray_train(model, metrics)\n",
    "        acc.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to build the data loader using Ray Data. Note we are prefetching 4 batches to keep the GPU saturated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_data_loader_ray_train_ray_data(\n",
    "    batch_size: int, prefetch_batches: int = 4\n",
    ") -> DataLoader:\n",
    "    dataset_iterator = ray.train.get_dataset_shard(\"train\")\n",
    "    data_loader = dataset_iterator.iter_torch_batches(\n",
    "        batch_size=batch_size, prefetch_batches=prefetch_batches\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we define the TorchTrainer and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasets = {\"train\": train_ds}\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_ray_train_ray_data,\n",
    "    train_loop_config={\"num_epochs\": 1, \"global_batch_size\": 512},\n",
    "    scaling_config=ScalingConfig(num_workers=2, use_gpu=True),\n",
    "    run_config=RunConfig(storage_path=storage_path, name=\"dist-cifar-vit-ray-data\"),\n",
    "    datasets=datasets,\n",
    ")\n",
    "\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the generated traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -lla /mnt/cluster_storage/vit/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run tensorboard to visualize the profiling data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and paste the following command in a terminal to start TensorBoard\n",
    "# tensorboard --logdir /mnt/cluster_storage/vit/ray_train_and_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can inspect the trace in TensorBoard.\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/apple/tensorboard_trace_data.png\" width=900>\n",
    "\n",
    "\n",
    "Note, in the above trace, we can see the GPU idling on ingest has resolved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orphan": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
