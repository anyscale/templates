{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Unstructured Data Ingestion and Processing With Ray Data\n",
        "\n",
        "**Time to complete**: 35 min | **Difficulty**: Advanced | **Prerequisites**: Data engineering experience, document processing, basic NLP knowledge\n",
        "\n",
        "## What you'll build\n",
        "\n",
        "Build a comprehensive document ingestion pipeline that transforms unstructured documents from data lakes into structured, analytics-ready datasets using Ray Data's distributed processing capabilities for enterprise data warehouse workflows.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. [Data Lake Document Discovery](#step-1-data-lake-document-discovery) (8 min)\n",
        "2. [Document Processing and Classification](#step-2-document-processing-and-classification) (10 min)\n",
        "3. [Text Extraction and Enrichment](#step-3-text-extraction-and-enrichment) (8 min)\n",
        "4. [LLM-Powered Content Analysis](#step-4-llm-powered-content-analysis) (6 min)\n",
        "5. [Data Warehouse Output](#step-5-data-warehouse-output) (3 min)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n",
        "\n",
        "**Why unstructured data ingestion matters**: Enterprise data lakes contain vast amounts of unstructured documents (PDFs, Word docs, presentations, reports) that need systematic processing to extract business value for analytics and reporting.\n",
        "\n",
        "**Ray Data's ingestion capabilities**: Distribute document processing across clusters to handle large-scale document collections, extract structured data, and prepare analytics-ready datasets for data warehouse consumption.\n",
        "\n",
        "**Data lake to warehouse patterns**: Techniques used by data engineering teams to systematically process document collections, extract structured information, and create queryable datasets for business intelligence.\n",
        "\n",
        "**Production ingestion workflows**: Scalable document processing patterns that handle diverse file formats, extract metadata, and create structured schemas for downstream analytics systems.\n",
        "\n",
        "**LLM integration strategies**: Document processing workflows that can use advanced analysis for content extraction from unstructured text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "**Challenge**: Enterprise data lakes contain millions of unstructured documents (PDFs, Word docs, presentations) across multiple formats that need systematic processing to extract business value. Traditional document processing approaches struggle with:\n",
        "- **Scale**: Single-machine processing limits document volume\n",
        "- **Consistency**: Manual extraction creates inconsistent schemas  \n",
        "- **Integration**: Complex infrastructure for analysis\n",
        "- **Warehouse integration**: Manual data modeling and ETL processes\n",
        "\n",
        "**Solution**: Ray Data enables end-to-end document ingestion pipelines:\n",
        "\n",
        "| Pipeline Stage | Traditional Approach | Ray Data Approach | Benefit |\n",
        "|------------------|-----------------------|---------------------|-----------|\n",
        "| **Document Discovery** | Sequential file listing | Parallel `read_binary_files()` | Process millions of files |\n",
        "| **Text Extraction** | Single-threaded parsing | Distributed `map_batches()` | Extract from all docs simultaneously |\n",
        "| **Content Analysis** | Manual processing | Distributed analysis | Built-in batch processing |\n",
        "| **Data Warehouse** | Custom ETL scripts | Native `write_parquet()` with partitioning | Production-ready output |\n",
        "\n",
        "**Data Lake to Warehouse Flow**: This template demonstrates a complete pipeline from raw documents in data lakes to structured, queryable datasets ready for business intelligence and analytics workflows using Ray Data native operations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites Checklist\n",
        "\n",
        "Before starting, ensure you have:\n",
        "- [ ] Understanding of data lake and data warehouse concepts\n",
        "- [ ] Experience with document processing and text extraction\n",
        "- [ ] Knowledge of structured data formats (Parquet, Delta Lake, Iceberg)\n",
        "- [ ] Python environment with Ray Data and document processing libraries\n",
        "- [ ] Access to S3 or other cloud storage for document sources\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick start (3 minutes)\n",
        "\n",
        "This section demonstrates large-scale document ingestion using Ray Data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import logging\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ray\n",
        "\n",
        "# Configure Ray Data \n",
        "ctx = ray.data.DataContext.get_current()\n",
        "ctx.enable_progress_bars = False\n",
        "ctx.enable_operator_progress_bars = False\n",
        "\n",
        "# Initialize Ray for distributed processing\n",
        "ray.init(ignore_reinit_error=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Data Lake Document Discovery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Discover document collections in data lake\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Load document collection from data lake\n",
        "document_collection = ray.data.read_binary_files(\n",
        "    \"s3://anyscale-rag-application/1000-docs/\",\n",
        "    include_paths=True,\n",
        "    ray_remote_args={\"num_cpus\":0.025}  # High I/O concurrency for large document collections\n",
        ").limit(100)\n",
        "\n",
        "print(f\"Dataset schema: {document_collection.schema()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Document metadata extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_file(record: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Extract text content from document files.\n",
        "    \n",
        "    Processes the bytes field immediately to avoid passing large binary data\n",
        "    through multiple Ray Data operations. Returns basic file metadata and\n",
        "    extracted text.\n",
        "    \"\"\"\n",
        "    import io\n",
        "    from pathlib import Path\n",
        "    from unstructured.partition.auto import partition\n",
        "    \n",
        "    file_path = Path(record[\"path\"])\n",
        "    file_bytes = record[\"bytes\"]\n",
        "    file_size = len(file_bytes)\n",
        "    file_extension = file_path.suffix.lower()\n",
        "    file_name = file_path.name\n",
        "    \n",
        "    # Only process supported file extensions\n",
        "    supported_extensions = {\".pdf\", \".docx\", \".doc\", \".pptx\", \".ppt\", \".html\", \".txt\"}\n",
        "    \n",
        "    if file_extension not in supported_extensions:\n",
        "        return {\n",
        "            \"document_id\": str(uuid.uuid4()),\n",
        "            \"file_path\": str(file_path),\n",
        "            \"file_name\": file_name,\n",
        "            \"file_extension\": file_extension,\n",
        "            \"file_size_bytes\": file_size,\n",
        "            \"file_size_mb\": round(file_size / (1024 * 1024), 2),\n",
        "            \"discovery_timestamp\": datetime.now().isoformat(),\n",
        "            \"extracted_text\": \"\",\n",
        "            \"text_length\": 0,\n",
        "            \"word_count\": 0,\n",
        "            \"extraction_status\": \"unsupported_format\"\n",
        "        }\n",
        "    \n",
        "    try:\n",
        "        with io.BytesIO(file_bytes) as stream:\n",
        "            elements = partition(file=stream)\n",
        "            \n",
        "            # Combine all text elements\n",
        "            extracted_text = \" \".join([str(el) for el in elements]).strip()\n",
        "            text_length = len(extracted_text)\n",
        "            word_count = len(extracted_text.split()) if extracted_text else 0\n",
        "            extraction_status = \"success\"\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Cannot process file {file_path}: {e}\")\n",
        "        extracted_text = \"\"\n",
        "        text_length = 0\n",
        "        word_count = 0\n",
        "        extraction_status = f\"error: {str(e)[:100]}\"\n",
        "    \n",
        "    return {\n",
        "        \"document_id\": str(uuid.uuid4()),\n",
        "        \"file_path\": str(file_path),\n",
        "        \"file_name\": file_name,\n",
        "        \"file_extension\": file_extension,\n",
        "        \"file_size_bytes\": file_size,\n",
        "        \"file_size_mb\": round(file_size / (1024 * 1024), 2),\n",
        "        \"discovery_timestamp\": datetime.now().isoformat(),\n",
        "        \"extracted_text\": extracted_text,\n",
        "        \"text_length\": text_length,\n",
        "        \"word_count\": word_count,\n",
        "        \"extraction_status\": extraction_status\n",
        "    }\n",
        "\n",
        "# Apply text extraction\n",
        "print(\"Extracting text from documents...\")\n",
        "documents_with_text = document_collection.map(\n",
        "    process_file,\n",
        "    concurrency=8,\n",
        "    num_cpus=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "documents_with_text.limit(25).to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def enrich_business_metadata(record: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Classify documents by business category and assign processing priority.\n",
        "    \n",
        "    This is a separate stage that operates on already-extracted text,\n",
        "    performing pure metadata enrichment based on filename patterns.\n",
        "    \"\"\"\n",
        "    file_name = record[\"file_name\"]\n",
        "    filename_lower = file_name.lower()\n",
        "    file_size = record[\"file_size_bytes\"]\n",
        "    \n",
        "    # Business classification for data warehouse categorization\n",
        "    if any(keyword in filename_lower for keyword in [\"financial\", \"earnings\", \"revenue\", \"profit\"]):\n",
        "        doc_type = \"financial_document\"\n",
        "        business_category = \"finance\"\n",
        "    elif any(keyword in filename_lower for keyword in [\"legal\", \"contract\", \"agreement\", \"terms\"]):\n",
        "        doc_type = \"legal_document\"\n",
        "        business_category = \"legal\"\n",
        "    elif any(keyword in filename_lower for keyword in [\"regulatory\", \"compliance\", \"filing\", \"sec\"]):\n",
        "        doc_type = \"regulatory_document\"\n",
        "        business_category = \"compliance\"\n",
        "    elif any(keyword in filename_lower for keyword in [\"client\", \"customer\", \"portfolio\"]):\n",
        "        doc_type = \"client_document\"\n",
        "        business_category = \"client_services\"\n",
        "    elif any(keyword in filename_lower for keyword in [\"market\", \"research\", \"analysis\", \"report\"]):\n",
        "        doc_type = \"research_document\"\n",
        "        business_category = \"research\"\n",
        "    else:\n",
        "        doc_type = \"general_document\"\n",
        "        business_category = \"general\"\n",
        "    \n",
        "    # Processing priority for workflow optimization\n",
        "    if any(keyword in filename_lower for keyword in [\"urgent\", \"critical\", \"deadline\"]):\n",
        "        priority = \"high\"\n",
        "        priority_score = 3\n",
        "    elif any(keyword in filename_lower for keyword in [\"important\", \"quarterly\", \"annual\"]):\n",
        "        priority = \"medium\"\n",
        "        priority_score = 2\n",
        "    else:\n",
        "        priority = \"low\"\n",
        "        priority_score = 1\n",
        "    \n",
        "    return {\n",
        "        **record,\n",
        "        \"document_type\": doc_type,\n",
        "        \"business_category\": business_category,\n",
        "        \"processing_priority\": priority,\n",
        "        \"priority_score\": priority_score,\n",
        "        \"estimated_pages\": max(1, file_size // 50000),\n",
        "        \"processing_status\": \"classified\"\n",
        "    }\n",
        "\n",
        "\n",
        "# Apply business metadata enrichment\n",
        "print(\"\\nEnriching with business metadata...\")\n",
        "documents_with_metadata = documents_with_text.map(\n",
        "    enrich_business_metadata,\n",
        "    concurrency=10,\n",
        "    num_cpus=0.25\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "documents_with_metadata.limit(5).to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use Ray Data native operations for document collection analysis\n",
        "from ray.data.aggregate import Count, Sum, Mean, Max, Min\n",
        "\n",
        "print(\"Analyzing document collection using Ray Data native operations...\")\n",
        "\n",
        "# Document type distribution using native groupby\n",
        "doc_type_stats = documents_with_metadata.groupby(\"document_type\").aggregate(\n",
        "    Count(),\n",
        "    Sum(\"file_size_bytes\"),\n",
        "    Mean(\"file_size_mb\"),\n",
        "    Max(\"estimated_pages\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Business category analysis\n",
        "category_stats = documents_with_metadata.groupby(\"business_category\").aggregate(\n",
        "    Count(),\n",
        "    Mean(\"priority_score\"),\n",
        "    Sum(\"file_size_mb\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Document Processing and Classification\n",
        "\n",
        "###  Text extraction and quality assessment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ray.data.expressions import col, lit\n",
        "\n",
        "def assess_document_quality(record: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"Assess document quality for data warehouse ingestion.\"\"\"\n",
        "    \n",
        "    quality_score = 0\n",
        "    quality_issues = []\n",
        "    \n",
        "    if record[\"file_size_mb\"] > 0.01:\n",
        "        quality_score += 1\n",
        "    else:\n",
        "        quality_issues.append(\"file_too_small\")\n",
        "    \n",
        "    if record[\"text_length\"] > 100:\n",
        "        quality_score += 1\n",
        "    else:\n",
        "        quality_issues.append(\"insufficient_text\")\n",
        "    \n",
        "    if record[\"business_category\"] != \"general\":\n",
        "        quality_score += 1\n",
        "    else:\n",
        "        quality_issues.append(\"low_business_relevance\")\n",
        "    \n",
        "    if record[\"word_count\"] > 20:\n",
        "        quality_score += 1\n",
        "    else:\n",
        "        quality_issues.append(\"insufficient_content\")\n",
        "    \n",
        "    quality_rating = \"high\" if quality_score >= 4 else \"medium\" if quality_score >= 2 else \"low\"\n",
        "    \n",
        "    return {\n",
        "        **record,\n",
        "        \"quality_score\": quality_score,\n",
        "        \"quality_rating\": quality_rating,\n",
        "        \"quality_issues\": json.dumps(quality_issues)\n",
        "    }\n",
        "\n",
        "# Apply quality assessment (text extraction already done in previous step)\n",
        "quality_assessed_docs = documents_with_metadata.map_batches(\n",
        "    process_quality_assessment_batch,\n",
        "    num_cpus=0.25,\n",
        "    batch_size=2000\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Text Chunking and Enrichment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_text_chunks(record: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Create text chunks optimized for processing and analytics.\"\"\"\n",
        "    \n",
        "    text = record[\"extracted_text\"]\n",
        "    chunk_size = 1500\n",
        "    overlap = 150\n",
        "    \n",
        "    chunks = []\n",
        "    start = 0\n",
        "    chunk_index = 0\n",
        "    \n",
        "    while start < len(text):\n",
        "        end = min(start + chunk_size, len(text))\n",
        "        chunk_text = text[start:end]\n",
        "        \n",
        "        chunk_record = {\n",
        "            **record,\n",
        "            \"chunk_id\": str(uuid.uuid4()),\n",
        "            \"chunk_index\": chunk_index,\n",
        "            \"chunk_text\": chunk_text,\n",
        "            \"chunk_length\": len(chunk_text),\n",
        "            \"chunk_word_count\": len(chunk_text.split())\n",
        "        }\n",
        "        \n",
        "        chunks.append(chunk_record)\n",
        "        \n",
        "        # If we've reached the end of the text, stop\n",
        "        if end >= len(text):\n",
        "            break\n",
        "            \n",
        "        start = end - overlap\n",
        "        chunk_index += 1\n",
        "    \n",
        "    # Update total chunks\n",
        "    for chunk in chunks:\n",
        "        chunk[\"total_chunks\"] = len(chunks)\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "# Apply text chunking using Ray Data flat_map\n",
        "print(\"Creating text chunks...\")\n",
        "\n",
        "chunked_documents = quality_assessed_docs.flat_map(\n",
        "    create_text_chunks,\n",
        "    num_cpus=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Data Warehouse Schema and Output\n",
        "\n",
        "### Create data warehouse schema\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Apply warehouse schema transformation using expressions API\n",
        "print(\"Creating data warehouse schema...\")\n",
        "\n",
        "processing_date = datetime.now().isoformat()[:10]\n",
        "\n",
        "warehouse_dataset = chunked_documents.select_columns([\n",
        "    # Primary identifiers\n",
        "    \"document_id\",\n",
        "    \"chunk_id\",\n",
        "    \n",
        "    # Dimensional attributes\n",
        "    \"business_category\",\n",
        "    \"document_type\",\n",
        "    \"file_extension\",\n",
        "    \"quality_rating\",\n",
        "    \"processing_priority\",\n",
        "    \n",
        "    # Fact measures\n",
        "    \"file_size_mb\",\n",
        "    \"word_count\",\n",
        "    \"chunk_word_count\",\n",
        "    \"quality_score\",\n",
        "    \"priority_score\",\n",
        "    \"estimated_pages\",\n",
        "    \"chunk_index\",\n",
        "    \"total_chunks\",\n",
        "    \n",
        "    # Content fields\n",
        "    \"chunk_text\",\n",
        "    \"file_name\",\n",
        "    \"file_path\",\n",
        "    \n",
        "    # Existing metadata\n",
        "    \"discovery_timestamp\",\n",
        "    \"extraction_status\",\n",
        "    \"processing_status\"\n",
        "]).rename_columns({\n",
        "    \"chunk_text\": \"text_content\"\n",
        "}).add_column(\n",
        "    \"processing_date\", lambda df: processing_date\n",
        ").add_column(\n",
        "    \"pipeline_version\", lambda df: \"1.0\"\n",
        ").add_column(\n",
        "    \"processing_engine\", lambda df: \"ray_data\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Write to data warehouse with partitioning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write main warehouse table with partitioning\n",
        "print(\"Writing to data warehouse...\")\n",
        "\n",
        "OUTPUT_WAREHOUSE_PATH = \"/mnt/cluster_storage\"\n",
        "\n",
        "warehouse_dataset.write_parquet(\n",
        "    f\"{OUTPUT_WAREHOUSE_PATH}/main_table/\",\n",
        "    partition_cols=[\"business_category\", \"processing_date\"],\n",
        "    compression=\"snappy\",\n",
        "    ray_remote_args={\"num_cpus\":0.1}\n",
        ")\n",
        "\n",
        "print(\"Main warehouse table written successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Creating business-specific datasets...\")\n",
        "\n",
        "# Financial documents dataset\n",
        "financial_analytics = warehouse_dataset.filter(\n",
        "    expr=\"business_category == 'finance'\",\n",
        "    num_cpus=0.1\n",
        ").select_columns([\n",
        "    \"document_id\", \"chunk_id\", \"text_content\", \"summary\", \n",
        "    \"quality_score\", \"processing_date\", \"metrics_count\"\n",
        "])\n",
        "\n",
        "financial_analytics.write_parquet(\n",
        "    f\"{OUTPUT_WAREHOUSE_PATH}/analytics/financial/\",\n",
        "    partition_cols=[\"processing_date\"],\n",
        "    compression=\"snappy\",\n",
        "    ray_remote_args={\"num_cpus\":0.1}\n",
        ")\n",
        "\n",
        "# Compliance documents dataset\n",
        "compliance_analytics = warehouse_dataset.filter(\n",
        "   expr=\"business_category == 'compliance'\",\n",
        "    num_cpus=0.1\n",
        ").select_columns([\n",
        "    \"document_id\", \"chunk_id\", \"text_content\", \"summary\",\n",
        "    \"quality_score\", \"content_priority\", \"processing_date\"\n",
        "])\n",
        "\n",
        "compliance_analytics.write_parquet(\n",
        "    f\"{OUTPUT_WAREHOUSE_PATH}/analytics/compliance/\",\n",
        "    partition_cols=[\"processing_date\"],\n",
        "    compression=\"snappy\",\n",
        "    ray_remote_args={\"num_cpus\":0.1}\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create analytics summary tables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Creating analytics summary tables...\")\n",
        "\n",
        "# Processing metrics by category and date\n",
        "processing_metrics = warehouse_dataset.groupby([\"business_category\", \"processing_date\"]).aggregate(\n",
        "    Count(),\n",
        "    Sum(\"file_size_mb\"),\n",
        "    Mean(\"word_count\"),\n",
        "    Mean(\"quality_score\")\n",
        ")\n",
        "\n",
        "processing_metrics.write_parquet(\n",
        "    f\"{OUTPUT_WAREHOUSE_PATH}/summaries/processing_metrics/\",\n",
        "    partition_cols=[\"processing_date\"],\n",
        "    compression=\"snappy\",\n",
        "    ray_remote_args={\"num_cpus\":0.1}\n",
        ")\n",
        "\n",
        "# Quality distribution analysis\n",
        "quality_distribution = warehouse_dataset.groupby([\"quality_rating\", \"business_category\"]).aggregate(\n",
        "    Count(),\n",
        "    Mean(\"word_count\"),\n",
        "    Mean(\"entities_count\"),\n",
        "    Mean(\"metrics_count\")\n",
        ")\n",
        "\n",
        "quality_distribution.write_parquet(\n",
        "    f\"{OUTPUT_WAREHOUSE_PATH}/summaries/quality_distribution/\",\n",
        "    compression=\"snappy\",\n",
        "    ray_remote_args={\"num_cpus\":0.1}\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verification and Summary\n",
        "\n",
        "### Verify data warehouse outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify warehouse outputs\n",
        "print(\"Verifying data warehouse integration...\")\n",
        "\n",
        "# Read back main table\n",
        "main_table_verify = ray.data.read_parquet(\n",
        "    f\"{OUTPUT_WAREHOUSE_PATH}/main_table/\",\n",
        "    num_cpus=0.025\n",
        ")\n",
        "\n",
        "# Read back summary tables\n",
        "metrics_verify = ray.data.read_parquet(\n",
        "    f\"{OUTPUT_WAREHOUSE_PATH}/summaries/processing_metrics/\",\n",
        "    num_cpus=0.025\n",
        ")\n",
        "\n",
        "print(f\"Data warehouse verification:\")\n",
        "print(f\"  Main table records: {main_table_verify.count():,}\")\n",
        "print(f\"  Processing metrics: {metrics_verify.count():,}\")\n",
        "print(f\"  Schema compatibility: Verified\")\n",
        "\n",
        "# Display sample data\n",
        "print(\"\\\\nSample warehouse records:\")\n",
        "samples = main_table_verify.take(10)\n",
        "for i, record in enumerate(samples):\n",
        "    print(f\"  {i+1}. Doc: {record['document_id'][:8]}, Category: {record['business_category']}, \"\n",
        "          f\"Words: {record['word_count']}, Quality: {record['quality_rating']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "This notebook demonstrates a complete document ingestion pipeline using Ray Data:\n",
        "\n",
        "### Key Features Demonstrated\n",
        "\n",
        "**Ray Data Operations**:\n",
        "- `read_binary_files()` for large-scale document discovery\n",
        "- `map()` and `map_batches()` for distributed processing\n",
        "- `filter()` with expressions API for efficient filtering\n",
        "- `flat_map()` for text chunking\n",
        "- `groupby().aggregate()` for analytics\n",
        "- `write_parquet()` with partitioning for data warehouse output\n",
        "\n",
        "**CPU-Based Processing**:\n",
        "- Pattern matching for content analysis\n",
        "- No GPU requirements\n",
        "- Scalable across CPU-only clusters\n",
        "\n",
        "**Data Warehouse Integration**:\n",
        "- Partitioned tables for query optimization\n",
        "- Business-specific datasets\n",
        "- Summary tables for analytics\n",
        "- Schema standardization\n",
        "\n",
        "### Enabling GPU-Accelerated LLM Processing\n",
        "\n",
        "For GPU-accelerated content analysis with vLLM:\n",
        "\n",
        "1. Install Ray Data LLM package: `pip install -U vllm==0.7.2`\n",
        "2. Configure GPU resources in your cluster\n",
        "3. Replace the CPU-based analysis in Step 4 with:\n",
        "\n",
        "```python\n",
        "from ray.data.llm import vLLMEngineProcessorConfig, build_llm_processor\n",
        "\n",
        "llm_config = vLLMEngineProcessorConfig(\n",
        "    model_source=\"unsloth/Llama-3.1-8B-Instruct\",\n",
        "    engine_kwargs={\n",
        "        \"max_model_len\": 16384,\n",
        "        \"enable_chunked_prefill\": True,\n",
        "        \"max_num_batched_tokens\": 4096,\n",
        "        \"tensor_parallel_size\": 1,\n",
        "    },\n",
        "    concurrency=1,\n",
        "    batch_size=32,\n",
        "    accelerator_type=\"A10G\"\n",
        ")\n",
        "\n",
        "llm_processor = build_llm_processor(\n",
        "    llm_config,\n",
        "    preprocess=create_prompts,\n",
        "    postprocess=extract_structured_data\n",
        ")\n",
        "\n",
        "analyzed_docs = llm_processor(chunked_documents)\n",
        "```\n",
        "\n",
        "### Production Recommendations\n",
        "\n",
        "1. **Use real text extraction libraries**: PyPDF2, python-docx, python-pptx, BeautifulSoup\n",
        "2. **Tune batch sizes**: Adjust based on document size and cluster resources\n",
        "3. **Monitor progress**: Use Ray dashboard for performance visibility\n",
        "4. **Scale horizontally**: Add workers to increase throughput\n",
        "5. **Optimize partitioning**: Match partitioning strategy to query patterns\n",
        "\n",
        "This pipeline transforms unstructured documents from data lakes into structured, analytics-ready datasets for enterprise data warehouse consumption and business intelligence workflows.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
