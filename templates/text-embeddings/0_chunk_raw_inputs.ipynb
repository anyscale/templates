{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import numpy as np\n",
    "from time import time\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV file with a single column `files`, containing URLs to parquet files. \n",
    "INPUT_FILES_CSV = \"INPUT-FILE-PATHS.csv\"\n",
    "\n",
    "input_files = pd.read_csv(INPUT_FILES_CSV)['files'].tolist()\n",
    "len(input_files), input_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the read parallelism here, to avoid unnecessary excessive splitting.\n",
    "ds = ray.data.read_parquet(\n",
    "    input_files, \n",
    "    columns=['content', 'url', 'timestamp'],\n",
    "    parallelism=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main chunking logic\n",
    "chunk_size = 512\n",
    "words_to_tokens = 1.2\n",
    "num_tokens = int(chunk_size // words_to_tokens)\n",
    "\n",
    "num_words = lambda x: len(x.split())\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=num_tokens,\n",
    "    keep_separator=True, \n",
    "    length_function=num_words, \n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "def chunk(row):\n",
    "    length = num_words(row['content']) * 1.2\n",
    "    if length < 20 or length > 4000:\n",
    "        return []\n",
    "    chunks = splitter.split_text(row['content'])\n",
    "    document_id = str(uuid.uuid5(uuid.NAMESPACE_URL, row['url']))\n",
    "    rand_coeff = np.random.rand(1).astype(np.float32)\n",
    "    metadata = {\n",
    "        'source': row['url'],\n",
    "        'timestamp': row['timestamp'].timestamp(),\n",
    "        'document_id': document_id,\n",
    "        'rand_coeff': rand_coeff\n",
    "    }\n",
    "    \n",
    "    new_rows = [\n",
    "        {\n",
    "            'id': f\"{document_id}_{i}\",\n",
    "            'text': chunk,\n",
    "            'metadata': metadata\n",
    "        } \n",
    "        for i, chunk in enumerate(chunks)]\n",
    "\n",
    "    return new_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_context = ray.data.DataContext.get_current()\n",
    "\n",
    "# Allow for up to 1 write failure per block, since the job is long.\n",
    "data_context.max_errored_blocks = len(input_files)\n",
    "\n",
    "# Set the max target block size and batch size in the `map_batches` call below\n",
    "# in order to control the output file sizes in the hundreds of MBs.\n",
    "data_context.target_max_block_size = 400_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "chunked_ds = (\n",
    "    ds.flat_map(\n",
    "        chunk\n",
    "    ).map_batches(lambda x: x, batch_size=350_000)\n",
    ")\n",
    "\n",
    "# Write the chunked input files to cloud storage, \n",
    "# e.g. add an S3 path below.\n",
    "CHUNK_OUTPUT_PATH = \"YOUR-OUTPUT-BUCKET-HERE\"\n",
    "chunked_ds.write_parquet(CHUNK_OUTPUT_PATH)\n",
    "print(f\"===> Done writing chunked files to {CHUNK_OUTPUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray-build",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
