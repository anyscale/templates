{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "from optimum.bettertransformer import BetterTransformer\n",
    "\n",
    "ray.init(\n",
    "    runtime_env={\n",
    "        \"env_vars\": {\n",
    "          \"TOKENIZERS_PARALLELISM\": \"false\",\n",
    "          \"AIM_UI_TELEMETRY_ENABLED\": \"0\",\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedOptimum:\n",
    "    \"\"\"Main actor class used to generate embeddings.\"\"\"\n",
    "    def __init__(self, model_name=\"thenlper/gte-large\", chunk_size=512):\n",
    "        self.model_name = model_name\n",
    "        self.chunk_size = chunk_size\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(\"cuda\")\n",
    "        self.model = BetterTransformer.transform(self.model)\n",
    "        self.model = torch.compile(self.model)\n",
    "\n",
    "    def _average_pool(self, last_hidden_states, attention_mask):\n",
    "        last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        batch_text = batch[\"text\"].tolist()\n",
    "        # Directly return torch tensors.\n",
    "        tokenize_results = self.tokenizer(batch_text, max_length=self.chunk_size, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        # can use either torch.no_grad() or torch.inference_mode() here, not much difference in this case.\n",
    "        # this significantly reduces GRAM usage and prevents OOMing during embed model call.\n",
    "        with torch.no_grad():\n",
    "            model_input = {\n",
    "                \"input_ids\": tokenize_results[\"input_ids\"],\n",
    "                \"token_type_ids\": tokenize_results[\"token_type_ids\"],\n",
    "                \"attention_mask\": tokenize_results[\"attention_mask\"],\n",
    "            }\n",
    "            model_input = {k: v.to(\"cuda\") for k, v in model_input.items()}\n",
    "\n",
    "            outputs = self.model(**model_input)\n",
    "            embeddings = self._average_pool(outputs.last_hidden_state, model_input['attention_mask'])\n",
    "            embeddings = F.normalize(embeddings)\n",
    "\n",
    "            batch[\"values\"] = embeddings.detach().cpu().numpy().astype(np.float32)\n",
    "            return batch\n",
    "\n",
    "\n",
    "def flatten_metadata_col(row):\n",
    "    \"\"\"Helper function to \"pop out\" metadata key/value pairs into their own columns.\n",
    "    This allows us to use numpy as the default batch format instead of pandas,\n",
    "    which can be more memory demanding.\"\"\"\n",
    "    for k, v in row[\"metadata\"].items():\n",
    "        row[\"metadata_\" + k] = v\n",
    "    del row[\"metadata\"]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get chunked input files to read.\n",
    "\n",
    "# CSV file with a single column `files`, containing URLs to parquet files \n",
    "# containing chunked input data.\n",
    "# These are files generated from notebook `0_chunk_raw_inputs.ipynb`.\n",
    "chunked_file_paths = pd.read_csv(\"CHUNKED-FILES-PATHS.csv\")['files'].tolist()\n",
    "NUM_CHUNKED_FILES = len(chunked_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data._internal.execution.backpressure_policy import (\n",
    "    ENABLED_BACKPRESSURE_POLICIES_CONFIG_KEY,\n",
    "    ConcurrencyCapBackpressurePolicy,\n",
    "    StreamingOutputBackpressurePolicy,\n",
    ")\n",
    "\n",
    "# Number of (GPU) workers used to generate embeddings.\n",
    "embed_num_workers = 16\n",
    "\n",
    "# Concurrency cap to improve stability of long running job.\n",
    "# With 16 workers, use a max read parallelism of 200.\n",
    "# Scale accordingly based on number of GPU workers.\n",
    "embed_batch_size = 1000\n",
    "MAX_CONCURRENCY_PER_16_WORKERS = 200\n",
    "MAX_CONCURRENCY = max(\n",
    "    int(MAX_CONCURRENCY_PER_16_WORKERS * embed_num_workers / 16), \n",
    "    embed_num_workers,\n",
    ")\n",
    "\n",
    "configs = {\n",
    "    ENABLED_BACKPRESSURE_POLICIES_CONFIG_KEY: [\n",
    "        ConcurrencyCapBackpressurePolicy,\n",
    "        StreamingOutputBackpressurePolicy,\n",
    "    ],\n",
    "    ConcurrencyCapBackpressurePolicy.INIT_CAP_CONFIG_KEY: MAX_CONCURRENCY,\n",
    "    ConcurrencyCapBackpressurePolicy.CAP_MULTIPLIER_CONFIG_KEY: 1,\n",
    "}\n",
    "\n",
    "ctx = ray.data.DataContext.get_current()\n",
    "for k, v in configs.items():\n",
    "    ctx.set_config(k, v)\n",
    "# Set the max target block size and batch size in the `map_batches` call below\n",
    "# in order to control the output file sizes in the hundreds of MBs.\n",
    "ctx.target_max_block_size = 300_000_000\n",
    "\n",
    "# Allow for up to 1 write failure per block, since the job is long.\n",
    "ctx.max_errored_blocks = NUM_CHUNKED_FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Ray Data code.\n",
    "print(f\"===> Starting embedding for {len(chunked_file_paths)} chunked files\")\n",
    "\n",
    "start_t = time()\n",
    "chunked_ds_read = ray.data.read_parquet(chunked_file_paths)\n",
    "chunked_ds_read = chunked_ds_read.map(flatten_metadata_col)\n",
    "embedded_ds = (\n",
    "    chunked_ds_read.map_batches(\n",
    "        EmbedOptimum,\n",
    "        concurrency=embed_num_workers,\n",
    "        batch_size=embed_batch_size,\n",
    "        num_gpus=1,  # 1 GPU for each actor.\n",
    "        max_concurrency=2, # Reduce GPU idle time.\n",
    "    # This second map_batches call is used to control the output file sizes.\n",
    "    ).map_batches(lambda x: x, batch_size=10_000)\n",
    ")\n",
    "\n",
    "embeddings_output_path = \"YOUR-OUTPUT-BUCKET-HERE\"\n",
    "embedded_ds.write_parquet(embeddings_output_path)\n",
    "print(f\"===> Finished embedding {len(chunked_file_paths)} chunked files in {time() - start_t} s\")\n",
    "print(f\"===> Wrote output files to {embeddings_output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
