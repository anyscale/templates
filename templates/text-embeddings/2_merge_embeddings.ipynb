{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUN_ROWS_PER_ROW_GROUP = 100_000\n",
    "COLUMN_TO_CAST_FLOAT_LIST_TYPE = \"values\"\n",
    "\n",
    "# SPREAD strategy ensures that tasks are scheduled across cluster evenly.\n",
    "# Retry on `OSError` to avoid failure caused by S3 rate limiting/other related issues.\n",
    "@ray.remote(num_cpus=1, max_retries=20, scheduling_strategy=\"SPREAD\", retry_exceptions=[OSError])\n",
    "def merge_files(bucket, task_index, *read_task_list):\n",
    "    \"\"\" Read files from read_task_list, and write to parquet files to `bucket`.\n",
    "    In each output file, each row group has roughly `NUN_ROWS_PER_ROW_GROUP` rows\n",
    "    and `NUM_ROWS_PER_FILE` total rows per file.\"\"\"\n",
    "    file_name = f\"{bucket}/{task_index}.parquet\"\n",
    "    writer = None\n",
    "    total_rows = 0\n",
    "    current_rows = 0\n",
    "    current_batches = []\n",
    "\n",
    "    def write_current_batches():\n",
    "        nonlocal current_batches, current_rows, writer, total_rows, file_name\n",
    "        new_batch = pa.concat_tables(current_batches)\n",
    "        \n",
    "        # Cast the specified column to list<float32> data type.\n",
    "        values_field = new_batch.schema.field(COLUMN_TO_CAST_FLOAT_LIST_TYPE)\n",
    "        values_field_idx = new_batch.schema.get_field_index(COLUMN_TO_CAST_FLOAT_LIST_TYPE)\n",
    "        new_values_type = pa.list_(pa.float32())\n",
    "        new_values_field = values_field.with_type(new_values_type)\n",
    "        new_schema = new_batch.schema.set(values_field_idx, new_values_field)\n",
    "        new_batch = new_batch.cast(new_schema)\n",
    "\n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(file_name, schema=new_schema)\n",
    "        writer.write(new_batch)\n",
    "        total_rows += new_batch.num_rows\n",
    "        current_batches = []\n",
    "        current_rows = 0\n",
    "\n",
    "    for read_task in read_task_list:\n",
    "        for batch in read_task():\n",
    "            batch.replace_schema_metadata()\n",
    "\n",
    "            current_rows += batch.num_rows\n",
    "            current_batches.append(batch)\n",
    "            if current_rows >= NUN_ROWS_PER_ROW_GROUP:\n",
    "                write_current_batches()\n",
    "\n",
    "    if current_rows > 0:\n",
    "        write_current_batches()\n",
    "    writer.close()\n",
    "    return total_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output path containing embeddings generated from notebook `1_generate_embeddings.ipynb`\n",
    "embedding_input_path = \"YOUR-EMBEDDINGS-BUCKET-HERE\"\n",
    "ds = ray.data.read_parquet(embedding_input_path)\n",
    "\n",
    "num_rows = ds.count()\n",
    "num_input_files = len(ds._logical_plan._dag._datasource._pq_fragments)\n",
    "read_tasks = ds._logical_plan._dag._datasource.get_read_tasks(num_input_files)\n",
    "\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "print(f\"Number of input files: {num_input_files}\")\n",
    "print(f\"Number of read tasks: {len(read_tasks)}\")\n",
    "\n",
    "output_path = \"YOUR-OUTPUT-BUCKET-HERE\"\n",
    "print(f\"Output path: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write 1M rows per file.\n",
    "NUM_ROWS_PER_FILE = 1e6\n",
    "\n",
    "num_rows_per_file = [i.num_rows for i in ds._logical_plan._dag._datasource._metadata]\n",
    "assert len(num_rows_per_file) == len(read_tasks)\n",
    "read_tasks_with_rows = list(zip(num_rows_per_file, read_tasks))\n",
    "\n",
    "# Decide grouping of input files such that the merged output \n",
    "# contains NUM_ROWS_PER_FILE rows per file.\n",
    "# Use First Fit Decreasing algorithm to merge input files into output files (bin packing).\n",
    "read_tasks_with_rows.sort(key=lambda t: t[0], reverse=True)\n",
    "merged_read_tasks = []\n",
    "for current_num_rows, current_task in read_tasks_with_rows:\n",
    "    found_bin = False\n",
    "    for idx, this_bin in enumerate(merged_read_tasks):\n",
    "        total_rows, task_list = this_bin\n",
    "        if total_rows + current_num_rows <= NUM_ROWS_PER_FILE:\n",
    "            found_bin = True\n",
    "            total_rows += current_num_rows\n",
    "            task_list.append(current_task)\n",
    "            merged_read_tasks[idx] = (total_rows, task_list)\n",
    "            break\n",
    "    if not found_bin:\n",
    "        merged_read_tasks.append((current_num_rows, [current_task]))\n",
    "\n",
    "num_output_files = len(merged_read_tasks)\n",
    "print(f\"Number of output files: {num_output_files}\")\n",
    "for num_rows, task_list in merged_read_tasks:\n",
    "    print(f\"Number of output rows: {num_rows}, Number of input files: {len(task_list)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Ray code to run the file merging and write.\n",
    "%%time\n",
    "\n",
    "result = []\n",
    "for i, this_bin in enumerate(merged_read_tasks):\n",
    "    result.append(merge_files.remote(output_path, i, *(this_bin[1])))\n",
    "ray.get(result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
