{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone.grpc import PineconeGRPC\n",
    "import os\n",
    "from datetime import datetime\n",
    "import ray\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = 'YOUR-API-KEY-HERE'\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(\n",
    "    runtime_env={\n",
    "        \"env_vars\": {\n",
    "          \"PINECONE_API_KEY\": os.getenv(\"PINECONE_API_KEY\"),\n",
    "        }\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"YOUR-PINECONE-INDEX-NAME-HERE\"\n",
    "\n",
    "batch_size = 350\n",
    "# from https://www.pinecone.io/blog/working-at-scale/\n",
    "MAX_BYTES_SIZE_PER_REQUEST = 2 * 1024 * 1024\n",
    "MAX_BATCH_LENGTH = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_records(row):\n",
    "    \"\"\"Format each row before upserting. Notably, we apply the inverse of\n",
    "    `flatten_metadata_col()` in `2_merge_embeddings.ipynb` to properly\n",
    "    format the metadata.\"\"\"\n",
    "    metadata_dict = {\n",
    "        \"document_id\": row[\"metadata_document_id\"],\n",
    "        # cannot upsert raw np.ndarray of floats in rand_coeff, get the following error:\n",
    "        # \"Metadata value must be a string, number, boolean or list of strings\"\n",
    "        \"rand_coeff\": [str(elem) for elem in row[\"metadata_rand_coeff\"].tolist()],\n",
    "        \"source\": row[\"metadata_source\"],\n",
    "        \"timestamp\": row[\"metadata_timestamp\"],\n",
    "        \"text\": row[\"text\"],\n",
    "    }\n",
    "\n",
    "    row[\"metadata\"] = metadata_dict\n",
    "    del row[\"metadata_document_id\"]\n",
    "    del row[\"metadata_rand_coeff\"]\n",
    "    del row[\"metadata_source\"]\n",
    "    del row[\"metadata_timestamp\"]\n",
    "    del row[\"text\"]\n",
    "\n",
    "    return row\n",
    "\n",
    "def process_batch(batch):\n",
    "    batch_keys = batch.keys()\n",
    "    batch_records = [dict(zip(batch_keys, vals)) for vals in zip(*(batch[k] for k in batch_keys))]\n",
    "    batch_records = [format_records(row) for row in batch_records]\n",
    "    return batch_records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities to calculate row byte size, used in determining how \n",
    "# many rows to batch together for the upsert call.\n",
    "def _get_row_size(row):\n",
    "    return (\n",
    "        sys.getsizeof(row[\"id\"]) + row[\"values\"].nbytes\n",
    "        + _get_row_md_size(row)\n",
    "    )\n",
    "\n",
    "def _get_row_md_size(row):\n",
    "    md = row[\"metadata\"]\n",
    "    return (\n",
    "        # from https://www.pinecone.io/blog/working-at-scale/,\n",
    "        # get utf-8 encoded size to get the most accurate estimate for grpc call.\n",
    "        # + sys.getsizeof(md[\"document_id\"]) + sys.getsizeof(md[\"source\"])\n",
    "        + sys.getsizeof(md[\"document_id\"]) + len(md[\"source\"].encode('utf-8'))\n",
    "        + len(md[\"text\"].encode('utf-8')) + md[\"timestamp\"].nbytes\n",
    "        # + sys.getsizeof(md[\"text\"]) + md[\"timestamp\"].nbytes\n",
    "        + sum([sys.getsizeof(c) for c in md[\"rand_coeff\"]])\n",
    "    )\n",
    "\n",
    "def chunker(seq):\n",
    "    # Chunk `seq` into batches based on byte size, to avoid errors like:\n",
    "    # UNKNOWN:Error received from peer ipv4:52.41.228.72:443 grpc_message:\"Request size 4MB exceeds the maximum supported size of 2MB\"\n",
    "    curr_batch = []\n",
    "    curr_batch_size = 0\n",
    "    for pos in range(len(seq)):\n",
    "        curr_row = seq[pos]\n",
    "        row_size = _get_row_size(curr_row)\n",
    "        if curr_batch_size + row_size < MAX_BYTES_SIZE_PER_REQUEST and len(curr_batch) < MAX_BATCH_LENGTH:\n",
    "            curr_batch.append(seq[pos])\n",
    "            curr_batch_size += row_size\n",
    "        else:\n",
    "            yield curr_batch\n",
    "            curr_batch = [seq[pos]]\n",
    "            curr_batch_size = row_size\n",
    "    yield curr_batch\n",
    "\n",
    "def split_batch(batch, n):\n",
    "    \"\"\"Split `batch` into `n` sub-batches, returned as a list of `n` lists.\"\"\"\n",
    "    sub_batch_size = len(batch) // n\n",
    "    for i in range(0, len(batch), sub_batch_size):\n",
    "        yield batch[i:i + sub_batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload(big_batch):\n",
    "    \"\"\"Chunk `big_batch` into smaller batches, each batch is upserted to Pinecone\n",
    "    with a basic wait-and-retry scheme on failure.\n",
    "    In the case of a payload size error, we try to upload each vector individually.\"\"\"\n",
    "    client = PineconeGRPC(PINECONE_API_KEY)\n",
    "    index = client.Index(index_name)\n",
    "    total_vectors = 0\n",
    "    num_failures_payload_size = 0\n",
    "    num_failures_other = 0\n",
    "    max_attempts_per_batch = 5\n",
    "    data = process_batch(big_batch)\n",
    "    \n",
    "    for batch in chunker(data):\n",
    "        upsert_successful = False\n",
    "        attempt = 0\n",
    "        recent_exception = None\n",
    "        upload_vector_individually = False\n",
    "        while not upsert_successful and attempt < max_attempts_per_batch:\n",
    "            if not upload_vector_individually:\n",
    "                try:\n",
    "                    result = index.upsert(vectors=batch)\n",
    "                    total_vectors += result.upserted_count\n",
    "                    upsert_successful = True\n",
    "                except Exception as e:\n",
    "                    recent_exception = e\n",
    "\n",
    "                    if \"which exceeds the limit of\" in str(e):\n",
    "                        upload_vector_individually = True\n",
    "                        print(f\"===> Payload size exceeds limit, try uploading individually next\")\n",
    "\n",
    "                    # wait at least 5 seconds per retry\n",
    "                    wait_s = max(5, random.randrange(0, 16 * 2 ** attempt))\n",
    "                    attempt += 1\n",
    "                    print(f\"===> Exception on attempt {attempt}/{max_attempts_per_batch}: {e}\")\n",
    "                    if attempt < max_attempts_per_batch:\n",
    "                        print(f\"===> Wait {wait_s} for next retry attempt\")\n",
    "                        time.sleep(wait_s)\n",
    "            else:\n",
    "                curr_success = 0\n",
    "                upsert_successful = True\n",
    "                for sub_batch in split_batch(batch, len(batch)):\n",
    "                    try:\n",
    "                        result = index.upsert(vectors=sub_batch)\n",
    "                        total_vectors += result.upserted_count\n",
    "                        curr_success += result.upserted_count\n",
    "                    except Exception as e:\n",
    "                        if \"which exceeds the limit of\" in str(e):\n",
    "                            print(f\"===> Embedding with id {sub_batch[0]['id']} exceeds size limit: {e}\")\n",
    "                            print(f\"===> Embedding text: {sub_batch[0]['metadata']['text']}\")\n",
    "                        num_failures_payload_size += len(sub_batch)\n",
    "                        recent_exception = e\n",
    "                        print(f\"===> Exception when uploading individual embedding: {e}\")\n",
    "                        print(f\"===> Embedding: {sub_batch[0]}\")\n",
    "                # after an individual upload attempt, move on to next batch, \n",
    "                # even if not all the vectors succeeded.\n",
    "                print(f\"===> Successfully uploaded {curr_success}/{len(batch)} individual embeddings\")\n",
    "                upsert_successful = True \n",
    "\n",
    "        if not upsert_successful and not upload_vector_individually:\n",
    "            num_failures_other += len(batch)\n",
    "            print(f\"===> Exception after {max_attempts_per_batch} attempts: {recent_exception}\")\n",
    "    return {\n",
    "        'upserted': np.array([total_vectors]), \n",
    "        'errors_payload_size': np.array([num_failures_payload_size]),\n",
    "        'errors_other': np.array([num_failures_other])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output path containing merged embeddings files \n",
    "# generated from notebook `2_merge_embeddings.ipynb`\n",
    "merged_output_prefix = \"YOUR-MERGED-EMBEDDINGS-BUCKET-HERE\"\n",
    "\n",
    "# Range of merged embedded files to upsert, from [f_start_index, f_end_index).\n",
    "# For example, the values below will upsert embeddings from all 1413 merged files.\n",
    "f_start_index = 0\n",
    "f_end_index = 1413\n",
    "\n",
    "merged_output_paths = [\n",
    "    f\"{merged_output_prefix}/{i}.parquet\"\n",
    "    for i in range(f_start_index, f_end_index)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Ray Data code to run upsert.\n",
    "start_t = datetime.now()\n",
    "\n",
    "embedded_ds_read = ray.data.read_parquet(merged_output_paths)\n",
    "new_ds = embedded_ds_read.map_batches(\n",
    "    upload, \n",
    "    batch_size=batch_size * 20,\n",
    ")\n",
    "summary = new_ds.sum(['upserted', 'errors_other', 'errors_payload_size'])\n",
    "\n",
    "duration = datetime.now() - start_t\n",
    "print(f\"===> Finished upserting {len(merged_output_paths)} merged files in {duration}\")\n",
    "print(\"===> Summary:\", {k: f\"{v: ,}\" for k,v in summary.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check upserted index info. This may not reflect the exact nmber of vectors upserted\n",
    "# from above, since there seems to be some delay in registering the vectors.\n",
    "client = PineconeGRPC(PINECONE_API_KEY)\n",
    "index = client.Index(index_name)\n",
    "print(client.describe_index(index_name))\n",
    "print(index.describe_index_stats())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray-build",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
