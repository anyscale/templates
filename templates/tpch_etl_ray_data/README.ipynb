{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d3427c1",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Ray Data for ETL: A Comprehensive Beginner's Guide\n",
    "\n",
    "This notebook provides a complete introduction to Ray Data for Extract, Transform, Load (ETL) workflows. We'll cover both the practical aspects of building ETL pipelines and the underlying architecture that makes Ray Data powerful for distributed data processing.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Learning Roadmap:</b>\n",
    "<ul>\n",
    "    <li><b>Part 1:</b> What is Ray Data and ETL?</li>\n",
    "    <li><b>Part 2:</b> Ray Data Architecture & Concepts</li>\n",
    "    <li><b>Part 3:</b> Extract - Reading Data</li>\n",
    "    <li><b>Part 4:</b> Transform - Processing Data</li>\n",
    "    <li><b>Part 5:</b> Load - Writing Data</li>\n",
    "    <li><b>Part 6:</b> Advanced ETL Patterns</li>\n",
    "    <li><b>Part 7:</b> Performance & Best Practices</li>\n",
    "    <li><b>Part 8:</b> Troubleshooting Common Issues</li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530af088",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa22fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "from typing import Dict, Any\n",
    "import time\n",
    "\n",
    "# Initialize Ray\n",
    "if not ray.is_initialized():\n",
    "    ray.init()\n",
    "\n",
    "print(f\"Ray version: {ray.__version__}\")\n",
    "print(f\"Ray cluster resources: {ray.cluster_resources()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416a0c3c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Part 1: What is Ray Data and ETL?\n",
    "\n",
    "### Understanding ETL\n",
    "\n",
    "**ETL** stands for Extract, Transform, Load - a fundamental pattern in data engineering:\n",
    "\n",
    "- **Extract**: Reading data from various sources (databases, files, APIs, etc.)\n",
    "- **Transform**: Processing, cleaning, and enriching the data\n",
    "- **Load**: Writing the processed data to destination systems\n",
    "\n",
    "### What is Ray Data?\n",
    "\n",
    "Ray Data is a distributed data processing library built on top of Ray that has recently reached **General Availability (GA)**. As the fastest-growing use case for Ray, it's designed to handle **both traditional ETL/ML workloads and next-generation AI applications**, providing a unified platform that scales from CPU clusters to heterogeneous GPU environments.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> Ray Data: One Platform for All Data Workloads</b><br>\n",
    "Ray Data is part of Ray, the AI Compute Engine that now orchestrates <b>over 1 million clusters per month</b>. Whether you're running traditional ETL on CPU clusters or cutting-edge multimodal AI pipelines, Ray Data provides a unified solution that evolves with your needs.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Ray Data: From Traditional to Transformational:</b>\n",
    "<ul>\n",
    "    <li><b>Traditional ETL:</b> Excellent for structured data processing, business intelligence, and reporting</li>\n",
    "    <li><b>ML Workflows:</b> Perfect for feature engineering, model training pipelines, and batch scoring</li>\n",
    "    <li><b>Scalable Processing:</b> Automatically scales from single machines to thousands of CPU cores</li>\n",
    "    <li><b>Future-Ready:</b> Seamlessly extends to GPU workloads and multimodal data when needed</li>\n",
    "    <li><b>Python-Native:</b> No JVM overhead - pure Python performance at scale</li>\n",
    "    <li><b>Streaming Architecture:</b> Handle datasets larger than memory with ease</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "### Today's Workloads, Tomorrow's Possibilities\n",
    "\n",
    "Ray Data excels across the entire spectrum of data processing needs:\n",
    "\n",
    "**Traditional & Current Workloads:**\n",
    "- **Business ETL**: Customer analytics, financial reporting, operational dashboards\n",
    "- **Classical ML**: Recommendation systems, fraud detection, predictive analytics\n",
    "- **Data Engineering**: Large-scale data cleaning, transformation, and aggregation\n",
    "- **BI Pipelines**: Data warehouse loading, metric computation, and reporting\n",
    "\n",
    "**Next-Generation Workloads:**\n",
    "- **Multimodal AI**: Processing text, images, video, and audio together\n",
    "- **LLM Pipelines**: Fine-tuning, embedding generation, and batch inference\n",
    "- **Computer Vision**: Image preprocessing and model inference at scale\n",
    "- **Compound AI Systems**: Orchestrating multiple models and traditional ML\n",
    "\n",
    "### Ray Data vs Traditional Tools\n",
    "\n",
    "Let's understand how Ray Data compares to other data processing tools across traditional and modern workloads:\n",
    "\n",
    "| Feature | Ray Data | Pandas | Spark | Dask |\n",
    "|---------|----------|--------|-------|------|\n",
    "| **Traditional ETL** | Excellent | Good | Excellent | Good |\n",
    "| **Scale** | Multi-machine | Single-machine | Multi-machine | Multi-machine |\n",
    "| **Memory** | Streaming | In-memory | Mixed | Lazy evaluation |\n",
    "| **Python Performance** | Native (no JVM) | Native | JVM overhead | Native |\n",
    "| **CPU Clusters** | Optimized | Single-node | Good | Good |\n",
    "| **GPU Support** | Native | None | Limited | Limited |\n",
    "| **Classical ML** | Excellent | Limited | Good | Good |\n",
    "| **Multimodal Data** | Optimized | Limited | Limited | Limited |\n",
    "| **Fault Tolerance** | Built-in | None | Built-in | Limited |\n",
    "\n",
    "### Real-World Impact Across All Workloads\n",
    "\n",
    "Organizations worldwide are seeing dramatic results with Ray Data for both traditional and advanced workloads:\n",
    "\n",
    "**Traditional ETL & Analytics:**\n",
    "- **Amazon**: Migrated an exabyte-scale workload from Spark to Ray Data, cutting costs by **82%** and saving **$120 million annually**\n",
    "- **Instacart**: Processing **100x more data** for recommendation systems and business analytics\n",
    "- **Financial Services**: Major banks using Ray Data for fraud detection and risk analytics at scale\n",
    "\n",
    "**Modern AI & ML:**\n",
    "- **Niantic**: Reduced code complexity by **85%** while scaling AR/VR data pipelines\n",
    "- **Canva**: Cut cloud costs in **half** while processing design assets and user data\n",
    "- **Pinterest**: Boosted GPU utilization to **90%+** for image processing and recommendations\n",
    "\n",
    "Ray Data provides a unified platform that excels at traditional ETL, classical ML, and next-generation AI workloads - eliminating the need for multiple specialized systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7280fcaf",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Part 2: Ray Data Architecture & Concepts\n",
    "\n",
    "### The AI Compute Engine Architecture\n",
    "\n",
    "Ray Data is built on Ray, the AI Compute Engine that powers the most demanding AI workloads in production. Ray's architecture addresses the core challenges of modern AI infrastructure:\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> Ray: Built for the AI Era</b><br>\n",
    "Unlike traditional distributed systems designed for structured data and CPU workloads, Ray was purpose-built for:<br>\n",
    "<ul>\n",
    "    <li><b>Python-Native:</b> No JVM overhead or serialization bottlenecks</li>\n",
    "    <li><b>Heterogeneous Compute:</b> Seamlessly orchestrates CPUs, GPUs, and other accelerators</li>\n",
    "    <li><b>Dynamic Workloads:</b> Adapts to varying compute needs in real-time</li>\n",
    "    <li><b>Fault Tolerance:</b> Handles failures gracefully at massive scale</li>\n",
    "</ul>\n",
    "Ray now supports clusters up to <b>8,000 nodes</b> with 4x improved scalability.\n",
    "</div>\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "Before diving into ETL examples, let's understand the fundamental concepts that power Ray Data.\n",
    "\n",
    "#### 1. Datasets and Blocks\n",
    "\n",
    "A **Dataset** in Ray Data is a distributed collection of data that's divided into **blocks**. Think of blocks as chunks of your data that can be processed independently.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Understanding Blocks:</b>\n",
    "<ul>\n",
    "    <li>Each block contains a subset of your data (typically 1-128 MB)</li>\n",
    "    <li>Blocks are stored in Ray's distributed object store</li>\n",
    "    <li>Operations are applied to blocks in parallel across the cluster</li>\n",
    "    <li>Block size affects performance - too small causes overhead, too large causes memory issues</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "### Performance Innovations: RayTurbo\n",
    "\n",
    "For production workloads, **Anyscale** offers **RayTurbo**, an optimized runtime with 30+ performance improvements:\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b> RayTurbo Performance Improvements:</b>\n",
    "<ul>\n",
    "    <li><b>Ray Data:</b> Up to 4.5x faster with streaming metadata fetching</li>\n",
    "    <li><b>Ray Serve:</b> Up to 56% faster inference with replica compaction</li>\n",
    "    <li><b>Ray Train:</b> Up to 60% cost reduction with elastic training on spot instances</li>\n",
    "    <li><b>Autoscaling:</b> 5.1x faster node autoscaling</li>\n",
    "    <li><b>Batch Inference:</b> Up to 6x lower costs compared to AWS Bedrock</li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5706ab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a simple dataset to understand blocks\n",
    "# Create sample data\n",
    "data = [(i, f\"name_{i}\", np.random.rand()) for i in range(1000)]\n",
    "ds = ray.data.from_items(data)\n",
    "\n",
    "print(f\"Dataset: {ds}\")\n",
    "print(f\"Number of blocks: {ds.num_blocks()}\")\n",
    "print(f\"Schema: {ds.schema()}\")\n",
    "\n",
    "# Look at a few rows\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "for i, row in enumerate(ds.take(3)):\n",
    "    print(f\"Row {i}: {row}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366da2ff",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### 2. Lazy vs Eager Execution\n",
    "\n",
    "Ray Data uses **lazy execution** by default, meaning operations are not executed immediately but are planned and optimized before execution.\n",
    "\n",
    "**Lazy Execution Benefits:**\n",
    "- **Optimization**: Ray Data can optimize the entire pipeline before execution\n",
    "- **Memory efficiency**: Only necessary data is loaded into memory\n",
    "- **Fault tolerance**: Can restart from intermediate points if failures occur\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b> Understanding Execution:</b><br>\n",
    "<b>Lazy:</b> Build a plan first, then execute (default)<br>\n",
    "<b>Eager:</b> Execute operations immediately as they're called<br><br>\n",
    "Lazy execution allows Ray Data to optimize your entire pipeline for better performance!\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c62bdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate lazy execution\n",
    "print(\"Creating a lazy dataset pipeline...\")\n",
    "\n",
    "# These operations are not executed yet - they're just planned\n",
    "ds_lazy = (ds\n",
    "    .map(lambda x: {\"id\": x[0], \"name\": x[1], \"value\": x[2] * 2})\n",
    "    .filter(lambda x: x[\"value\"] > 1.0)\n",
    "    .map(lambda x: {**x, \"category\": \"high\" if x[\"value\"] > 1.5 else \"medium\"})\n",
    ")\n",
    "\n",
    "print(f\"Lazy dataset: {ds_lazy}\")\n",
    "print(\"Notice: No actual computation has happened yet!\")\n",
    "\n",
    "# Execution happens when we materialize the data\n",
    "print(\"\\nExecuting pipeline...\")\n",
    "result = ds_lazy.take(5)  # This triggers execution\n",
    "print(f\"Results: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f12cce6",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Part 3: Extract - Reading Data\n",
    "\n",
    "The **Extract** phase involves reading data from various sources. Ray Data provides built-in connectors for many common data sources and makes it easy to scale data reading across a distributed cluster, especially for the **multimodal data** that powers modern AI applications.\n",
    "\n",
    "### The Multimodal Data Revolution\n",
    "\n",
    "Today's AI applications process vastly more complex data than traditional ETL pipelines:\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> The Scale of Modern Data:</b>\n",
    "<ul>\n",
    "    <li><b>Unstructured Data Growth:</b> Now outpaces structured data by 10x+ in most organizations</li>\n",
    "    <li><b>Video Processing:</b> Companies like OpenAI (Sora), Pinterest, and Apple process petabytes of multimodal data daily</li>\n",
    "    <li><b>Foundation Models:</b> Require processing millions of images, videos, and documents</li>\n",
    "    <li><b>AI-Powered Processing:</b> Every aspect of data processing is becoming AI-enhanced</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "### How Ray Data Reads Data Under the Hood\n",
    "\n",
    "When you read data with Ray Data, here's what happens:\n",
    "\n",
    "1. **File Discovery**: Ray Data discovers all files matching your path pattern\n",
    "2. **Task Creation**: Files are distributed across Ray tasks (typically one file per task)\n",
    "3. **Parallel Reading**: Multiple tasks read files simultaneously across the cluster\n",
    "4. **Block Creation**: Each task creates data blocks stored in Ray's object store\n",
    "5. **Lazy Planning**: The dataset is created but data isn't loaded until needed\n",
    "\n",
    "This architecture enables Ray Data to efficiently handle both traditional structured data and modern unstructured formats that power AI applications.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Built-in Data Sources:</b>\n",
    "<ul>\n",
    "    <li><b>Structured:</b> Parquet, CSV, JSON, Arrow</li>\n",
    "    <li><b>Unstructured:</b> Images, Videos, Audio, Binary files</li>\n",
    "    <li><b>Databases:</b> MongoDB, MySQL, PostgreSQL, Snowflake</li>\n",
    "    <li><b>Cloud Storage:</b> S3, GCS, Azure Blob Storage</li>\n",
    "    <li><b>Data Lakes:</b> Delta Lake, Iceberg (via RayTurbo)</li>\n",
    "    <li><b>ML Formats:</b> TensorFlow Records, PyTorch datasets</li>\n",
    "    <li><b>Memory:</b> Python lists, NumPy arrays, Pandas DataFrames</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "### Enterprise-Grade Data Connectivity\n",
    "\n",
    "For enterprise environments, **Anyscale** provides additional connectors and optimizations:\n",
    "- **Enhanced Security**: Integration with enterprise identity systems\n",
    "- **Governance Controls**: Data lineage and access controls\n",
    "- **Performance Optimization**: RayTurbo's streaming metadata fetching provides up to **4.5x faster** data loading\n",
    "- **Hybrid Deployment**: Support for Kubernetes, on-premises, and multi-cloud environments\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "02aa5e8c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Accessing TPC-H Benchmark Data for Our ETL Examples\n",
    "\n",
    "Instead of generating sample data, we'll use the industry-standard TPC-H benchmark dataset. This provides realistic enterprise-scale data that's used by companies worldwide to evaluate data processing systems and represents real business scenarios with complex relationships between customers, orders, suppliers, and products.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d452fc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TPC-H Benchmark Dataset - Industry Standard for Data Processing\n",
    "# TPC-H is the gold standard benchmark for decision support systems and analytics\n",
    "print(\" Accessing TPC-H Benchmark Dataset (Scale Factor 1000)\")\n",
    "print(\"   The Transaction Processing Performance Council Benchmark H\")\n",
    "print(\"   - Industry standard for testing data processing systems\")\n",
    "print(\"   - Scale Factor 1000 = ~1TB of data across 8 tables\")\n",
    "print(\"   - Used by enterprises worldwide for performance evaluation\")\n",
    "\n",
    "# TPC-H S3 data location\n",
    "TPCH_S3_PATH = \"s3://ray-benchmark-data/tpch/parquet/sf1000\"\n",
    "\n",
    "print(f\"\\n TPC-H Dataset Overview:\")\n",
    "print(f\"    Source: {TPCH_S3_PATH}\")\n",
    "print(f\"    Scale: 1000 (approximately 1TB)\")\n",
    "print(f\"    Use Case: Enterprise decision support and business intelligence\")\n",
    "\n",
    "# TPC-H Schema Overview\n",
    "tpch_tables = {\n",
    "    \"customer\": \"Customer master data with demographics and market segments\",\n",
    "    \"orders\": \"Order header information with dates, priorities, and status\",\n",
    "    \"lineitem\": \"Detailed line items for each order (largest table ~6B rows)\",\n",
    "    \"part\": \"Parts catalog with specifications and retail prices\", \n",
    "    \"supplier\": \"Supplier information including contact details and geography\",\n",
    "    \"partsupp\": \"Part-supplier relationships with costs and availability\",\n",
    "    \"nation\": \"Nation reference data with geographic regions\",\n",
    "    \"region\": \"Regional groupings for geographic analysis\"\n",
    "}\n",
    "\n",
    "print(f\"\\n TPC-H Schema (8 Tables):\")\n",
    "for table, description in tpch_tables.items():\n",
    "    print(f\"    {table.upper()}: {description}\")\n",
    "\n",
    "print(f\"\\n Business Scenario:\")\n",
    "print(f\"   Global supply chain and retail operation\")\n",
    "print(f\"   - Multi-national customer base\")\n",
    "print(f\"   - Complex supplier relationships\") \n",
    "print(f\"   - Detailed transaction history\")\n",
    "print(f\"   - Perfect for traditional BI and modern AI/ML applications\")\n",
    "\n",
    "print(f\"\\n TPC-H dataset ready for analysis!\")\n",
    "print(f\" This represents real-world enterprise-scale data processing challenges\")\n",
    "print(f\" Demonstrates Ray Data's capabilities on industry-standard benchmarks\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4bc4a433",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Reading Different Data Formats\n",
    "\n",
    "Now let's learn how to read data from different formats using Ray Data's built-in connectors.\n",
    "\n",
    "#### Reading CSV Files\n",
    "\n",
    "CSV files are common for data exchange, though not the most efficient format for large-scale processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58ee58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read TPC-H Customer Master Data (Traditional Structured Data Processing)\n",
    "customers_ds = ray.data.read_parquet(f\"{TPCH_S3_PATH}/customer\")\n",
    "\n",
    "print(\" TPC-H Customer Master Data (Traditional ETL):\")\n",
    "print(f\"    Schema: {customers_ds.schema()}\")\n",
    "print(f\"    Blocks: {customers_ds.num_blocks()}\")\n",
    "print(f\"    Total customers: {customers_ds.count():,}\")\n",
    "print(f\"    Estimated size: {customers_ds.size_bytes() / (1024*1024):.1f} MB\")\n",
    "\n",
    "print(\"\\n Sample customer records:\")\n",
    "customers_ds.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2e6f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer Market Segment Analysis - Traditional BI Workload\n",
    "print(\" Customer Market Segment Distribution:\")\n",
    "print(\"   Analyzing customer segments for business intelligence...\")\n",
    "\n",
    "segment_analysis = customers_ds.groupby('c_mktsegment').agg(\n",
    "    customer_count=('c_custkey', 'count'),\n",
    "    avg_account_balance=('c_acctbal', 'mean')\n",
    ")\n",
    "segment_analysis.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc44024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geographic Reference Data - Nations Table\n",
    "print(\" TPC-H Nations Reference Data:\")\n",
    "print(\"   Loading geographic data for customer demographics...\")\n",
    "\n",
    "nation_ds = ray.data.read_parquet(f\"{TPCH_S3_PATH}/nation\")\n",
    "print(f\"    Total nations: {nation_ds.count():,}\")\n",
    "print(f\"    Size: {nation_ds.size_bytes() / 1024:.1f} KB\")\n",
    "\n",
    "print(\"\\n Sample nation records:\")\n",
    "nation_ds.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eab8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer Demographics by Nation - Join Analysis\n",
    "print(\" Customer Demographics by Nation:\")\n",
    "print(\"   Joining customer and nation data for geographic analysis...\")\n",
    "\n",
    "customer_nation_analysis = (\n",
    "    customers_ds\n",
    "    .join(nation_ds, left_on='c_nationkey', right_on='n_nationkey')\n",
    "    .groupby('n_name')\n",
    "    .agg(\n",
    "        customer_count=('c_custkey', 'count'),\n",
    "        avg_balance=('c_acctbal', 'mean'),\n",
    "        total_balance=('c_acctbal', 'sum')\n",
    "    )\n",
    ")\n",
    "customer_nation_analysis.sort('customer_count', descending=True).show(10)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "38fa802d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### Reading Parquet Files\n",
    "\n",
    "Parquet is a columnar storage format that's highly efficient for analytics workloads. It's the preferred format for large-scale data processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a6d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read TPC-H High-Volume Transactional Data (Orders + Line Items)\n",
    "# This demonstrates Ray Data's strength in traditional ETL: processing massive, enterprise-scale datasets\n",
    "\n",
    "# Read Orders table (header information)\n",
    "orders_ds = ray.data.read_parquet(f\"{TPCH_S3_PATH}/orders\")\n",
    "\n",
    "print(\" TPC-H Orders Data (Enterprise Transaction Processing):\")\n",
    "print(f\"    Schema: {orders_ds.schema()}\")\n",
    "print(f\"    Blocks: {orders_ds.num_blocks()}\")\n",
    "print(f\"    Total orders: {orders_ds.count():,}\")\n",
    "print(f\"    Estimated size: {orders_ds.size_bytes() / (1024*1024):.1f} MB\")\n",
    "\n",
    "print(\"\\n Sample order records:\")\n",
    "orders_ds.show(3)\n",
    "\n",
    "# Read Line Items table (detailed transaction data - largest table in TPC-H)\n",
    "lineitem_ds = ray.data.read_parquet(f\"{TPCH_S3_PATH}/lineitem\")\n",
    "\n",
    "print(f\"\\n TPC-H Line Items Data (Detailed Transaction Processing):\")\n",
    "print(f\"    Schema: {lineitem_ds.schema()}\")\n",
    "print(f\"    Blocks: {lineitem_ds.num_blocks()}\")\n",
    "print(f\"    Total line items: {lineitem_ds.count():,}\")\n",
    "print(f\"    Estimated size: {lineitem_ds.size_bytes() / (1024*1024):.1f} MB\")\n",
    "\n",
    "print(\"\\n Sample line item records:\")\n",
    "lineitem_ds.show(3)\n",
    "\n",
    "# Demonstrate column pruning optimization (common ETL optimization)\n",
    "lineitem_subset = ray.data.read_parquet(\n",
    "    f\"{TPCH_S3_PATH}/lineitem\",\n",
    "    columns=['l_orderkey', 'l_partkey', 'l_quantity', 'l_extendedprice', 'l_discount', 'l_shipdate']\n",
    ")\n",
    "print(f\"\\n Column Pruning Optimization on Line Items:\")\n",
    "print(f\"   Original columns: {len(lineitem_ds.schema())}\")\n",
    "print(f\"   Selected columns: {len(lineitem_subset.schema())}\")\n",
    "print(f\"   Data reduction: {(1 - lineitem_subset.size_bytes()/lineitem_ds.size_bytes())*100:.1f}% size reduction\")\n",
    "\n",
    "# Traditional ETL analytics - business KPIs on enterprise data\n",
    "print(\"\\n Traditional Business Analytics (Enterprise Scale - Billions of Records):\")\n",
    "\n",
    "# Order priority analysis - typical business reporting\n",
    "order_priority_analysis = orders_ds.groupby('o_orderpriority').agg(\n",
    "    order_count=('o_orderkey', 'count'),\n",
    "    avg_total_price=('o_totalprice', 'mean'),\n",
    "    total_value=('o_totalprice', 'sum')\n",
    ")\n",
    "print(\"Order Priority Distribution:\")\n",
    "order_priority_analysis.sort('total_value', descending=True).show()\n",
    "\n",
    "# Time-based order analysis - common time-series analysis\n",
    "orders_with_year = orders_ds.map(lambda x: {\n",
    "    **x,\n",
    "    'order_year': int(str(x['o_orderdate'])[:4])\n",
    "})\n",
    "yearly_revenue = orders_with_year.groupby('order_year').agg(\n",
    "    yearly_orders=('o_orderkey', 'count'),\n",
    "    yearly_revenue=('o_totalprice', 'sum'),\n",
    "    avg_order_value=('o_totalprice', 'mean')\n",
    ")\n",
    "print(\"\\nYearly Revenue Trends:\")\n",
    "yearly_revenue.sort('order_year').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca038c3e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Part 4: Transform - Processing Data\n",
    "\n",
    "The **Transform** phase is where the real data processing happens. Ray Data provides several transformation operations that can be applied to datasets, and understanding how they work under the hood is key to building efficient ETL pipelines that power modern AI applications.\n",
    "\n",
    "### Transformations for the AI Era\n",
    "\n",
    "Modern AI workloads require more than traditional data transformations. Ray Data is designed for the era of **compound AI systems** and **agentic workflows** where:\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> AI-Powered Transformations:</b>\n",
    "<ul>\n",
    "    <li><b>Multimodal Processing:</b> Simultaneously process text, images, video, and audio</li>\n",
    "    <li><b>Model Inference:</b> Embed ML models directly into transformation pipelines</li>\n",
    "    <li><b>GPU Acceleration:</b> Seamlessly utilize both CPU and GPU resources</li>\n",
    "    <li><b>Compound AI:</b> Orchestrate multiple models and traditional ML within single workflows</li>\n",
    "    <li><b>AI-Enhanced ETL:</b> Use AI to optimize every aspect of data processing</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "### How Ray Data Processes Transformations\n",
    "\n",
    "When you apply transformations with Ray Data:\n",
    "\n",
    "1. **Task Distribution**: Transformations are distributed across Ray tasks/actors\n",
    "2. **Block-level Processing**: Each task processes one or more blocks independently  \n",
    "3. **Streaming Execution**: Blocks flow through the pipeline without waiting for all data\n",
    "4. **Operator Fusion**: Compatible operations are automatically combined for efficiency\n",
    "5. **Heterogeneous Compute**: Intelligently schedules CPU and GPU work\n",
    "6. **Fault Tolerance**: Failed tasks are automatically retried\n",
    "\n",
    "This architecture enables Ray Data to handle everything from traditional business logic to cutting-edge AI inference within the same pipeline.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Transformation Categories:</b>\n",
    "<ul>\n",
    "    <li><b>Row-wise operations:</b> <code>map()</code> - Transform individual rows</li>\n",
    "    <li><b>Batch operations:</b> <code>map_batches()</code> - Transform groups of rows (ideal for ML inference)</li>\n",
    "    <li><b>Filtering:</b> <code>filter()</code> - Remove rows based on conditions</li>\n",
    "    <li><b>Aggregations:</b> <code>groupby()</code> - Group and aggregate data</li>\n",
    "    <li><b>Joins:</b> <code>join()</code> - Combine datasets</li>\n",
    "    <li><b>AI Operations:</b> Embed models for inference, embeddings, and feature extraction</li>\n",
    "    <li><b>Shuffling:</b> <code>random_shuffle()</code>, <code>sort()</code> - Reorder data</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "### Enterprise-Scale Transformation Performance\n",
    "\n",
    "With **RayTurbo**, transformation performance reaches new levels:\n",
    "- **Compiled Graphs**: Up to 17x faster GPU communication and 2.8x faster multi-node performance\n",
    "- **Advanced Scheduling**: Intelligent resource allocation across heterogeneous clusters\n",
    "- **Memory Optimization**: Reduced overhead for small tasks and efficient peer-to-peer communication\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c2aedd",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Practical ETL Transformations\n",
    "\n",
    "Let's implement common ETL transformations using our e-commerce data:\n",
    "\n",
    "#### 1. Data Enrichment with Business Logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab097e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traditional_etl_enrichment_tpch(batch):\n",
    "    \"\"\"\n",
    "    Traditional ETL transformations for TPC-H business intelligence and reporting\n",
    "    This demonstrates classic data warehouse-style transformations on enterprise data\n",
    "    \"\"\"\n",
    "    df = batch.to_pandas() if hasattr(batch, 'to_pandas') else pd.DataFrame(batch)\n",
    "    \n",
    "    # Parse order date and create time dimensions (standard BI practice)\n",
    "    df['o_orderdate'] = pd.to_datetime(df['o_orderdate'])\n",
    "    df['order_year'] = df['o_orderdate'].dt.year\n",
    "    df['order_quarter'] = df['o_orderdate'].dt.quarter\n",
    "    df['order_month'] = df['o_orderdate'].dt.month\n",
    "    df['order_day_of_week'] = df['o_orderdate'].dt.dayofweek\n",
    "    \n",
    "    # Business day classifications (common in traditional ETL)\n",
    "    df['is_weekend'] = df['order_day_of_week'].isin([5, 6])\n",
    "    df['quarter_name'] = 'Q' + df['order_quarter'].astype(str)\n",
    "    df['month_name'] = df['o_orderdate'].dt.month_name()\n",
    "    \n",
    "    # Revenue and profit calculations (standard BI metrics)\n",
    "    df['revenue_tier'] = pd.cut(\n",
    "        df['o_totalprice'],\n",
    "        bins=[0, 50000, 150000, 300000, float('inf')],\n",
    "        labels=['Small', 'Medium', 'Large', 'Enterprise']\n",
    "    )\n",
    "    \n",
    "    # Order priority business rules (TPC-H specific)\n",
    "    priority_weights = {\n",
    "        '1-URGENT': 1.0,\n",
    "        '2-HIGH': 0.8,\n",
    "        '3-MEDIUM': 0.6,\n",
    "        '4-NOT SPECIFIED': 0.4,\n",
    "        '5-LOW': 0.2\n",
    "    }\n",
    "    df['priority_weight'] = df['o_orderpriority'].map(priority_weights).fillna(0.4)\n",
    "    df['weighted_revenue'] = df['o_totalprice'] * df['priority_weight']\n",
    "    \n",
    "    # Order status analysis\n",
    "    df['is_urgent'] = df['o_orderpriority'].isin(['1-URGENT', '2-HIGH'])\n",
    "    df['is_large_order'] = df['o_totalprice'] > 200000\n",
    "    df['requires_expedited_processing'] = df['is_urgent'] | df['is_large_order']\n",
    "    \n",
    "    # Date-based business logic\n",
    "    df['days_to_process'] = (pd.to_datetime(df['o_orderdate']) - pd.Timestamp('1992-01-01')).dt.days\n",
    "    df['is_peak_season'] = df['order_month'].isin([11, 12])  # Nov-Dec peak\n",
    "    \n",
    "    return df\n",
    "\n",
    "def ml_ready_feature_engineering_tpch(batch):\n",
    "    \"\"\"\n",
    "    Modern ML feature engineering for TPC-H data\n",
    "    This prepares enterprise data for machine learning models\n",
    "    \"\"\"\n",
    "    df = batch.to_pandas() if hasattr(batch, 'to_pandas') else pd.DataFrame(batch)\n",
    "    \n",
    "    # Temporal features for ML models\n",
    "    df['days_since_epoch'] = (df['o_orderdate'] - pd.Timestamp('1992-01-01')).dt.days\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['order_month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['order_month'] / 12)\n",
    "    df['quarter_sin'] = np.sin(2 * np.pi * df['order_quarter'] / 4)\n",
    "    df['quarter_cos'] = np.cos(2 * np.pi * df['order_quarter'] / 4)\n",
    "    \n",
    "    # Priority encoding for ML (one-hot style features)\n",
    "    for priority in ['1-URGENT', '2-HIGH', '3-MEDIUM']:\n",
    "        df[f'is_priority_{priority.split(\"-\")[0]}'] = (df['o_orderpriority'] == priority).astype(int)\n",
    "    \n",
    "    # Revenue-based features (common in ML)\n",
    "    df['log_total_price'] = np.log1p(df['o_totalprice'])  # Log transformation for ML\n",
    "    df['revenue_per_priority'] = df['o_totalprice'] * df['priority_weight']\n",
    "    df['weekend_large_order'] = (df['is_weekend'] & df['is_large_order']).astype(int)\n",
    "    \n",
    "    # Time-series features for predictive modeling\n",
    "    df['year_normalized'] = (df['order_year'] - df['order_year'].min()) / (df['order_year'].max() - df['order_year'].min())\n",
    "    df['seasonal_revenue_multiplier'] = np.where(df['is_peak_season'], 1.2, 1.0)\n",
    "    \n",
    "    # Customer key features (for customer analytics)\n",
    "    df['customer_id_mod_100'] = df['o_custkey'] % 100  # Simple customer segmentation feature\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Traditional ETL Processing on TPC-H Data (CPU-based, enterprise business logic)\n",
    "print(\" Traditional ETL Processing on TPC-H Data (Business Intelligence Focus):\")\n",
    "print(\"   Processing millions of enterprise orders with standard BI transformations...\")\n",
    "\n",
    "traditional_enriched = orders_ds.map_batches(\n",
    "    traditional_etl_enrichment_tpch,\n",
    "    batch_format=\"pyarrow\",\n",
    "    batch_size=10000  # Larger batches for efficiency on CPU clusters with enterprise data\n",
    ")\n",
    "\n",
    "print(\"\\n Traditional ETL Results:\")\n",
    "traditional_enriched.show(3)\n",
    "\n",
    "# ML-Ready Feature Engineering (Preparing enterprise data for model training/inference)\n",
    "print(\"\\n ML-Ready Feature Engineering (Next-Generation Capabilities):\")\n",
    "print(\"   Adding ML features for predictive analytics on enterprise transaction data...\")\n",
    "\n",
    "ml_ready_data = traditional_enriched.map_batches(\n",
    "    ml_ready_feature_engineering_tpch,\n",
    "    batch_format=\"pyarrow\",\n",
    "    batch_size=10000\n",
    ")\n",
    "\n",
    "print(\"\\n ML-Ready Data Sample:\")\n",
    "ml_ready_data.show(3)\n",
    "\n",
    "print(f\"\\n Complete TPC-H ETL Pipeline:\")\n",
    "print(f\"    Original TPC-H columns: {len(orders_ds.schema())}\")\n",
    "print(f\"    After traditional ETL: {len(traditional_enriched.schema())}\")\n",
    "print(f\"    After ML enrichment: {len(ml_ready_data.schema())}\")\n",
    "print(f\"    Total processing: {orders_ds.count():,} enterprise orders with {len(ml_ready_data.schema())} features\")\n",
    "\n",
    "# Store the enriched dataset for later use\n",
    "enriched_orders = ml_ready_data\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a925b36",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### 2. Aggregations and Analytics\n",
    "\n",
    "Aggregations are essential for creating summary statistics and business metrics. Ray Data's `groupby()` operations distribute the computation across the cluster.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b> Under the Hood - GroupBy Operations:</b><br>\n",
    "When you perform a GroupBy operation, Ray Data:<br>\n",
    "<ol>\n",
    "    <li><b>Shuffle Phase:</b> Data is redistributed so all records with the same key end up on the same node</li>\n",
    "    <li><b>Local Aggregation:</b> Each node performs aggregation on its subset of data</li>\n",
    "    <li><b>Result Collection:</b> Final aggregated results are collected</li>\n",
    "</ol>\n",
    "This is a distributed operation that can handle massive datasets efficiently!\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9543dc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional Business Intelligence & Data Warehouse Analytics on TPC-H Enterprise Data\n",
    "print(\" TPC-H Business Intelligence & Reporting (Enterprise CPU Cluster Workloads)\")\n",
    "print(\"   Generating executive dashboards and operational reports on industry-standard benchmark data...\")\n",
    "\n",
    "# Executive Summary Dashboard - typical BI metrics on enterprise data\n",
    "print(\"\\n Executive Dashboard (Traditional BI on TPC-H):\")\n",
    "executive_summary = (\n",
    "    enriched_orders\n",
    "    .groupby('order_quarter')\n",
    "    .agg(\n",
    "        total_orders=('o_orderkey', 'count'),\n",
    "        total_revenue=('o_totalprice', 'sum'),\n",
    "        avg_order_value=('o_totalprice', 'mean'),\n",
    "        weighted_revenue=('weighted_revenue', 'sum'),\n",
    "        urgent_order_percentage=('is_urgent', 'mean')\n",
    "    )\n",
    ")\n",
    "print(\"Quarterly Business Performance:\")\n",
    "executive_summary.show()\n",
    "\n",
    "# Operational Analytics - business process optimization\n",
    "print(\"\\n Operational Analytics (Enterprise Process Optimization):\")\n",
    "operational_metrics = (\n",
    "    enriched_orders\n",
    "    .groupby('revenue_tier')\n",
    "    .agg(\n",
    "        order_volume=('o_orderkey', 'count'),\n",
    "        total_revenue=('o_totalprice', 'sum'),\n",
    "        avg_priority_weight=('priority_weight', 'mean'),\n",
    "        expedited_processing_rate=('requires_expedited_processing', 'mean'),\n",
    "        peak_season_orders=('is_peak_season', 'sum')\n",
    "    )\n",
    ")\n",
    "print(\"Performance by Revenue Tier:\")\n",
    "operational_metrics.show()\n",
    "\n",
    "# Priority-Based Analysis - enterprise order management\n",
    "print(\"\\n Priority-Based Analysis (Order Management Insights):\")\n",
    "priority_performance = (\n",
    "    enriched_orders\n",
    "    .groupby('o_orderpriority')\n",
    "    .agg(\n",
    "        priority_orders=('o_orderkey', 'count'),\n",
    "        priority_revenue=('o_totalprice', 'sum'),\n",
    "        avg_order_value=('o_totalprice', 'mean'),\n",
    "        large_order_rate=('is_large_order', 'mean'),\n",
    "        weekend_order_rate=('is_weekend', 'mean')\n",
    "    )\n",
    ")\n",
    "print(\"Performance by Order Priority:\")\n",
    "priority_performance.sort('priority_revenue', descending=True).show()\n",
    "\n",
    "# Temporal Business Analysis - time-series insights\n",
    "print(\"\\n Temporal Analysis (Time-Series Business Intelligence):\")\n",
    "temporal_intelligence = (\n",
    "    enriched_orders\n",
    "    .groupby('order_year')\n",
    "    .agg(\n",
    "        yearly_orders=('o_orderkey', 'count'),\n",
    "        yearly_revenue=('o_totalprice', 'sum'),\n",
    "        avg_order_value=('o_totalprice', 'mean'),\n",
    "        peak_season_revenue=('seasonal_revenue_multiplier', lambda x: (x > 1.0).mean()),\n",
    "        large_order_percentage=('is_large_order', 'mean')\n",
    "    )\n",
    ")\n",
    "print(\"Year-over-Year Performance:\")\n",
    "temporal_intelligence.sort('order_year').show()\n",
    "\n",
    "# Advanced Analytics for ML/AI Applications on Enterprise Data\n",
    "print(\"\\n Advanced Analytics (Enterprise ML/AI Preparation):\")\n",
    "print(\"   Preparing aggregated features for machine learning models on TPC-H data...\")\n",
    "\n",
    "# Customer behavior patterns - ML feature engineering on enterprise scale\n",
    "customer_behavior = (\n",
    "    enriched_orders\n",
    "    .groupby('o_custkey')\n",
    "    .agg(\n",
    "        order_frequency=('o_orderkey', 'count'),\n",
    "        total_lifetime_value=('o_totalprice', 'sum'),\n",
    "        avg_order_value=('o_totalprice', 'mean'),\n",
    "        priority_preference=('priority_weight', 'mean'),\n",
    "        ml_features_avg_log_price=('log_total_price', 'mean'),\n",
    "        ml_features_weekend_large_orders=('weekend_large_order', 'sum'),\n",
    "        customer_segment_mod=('customer_id_mod_100', 'max'),\n",
    "        peak_season_preference=('is_peak_season', 'mean')\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Customer Behavior Patterns (Enterprise ML Features):\")\n",
    "customer_behavior.sort('total_lifetime_value', descending=True).show(5)\n",
    "\n",
    "# Advanced time-series features for forecasting models\n",
    "monthly_trends = (\n",
    "    enriched_orders\n",
    "    .groupby(['order_year', 'order_month'])\n",
    "    .agg(\n",
    "        monthly_orders=('o_orderkey', 'count'),\n",
    "        monthly_revenue=('o_totalprice', 'sum'),\n",
    "        urgent_order_ratio=('is_urgent', 'mean'),\n",
    "        seasonal_multiplier=('seasonal_revenue_multiplier', 'mean')\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nMonthly Trends (Time-Series ML Features):\")\n",
    "monthly_trends.sort(['order_year', 'order_month']).show(10)\n",
    "\n",
    "print(f\"\\n TPC-H Analytics Summary:\")\n",
    "print(f\"    Processed {enriched_orders.count():,} enterprise orders\")\n",
    "print(f\"    Traditional BI: Quarterly, operational, priority-based, and temporal analytics\")\n",
    "print(f\"    ML Preparation: Customer behavior patterns and time-series features\")\n",
    "print(f\"    All analytics computed on distributed CPU cluster using Ray Data\")\n",
    "print(f\"    Industry-standard TPC-H benchmark demonstrates real-world enterprise capabilities\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8be63c5e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Part 5: Load - Writing Data\n",
    "\n",
    "The **Load** phase involves writing the processed data to destination systems. Ray Data supports writing to various formats and destinations, and understanding how this works helps you optimize for your use case.\n",
    "\n",
    "### How Ray Data Writes Data\n",
    "\n",
    "When you write data with Ray Data:\n",
    "\n",
    "1. **Parallel Writing**: Multiple tasks write data simultaneously across the cluster\n",
    "2. **Partitioned Output**: Data is written as multiple files (one per block typically)\n",
    "3. **Format Optimization**: Ray Data optimizes the writing process for each format\n",
    "4. **Streaming Writes**: Large datasets can be written without loading everything into memory\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Supported Output Formats:</b>\n",
    "<ul>\n",
    "    <li><b>Files:</b> Parquet, CSV, JSON</li>\n",
    "    <li><b>Databases:</b> MongoDB, MySQL, PostgreSQL</li>\n",
    "    <li><b>Cloud Storage:</b> S3, GCS, Azure Blob Storage</li>\n",
    "    <li><b>Data Formats:</b> Delta Lake, Iceberg (via plugins)</li>\n",
    "    <li><b>Custom:</b> Implement your own writers using <code>map_batches</code></li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb17987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories for TPC-H processed data\n",
    "import os\n",
    "os.makedirs(\"/tmp/tpch_etl_output\", exist_ok=True)\n",
    "os.makedirs(\"/tmp/tpch_etl_output/analytics\", exist_ok=True)\n",
    "\n",
    "print(\" Writing TPC-H processed data to various formats...\")\n",
    "\n",
    "# Write enriched TPC-H orders to Parquet (best for large enterprise datasets)\n",
    "print(\" Writing enriched TPC-H orders to Parquet...\")\n",
    "enriched_orders.write_parquet(\"/tmp/tpch_etl_output/enriched_tpch_orders\")\n",
    "\n",
    "# Write analytics results to CSV (good for business users)\n",
    "print(\" Writing priority analytics to CSV...\")\n",
    "priority_performance.write_csv(\"/tmp/tpch_etl_output/analytics/priority_performance.csv\")\n",
    "\n",
    "# Write customer analytics to JSON (good for APIs and downstream systems)\n",
    "print(\" Writing customer behavior analytics to JSON...\")\n",
    "customer_behavior.limit(1000).write_json(\"/tmp/tpch_etl_output/analytics/top_customers_tpch.json\")\n",
    "\n",
    "# Custom writer example - create a TPC-H executive summary report\n",
    "def create_tpch_executive_summary(batch):\n",
    "    \"\"\"Create a custom TPC-H executive summary report\"\"\"\n",
    "    df = batch.to_pandas() if hasattr(batch, 'to_pandas') else pd.DataFrame(batch)\n",
    "    \n",
    "    summary = {\n",
    "        'report_timestamp': pd.Timestamp.now().isoformat(),\n",
    "        'report_type': 'TPC-H Executive Summary',\n",
    "        'total_customers_analyzed': len(df),\n",
    "        'total_customer_lifetime_value': float(df['total_lifetime_value'].sum()),\n",
    "        'average_customer_lifetime_value': float(df['total_lifetime_value'].mean()),\n",
    "        'top_customer_ltv': float(df['total_lifetime_value'].max()),\n",
    "        'customers_by_order_frequency': {\n",
    "            'single_order': int((df['order_frequency'] == 1).sum()),\n",
    "            'repeat_customers': int((df['order_frequency'] >= 2).sum()),\n",
    "            'high_frequency': int((df['order_frequency'] >= 10).sum())\n",
    "        },\n",
    "        'average_priority_preference': float(df['priority_preference'].mean()),\n",
    "        'peak_season_customers': int((df['peak_season_preference'] > 0.5).sum()),\n",
    "        'enterprise_insights': {\n",
    "            'benchmark': 'TPC-H Scale Factor 1000',\n",
    "            'data_size': '~1TB processed',\n",
    "            'processing_engine': 'Ray Data distributed CPU cluster'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Write summary to file\n",
    "    with open('/tmp/tpch_etl_output/analytics/tpch_executive_summary.json', 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(\" Generated TPC-H executive summary report\")\n",
    "    return [{\"summary_created\": True, \"customers_processed\": len(df)}]\n",
    "\n",
    "# Generate executive summary using custom writer\n",
    "summary_result = customer_behavior.map_batches(create_tpch_executive_summary, batch_size=None)\n",
    "summary_result.take()  # Trigger execution\n",
    "\n",
    "# Write time-series data for forecasting models\n",
    "print(\" Writing monthly trends for time-series analysis...\")\n",
    "monthly_trends.write_parquet(\"/tmp/tpch_etl_output/analytics/monthly_trends_tpch\")\n",
    "\n",
    "# Display what was created\n",
    "print(\"\\n TPC-H ETL Output files created:\")\n",
    "for root, dirs, files in os.walk(\"/tmp/tpch_etl_output\"):\n",
    "    level = root.replace(\"/tmp/tpch_etl_output\", \"\").count(os.sep)\n",
    "    indent = \" \" * 2 * level\n",
    "    print(f\"{indent} {os.path.basename(root)}/\")\n",
    "    sub_indent = \" \" * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        print(f\"{sub_indent} {file} ({size_mb:.1f} MB)\")\n",
    "\n",
    "print(f\"\\n TPC-H ETL Pipeline Complete!\")\n",
    "print(f\"    Industry-standard benchmark data processed\")\n",
    "print(f\"    Traditional BI and modern ML features generated\") \n",
    "print(f\"    Enterprise-scale ETL demonstrated on CPU cluster\")\n",
    "print(f\"    Ready for both business intelligence and AI/ML applications\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23f3a0e4",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Part 6: Advanced ETL Patterns & Best Practices\n",
    "\n",
    "### Building a Production-Ready ETL Pipeline\n",
    "\n",
    "Now let's combine everything we've learned into a comprehensive, production-ready ETL pipeline that demonstrates advanced patterns and best practices.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> Production ETL Best Practices:</b>\n",
    "<ul>\n",
    "    <li><b>Error Handling:</b> Implement robust error handling and data validation</li>\n",
    "    <li><b>Monitoring:</b> Add logging and metrics for observability</li>\n",
    "    <li><b>Data Quality:</b> Validate data at each step</li>\n",
    "    <li><b>Performance:</b> Optimize batch sizes and resource usage</li>\n",
    "    <li><b>Fault Tolerance:</b> Design for failures and retries</li>\n",
    "    <li><b>Modularity:</b> Create reusable transformation functions</li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4069e00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionETLPipeline:\n",
    "    \"\"\"\n",
    "    A production-ready ETL pipeline class demonstrating best practices:\n",
    "    - Error handling and data validation\n",
    "    - Logging and monitoring\n",
    "    - Resource management\n",
    "    - Modular design\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size=1000, concurrency=2):\n",
    "        self.batch_size = batch_size\n",
    "        self.concurrency = concurrency\n",
    "        self.processed_records = 0\n",
    "        self.error_count = 0\n",
    "        self.quality_metrics = {}\n",
    "        \n",
    "    def validate_data_quality(self, batch, dataset_name=\"dataset\"):\n",
    "        \"\"\"Validate data quality and collect metrics\"\"\"\n",
    "        df = batch.to_pandas() if hasattr(batch, 'to_pandas') else pd.DataFrame(batch)\n",
    "        initial_count = len(df)\n",
    "        \n",
    "        # Track quality metrics\n",
    "        quality_checks = {}\n",
    "        \n",
    "        if 'customer_id' in df.columns:\n",
    "            # Check for missing customer IDs\n",
    "            missing_customer_ids = df['customer_id'].isna().sum()\n",
    "            quality_checks['missing_customer_ids'] = missing_customer_ids\n",
    "            df = df[df['customer_id'].notna()]\n",
    "            \n",
    "        if 'price' in df.columns:\n",
    "            # Check for negative prices\n",
    "            negative_prices = (df['price'] < 0).sum()\n",
    "            quality_checks['negative_prices'] = negative_prices\n",
    "            df = df[df['price'] >= 0]\n",
    "            \n",
    "        if 'quantity' in df.columns:\n",
    "            # Check for invalid quantities\n",
    "            invalid_quantities = (df['quantity'] <= 0).sum()\n",
    "            quality_checks['invalid_quantities'] = invalid_quantities\n",
    "            df = df[df['quantity'] > 0]\n",
    "        \n",
    "        final_count = len(df)\n",
    "        dropped_count = initial_count - final_count\n",
    "        \n",
    "        if dropped_count > 0:\n",
    "            print(f\"  Data quality issues in {dataset_name}: dropped {dropped_count}/{initial_count} records\")\n",
    "            self.error_count += dropped_count\n",
    "            \n",
    "        # Store quality metrics\n",
    "        self.quality_metrics[dataset_name] = quality_checks\n",
    "        \n",
    "        # Add quality metadata to the data\n",
    "        df['data_quality_score'] = 1.0 - (dropped_count / initial_count if initial_count > 0 else 0)\n",
    "        df['validation_timestamp'] = pd.Timestamp.now()\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def enrich_with_advanced_features(self, batch):\n",
    "        \"\"\"Advanced feature engineering with error handling\"\"\"\n",
    "        try:\n",
    "            df = batch.to_pandas() if hasattr(batch, 'to_pandas') else pd.DataFrame(batch)\n",
    "            \n",
    "            # Calculate advanced business metrics\n",
    "            df['total_value'] = df['quantity'] * df['price']\n",
    "            \n",
    "            # Customer value segments (using percentiles)\n",
    "            value_percentiles = df['total_value'].quantile([0.33, 0.66, 1.0])\n",
    "            df['value_segment'] = pd.cut(\n",
    "                df['total_value'],\n",
    "                bins=[-float('inf')] + value_percentiles.tolist(),\n",
    "                labels=['Economy', 'Standard', 'Premium', 'Luxury']\n",
    "            )\n",
    "            \n",
    "            # Time-based features\n",
    "            df['order_date'] = pd.to_datetime(df['order_date'])\n",
    "            df['order_hour'] = df['order_date'].dt.hour\n",
    "            df['is_business_hours'] = df['order_hour'].between(9, 17)\n",
    "            df['order_season'] = df['order_date'].dt.month % 12 // 3\n",
    "            \n",
    "            # RFM Analysis components (Recency, Frequency, Monetary)\n",
    "            reference_date = df['order_date'].max()\n",
    "            df['days_since_order'] = (reference_date - df['order_date']).dt.days\n",
    "            \n",
    "            # Product affinity scoring\n",
    "            product_scores = {'laptop': 5, 'phone': 4, 'tablet': 3, 'watch': 2, 'headphones': 1}\n",
    "            df['product_affinity_score'] = df['product'].map(product_scores).fillna(0)\n",
    "            \n",
    "            # Calculate expected shipping costs (business rule)\n",
    "            df['estimated_shipping_cost'] = np.where(\n",
    "                df['total_value'] > 500,\n",
    "                0,  # Free shipping for orders over $500\n",
    "                np.where(df['is_business_hours'], 15, 25)  # Higher cost outside business hours\n",
    "            )\n",
    "            \n",
    "            self.processed_records += len(df)\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" Error in feature engineering: {str(e)}\")\n",
    "            # Return original batch to continue processing\n",
    "            return batch\n",
    "    \n",
    "    def create_customer_360_view(self, orders_df, customers_df):\n",
    "        \"\"\"Create a comprehensive customer view\"\"\"\n",
    "        try:\n",
    "            # Aggregate order data by customer\n",
    "            customer_order_metrics = (\n",
    "                orders_df\n",
    "                .groupby('customer_id')\n",
    "                .agg(\n",
    "                    total_orders=('order_id', 'count'),\n",
    "                    total_lifetime_value=('total_value', 'sum'),\n",
    "                    avg_order_value=('total_value', 'mean'),\n",
    "                    max_order_value=('total_value', 'max'),\n",
    "                    preferred_product=('product', lambda x: x.mode().iloc[0] if not x.mode().empty else 'unknown'),\n",
    "                    avg_product_affinity=('product_affinity_score', 'mean'),\n",
    "                    total_shipping_saved=('estimated_shipping_cost', lambda x: (x == 0).sum() * 20)  # Estimated savings\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Join with customer data\n",
    "            customer_360 = customers_df.join(\n",
    "                customer_order_metrics,\n",
    "                key='customer_id',\n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "            return customer_360\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" Error creating customer 360 view: {str(e)}\")\n",
    "            return customers_df\n",
    "    \n",
    "    def run_pipeline(self):\n",
    "        \"\"\"Execute the complete ETL pipeline\"\"\"\n",
    "        print(\" Starting Production ETL Pipeline...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # EXTRACT: Read data with validation\n",
    "            print(\" Phase 1: Extracting data...\")\n",
    "            raw_orders = ray.data.read_parquet('/tmp/sample_data/orders.parquet')\n",
    "            raw_customers = ray.data.read_csv('/tmp/sample_data/customers.csv')\n",
    "            \n",
    "            # TRANSFORM: Data quality validation\n",
    "            print(\" Phase 2: Data quality validation...\")\n",
    "            clean_orders = raw_orders.map_batches(\n",
    "                lambda batch: self.validate_data_quality(batch, \"orders\"),\n",
    "                batch_format=\"pyarrow\",\n",
    "                batch_size=self.batch_size\n",
    "            )\n",
    "            \n",
    "            clean_customers = raw_customers.map_batches(\n",
    "                lambda batch: self.validate_data_quality(batch, \"customers\"),\n",
    "                batch_format=\"pyarrow\", \n",
    "                batch_size=self.batch_size\n",
    "            )\n",
    "            \n",
    "            # TRANSFORM: Feature engineering\n",
    "            print(\" Phase 3: Feature engineering...\")\n",
    "            enriched_orders = clean_orders.map_batches(\n",
    "                self.enrich_with_advanced_features,\n",
    "                batch_format=\"pyarrow\",\n",
    "                batch_size=self.batch_size,\n",
    "                concurrency=self.concurrency\n",
    "            )\n",
    "            \n",
    "            # TRANSFORM: Create customer 360 view\n",
    "            print(\" Phase 4: Creating customer 360 view...\")\n",
    "            customer_360 = self.create_customer_360_view(enriched_orders, clean_customers)\n",
    "            \n",
    "            # LOAD: Write results\n",
    "            print(\" Phase 5: Loading results...\")\n",
    "            os.makedirs(\"/tmp/production_output\", exist_ok=True)\n",
    "            \n",
    "            # Write different outputs for different use cases\n",
    "            enriched_orders.write_parquet(\"/tmp/production_output/enriched_orders\")\n",
    "            customer_360.write_parquet(\"/tmp/production_output/customer_360\")\n",
    "            \n",
    "            # Generate final metrics\n",
    "            execution_time = time.time() - start_time\n",
    "            total_orders = enriched_orders.count()\n",
    "            total_customers = customer_360.count()\n",
    "            \n",
    "            pipeline_metrics = {\n",
    "                'execution_time_seconds': round(execution_time, 2),\n",
    "                'total_orders_processed': total_orders,\n",
    "                'total_customers_processed': total_customers,\n",
    "                'records_per_second': round(self.processed_records / execution_time, 2),\n",
    "                'error_count': self.error_count,\n",
    "                'data_quality_metrics': self.quality_metrics\n",
    "            }\n",
    "            \n",
    "            # Save metrics\n",
    "            with open('/tmp/production_output/pipeline_metrics.json', 'w') as f:\n",
    "                json.dump(pipeline_metrics, f, indent=2)\n",
    "            \n",
    "            print(\" Pipeline completed successfully!\")\n",
    "            print(f\" Processed {total_orders:,} orders and {total_customers:,} customers in {execution_time:.2f}s\")\n",
    "            print(f\" Throughput: {pipeline_metrics['records_per_second']:,.0f} records/second\")\n",
    "            \n",
    "            return pipeline_metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" Pipeline failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Execute the production pipeline\n",
    "pipeline = ProductionETLPipeline(batch_size=500, concurrency=2)\n",
    "metrics = pipeline.run_pipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39b76d7",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Summary: Your Journey with Ray Data ETL\n",
    "\n",
    "Congratulations! You've completed a comprehensive journey through Ray Data for ETL. Let's summarize what you've learned and explore how to take your AI data pipelines to production.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> What You've Mastered:</b>\n",
    "<ul>\n",
    "    <li><b>Ray Data Fundamentals:</b> Blocks, lazy execution, streaming processing</li>\n",
    "    <li><b>Extract Phase:</b> Reading from multiple data sources efficiently, including multimodal data</li>\n",
    "    <li><b>Transform Phase:</b> Distributed data processing and feature engineering</li>\n",
    "    <li><b>Load Phase:</b> Writing to various destinations with optimization</li>\n",
    "    <li><b>Production Patterns:</b> Error handling, monitoring, and data quality</li>\n",
    "    <li><b>Performance Optimization:</b> Understanding bottlenecks and solutions</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "### When to Use Ray Data\n",
    "\n",
    "**Ray Data excels across the full spectrum of data workloads:**\n",
    "\n",
    "**Traditional ETL & Business Intelligence:**\n",
    "- **High-volume transaction processing** for e-commerce, finance, and operations\n",
    "- **Business intelligence** and executive reporting at scale\n",
    "- **Data warehouse** loading and transformation pipelines\n",
    "- **CPU cluster optimization** with pure Python performance (no JVM overhead)\n",
    "- **Traditional analytics** that need to scale beyond single-node tools\n",
    "\n",
    "**Modern ML & AI Workloads:**\n",
    "- **Feature engineering** for machine learning at scale\n",
    "- **Batch inference** on foundation models and LLMs\n",
    "- **Multimodal data processing** (text, images, video, audio)\n",
    "- **GPU-accelerated pipelines** for AI applications\n",
    "- **Real-time model serving** and inference workloads\n",
    "\n",
    "**Ray Data's Unified Platform Advantage:**\n",
    "- **One system** for both traditional ETL and cutting-edge AI\n",
    "- **Seamless evolution** from CPU-based analytics to GPU-powered AI\n",
    "- **No migration** required as your data needs grow and change\n",
    "- **Consistent APIs** whether processing structured business data or unstructured AI content\n",
    "\n",
    "**Ray Data is proven at scale:**\n",
    "- Processing **exabyte-scale** workloads (Amazon's migration from Spark)\n",
    "- **1M+ clusters** orchestrated monthly across the Ray ecosystem\n",
    "- **$120M annual savings** achieved by leading enterprises\n",
    "- **Traditional workloads** running alongside **next-generation AI** on the same platform\n",
    "\n",
    "### From Open Source to Enterprise: Anyscale Platform\n",
    "\n",
    "While Ray Data open source provides powerful capabilities, **Anyscale** offers a unified AI platform for production deployments:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Anyscale: The Unified AI Platform</b>\n",
    "<ul>\n",
    "    <li><b>RayTurbo Runtime:</b> Up to 5.1x performance improvements over open source</li>\n",
    "    <li><b>Enterprise Governance:</b> Resource quotas, usage tracking, and advanced observability</li>\n",
    "    <li><b>AI Anywhere:</b> Deploy on Kubernetes, hybrid cloud, or any infrastructure</li>\n",
    "    <li><b>LLM Suite:</b> Complete capabilities for embeddings, fine-tuning, and serving</li>\n",
    "    <li><b>Marketplace Ready:</b> Available on AWS and GCP Marketplaces</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "### Production Deployment Options\n",
    "\n",
    "**Getting Started:**\n",
    "1. **Ray Open Source**: Perfect for development and smaller workloads\n",
    "2. **Anyscale Platform**: Enterprise features with RayTurbo optimizations\n",
    "3. **Marketplace Deployment**: One-click setup via AWS or GCP Marketplace\n",
    "\n",
    "### Key Architectural Insights\n",
    "\n",
    "Understanding how Ray Data works under the hood helps you build better pipelines:\n",
    "\n",
    "1. **AI-Native Architecture**: Purpose-built for Python, GPUs, and multimodal data\n",
    "2. **Streaming Execution**: Process datasets larger than cluster memory\n",
    "3. **Heterogeneous Compute**: Seamlessly orchestrate CPUs, GPUs, and other accelerators\n",
    "4. **Operator Fusion**: Combines compatible operations for efficiency\n",
    "5. **Enterprise Scalability**: Proven to scale to 8,000+ nodes\n",
    "\n",
    "### Production Readiness Checklist\n",
    "\n",
    "Before deploying Ray Data pipelines to production:\n",
    "\n",
    "-  **Architecture**: Choose between Ray OSS and Anyscale based on your needs\n",
    "-  **Performance**: Consider RayTurbo for production workloads requiring maximum efficiency\n",
    "-  **Governance**: Implement enterprise controls for AI sprawl and cost management\n",
    "-  **Security**: Leverage enterprise identity integration and access controls\n",
    "-  **Monitoring**: Use advanced observability tools for optimization insights\n",
    "-  **Scalability**: Test with realistic data volumes and cluster sizes\n",
    "\n",
    "### Join the Ray Ecosystem\n",
    "\n",
    "The Ray community is thriving with **1,000+ contributors** and growing:\n",
    "\n",
    "1. **Community**: Join the Ray Slack community for support and discussions\n",
    "2. **Learning**: Access Ray Summit sessions and technical deep-dives\n",
    "3. **Contributing**: Contribute to the fastest-growing AI infrastructure project\n",
    "4. **Enterprise Support**: Explore Anyscale for production deployments\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> One Platform for All Your Data Workloads</b><br>\n",
    "You now have the knowledge to build production-ready, scalable data pipelines that handle everything from traditional business ETL to cutting-edge AI applications. Whether you're processing millions of e-commerce transactions for business intelligence or preparing multimodal data for foundation models, Ray Data provides a unified platform that scales with your needs.<br><br>\n",
    "<b>Start with traditional ETL today, evolve to AI tomorrow - all on the same platform.</b> Ray Data and Anyscale eliminate the complexity of managing multiple systems as your data requirements grow.\n",
    "</div>\n",
    "\n",
    "### Get Started Today\n",
    "\n",
    "- ** Ray Documentation**: [docs.ray.io](https://docs.ray.io/en/latest/data/)\n",
    "- ** Try Anyscale**: Available on [AWS](https://aws.amazon.com/marketplace) and [GCP](https://console.cloud.google.com/marketplace) Marketplaces\n",
    "- ** Community**: Join the conversation on [Ray Slack](https://ray-distributed.slack.com)\n",
    "- ** Learn More**: Watch [Ray Summit sessions](https://www.youtube.com/c/RayProject) for deeper insights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e9e932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
